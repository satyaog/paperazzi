{
  "pages": [
    {
      "markdown": "# Optimal Approximate Minimization of One-Letter Weighted Finite Automata \n\nClara Lacroce ${ }^{* 1,2}$, Borja Balle ${ }^{3}$, Prakash Panangaden ${ }^{1,2}$, and Guillaume Rabusseau ${ }^{2,4,5}$<br>${ }^{1}$ School of Computer Science, McGill University, Montréal, Canada<br>${ }^{2}$ Mila, Montréal, Canada<br>${ }^{3}$ DeepMind, London, United Kingdom.<br>${ }^{4}$ DIRO, Université de Montréal, Montréal, Canada<br>${ }^{5}$ CIFAR AI Chair\n\n\n#### Abstract\n\nIn this paper, we study the approximate minimization problem of weighted finite automata (WFAs): to compute the best possible approximation of a WFA given a bound on the number of states. By reformulating the problem in terms of Hankel matrices, we leverage classical results on the approximation of Hankel operators, namely the celebrated Adamyan-Arov-Krein (AAK) theory.\n\nWe solve the optimal spectral-norm approximate minimization problem for irredundant WFAs with real weights, defined over a one-letter alphabet. We present a theoretical analysis based on AAK theory, and bounds on the quality of the approximation in the spectral norm and $\\ell^{2}$ norm. Moreover, we provide a closed-form solution, and an algorithm, to compute the optimal approximation of a given size in polynomial time.\n\n\n## 1 Introduction\n\nWeighted finite automata (WFAs) are an expressive class of models representing functions defined over sequences. The approximate minimization problem is concerned with finding an automaton that approximates the behaviour of a given minimal WFA, while being smaller in size. Clearly, the two automata compute different languages, so the objective is to minimize the approximation error (Balle et al. 2015, 2019). Approximate minimization can be particularly useful in the context of spectral learning algorithms (Bailly et al. 2009, Hsu et al. 2012, Balle, Carreras, Luque \\& Quattoni 2014, Balle, Hamilton \\& Pineau 2014). When applied to a learning task, such algorithms can be viewed as working in two steps. First, they compute a minimal WFA that explains the training data exactly. Then, they obtain a model that generalizes to the unseen data by producing a smaller approximation to the minimal WFA, thus preventing overfitting of the data.\n\nA key point in solving approximation tasks is to choose how to quantify the error. We propose to rewrite the problem in terms of Hankel matrices, mathematical objects strictly related to WFAs, and to measure the error in terms of the spectral norm. This allows us to exploit the work of Adamyan, Arov and Krein which has come to be known as AAK theory (Adamyan et al. 1971): a series of results connecting the theory of complex functions to Hankel matrices.\n\n[^0]\n[^0]:    *Corresponding author: clara.lacroce@mail.mcgill.ca ORCID\n\n    Preprint under consideration for publication in Mathematical Structures in Computer Science."
    },
    {
      "markdown": "The core of this theory provides us with theoretical guarantees for the exact computation of the spectral norm of the error, and a method to construct the optimal approximation. We show that the spectral norm of the Hankel matrix of a WFA can be computed accurately in polynomial time (cubic in the number of states of the automaton). This is a great advantage compared, for example, to behavioural norms, which are easier to interpret but harder to compute (Balle et al. 2017, 2022). The spectral norm has another advantage over WFA-specific behavioural metrics. In fact, an important extension of this work is the application of the method to other classes of models. In the one-letter case, a similar algorithm can be found to approximate a black-box model over sequential data using a WFA (Lacroce et al. 2021). With this in mind, we think that it is preferable to consider a norm defined on the input-output function - or the Hankel matrix - rather than the parameters of the specific model considered.\n\nWe summarize our main contributions:\n\n- We apply AAK theory to the approximate minimization problem of WFAs by establishing a correspondence between the parameters of a WFA and the coefficients of a complex function on the unit circle. To the best of our knowledge, this paper represents the first attempt to apply AAK theory to WFAs.\n- We present a theoretical analysis of the optimal spectral-norm approximate minimization problem of WFAs, based on their connection with finite-rank infinite Hankel matrices. We provide a closed form solution for real weighted automata $A=\\langle\\boldsymbol{\\alpha}, \\mathbf{A}, \\boldsymbol{\\beta}\\rangle$ over a one-letter alphabet, under the assumption $\\rho(\\mathbf{A})<1$ on the spectral radius. We bound the approximation error, both in terms of the Hankel matrix (spectral norm) and of the rational function computed by the WFA ( $\\ell^{2}$ norm).\n- We propose a self-contained algorithm that returns the unique optimal spectral-norm approximation of a given size in polynomial time.\n- We tighten the connection, made in Balle et al. (2019), between WFAs and discrete dynamical systems, by adapting some of the control theory concepts to this setting, e.g. the allpass system (Glover 1984).\nIn this paper we present and expand the results of our previous work (Balle et al. 2021). The contents of this paper are organized as follows. In Section 2 we define the notation that will be used throughout the paper and review a series of well-known results from the theory of automata and from functional analysis. In Section 3 we establish the framework to reformulate the approximate minimization problem in terms of Hankel operators and AAK theory. Section 4 presents the theoretical foundation of our contribution and a closed-form solution for our problem. Section 5 shows how to implement the algorithm derived from the solution obtained in the previous section, while in Section 6 we provide an example and compute the optimal approximation of a given WFA. Section 7 discusses the related work in approximate minimization and control theory. Finally, in Sections 8 and 9 we highlight possible directions for future work, analyze the limitations of this approach and summarize our contribution.\n\n\n# 2 Background \n\nIn this section, we recall the fundamental definitions and preliminary results that are used throughout the paper. After defining weighted finite automata and Hankel matrices, we will provide an overview of AAK theory. We will see in the next section that our objective is to rewrite the approximate minimization problems as low-rank approximation of a Hankel matrix. In the paper, We use AAK theory to solve the low-rank approximation problem while preserving the Hankel property."
    },
    {
      "markdown": "# 2.1 Preliminaries \n\nWe denote with $\\mathbb{N}, \\mathcal{Z}$ and $\\mathbb{R}$ the set of natural, integers and real numbers, respectively. We use bold letters for vectors and matrices; all vectors considered are column vectors. We denote with 1 the identity matrix, specifying its dimension only when not clear from the context. We refer to the $i$-th row and the $j$-th column of $\\mathbf{M}$ by $\\mathbf{M}(i,:)$ and $\\mathbf{M}(:, j)$. Given a matrix $\\mathbf{M} \\in \\mathbb{R}^{p \\times q}$ of rank $n$, a rank factorization is a factorization $\\mathbf{M}=\\mathbf{P Q}$, where $\\mathbf{P} \\in \\mathbb{R}^{p \\times n}, \\mathbf{Q} \\in \\mathbb{R}^{n \\times q}$ and $\\operatorname{rank}(\\mathbf{M})=\\operatorname{rank}(\\mathbf{P})=\\operatorname{rank}(\\mathbf{Q})=n$. Let $\\mathbf{M} \\in \\mathbb{R}^{p \\times q}$ of rank $n$, the compact singular value decomposition SVD of $\\mathbf{M}$ is the factorization $\\mathbf{M}=\\mathbf{U D V}^{\\top}$, where $\\mathbf{U} \\in \\mathbb{R}^{p \\times n}, \\mathbf{D} \\in \\mathbb{R}^{n \\times n}$, $\\mathbf{V} \\in \\mathbb{R}^{q \\times n}$ are such that $\\mathbf{U}^{\\top} \\mathbf{U}=\\mathbf{V}^{\\top} \\mathbf{V}=\\mathbf{1}$, and $\\mathbf{D}$ is a diagonal matrix. The columns of $\\mathbf{U}$ and $\\mathbf{V}$ are called left and right singular vectors, while the entries of $\\mathbf{D}$ are the singular values. The Moore-Penrose pseudo-inverse $\\mathbf{M}^{+}$of $\\mathbf{M}$ is the unique matrix such that $\\mathbf{M M}^{+} \\mathbf{M}=\\mathbf{M}$, $\\mathbf{M}^{+} \\mathbf{M} \\mathbf{M}^{+}=\\mathbf{M}^{+}$, with $\\mathbf{M}^{+} \\mathbf{M}$ and $\\mathbf{M} \\mathbf{M}^{+}$Hermitian (Zhu 1990). The spectral radius $\\rho(\\mathbf{M})$ of a matrix $\\mathbf{M}$ is the largest modulus among its eigenvalues.\n\nA Hilbert space is a complete normed vector space where the norm arises from an inner product. A linear operator $T: X \\rightarrow Y$ between Hilbert spaces is bounded if it has finite operator norm, i.e. $\\|T\\|_{o p}=\\sup _{\\|g\\|_{X} \\leq 1}\\|T g\\|_{Y}<\\infty$. We denote by $\\mathbf{T}$ the (infinite) matrix associated with $T$ by some (canonical) orthonormal basis on $H$. An operator is compact if the image of the unit ball in $X$ is relatively compact. Given Hilbert spaces $X, Y$ and a compact operator $T: X \\rightarrow Y$, we denote its adjoint by $T^{*}$. The singular numbers $\\left\\{\\sigma_{n}\\right\\}_{n \\geq 0}$ of $T$ are the square roots of the eigenvalues of the self-adjoint operator $T^{*} T$, arranged in decreasing order. A $\\sigma$-Schmidt pair $\\{\\boldsymbol{\\xi}, \\boldsymbol{\\eta}\\}$ for $T$ is a couple of norm 1 vectors such that: $\\mathbf{T} \\boldsymbol{\\xi}=\\sigma \\boldsymbol{\\eta}$ and $\\mathbf{T}^{*} \\boldsymbol{\\eta}=\\sigma \\boldsymbol{\\xi}$. The Hilbert-Schmidt decomposition provides a generalization of the compact SVD for the infinite matrix of a compact operator $T$ using singular numbers and orthonormal Schmidt pairs: $\\mathbf{T x}=\\sum_{n \\geq 0} \\sigma_{n}\\left\\langle\\mathbf{x}, \\boldsymbol{\\xi}_{n}\\right\\rangle \\boldsymbol{\\eta}_{k}$ (Zhu 1990). The spectral norm $\\|\\mathbf{T}\\|$ of the matrix representing the operator $T$ is the largest singular number of $T$. Note that the spectral norm of $\\mathbf{T}$ corresponds to the operator norm of $T$.\n\nLet $\\ell^{2}$ be the Hilbert space of square-summable sequences over $\\Sigma^{*}$, with norm $\\|f\\|_{2}^{2}=$ $\\sum_{x \\in \\Sigma^{*}}|f(x)|^{2}$ and inner product $\\langle f, g\\rangle=\\sum_{x \\in \\Sigma^{*}} f(x) g(x)$ for $f, g \\in \\mathbb{R}^{\\Sigma^{*}}$. Let $\\mathbb{T}=\\{z \\in \\mathbb{C}$ : $|z|=1\\}$ be the complex unit circle, $\\mathbb{D}=\\{z \\in \\mathbb{C}:|z|<1\\}$ the (open) complex unit disc. Let $1<p<\\infty, \\mathcal{L}^{p}(\\mathbb{T})$ be the space of measurable functions on $\\mathbb{T}$ for which the $p$-th power of the absolute value is Lebesgue integrable. For $p=\\infty$, we denote with $\\mathcal{L}^{\\infty}(\\mathbb{T})$ the space of measurable functions that are bounded, with norm $\\|f\\|_{\\infty}=\\sup \\{|f(x)|: x \\in \\mathbb{T}\\}$.\n\n### 2.2 Hankel matrix and Weighted Automata\n\nLet $\\Sigma$ be a fixed finite alphabet and $\\Sigma^{*}$ be the set of all finite strings with symbols in $\\Sigma$. We denote with $\\varepsilon$ the empty string. Given $p, s \\in \\Sigma^{*}$, we denote with $p s$ the string obtained by their concatenation. Let $f: \\Sigma^{*} \\rightarrow \\mathbb{R}$ be a function defined on sequences, we consider the bi-infinite matrix $\\mathbf{H}_{f} \\in \\mathbb{R}^{\\Sigma^{*} \\times \\Sigma^{*}}$ having rows and columns indexed by strings and defined by $\\mathbf{H}_{f}(p, s)=f(p s)$ for $p, s \\in \\Sigma^{*}$.\n\nDefinition 1. A matrix $\\mathbf{H} \\in \\mathbb{R}^{\\Sigma^{*} \\times \\Sigma^{*}}$ is Hankel if for all $p, p^{\\prime}, s, s^{\\prime} \\in \\Sigma^{*}$ such that $p s=p^{\\prime} s^{\\prime}$, we have $\\mathbf{H}(p, s)=\\mathbf{H}\\left(p^{\\prime}, s^{\\prime}\\right)$.\n\nGiven a Hankel matrix $\\mathbf{H} \\in \\mathbb{R}^{\\Sigma^{*} \\times \\Sigma^{*}}$, there is a unique function $f: \\Sigma^{*} \\rightarrow \\mathbb{R}$ such that $\\mathbf{H}_{f}=\\mathbf{H}$. Intuitively, the Hankel property tells us that each entry of the matrix only depends on the composition of the coordinates. Since rows and columns are indexed using strings, then the value stored in each entry only depends on the string obtained by concatenating the coordinates."
    },
    {
      "markdown": "Weighted finite automata are a class of models defined over sequential data. A weighted finite automaton (WFA) of $n$ states over $\\Sigma$ is a tuple $A=\\left\\langle\\boldsymbol{\\alpha},\\left\\{\\mathbf{A}_{a}\\right\\}, \\boldsymbol{\\beta}\\right\\rangle$, where $\\boldsymbol{\\alpha}, \\boldsymbol{\\beta} \\in \\mathbb{R}^{n}$ are the vector of initial and final weights, respectively, and $\\mathbf{A}_{a} \\in \\mathbb{R}^{n \\times n}$ is the matrix containing the transition weights associated with each symbol $a \\in \\Sigma$. Every WFA $A$ with real weights realizes (or computes) a function $f_{A}: \\Sigma^{*} \\rightarrow \\mathbb{R}$, i.e. given a string $x=x_{1} \\cdots x_{t} \\in \\Sigma^{*}$, it returns $f_{A}(x)=\\boldsymbol{\\alpha}^{\\top} \\mathbf{A}_{x_{1}} \\cdots \\mathbf{A}_{x_{t}} \\boldsymbol{\\beta}=\\boldsymbol{\\alpha}^{\\top} \\mathbf{A}_{x} \\boldsymbol{\\beta}$. A function $f: \\Sigma^{*} \\rightarrow \\mathbb{R}$ is called rational if there exists a WFA $A$ that realizes it. Given $f: \\Sigma^{*} \\rightarrow \\mathbb{R}$, we can use the Hankel matrix $\\mathbf{H}_{f} \\in \\mathbb{R}^{\\Sigma^{*} \\times \\Sigma^{*}}$ to recover information about the weighted automaton computing $f$.\n\nTheorem 2 (Carlyle \\& Paz (1971), Fliess (1974)). A function $f: \\Sigma^{*} \\rightarrow \\mathbb{R}$ is realized by a WFA $A$ if and only if $\\mathbf{H}_{f}$ has finite rank. In that case, the rank of $\\mathbf{H}_{f}$ corresponds to the minimal number of states of any automaton realizing $f$.\n\nGiven a WFA $A=\\left\\langle\\boldsymbol{\\alpha},\\left\\{\\mathbf{A}_{a}\\right\\}, \\boldsymbol{\\beta}\\right\\rangle$, the forward matrix of $A$ is the infinite matrix $\\mathbf{F}_{A} \\in \\mathbb{R}^{\\Sigma^{*} \\times n}$ given by $\\mathbf{F}_{A}(p,:)=\\boldsymbol{\\alpha}^{\\top} \\mathbf{A}_{p}$ for any $p \\in \\Sigma^{*}$, while the backward matrix of $A$ is $\\mathbf{B}_{A} \\in \\mathbb{R}^{\\Sigma^{*} \\times n}$, given by $\\mathbf{B}_{A}(s,:)=\\left(\\mathbf{A}_{s} \\boldsymbol{\\beta}\\right)^{\\top}$ for any $s \\in \\Sigma^{*}$. Let $\\mathbf{H}_{f}$ be the Hankel matrix of $f$, its forward-backward (FB) factorization is: $\\mathbf{H}_{f}=\\mathbf{F B}^{\\top}$. A WFA with $n$ states is reachable if $\\operatorname{rank}\\left(\\mathbf{F}_{A}\\right)=n$, while it is observable if $\\operatorname{rank}\\left(\\mathbf{B}_{A}\\right)=n$. A WFA is minimal if it is reachable and observable. If $A$ is minimal, the (unique) FB factorization is a rank factorization (Balle, Carreras, Luque \\& Quattoni 2014).\n\nWe recall the definition of the singular value automaton, a canonical form for WFAs (Balle et al. 2015).\n\nDefinition 3. Let $f: \\Sigma^{*} \\rightarrow \\mathbb{R}$ be a rational function and suppose $\\mathbf{H}_{f}$ admits an $S V D, \\mathbf{H}_{f}=$ $\\mathbf{U D V}^{\\top}$. A singular value automaton ( $S V A$ ) for $f$ is the minimal WFA $A$ realizing $f$ such that $\\mathbf{F}_{A}=\\mathbf{U D}^{1 / 2}$ and $\\mathbf{B}_{A}=\\mathbf{V D}^{1 / 2}$.\n\nThe SVA can be computed with an efficient algorithm relying on the following matrices (Balle et al. 2019).\n\nDefinition 4. Let $f: \\Sigma^{*} \\rightarrow \\mathbb{R}$ be a rational function, $\\mathbf{H}_{f}=\\mathbf{F B}^{\\top}$ the FB factorization. If the matrices $\\mathbf{P}=\\mathbf{F}^{\\top} \\mathbf{F}$ and $\\mathbf{Q}=\\mathbf{B}^{\\top} \\mathbf{B}$ are well defined (i.e. the inner products of their columns are finite for any column), we call $\\mathbf{P}$ the reachability Gramian and $\\mathbf{Q}$ the observability Gramian.\n\nIf $A$ is in its SVA form, the Gramians associated with its FB factorization satisfy $\\mathbf{P}_{A}=$ $\\mathbf{Q}_{A}=\\mathbf{D}$, where $\\mathbf{D}$ is the matrix of singular values of the corresponding Hankel matrix. The Gramians can alternatively be characterized and computed (Balle et al. 2019)) using fixed point equations, corresponding to Lyapunov equations when $|\\Sigma|=1$ (Lyapunov 1950).\n\nTheorem 5. Let $|\\Sigma|=1, A=\\langle\\boldsymbol{\\alpha}, \\mathbf{A}, \\boldsymbol{\\beta}\\rangle$ a WFA with $n$ states and well-defined Gramians $\\mathbf{P}$, Q. Then $X=\\mathbf{P}$ and $Y=\\mathbf{Q}$ solve the equations $X-\\mathbf{A} X \\mathbf{A}^{\\top}=\\boldsymbol{\\beta} \\boldsymbol{\\beta}^{\\top}$ and $Y-\\mathbf{A}^{\\top} Y \\mathbf{A}=\\boldsymbol{\\alpha} \\boldsymbol{\\alpha}^{\\top}$.\n\nFinally, we recall the definition of generative probabilistic automata (GPA). A WFA $A=$ $\\left\\langle\\boldsymbol{\\alpha},\\left\\{\\mathbf{A}_{a}\\right\\}, \\boldsymbol{\\beta}\\right\\rangle$ is a GPA if $f_{A}(x) \\geq 0$ for every $x$ and $\\sum_{x \\in \\Sigma^{*}} f_{A}(x)=1$, i.e. if $f_{A}$ computes a probability distribution over $\\Sigma^{*}$. In general this class of automata can contain pathological examples having states not connected to any final state. To avoid these cases, we introduce the following property on the spectral radius of the transition matrix.\n\nDefinition 6. Given a WFA $A=\\left\\langle\\boldsymbol{\\alpha},\\left\\{\\mathbf{A}_{a}\\right\\}_{a \\in \\Sigma}, \\boldsymbol{\\beta}\\right\\rangle$, let $\\mathbf{A}=\\sum_{a \\in \\Sigma} \\mathbf{A}_{a}$. The WFA $A$ is irredundant if $\\rho(\\mathbf{A})<1$."
    },
    {
      "markdown": "# 2.3 AAK Theory \n\nIn this section, we introduce the theory of optimal approximation for Hankel operators and complex functions known as AAK theory (Adamyan et al. 1971). A comprehensive presentation of the concepts recalled in this section can be found in Nikol'Skii (2002), Peller (2012).\n\nWe consider the space of square-integrable complex functions on the unit circle $\\mathcal{L}^{2}(\\mathbb{T})$. To avoid any confusion with functions defined over sequences, when dealing with complex function we make explicit the dependence on the complex variable $z=e^{i t}$. Note that a function $\\phi(z) \\in \\mathcal{L}^{2}(\\mathbb{T})$ can be represented, using the orthonormal basis $\\left\\{z^{n}\\right\\}_{n \\in \\mathbb{Z}}$, by means of its Fourier series: $\\phi(z)=\\sum_{n \\in \\mathbb{Z}} \\widehat{\\phi}(n) z^{n}$, with Fourier coefficients $\\widehat{\\phi}(n)=\\int_{\\mathbb{Z}} \\phi(z) \\bar{z}^{n} d z, n \\in \\mathbb{Z}$. Thus, we can partition the function space $\\mathcal{L}^{2}(\\mathbb{T})$ into two subspaces.\n\nDefinition 7. The Hardy space $\\mathcal{H}^{2}$ and the negative Hardy space $\\mathcal{H}_{-}^{2}$ on $\\mathbb{T}$ are the subspaces of $\\mathcal{L}^{2}(\\mathbb{T})$ defined as:\n\n$$\n\\begin{aligned}\n& \\mathcal{H}^{2}=\\left\\{\\phi(z) \\in \\mathcal{L}^{2}(\\mathbb{T}): \\widehat{\\phi}(n)=0, n<0\\right\\} \\\\\n& \\mathcal{H}_{-}^{2}=\\left\\{\\phi(z) \\in \\mathcal{L}^{2}(\\mathbb{T}): \\widehat{\\phi}(n)=0, n \\geq 0\\right\\}\n\\end{aligned}\n$$\n\nInterestingly, the elements of the Hardy space can be canonically identified with the set of functions analytic in the unit disc $\\mathbb{D}$, with the property that the square of their absolute value is integrable on $\\mathbb{T}$ (a proof can be found in Nikol'Skii (2002)). Thus, we will make no difference between these functions in the unit disc and their boundary value on the circle. Moreover, we remark that the definition of Hardy space can be generalized for any $p$-th power of the functions' absolute value, for $0<p \\leq \\infty$.\n\nWe define Hankel operators in the Hardy spaces.\nDefinition 8. Let $\\phi(z)$ be a function in the space $\\mathcal{L}^{2}(\\mathbb{T})$. A Hankel operator is an operator $H_{\\phi}: \\mathcal{H}^{2} \\rightarrow \\mathcal{H}_{-}^{2}$ defined by $H_{\\phi} f(z)=\\mathbb{P}_{-} \\phi f(z)$, where $\\mathbb{P}_{-}$is the orthogonal projection from $\\mathcal{L}^{2}(\\mathbb{T})$ onto $\\mathcal{H}_{-}^{2}$. The function $\\phi(z)$ is called a symbol of the Hankel operator $H_{\\phi}$.\n\nThe matrix $\\mathbf{H}_{\\phi}$ associated to the Hankel operator $H_{\\phi}: \\mathcal{H}^{2} \\rightarrow \\mathcal{H}_{-}^{2}$ is:\n\n$$\n\\mathbf{H}_{\\phi}=\\left(\\begin{array}{cccc}\n\\widehat{\\phi}(-1) & \\widehat{\\phi}(-2) & \\widehat{\\phi}(-3) & \\ldots \\\\\n\\widehat{\\phi}(-2) & \\widehat{\\phi}(-3) & \\widehat{\\phi}(-4) & \\ldots \\\\\n\\widehat{\\phi}(-3) & \\widehat{\\phi}(-4) & \\widehat{\\phi}(-5) & \\ldots \\\\\n\\vdots & \\vdots & \\vdots & \\ddots\n\\end{array}\\right)\n$$\n\nNote that this matrix satisfies the Hankel property, as each entry only depends on the composition of the corresponding coordinates.\n\nNehari's theorem (Nehari 1957), characterizes bounded Hankel operators and their norm.\nTheorem 9 (Nehari (1957)). Let $\\phi \\in \\mathcal{L}^{2}(\\mathbb{T})$ be a symbol of the Hankel operator on Hardy spaces $H_{\\phi}: \\mathcal{H}^{2} \\rightarrow \\mathcal{H}_{-}^{2}$. Then, $H_{\\phi}$ is bounded on $\\mathcal{H}^{2}$ if and only if there exists $\\psi \\in \\mathcal{L}^{\\infty}(\\mathbb{T})$ such that $\\widehat{\\psi}(m)=\\widehat{\\phi}(m)$ for all $m<0$. If the conditions above are satisfied, then:\n\n$$\n\\left\\|H_{\\phi}\\right\\|=\\inf \\{\\|\\psi\\|_{\\infty}: \\widehat{\\psi}(m)=\\widehat{\\phi}(m), m<0\\}\n$$\n\nAs a consequence of Theorem 9 , if $H_{\\phi}$ is a bounded operator, we can consider without loss of generality $\\phi(z) \\in \\mathcal{L}^{\\infty}(\\mathbb{T})$. We remark that a Hankel operator has infinitely many different symbols, since $H_{\\phi}=H_{\\phi+\\psi}$ for $\\psi(z) \\in \\mathcal{H}^{\\infty}$."
    },
    {
      "markdown": "Definition 10. The complex function $\\phi(z)$ is rational if $\\phi(z)=p(z) / q(z)$, with $p(z)$ and $q(z)$ polynomials. The rank of $\\phi(z)$ is the maximum between the degrees of $p(z)$ and $q(z)$. A rational function is strictly proper if the degree of $p(z)$ is strictly smaller than that of $q(z)$.\n\nThe following result of Kronecker relates finite-rank infinite Hankel matrices to rational functions.\n\nTheorem 11 (Kronecker (1881)). Let $H_{\\phi}$ be a bounded Hankel operator with matrix H. Then $\\mathbf{H}$ has finite rank if and only if $\\mathbb{P}_{-} \\phi$ is a strictly proper rational function. Moreover the rank of $\\mathbf{H}$ is equal to the number of poles (with multiplicities) of $\\mathbb{P}_{-} \\phi$ inside the unit disc.\n\nWe are ready to state the main result of Adamyan et al. (1971). The theorem shows that for infinite dimensional Hankel matrices the constraint of preserving the Hankel property does not affect the achievable approximation error.\n\nTheorem 12 (Adamyan et al. (1971)). Let $H_{\\phi}$ be a compact Hankel operator of rank $n$, matrix $\\mathbf{H}$ and singular numbers $\\sigma_{0} \\geq \\cdots \\geq \\sigma_{n-1}>0$. Then there exists a unique Hankel operator $H_{g}$ with matrix $\\mathbf{G}$ of rank $k<n$ such that:\n\n$$\n\\left\\|H_{\\phi}-H_{g}\\right\\|=\\|\\mathbf{H}-\\mathbf{G}\\|=\\sigma_{k}\n$$\n\nWe denote with $\\mathcal{R}_{k} \\subset \\mathcal{H}_{-}^{\\infty}$ the set of strictly proper rational functions of rank $k$, and we consider the set of functions:\n\n$$\n\\mathcal{H}_{k}^{\\infty}=\\left\\{\\psi \\in \\mathcal{L}^{\\infty}(\\mathbb{T}): \\exists g \\in \\mathcal{R}_{k}, \\exists l \\in \\mathcal{H}^{\\infty}, \\psi=g+l\\right\\}\n$$\n\nThe proof of the AAK theorem is directly connected with the problem of approximating a bounded function defined on the unit circle. In fact, the theorem can be reformulated in terms of the symbols associated with the Hankel operators.\n\nTheorem 13 (Adamyan et al. (1971)). Let $\\phi \\in \\mathcal{L}^{\\infty}(\\mathbb{T})$. Then there exists a complex function $\\psi \\in \\mathcal{H}_{k}^{\\infty}$ such that:\n\n$$\n\\|\\phi-\\psi\\|_{\\infty}=\\sigma_{k}\\left(H_{\\phi}\\right)\n$$\n\nThis theorem provides us with an alternative interpretation of singular numbers, relating them to the \"smoothness\" of the corresponding operator (or symbol). The advantage of this second formulation is that its proof is constructive, and tells us how to find the function $\\psi$.\n\nWe state as a corollary the critical steps of the proof, that allows us to find the best approximating symbol.\n\nCorollary 14. Let $\\phi$ and $\\left\\{\\boldsymbol{\\xi}_{k}, \\boldsymbol{\\eta}_{k}\\right\\}$ be a symbol and a $\\sigma_{k}$-Schmidt pair for $H_{\\phi}$. A function $\\psi \\in \\mathcal{L}^{\\infty}(\\mathbb{T})$ is the best AAK approximation according to Theorem 13, if and only if:\n\n$$\n(\\phi-\\psi) \\xi_{k}^{+}=\\sigma_{k} \\eta_{k}^{-}\n$$\n\nMoreover, the function $\\psi$ does not depend on the particular choice of the pair $\\left\\{\\boldsymbol{\\xi}_{k}, \\boldsymbol{\\eta}_{k}\\right\\}$.\nNote that the solutions of Theorem 12 and 13 are strictly related.\nCorollary 15. Let $\\psi \\in \\mathcal{H}_{k}^{\\infty}$, with $\\psi=l+g, g \\in \\mathcal{R}_{k}, l \\in \\mathcal{H}^{\\infty}$. If $\\psi$ solves Equation 5, then $H_{g}$ is the unique Hankel operator from Theorem 12.\n\nIn particular, this means that to find the Hankel operator $H_{g}$ corresponding to the optimal approximation, we can first obtain $\\psi$ by applying Corollary 14. Then, we can extract the rational component $g$ of $\\psi$ : this will correspond to a symbol for $\\mathbf{H}_{g}$."
    },
    {
      "markdown": "# 3 AAK Theory and Approximate Minimization \n\nTheorem 2 establishes a correspondence between a given minimal WFA $A$ with $n$ states and a Hankel matrix $\\mathbf{H}$ of rank $n$. The relation between rank and number of states is what motivates our choice to reformulate the approximate minimization problem as low-rank approximation of the Hankel matrix. The approach that we propose to approximate $A$ is to find the minimal WFA corresponding to the Hankel matrix that minimizes $\\mathbf{H}$ optimally in the spectral norm. We recall the fundamental result of Eckart \\& Young (1936).\nTheorem 16 (Eckart \\& Young (1936)). Let $\\mathbf{H}$ be a Hankel matrix corresponding to a compact Hankel operator of rank $n$, and $\\sigma_{m}$, with $0 \\leq m<n$ and $\\sigma_{0} \\geq \\cdots \\geq \\sigma_{n-1}>0$, its singular numbers. Then, if $\\mathbf{R}$ is a matrix of rank $k$, we have: $\\|\\mathbf{H}-\\mathbf{R}\\| \\geq \\sigma_{k}$. The equality is attained when $\\mathbf{R}$ corresponds to the truncated $S V D$ of $\\mathbf{H}$.\n\nIn the following example, we compute the low-rank approximation of a finite Hankel matrix using the truncated SVD.\nExample 1. We consider the Hankel matrix $\\mathbf{M} \\in \\mathbb{R}^{3 \\times 3}$,\n\n$$\n\\mathbf{M}=\\left(\\begin{array}{lll}\n1 & 2 & 3 \\\\\n2 & 3 & 1 \\\\\n3 & 1 & 2\n\\end{array}\\right)\n$$\n\nThe singular value decomposition of $\\mathbf{M}$ is $\\mathbf{M}=\\mathbf{U D V}^{\\top}$, with\n\n$$\n\\mathbf{U}=\\left(\\begin{array}{ccc}\n\\frac{1}{\\sqrt{3}} & \\sqrt{\\frac{2}{3}} & 0 \\\\\n\\frac{1}{\\sqrt{3}} & -\\frac{1}{\\sqrt{6}} & \\frac{1}{\\sqrt{2}} \\\\\n\\frac{1}{\\sqrt{3}} & -\\frac{1}{\\sqrt{6}} & -\\frac{1}{\\sqrt{2}}\n\\end{array}\\right), \\quad \\mathbf{D}=\\left(\\begin{array}{ccc}\n6 & 0 & 0 \\\\\n0 & \\sqrt{3} & 0 \\\\\n0 & 0 & \\sqrt{3}\n\\end{array}\\right) \\quad \\mathbf{V}=\\left(\\begin{array}{ccc}\n\\frac{1}{\\sqrt{3}} & -\\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{9}} \\\\\n\\frac{1}{\\sqrt{3}} & 0 & \\sqrt{\\frac{2}{3}} \\\\\n\\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{6}}\n\\end{array}\\right)\n$$\n\nThe rank 2 matrix $\\overline{\\mathbf{M}}$ obtained by truncating the $S V D$ is not Hankel:\n\n$$\n\\overline{\\mathbf{M}}=\\left(\\begin{array}{ccc}\n1 & 2 & 3 \\\\\n\\frac{5}{2} & 2 & \\frac{3}{2} \\\\\n\\frac{8}{2} & 2 & \\frac{3}{2}\n\\end{array}\\right)\n$$\n\nIt is easy to see that the low-rank approximation of a Hankel matrix obtained by truncating its SVD is not in general a Hankel matrix. This is problematic, since the low-rank approximation needs to be a Hankel matrix in order to correspond to a WFA. On the other hand, we have seen that, by applying AAK theory, we can find the optimal Hankel matrix minimizing the (Hankel) matrix of a Hankel operator in the Hardy spaces. Our objective is to find a way to apply AAK theory to solve the approximate minimization problem of WFAs. To do this, we need an appropriate framework to reformulate this task in terms of Hankel operators and complex functions.\n\n### 3.1 Defining a Hankel Operator: the one-letter assumption\n\nAs a first step, we want to understand whether or not a Hankel operator on the Hardy space can be associated to the Hankel matrix of a weighted automaton. To do so, we compare the Hankel matrix $\\mathbf{H}_{f}$ of a WFA realizing a function $f$ over an alphabet $\\Sigma$, to the Hankel matrix $\\mathbf{H}_{\\phi}$ of a Hankel operator in the Hardy space:\n\n$$\n\\mathbf{H}_{f}=\\left(\\begin{array}{cccc}\nf(\\varepsilon) & f(a) & f(b) & \\ldots \\\\\nf(a) & f(a a) & f(a b) & \\ldots \\\\\nf(b) & f(b a) & f(b b) & \\ldots \\\\\n\\vdots & \\vdots & \\vdots & \\ddots\n\\end{array}\\right) \\quad \\mathbf{H}_{\\phi}=\\left(\\begin{array}{ccc}\n\\widehat{\\phi}(-1) & \\widehat{\\phi}(-2) & \\widehat{\\phi}(-3) & \\ldots \\\\\n\\widehat{\\phi}(-2) & \\widehat{\\phi}(-3) & \\widehat{\\phi}(-4) & \\ldots \\\\\n\\widehat{\\phi}(-3) & \\widehat{\\phi}(-4) & \\widehat{\\phi}(-5) & \\ldots \\\\\n\\vdots & \\vdots & \\vdots & \\ddots\n\\end{array}\\right)\n$$"
    },
    {
      "markdown": "We remark that the columns and rows of $\\mathbf{H}_{f}$ are indexed using the letters of the alphabet $\\Sigma$ :\n\n$$\n\\mathbf{H}_{f}(p, s)=f(p s) \\quad \\text { for } p, s \\in \\Sigma\n$$\n\nwhile in the case of $\\mathbf{H}_{\\phi}$, the entries are indexed using natural numbers\n\n$$\n\\mathbf{H}_{\\phi}(j, k)=\\widehat{\\phi}(-j-k-1) \\quad \\text { for } j, k \\geq 0\n$$\n\nIf we think of the intuitive definition of the Hankel property presented in the previous section, we have that it holds in both cases the entries of the matrices only depend on the composition of the coordinate. Note that \"composition\" means concatenation of letters in the first case, and sum of numbers in the second one. One fundamental difference is that adding natural numbers is a commutative operation, while concatenating letters is not. For example, while for the matrix corresponding to a Hankel operator in the Hardy space we have:\n\n$$\n\\mathbf{H}_{\\phi}(0,1)=\\widehat{\\phi}(-2)=\\mathbf{H}_{\\phi}(1,0)\n$$\n\nin the case of the WFA's matrix, this is not true:\n\n$$\n\\mathbf{H}_{f}(a, b)=f(a b) \\neq f(b a)=\\mathbf{H}_{f}(b, a)\n$$\n\nThis fact reflects in the much stronger structural property satisfied by Hankel of matrices in the Hardy spaces, where the Hankel property implies that the anti-diagonals have constant entries. This property is not reflected by the matrix of an arbitrary WFA, so it is not always possible to associate a Hankel operator to an automaton over an alphabet of arbitrary size, and AAK theory cannot be generally applied. The only case in which concatenation of strings is commutative, is when we are restricting our focus on alphabets of one letter. In particular, when $|\\Sigma|=1$, the set of strings $\\Sigma^{*}$ can be identified with $\\mathbb{N}$. Therefore, the function $f: \\Sigma^{*} \\rightarrow \\mathbb{R}$ recognized by a minimal WFA can be rewritten as $f: \\mathbb{N} \\rightarrow \\mathbb{R}$, and the Hankel matrix $\\mathbf{H}_{f}$ associated with it can be interpreted as the matrix of a Hankel operator between sequences $H_{f}: \\ell^{2} \\rightarrow \\ell^{2}$. In this case, the Hankel matrix is defined by $\\mathbf{H}(i, j)=f(i+j)$, for $i, j \\geq 0$ :\n\n$$\n\\mathbf{H}_{f}=\\left(\\begin{array}{cccc}\nf(0) & f(1) & f(2) & \\ldots \\\\\nf(1) & f(2) & f(3) & \\ldots \\\\\nf(2) & f(3) & f(4) & \\ldots \\\\\n\\vdots & \\vdots & \\vdots & \\ddots\n\\end{array}\\right)\n$$\n\nUsing the Fourier isomorphism, we can interpret $\\mathbf{H}_{f}$ as the matrix $\\mathbf{H}_{\\phi}$ of a Hankel operator over Hardy spaces, associated to a complex function $\\phi \\in \\mathcal{L}^{2}(\\mathbb{T})$. In particular, we can embed the sequence space $\\ell^{2}$ into $\\ell^{2}(\\mathbb{Z})$ by \"duplicating\" each vector, i.e. by associating $\\boldsymbol{\\mu}=\\left(\\mu_{0}, \\mu_{1}, \\ldots\\right) \\in$ $\\ell^{2}$ to $\\boldsymbol{\\mu}^{(2)}=\\left(\\ldots, \\mu_{1}, \\mu_{0}, \\mu_{1}, \\ldots\\right) \\in \\ell^{2}(\\mathbb{Z})$. Then, we can use the Fourier isomorphism to map the vector $\\boldsymbol{\\mu}^{(2)} \\in \\ell^{2}(\\mathbb{Z})$ to the complex function space $\\mathcal{L}^{2}(\\mathbb{T})$. In this way, each vector $\\boldsymbol{\\mu} \\in \\ell^{2}$ corresponds to two functions in the Hardy spaces:\n\n$$\n\\begin{aligned}\n& \\mu^{-}(z)=\\sum_{j=0}^{\\infty} \\boldsymbol{\\mu}_{j} z^{-j-1} \\in \\mathcal{H}_{-}^{2} \\\\\n& \\mu^{+}(z)=\\sum_{j=0}^{\\infty} \\boldsymbol{\\mu}_{j} z^{j} \\in \\mathcal{H}^{2}\n\\end{aligned}\n$$"
    },
    {
      "markdown": "![img-0.jpeg](img-0.jpeg)\n\nFigure 1: Graphical representation of the generative probabilistic automaton described in Example 2 .\n\nMoreover, we can derive the relationship between $f$ and $\\phi$ :\n\n$$\n\\left(\\begin{array}{cccc}\nf_{A}(0) & f_{A}(1) & f_{A}(2) & \\ldots \\\\\nf_{A}(1) & f_{A}(2) & f_{A}(3) & \\ldots \\\\\nf_{A}(2) & f_{A}(3) & f_{A}(4) & \\ldots \\\\\n\\vdots & \\vdots & \\vdots & \\ddots\n\\end{array}\\right)=\\left(\\begin{array}{cccc}\n\\widehat{\\phi}(-1) & \\widehat{\\phi}(-2) & \\widehat{\\phi}(-3) & \\ldots \\\\\n\\widehat{\\phi}(-2) & \\widehat{\\phi}(-3) & \\widehat{\\phi}(-4) & \\ldots \\\\\n\\widehat{\\phi}(-3) & \\widehat{\\phi}(-4) & \\widehat{\\phi}(-5) & \\ldots \\\\\n\\vdots & \\vdots & \\vdots & \\ddots\n\\end{array}\\right)\n$$\n\nfrom which we obtain:\n\n$$\nf(n)=\\widehat{\\phi}(-n-1)\n$$\n\nSince we know how to express the function $f$ with respect to the parameters of the WFA, we can explicitly compute the rational component of the symbol:\n\n$$\n\\mathbb{P}_{-} \\phi=\\sum_{k \\geq 0} f(k) z^{-k-1}=\\sum_{k \\geq 0} \\boldsymbol{\\alpha}^{\\top} \\mathbf{A}^{k} \\boldsymbol{\\beta} z^{-k-1}=\\boldsymbol{\\alpha}^{\\top}(z \\mathbf{1}-\\mathbf{A})^{-1} \\boldsymbol{\\beta}\n$$\n\nwhere the last equality holds only if $\\rho(A)<1$.\nThe correspondence between symbol and function computed by a model allows us to reformulate the approximation problem in terms of Hankel operators and functions in the complex space, and to apply AAK theory.\n\nWe consider the following example, from Balle et al. (2021).\nExample 2. Let $|\\Sigma|=1, \\Sigma=\\{x\\}$, we consider the WFA $A=\\langle\\boldsymbol{\\alpha}, \\mathbf{A}, \\boldsymbol{\\beta}\\rangle$ represented in Figure 1, with:\n\n$$\n\\mathbf{A}=\\left(\\begin{array}{cc}\n0 & \\frac{1}{2} \\\\\n\\frac{1}{2} & 0\n\\end{array}\\right), \\quad \\boldsymbol{\\alpha}=\\binom{\\frac{\\sqrt{3}}{2}}{0}, \\quad \\boldsymbol{\\beta}=\\binom{\\frac{\\sqrt{3}}{2}}{0}\n$$\n\nNote that $A$ is a generative probabilistic automaton. Indeed, we have that\n\n- $f_{A}(x) \\geq 0$\n- $\\sum_{x \\in \\Sigma^{*}} f_{A}(x)=1$,\nsince the rational function realized by the WFA is defined as:\n\n$$\nf_{A}(x \\cdots x)=f_{A}(k)=\\boldsymbol{\\alpha}^{\\top} \\mathbf{A}^{k} \\boldsymbol{\\beta}= \\begin{cases}0 & \\text { if } k \\text { is odd } \\\\ \\frac{3}{4} 2^{-k} & \\text { if } k \\text { is even }\\end{cases}\n$$\n\nwhere $k$ corresponds to the string where $x$ is repeated $k$-times. We remark that $A$ is minimal and already in its SVA form, with Gramians\n\n$$\n\\mathbf{P}=\\mathbf{Q}=\\left(\\begin{array}{ll}\n\\frac{4}{5} & 0 \\\\\n0 & \\frac{1}{5}\n\\end{array}\\right)\n$$"
    },
    {
      "markdown": "The corresponding Hankel matrix, with entries defined as $\\mathbf{H}(i, j)=f(i+j)$, has rank 2 :\n\n$$\n\\mathbf{H}=\\left(\\begin{array}{cccc}\nf_{A}(0) & f_{A}(1) & f_{A}(2) & \\ldots \\\\\nf_{A}(1) & f_{A}(2) & f_{A}(3) & \\ldots \\\\\nf_{A}(2) & f_{A}(3) & f_{A}(4) & \\ldots \\\\\n\\vdots & \\vdots & \\vdots & \\ddots\n\\end{array}\\right)=\\left(\\begin{array}{cccc}\n\\frac{3}{4} & 0 & \\frac{3}{16} & \\ldots \\\\\n0 & \\frac{3}{16} & 0 & \\ldots \\\\\n\\frac{3}{16} & 0 & \\frac{3}{64} & \\ldots \\\\\n\\vdots & \\vdots & \\vdots & \\ddots\n\\end{array}\\right)\n$$\n\nNow, we can apply the second interpretation of the Hankel matrix and look at it with respect to the symbol, using the definition $\\mathbf{H}(j, k)=\\widetilde{\\phi}(-j-k-1)$. We have:\n\n$$\n\\mathbf{H}=\\left(\\begin{array}{cccc}\n\\frac{3}{4} & 0 & \\frac{3}{16} & \\ldots \\\\\n0 & \\frac{3}{16} & 0 & \\ldots \\\\\n\\frac{3}{16} & 0 & \\frac{3}{64} & \\ldots \\\\\n\\vdots & \\vdots & \\vdots & \\ddots\n\\end{array}\\right)=\\left(\\begin{array}{cccc}\n\\widetilde{\\phi}(-1) & \\widetilde{\\phi}(-2) & \\widetilde{\\phi}(-3) & \\ldots \\\\\n\\widetilde{\\phi}(-2) & \\widetilde{\\phi}(-3) & \\widetilde{\\phi}(-4) & \\ldots \\\\\n\\widetilde{\\phi}(-3) & \\widetilde{\\phi}(-4) & \\widetilde{\\phi}(-5) & \\ldots \\\\\n\\vdots & \\vdots & \\vdots & \\ddots\n\\end{array}\\right)\n$$\n\nWe can recover the rational component of a symbol, i.e. the projection of $\\phi$ on the negative Hardy space.\n\n$$\n\\mathbb{P}_{-} \\phi=\\sum_{n \\geq 0} \\widetilde{\\phi}(-n-1) z^{-n-1}=\\sum_{n \\geq 0} \\frac{3}{4} 4^{-n} z^{-2 n-1}=\\frac{3 z}{4 z^{2}-1}\n$$\n\nNote that this is a complex rational function having degree 2, and it has two poles inside the unit disc at $z= \\pm \\frac{1}{2}$ (as predicted by Theorem 11). It is important to remark that from the Hankel matrix we can only recover the negative Fourier coefficients of $\\phi$, meaning only the component of the symbol that belongs to the negative Hardy space.\n\n# 4 Solving the Approximate Minimization Problem \n\nIn this section we present the theoretical contribution of this paper, a closed form solution for the approximate minimization problem.\n\n### 4.1 Assumptions\n\nWe briefly list and analyze the assumptions made to solve the approximate minimization problem. A class of automata that automatically satisfies the following properties is that of generative probabilistic automata.\n\nOne-letter Alphabet We tackle the approximate minimization problem in the case of automata with real weights, defined over a one-letter alphabet. As discussed before, this assumption is needed in order to apply AAK theory, and will hold for the rest of the paper (see Section 3.1 for more details).\n\nSVA Form We assume that the minimal WFA $A=\\langle\\boldsymbol{\\alpha}, \\mathbf{A}, \\boldsymbol{\\beta}\\rangle$ is in SVA form. This assumption is not necessary, as the SVA can be efficiently computed from a WFA satisfying the set of assumptions stated above (Balle et al. 2019). Starting from a WFA in SVA form allows us to obtain results that are representation independent. Since the alphabet has size one, the Hankel matrix $\\mathbf{H}$ is symmetric. Therefore, if we denote with $\\lambda_{i}$ the $i$-th non-zero eigenvalue of $\\mathbf{H}$, and we consider the coordinates of $\\boldsymbol{\\alpha}$ and $\\boldsymbol{\\beta}$, we have that $\\boldsymbol{\\alpha}_{i}=\\operatorname{sgn}\\left(\\lambda_{i}\\right) \\boldsymbol{\\beta}_{i}$, where $\\operatorname{sgn}\\left(\\lambda_{i}\\right)=\\lambda_{i} /\\left|\\lambda_{i}\\right|$."
    },
    {
      "markdown": "Compactness of the Operator To apply Theorem 12 we need the Hankel operator $H$ to be compact. To ensure that this condition is satisfied, we study the respective Hankel matrix. In our setting, the Hankel matrix has finite rank (equal to the number of states of the minimal WFA that we are considering). Moreover, the singular values can be computed exactly using the Gramian matrices introduced in Definition 4. A finite rank operator is compact only if it is bounded. Therefore, we just need to check that the Hankel operator is bounded. To this extent, Balle et al. (2019) show that it is enough that the WFA being considered computes a function $f \\in \\ell^{2}$. We make the slightly stronger assumption that the transition matrix $A$ is irreduntant, i.e. that $\\rho(\\mathbf{A})<1$, where $\\rho$ is the spectral radius. This condition directly implies boundness and the existence of the SVA and the Gramian matrices $\\mathbf{P}$ and $\\mathbf{Q}$, where $\\mathbf{P}=\\mathbf{Q}$ and are diagonal matrices (Balle et al. 2019). Moreover, it allows us to compute a closed form for the symbol of a WFA, as seen in Equation 10.\n\n# 4.2 Problem Formulation \n\nLet $A=\\langle\\boldsymbol{\\alpha}, \\mathbf{A}, \\boldsymbol{\\beta}\\rangle$ be a minimal irredundant WFA with $n$ states and real weights, defined over a one-letter alphabet and represented in its SVA form. Let $\\mathbf{H}$ be the Hankel matrix of $A$, we denote with $\\sigma_{i}$, for $0 \\leq i<n$, the singular numbers. Given a target number of states $k<n$, we say that a WFA $\\widehat{A}_{k}$ with $k$ states solves the optimal spectral-norm approximate minimization problem if the Hankel matrix $\\mathbf{G}$ of $\\widehat{A}_{k}$ satisfies:\n\n$$\n\\|\\mathbf{H}-\\mathbf{G}\\|=\\sigma_{k}(\\mathbf{H})\n$$\n\nNote that the content of the \"optimal spectral-norm approximate minimization\" is equivalent to the problem solved by Theorem 12, with the exception that here we represent the inputs and outputs of the problem effectively by means of WFAs.\n\nBased on the AAK theory sketched in Section 2.3, we draw the following steps:\n(1) Compute a symbol for the WFA. Given an irredundant WFA on a one-letter alphabet, we consider its Hankel matrix $\\mathbf{H}$ and the function $f$ that it is computing. We use Equation 10 to associate a complex rational function to the WFA.\n(2) Compute the optimal symbol $\\psi(z)$ using Corollary 14. The main challenge here is to find a suitable representation for the functions $\\psi(z)$ and $e(z)=\\phi(z)-\\psi(z)$. We define them in terms of two auxiliary WFAs. The key point is to select constraints on their parameters to leverage the properties of weighted automata, while still keeping the formulation general.\n(3) Extracting the rational component by solving for $g(z)$ in Corollary 15. This step is arguably the most conceptually challenging, as it requires to identify the position of the function's poles. In fact, we know from Theorem 11 that $g(z)$ has $k$ poles, all inside the unit disc.\n(4) Find a WFA representation for $g(z)$. Since in Step 4.2 we parametrized the functions using WFAs, the expression of $g(z)$ directly reveals the WFA $\\widehat{A}_{k}$.\n\n### 4.3 Finding the Optimal Approximation\n\nWe analyze each of the steps detailed above.\n\n### 4.3.1 Finding a Symbol for the WFA\n\nLet $A=\\langle\\boldsymbol{\\alpha}, \\mathbf{A}, \\boldsymbol{\\beta}\\rangle$ be a minimal irredundant WFA with $n$ states, already represented in its in SVA form. $A$ realizes a function $f: \\Sigma^{*} \\rightarrow \\mathbb{R}$, defined over a one-letter alphabet $\\Sigma=\\{a\\}$. Let $\\mathbf{H}$"
    },
    {
      "markdown": "be its Hankel matrix, with corresponding bounded Hankel operator $H$, and singular numbers $\\sigma_{i}$, for $0 \\leq i<n$.\n\nAs seen in Subsection 3.1, we can associate a complex function to the WFA. In particular, since we are assuming that $A$ is irredundant, from Equation 10 we obtain an expression for the rational component of the symbol:\n\n$$\n\\mathbb{P}_{-} \\phi=\\boldsymbol{\\alpha}^{\\top}(z \\mathbf{1}-\\mathbf{A})^{-1} \\boldsymbol{\\beta}\n$$\n\n# 4.3.2 Finding the Optimal Symbol \n\nTo find the solution of Theorem 12, we need to first derive the function $\\psi$ from Theorem 13. Therefore, the second step to solve the approximate minimization problem is to find a proper expression for the complex functions $\\psi$ and $e=\\phi-\\psi$ described in Theorem 13. Since our objective is to find the WFA corresponding to the optimal approximation, we focus on representing these functions using the parameters of two auxiliary WFAs. We consider a WFA $\\widehat{A}=\\langle\\widehat{\\boldsymbol{\\alpha}}, \\widehat{\\mathbf{A}}, \\widehat{\\boldsymbol{\\beta}}\\rangle$ with more than $k$ states, such that the automaton $E=\\left\\langle\\boldsymbol{\\alpha}_{e}, \\mathbf{A}_{e}, \\boldsymbol{\\beta}_{e}\\right\\rangle$ computing the difference between $A$ and $\\widehat{A}$ is minimal, with:\n\n$$\n\\mathbf{A}_{e}=\\left(\\begin{array}{cc}\n\\mathbf{A} & \\mathbf{0} \\\\\n\\mathbf{0} & \\widehat{\\mathbf{A}}\n\\end{array}\\right), \\quad \\boldsymbol{\\alpha}_{e}=\\binom{\\boldsymbol{\\alpha}}{-\\widehat{\\boldsymbol{\\alpha}}}, \\quad \\boldsymbol{\\beta}_{e}=\\binom{\\boldsymbol{\\beta}}{\\widehat{\\boldsymbol{\\beta}}}\n$$\n\nNow, given $C \\in \\mathcal{H}^{\\infty}$, we consider the complex functions:\n\n$$\n\\begin{aligned}\n& \\psi=\\widehat{\\boldsymbol{\\alpha}}^{\\top}(z \\mathbf{1}-\\widehat{\\mathbf{A}})^{-1} \\widehat{\\boldsymbol{\\beta}}+C \\\\\n& e=\\phi-\\psi=\\boldsymbol{\\alpha}_{e}^{\\top}\\left(z \\mathbf{1}-\\mathbf{A}_{e}\\right)^{-1} \\boldsymbol{\\beta}_{e}-C\n\\end{aligned}\n$$\n\nThe idea is that we want to find the parameters of $\\widehat{A}$ that make $\\psi$ the solution of Theorem 13. By definition, $\\psi$ is the sum of two components, one that is bounded around the unit circle and one that has $k$ poles inside the unit disc (where $k$ is the size of the sought approximation). Therefore, there cannot be poles on the unit circle. By looking at the way we defined the function $\\psi$, we can see that its poles correspond to the eigenvalues of $\\widehat{\\mathbf{A}}$, counted with their multiplicities. Thus, in order for $\\psi$ to be the solution of Theorem 13, 1 cannot be an eigenvalue of $\\widehat{\\mathbf{A}}$, and the WFA $\\widehat{A}$ needs to have at least $k$ states.\n\nAs remarked in the previous section, the parameters of the automaton $A$ only encode the negative Fourier coefficients of the symbol. We add $C$ to the definition of $\\psi$ to account for the $\\mathcal{H}^{\\infty}$ component when considering the difference $\\phi-\\psi$. In fact, while this component of the symbol does not affect the spectral norm, it plays a role in the computation of the $\\mathcal{L}^{\\infty}$-norm (in Equation 5), so it cannot be entirely dismissed. Nonetheless, we won't need to find the value of $C$, as ultimately we are only interested in the WFA's parameters.\n\nNow that we have an expression for $\\psi$ and $e$, we can look back at Theorem 13. From this theorem, we know that by definition, $\\sigma_{k}^{1} e$ is a unimodular function. In Appendix A we show how to compute the functions corresponding to a $\\sigma_{k}$-Schmidt pair, and that their quotient is indeed unimodular. This property of $e$ can be used to derive a set of constraints on the parameters of the WFA $E=\\left\\langle\\boldsymbol{\\alpha}_{e}, \\mathbf{A}_{e}, \\boldsymbol{\\beta}_{e}\\right\\rangle$. In particular, it is possible to leverage the maximum modulus principle, according to which the maximum modulus of an holomorphic function is attained on the boundary of the domain. A similar result has been obtained in the control theory literature (Chui \\& Chen 1997), and can be easily applied to our setting. In fact, a parallel can be drawn between dynamical systems and automata, by noting that the impulse-response of a discrete time-invariant Single-Input-Single-Output SISO system can be parametrized as a WFA over a one-letter alphabet. This allows us to apply a theorem from Chui \\& Chen (1997, Theorem 6.3)"
    },
    {
      "markdown": "to find two matrices, $\\mathbf{P}_{e}$ and $\\mathbf{Q}_{e}$, satisfying properties similar to those of the Gramians. It is important to notice that, a priori, the controllability and observability Gramians of $E$ might not be well defined. We refer the reader to Appendix A for a sketch of the proof.\nTheorem 17 (Chui \\& Chen (1997)). Consider the function $e=\\boldsymbol{\\alpha}_{e}^{\\top}\\left(z \\mathbf{1}-\\mathbf{A}_{e}\\right)^{-1} \\boldsymbol{\\beta}_{e}-C$ and the corresponding minimal WFA $E=\\left\\langle\\boldsymbol{\\alpha}_{e}, \\mathbf{A}_{e}, \\boldsymbol{\\beta}_{e}\\right\\rangle$ associated with it. If $\\sigma_{k}^{-1} e$ is unimodular, then there exists a unique pair of symmetric invertible matrices $\\mathbf{P}_{e}$ and $\\mathbf{Q}_{e}$ satisfying:\n(1) $\\mathbf{P}_{e}-\\mathbf{A}_{e} \\mathbf{P}_{e} \\mathbf{A}_{e}^{\\top}=\\boldsymbol{\\beta}_{e} \\boldsymbol{\\beta}_{e}^{\\top}$\n(2) $\\mathbf{Q}_{e}-\\mathbf{A}_{e}^{\\top} \\mathbf{Q}_{e} \\mathbf{A}_{e}=\\boldsymbol{\\alpha}_{e} \\boldsymbol{\\alpha}_{e}^{\\top}$\n(3) $\\mathbf{P}_{e} \\mathbf{Q}_{e}=\\sigma_{k}^{2} \\mathbf{1}$\n\nWe can now derive the parameters of the WFA $\\widehat{A}=\\langle\\widehat{\\boldsymbol{\\alpha}}, \\widehat{\\mathbf{A}}, \\widehat{\\boldsymbol{\\beta}}\\rangle$ that make $\\psi$ the solution of Theorem 13 .\n\nTheorem 18. Let $A=\\langle\\boldsymbol{\\alpha}, \\mathbf{A}, \\boldsymbol{\\beta}\\rangle$ be a minimal WFA with $n$ states in its $S V A$ form, and let $\\phi=\\boldsymbol{\\alpha}^{\\top}(z \\mathbf{1}-\\mathbf{A})^{-1} \\boldsymbol{\\beta}$ be a symbol for its Hankel operator $H$. Let $\\sigma_{k}$ be a singular number of multiplicity $r$ for $H$, with:\n\n$$\n\\sigma_{0} \\geq \\cdots>\\sigma_{k}=\\cdots=\\sigma_{k+r-1}>\\sigma_{k+r} \\geq \\cdots \\geq \\sigma_{n-1}>0\n$$\n\nWe can partition the Gramian matrices $\\mathbf{P}, \\mathbf{Q}$ as follows:\n\n$$\n\\mathbf{P}=\\mathbf{Q}=\\left(\\begin{array}{cc}\n\\boldsymbol{\\Sigma} & \\mathbf{0} \\\\\n\\mathbf{0} & \\sigma_{k} \\mathbf{1}_{r}\n\\end{array}\\right)\n$$\n\nwhere $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{(n-r) \\times(n-r)}$ is the diagonal matrix containing the remaining singular numbers, and partition $\\mathbf{A}, \\boldsymbol{\\alpha}$ and $\\boldsymbol{\\beta}$ to conform with the Gramians:\n\n$$\n\\mathbf{A}=\\left(\\begin{array}{ll}\n\\mathbf{A}_{11} & \\mathbf{A}_{12} \\\\\n\\mathbf{A}_{21} & \\mathbf{A}_{22}\n\\end{array}\\right), \\quad \\boldsymbol{\\alpha}=\\binom{\\boldsymbol{\\alpha}_{1}}{\\boldsymbol{\\alpha}_{2}}, \\quad \\boldsymbol{\\beta}=\\binom{\\boldsymbol{\\beta}_{1}}{\\boldsymbol{\\beta}_{2}}\n$$\n\nLet $\\mathbf{R}=\\sigma_{k}^{2} \\mathbf{1}_{n-r}-\\boldsymbol{\\Sigma}^{2}$, we denote by $(\\cdot)^{+}$the Moore-Penrose pseudo-inverse. If the function $\\psi=\\widehat{\\boldsymbol{\\alpha}}^{\\top}(z \\mathbf{1}-\\widehat{\\mathbf{A}})^{-1} \\widehat{\\boldsymbol{\\beta}}+C$ is the best approximation of $\\phi$, then:\n\n- If $\\boldsymbol{\\alpha}_{2} \\neq \\mathbf{0}$ :\n\n$$\n\\left\\{\\begin{array}{l}\n\\widehat{\\boldsymbol{\\beta}}=-\\widehat{\\mathbf{A}} \\mathbf{A}_{21}^{\\top}\\left(\\boldsymbol{\\beta}_{2}^{\\top}\\right)^{+} \\\\\n\\widehat{\\boldsymbol{\\alpha}}=\\widehat{\\mathbf{A}}^{\\top} \\mathbf{R} \\mathbf{A}_{12}\\left(\\boldsymbol{\\alpha}_{2}^{\\top}\\right)^{+} \\\\\n\\widehat{\\mathbf{A}}\\left(\\mathbf{A}_{11}^{\\top}-\\mathbf{A}_{21}^{\\top}\\left(\\boldsymbol{\\beta}_{2}^{\\top}\\right)^{+} \\boldsymbol{\\beta}_{1}^{\\top}\\right)=\\mathbf{1}\n\\end{array}\\right.\n$$\n\n- If $\\boldsymbol{\\alpha}_{2}=\\mathbf{0}:$\n\n$$\n\\left\\{\\begin{array}{l}\n\\widehat{\\boldsymbol{\\beta}}=\\left(\\mathbf{1}-\\widehat{\\mathbf{A}} \\mathbf{A}_{11}^{\\top}\\right)\\left(\\boldsymbol{\\beta}_{1}^{\\top}\\right)^{+} \\\\\n\\widehat{\\boldsymbol{\\alpha}}=-\\left(\\mathbf{R}-\\widehat{\\mathbf{A}}^{\\top} \\mathbf{R} \\mathbf{A}_{11}\\right)\\left(\\boldsymbol{\\alpha}_{1}^{\\top}\\right)^{+} \\\\\n\\widehat{\\mathbf{A}} \\mathbf{A}_{21}^{\\top}=\\mathbf{0}\n\\end{array}\\right.\n$$\n\nProof. Since $\\sigma^{-1} e=\\phi-\\psi$ is unimodular, from Theorem 17 there exist two symmetric nonsingular matrices $\\mathbf{P}_{e}, \\mathbf{Q}_{e}$ satisfying the fixed point equations:\n\n$$\n\\begin{aligned}\n& \\mathbf{P}_{e}-\\mathbf{A}_{e} \\mathbf{P}_{e} \\mathbf{A}_{e}^{\\top}=\\boldsymbol{\\beta}_{e} \\boldsymbol{\\beta}_{e}^{\\top} \\\\\n& \\mathbf{Q}_{e}-\\mathbf{A}_{e}^{\\top} \\mathbf{Q}_{e} \\mathbf{A}_{e}=\\boldsymbol{\\alpha}_{e} \\boldsymbol{\\alpha}_{e}^{\\top}\n\\end{aligned}\n$$"
    },
    {
      "markdown": "and such that $\\mathbf{P}_{e} \\mathbf{Q}_{e}=\\sigma_{k}^{2} \\mathbf{1}$. We can partition $\\mathbf{P}_{e}$ and $\\mathbf{Q}_{e}$ according to the definition of $\\mathbf{A}_{e}$ (see Equation 15):\n\n$$\n\\mathbf{P}_{e}=\\left(\\begin{array}{ll}\n\\mathbf{P}_{11} & \\mathbf{P}_{12} \\\\\n\\mathbf{P}_{12}^{\\top} & \\mathbf{P}_{22}\n\\end{array}\\right), \\quad \\mathbf{Q}_{e}=\\left(\\begin{array}{ll}\n\\mathbf{Q}_{11} & \\mathbf{Q}_{12} \\\\\n\\mathbf{Q}_{12}^{\\top} & \\mathbf{Q}_{22}\n\\end{array}\\right)\n$$\n\nFrom Equation 21 and 22, we note that $\\mathbf{P}_{11}$ and $\\mathbf{Q}_{11}$ correspond to the controllability and observability Gramians of $A$ :\n\n$$\n\\mathbf{P}_{11}=\\mathbf{Q}_{11}=\\mathbf{P}=\\left(\\begin{array}{cc}\n\\boldsymbol{\\Sigma} & \\mathbf{0} \\\\\n\\mathbf{0} & \\sigma_{k} \\mathbf{1}\n\\end{array}\\right)\n$$\n\nMoreover, since $\\mathbf{P}_{e} \\mathbf{Q}_{e}=\\sigma_{k}^{2} \\mathbf{1}$, we get $\\mathbf{P}_{12} \\mathbf{Q}_{12}^{\\top}=\\sigma_{k}^{2} \\mathbf{1}-\\mathbf{P}^{2}$. It follows that $\\mathbf{P}_{12} \\mathbf{Q}_{12}^{\\top}$ has rank $n-r$. Without loss of generality we can set $\\operatorname{dim} \\widetilde{\\mathbf{A}}=j=n-r$, and choose an appropriate basis for the state space such that $\\mathbf{P}_{12}=\\left(\\begin{array}{ll}1 & \\mathbf{0}\\end{array}\\right)^{\\top}$ and $\\mathbf{Q}_{12}=\\left(\\begin{array}{ll}\\mathbf{R} & \\mathbf{0}\\end{array}\\right)^{\\top}$, with $\\mathbf{R}=\\sigma_{k}^{2} \\mathbf{1}-\\boldsymbol{\\Sigma}^{2}$. Once $\\mathbf{P}_{12}$ and $\\mathbf{Q}_{12}$ are fixed, the values of $\\mathbf{P}_{22}$ and $\\mathbf{Q}_{22}$ are automatically determined. We obtain:\n\n$$\n\\mathbf{P}_{e}=\\left(\\begin{array}{rrr}\n\\boldsymbol{\\Sigma} & \\mathbf{0} & \\mathbf{1} \\\\\n\\mathbf{0} & \\sigma_{k} \\mathbf{1} & \\mathbf{0} \\\\\n\\mathbf{1} & \\mathbf{0} & -\\boldsymbol{\\Sigma} \\mathbf{R}^{-1}\n\\end{array}\\right), \\quad \\mathbf{Q}_{e}=\\left(\\begin{array}{rrr}\n\\boldsymbol{\\Sigma} & \\mathbf{0} & \\mathbf{R} \\\\\n\\mathbf{0} & \\sigma_{k} \\mathbf{1} & \\mathbf{0} \\\\\n\\mathbf{R} & \\mathbf{0} & -\\boldsymbol{\\Sigma} \\mathbf{R}\n\\end{array}\\right)\n$$\n\nNow that we have an expression for the matrices $\\mathbf{P}_{e}$ and $\\mathbf{Q}_{e}$ of Theorem 17, we can rewrite the fixed point equations to derive the parameters $\\widetilde{\\boldsymbol{\\alpha}}, \\widetilde{\\mathbf{A}}$ and $\\widetilde{\\boldsymbol{\\beta}}$. We obtain the following systems:\n\n$$\n\\left\\{\\begin{array} { l } \n{ \\mathbf { P } - \\mathbf { A P A } ^ { \\top } = \\boldsymbol { \\beta } \\boldsymbol { \\beta } ^ { \\top } } \\\\\n{ \\mathbf { N } - \\mathbf { A N } \\tilde { \\mathbf { A } } ^ { \\top } = \\boldsymbol { \\beta } \\tilde { \\boldsymbol { \\beta } } ^ { \\top } } \\\\\n{ \\left. - \\boldsymbol { \\Sigma } \\mathbf { R } ^ { - 1 } + \\tilde { \\mathbf { A } } \\boldsymbol { \\Sigma } \\mathbf { R } ^ { - 1 } \\tilde { \\mathbf { A } } ^ { \\top } = \\widetilde { \\beta } \\widetilde { \\boldsymbol { \\beta } } ^ { \\top } }\n\\end{array} \\quad \\left\\{\\begin{array}{l}\n\\mathbf{P}-\\mathbf{A}^{\\top} \\mathbf{P A}=\\boldsymbol{\\alpha} \\boldsymbol{\\alpha}^{\\top} \\\\\n\\mathbf{M}-\\mathbf{A}^{\\top} \\mathbf{M} \\tilde{\\mathbf{A}}=-\\boldsymbol{\\alpha} \\tilde{\\boldsymbol{\\alpha}}^{\\top} \\\\\n-\\boldsymbol{\\Sigma} \\mathbf{R}+\\tilde{\\mathbf{A}}^{\\top} \\boldsymbol{\\Sigma} \\mathbf{R} \\tilde{\\mathbf{A}}=\\tilde{\\boldsymbol{\\alpha}} \\tilde{\\boldsymbol{\\alpha}}^{\\top}\n\\end{array}\\right.\\right.\n$$\n\nwhere $\\mathbf{N}=\\binom{\\mathbf{1}}{\\mathbf{0}}$ and $\\mathbf{M}=\\binom{\\mathbf{R}}{\\mathbf{0}}$.\nWe can rewrite the second equation of each system as follows:\n\n$$\n\\left\\{\\begin{array} { l } \n{ \\mathbf { 1 } - \\mathbf { A } _ { 1 1 } \\tilde { \\mathbf { A } } ^ { \\top } = \\boldsymbol { \\beta } _ { 1 } \\tilde { \\boldsymbol { \\beta } } ^ { \\top } } \\\\\n{ \\left. - \\mathbf { A } _ { 2 1 } \\tilde { \\mathbf { A } } ^ { \\top } = \\boldsymbol { \\beta } _ { 2 } \\tilde { \\boldsymbol { \\beta } } ^ { \\top } }\n\\end{array} \\quad \\left\\{\\begin{array}{l}\n\\mathbf{R}-\\mathbf{A}_{11}^{\\top} \\mathbf{R} \\tilde{\\mathbf{A}}=-\\boldsymbol{\\alpha}_{1} \\tilde{\\boldsymbol{\\alpha}}^{\\top} \\\\\n\\tilde{\\mathbf{A}}^{\\top} \\mathbf{R} \\mathbf{A}_{12}=\\tilde{\\boldsymbol{\\alpha}} \\boldsymbol{\\alpha}_{2}^{\\top}\n\\end{array}\\right.\\right.\n$$\n\nIf $\\boldsymbol{\\alpha}_{2} \\neq \\mathbf{0}$, then also $\\boldsymbol{\\beta}_{2} \\neq \\mathbf{0}$ (recall that $\\boldsymbol{\\alpha}_{i}=\\operatorname{sgn}\\left(\\lambda_{i}\\right) \\boldsymbol{\\beta}_{i}$ ), and we have:\n\n$$\n\\left\\{\\begin{array}{l}\n\\tilde{\\boldsymbol{\\beta}}=-\\tilde{\\mathbf{A}} \\mathbf{A}_{21}^{\\top}\\left(\\boldsymbol{\\beta}_{2}^{\\top}\\right)^{+} \\\\\n\\tilde{\\boldsymbol{\\alpha}}=\\tilde{\\mathbf{A}}^{\\top} \\mathbf{R} \\mathbf{A}_{12}\\left(\\boldsymbol{\\alpha}_{2}^{\\top}\\right)^{+} \\\\\n\\tilde{\\mathbf{A}}\\left(\\mathbf{A}_{11}^{\\top}-\\mathbf{A}_{21}^{\\top}\\left(\\boldsymbol{\\beta}_{2}^{\\top}\\right)^{+} \\boldsymbol{\\beta}_{1}^{\\top}\\right)=\\mathbf{1}\n\\end{array}\\right.\n$$\n\nwith $\\left(\\boldsymbol{\\alpha}_{2}^{\\top}\\right)^{+}=\\frac{\\boldsymbol{\\alpha}_{2}}{\\boldsymbol{\\alpha}_{2}^{\\top} \\boldsymbol{\\alpha}_{2}}$ and $\\left(\\boldsymbol{\\beta}_{2}^{\\top}\\right)^{+}=\\frac{\\boldsymbol{\\beta}_{2}}{\\boldsymbol{\\beta}_{2}^{\\top} \\boldsymbol{\\beta}_{2}}$.\nIf $\\boldsymbol{\\alpha}_{2}=\\mathbf{0}$, we have $\\tilde{\\mathbf{A}} \\mathbf{A}_{21}^{\\top}=\\mathbf{0}$. We remark that $\\tilde{\\mathbf{A}}$ has size $(n-r) \\times(n-r)$, while $\\mathbf{A}_{21}^{\\top}$ is $(n-r) \\times r$, so the system of equations corresponding to $\\tilde{\\mathbf{A}} \\mathbf{A}_{21}^{\\top}=\\mathbf{0}$ is underdetermined if $r<\\frac{n}{2}$, in which case we can find an alternative set of solutions:\n\n$$\n\\left\\{\\begin{array}{l}\n\\tilde{\\boldsymbol{\\beta}}=\\left(\\mathbf{1}-\\tilde{\\mathbf{A}} \\mathbf{A}_{11}^{\\top}\\right)\\left(\\boldsymbol{\\beta}_{1}^{\\top}\\right)^{+} \\\\\n\\tilde{\\boldsymbol{\\alpha}}=-\\left(\\mathbf{R}-\\tilde{\\mathbf{A}}^{\\top} \\mathbf{R} \\mathbf{A}_{11}\\right)\\left(\\boldsymbol{\\alpha}_{1}^{\\top}\\right)^{+} \\\\\n\\tilde{\\mathbf{A}} \\mathbf{A}_{21}^{\\top}=\\mathbf{0}\n\\end{array}\\right.\n$$"
    },
    {
      "markdown": "with $\\widehat{\\mathbf{A}} \\neq \\mathbf{0}$. On the other hand, if $r \\geq \\frac{n}{2}$, i.e. if the multiplicity of the singular number $\\sigma_{k}$ is more than half the size of the original WFA, the system might not have any solution unless $\\widehat{\\mathbf{A}}=\\mathbf{0}$ (or unless $\\mathbf{A}_{21}$ was zero to begin with). In this setting the method proposed returns $\\widehat{\\mathbf{A}}=\\mathbf{0}$.\n\nWe remark that in the (rare) case in which the algorithm returns $\\widehat{\\mathbf{A}}=\\mathbf{0}$, an alternative and preferable approach is to search for an approximation of size $k-1$ or $k+1$. This way, the multiplicity $r$ of the singular number $\\sigma_{k}$ is such that $r<\\frac{n}{2}$, and the system in Equation 27 is underdetermined.\n\nTheorem 18 provides us with a way to compute the coefficients of the function $\\psi$ solving Theorem 13. It is important to notice that the WFA $A_{k}$ is not necessarily the best approximation we are looking for. Intuitively, the problem is that it might be too big, as irredundancy is not guaranteed by the system of equations (while we know from AAK theory that the best approximation corresponds to a bounded operator). Therefore, in these cases we need to \"extract\" from $A_{k}$ a smaller WFA of size $k$. We do this by extracting the component of the function $\\psi$ that belongs to the negative Hardy space.\n\n# 4.3.3 Extracting the Rational Component \n\nThe objective of this section is to \"isolate\" the function $g \\in \\mathcal{R}_{k}$, i.e. the rational component of $\\psi$. To do this, we study the position of the poles of $\\psi$. In fact, we know from Theorem 11 that the poles of a strictly proper rational function lie inside the unit disc. As noted before, the key to solving our problem is the way we parametrized the functions. We defined $\\psi$ so that its poles correspond to the eigenvalues of $\\widehat{A}$. Therefore, we study the eigenvalues of $\\widehat{\\mathbf{A}}$ using the following auxiliary result from Ostrowski \\& Schneider (1962). A proof of this theorem can be found in Wimmer (1973).\n\nTheorem 19 (Ostrowski \\& Schneider (1962)). Let $|\\Sigma|=1$, and let $\\mathbf{P}$ be a solution to the fixed point equation $X-\\mathbf{A} X \\mathbf{A}^{\\top}=\\boldsymbol{\\beta} \\boldsymbol{\\beta}^{\\top}$ for the WFA $A=\\langle\\boldsymbol{\\alpha}, \\mathbf{A}, \\boldsymbol{\\beta}\\rangle$. If $A$ is reachable, then:\n\n- The number of eigenvalues $\\lambda$ of $\\mathbf{A}$ such that $|\\lambda|<1$ is equal to the number of positive eigenvalues of $\\mathbf{P}$.\n- The number of eigenvalues $\\lambda$ of $\\mathbf{A}$ such that $|\\lambda|>1$ is equal to the number of negative eigenvalues of $\\mathbf{P}$.\n\nAfter a change of basis (that we detail in Section 5 with the approximation algorithm), we can rewrite $\\widehat{\\mathbf{A}}$ in block-diagonal form:\n\n$$\n\\widehat{\\mathbf{A}}=\\left(\\begin{array}{cc}\n\\widehat{\\mathbf{A}}_{+} & \\mathbf{0} \\\\\n\\mathbf{0} & \\widehat{\\mathbf{A}}_{-}\n\\end{array}\\right)\n$$\n\nwhere the modulus of the eigenvalues of $\\widehat{\\mathbf{A}}_{+}$(resp. $\\widehat{\\mathbf{A}}_{-}$) is smaller (resp. greater) than one. We then apply the same change of coordinates on $\\widehat{\\boldsymbol{\\alpha}}$ and $\\widehat{\\boldsymbol{\\beta}}$.\n\nWe can finally find the rational component of the function $\\psi$, i.e. the function $g$ from Corollary 15 necessary to solve that approximate minimization problem.\nTheorem 20. Let $\\widehat{\\mathbf{A}}_{+}, \\widehat{\\boldsymbol{\\alpha}}_{+}, \\widehat{\\boldsymbol{\\beta}}_{+}$be as in Equation 28. The rational component of $\\psi$ is the function $g=\\widehat{\\boldsymbol{\\alpha}}_{+}^{\\top}\\left(z \\mathbf{1}-\\widehat{\\mathbf{A}}_{+}\\right)^{-1} \\widehat{\\boldsymbol{\\beta}}_{+}$.\n\nProof. Clearly $\\psi=g+l$, with $l=\\widehat{\\boldsymbol{\\alpha}}_{-}^{\\top}\\left(z \\mathbf{1}-\\widehat{\\mathbf{A}}_{-}\\right)^{-1} \\widehat{\\boldsymbol{\\beta}}_{-}, l \\in \\mathcal{H}^{\\infty}$. To conclude the proof we need to show that $g$ has $k$ poles inside the unit disc, and that therefore it has rank $k$. We do this by studying the modulus of the eigenvalues of $\\widehat{\\mathbf{A}}_{+}$."
    },
    {
      "markdown": "Since $E$ is minimal, $\\widehat{A}$ is reachable by definition, so we can use Theorem 19 and solve the problem by directly examining the eigenvalues of $-\\boldsymbol{\\Sigma} \\mathbf{R}$. From the proof of Theorem 18 we have $-\\boldsymbol{\\Sigma} \\mathbf{R}=\\boldsymbol{\\Sigma}\\left(\\boldsymbol{\\Sigma}^{2}-\\sigma_{k}^{2} \\mathbf{1}\\right)$, where $\\boldsymbol{\\Sigma}$ is the diagonal matrix having as elements the singular numbers of $H$ different from $\\sigma_{k}$. It follows that $-\\boldsymbol{\\Sigma} \\mathbf{R}$ has only $k$ strictly positive eigenvalues, and $\\widehat{\\mathbf{A}}$ has $k$ eigenvalues with modulus smaller than 1 . Thus, $\\widehat{\\mathbf{A}}_{+}$has $k$ eigenvalues, corresponding to the poles of $g$.\n\n# 4.3.4 Solving the Approximation Problem \n\nNow that we have found the rational function $g$, a symbol for the operator that solves Theorem 12 , we need to find the parameters of $\\widehat{A}_{k}$, the WFA corresponding to the optimal approximation. These are directly revealed by the expression of $g$, due to the function's parametrization.\nTheorem 21. Let $A=\\langle\\boldsymbol{\\alpha}, \\mathbf{A}, \\boldsymbol{\\beta}\\rangle$ be a minimal WFA with $n$ states over a one-letter alphabet. Let $A$ be in its SVA form. The optimal spectral-norm approximation of rank $k$ is given by the WFA $\\widehat{A}_{k}=\\left\\langle\\widehat{\\boldsymbol{\\alpha}}_{+}, \\widehat{\\mathbf{A}}_{+}, \\widehat{\\boldsymbol{\\beta}}_{+}\\right\\rangle$.\nProof. From Corollary 15 we know that $g$ is the rational function associated with the Hankel matrix of the best approximation. Given the correspondence between the Fourier coefficients of $g$ and the entries of the matrix, we have:\n\n$$\ng=\\widehat{\\boldsymbol{\\alpha}}_{+}^{\\top}\\left(z \\mathbf{1}-\\widehat{\\mathbf{A}}_{+}\\right)^{-1} \\widehat{\\boldsymbol{\\beta}}_{+}=\\sum_{k \\geq 0} \\widehat{\\boldsymbol{\\alpha}}_{+}^{\\top} \\widehat{\\mathbf{A}}_{+}^{k} \\widehat{\\boldsymbol{\\beta}}_{+} z^{-k-1}=\\sum_{k \\geq 0} \\bar{f}(k) z^{-k-1}\n$$\n\nwhere $\\bar{f}: \\Sigma^{*} \\rightarrow \\mathbb{R}$ is the function computed by $\\widehat{A}_{k}$ and $\\widehat{\\boldsymbol{\\alpha}}_{+}, \\widehat{\\mathbf{A}}_{+}, \\widehat{\\boldsymbol{\\beta}}_{+}$are the parameters.\n\n### 4.4 Error Analysis\n\nThanks to the use of AAK theory, the method outlined in the previous sections is guaranteed to return the rank $k$ optimal spectral-norm approximation of a WFA satisfying our assumptions, and the singular number $\\sigma_{k}$ provides the error. As noticed before, since the Hankel matrix has finite rank and we can derive the Gramian matrices of the WFA, the singular number corresponding to the error can be computed precisely, even though the Hankel matrix is infinite.\n\nSimilarly to the case of SVA truncation (Balle et al. 2019), owing to the ordering of the singular numbers, the error decreases when $k$ increases, meaning that allowing $\\widehat{A}_{k}$ to have more states guarantees a better approximation of $A$. Note that the solution we propose is optimal in the spectral norm, but it might not be the case in other norms. Nonetheless, we have the following bound between $\\ell^{2}$ norm and spectral norm.\nTheorem 22. Let $A$ be a minimal WFA computing $f: \\Sigma^{*} \\rightarrow \\mathbb{R}$, with matrix $\\mathbf{H}$. Let $\\widehat{A}_{k}$ be its optimal spectral-norm approximation, computing $g: \\Sigma^{*} \\rightarrow \\mathbb{R}$, with matrix $\\mathbf{G}$. Then:\n\n$$\n\\|f-g\\|_{\\ell^{2}} \\leq\\|\\mathbf{H}-\\mathbf{G}\\|=\\sigma_{k}\n$$\n\nProof. Let $\\mathbf{e}_{0}=\\left(\\begin{array}{lll}1 & 0 & \\cdots\\end{array}\\right)^{\\top}, f: \\Sigma^{*} \\rightarrow \\mathbb{R}, g: \\Sigma^{*} \\rightarrow \\mathbb{R}$ with Hankel matrices $\\mathbf{H}$ and $\\mathbf{G}$, respectively. We have:\n\n$$\n\\begin{aligned}\n\\|f-g\\|_{\\ell^{2}} & =\\left(\\sum_{n=0}^{\\infty}\\left|f_{n}-g_{n}\\right|^{2}\\right)^{1 / 2} \\\\\n& =\\left\\|(\\mathbf{H}-\\mathbf{G}) \\mathbf{e}_{0}\\right\\|_{\\ell^{2}} \\\\\n& \\leq \\sup _{\\|\\mathbf{x}\\|_{\\ell^{2}}=1}\\|(\\mathbf{H}-\\mathbf{G}) \\mathbf{x}\\|_{\\ell^{2}} \\\\\n& =\\|\\mathbf{H}-\\mathbf{G}\\|=\\sigma_{k}\n\\end{aligned}\n$$"
    },
    {
      "markdown": "```\nAlgorithm 1: AAKapproximation\n    input : A minimal WFA \\(A\\), with \\(\\boldsymbol{\\alpha}_{2} \\neq 0, n\\) states and in SVA form,\n        its Gramian \\(\\mathbf{P}\\), a target number of states \\(k<n\\)\n    output: A WFA \\(\\widehat{A}_{k}\\) with \\(k\\) states\n    1 Let \\(\\boldsymbol{\\alpha}_{1}, \\boldsymbol{\\alpha}_{2}, \\boldsymbol{\\beta}_{1}, \\boldsymbol{\\beta}_{2}, \\mathbf{A}_{11}, \\mathbf{A}_{12}, \\mathbf{A}_{22}, \\boldsymbol{\\Sigma}\\) be the blocks defined in Eq. 17\n    2 Let \\(\\left(\\boldsymbol{\\alpha}_{2}^{\\top}\\right)^{+}=\\frac{\\boldsymbol{\\alpha}_{2}}{\\boldsymbol{\\alpha}_{2}^{\\top} \\boldsymbol{\\alpha}_{2}},\\left(\\boldsymbol{\\beta}_{2}^{\\top}\\right)^{+}=\\frac{\\boldsymbol{\\beta}_{2}}{\\boldsymbol{\\beta}_{2}^{\\top} \\boldsymbol{\\beta}_{2}}\\)\n    3 Let \\(\\mathbf{R}=\\sigma_{k}^{2} \\mathbf{1}-\\boldsymbol{\\Sigma}^{2}\\)\n    4 Let \\(\\widehat{\\mathbf{A}}=\\left(\\mathbf{A}_{11}^{\\top}-\\mathbf{A}_{21}^{\\top}\\left(\\boldsymbol{\\beta}_{2}^{\\top}\\right)^{+} \\boldsymbol{\\beta}_{1}^{\\top}\\right)^{-1}\\)\n    5 Let \\(\\widehat{\\boldsymbol{\\alpha}}=\\widehat{\\mathbf{A}}^{\\top} \\mathbf{R} \\mathbf{A}_{12}\\left(\\boldsymbol{\\alpha}_{2}^{\\top}\\right)^{+}\\)\n    6 Let \\(\\widehat{\\boldsymbol{\\beta}}=-\\widehat{\\mathbf{A}} \\mathbf{A}_{21}^{\\top}\\left(\\boldsymbol{\\beta}_{2}^{\\top}\\right)^{+}\\)\n    7 Let \\(\\widehat{A}=\\langle\\widehat{\\boldsymbol{\\alpha}}, \\widehat{\\mathbf{A}}, \\widehat{\\boldsymbol{\\beta}}\\rangle\\)\n    8 Let \\(\\widehat{A}_{k} \\leftarrow \\operatorname{BlockDiagonalize}(\\widehat{A})\\)\n    9 return \\(\\widehat{A}_{k}\\)\n```\n\nwhere the second equation follows by definition and by observing that matrix difference is computed entry-wise.\n\n# 5 Algorithm \n\nWe now use the results obtained in the previous sections to define Algorithm 1, that we call AAKapproximation.\n\nThe algorithm takes as input a target number of states $k<n$, a minimal irredundant WFA $A n$ states and in SVA form, and its Gramian $\\mathbf{P}$. We assume $\\boldsymbol{\\alpha}_{2} \\neq 0$. If $\\boldsymbol{\\alpha}_{2}=0$, it is enough to substitute the Steps 4, 5, 6 with the analogues from Equation 20. As mentioned in Section 4.1, the constraints on the WFA $A$ to be minimal and in SVA form are not essential. In fact a WFA with $n$ states can be minimized in time $O\\left(n^{3}\\right)$ (Berstel \\& Reutenauer 2011), and the SVA computed in $O\\left(n^{3}\\right)$ (Balle et al. 2019). The algorithm applies the results of Theorem 18 in order to derive the parameters of the optimal WFA. The output of the algorithm is the WFA $\\widehat{A}_{k}$ corresponding to the unique optimal spectral-norm approximation of $A$.\n\nBlock Diagonalization The algorithm involves a call to Algorithm 2, BlockDiagonalize. This algorithm corresponds to the steps necessary to derive the WFA $\\widehat{A}_{k}$ associated to the rational function $g$. One way to solve the problem is to compute the Jordan form of the matrix. Unfortunately, this problem is ill-conditioned, so it is not suitable for our algorithmic purposes. Following an idea of Glover (1984), we compute the Schur decomposition, i.e. we find an orthogonal matrix $\\mathbf{U}$ such that the matrix $\\mathbf{U}^{\\top} \\widehat{\\mathbf{A}} \\mathbf{U}$ is upper triangular, with the eigenvalues of $\\widehat{\\mathbf{A}}$ on the diagonal. We obtain:\n\n$$\n\\mathbf{T}=\\mathbf{U}^{\\top} \\widehat{\\mathbf{A}} \\mathbf{U}=\\left(\\begin{array}{cc}\n\\widehat{\\mathbf{A}}_{+} & \\widehat{\\mathbf{A}}_{12} \\\\\n\\mathbf{0} & \\widehat{\\mathbf{A}}_{-}\n\\end{array}\\right)\n$$\n\nwhere the eigenvalues are arranged in increasing order of modulus, and the modulus of those in $\\widehat{\\mathbf{A}}_{+}$(resp. $\\widehat{\\mathbf{A}}_{-}$) is smaller (resp. greater) than one. To transform this upper triangular matrix into a block-diagonal one, we use the following result."
    },
    {
      "markdown": "```\n    input : A WFA \\(\\widehat{A}\\)\n    output: A WFA \\(\\widehat{A}_{k}\\) wit \\(\\rho<1\\)\n    if \\(\\operatorname{dim} \\widehat{\\mathbf{A}}=k\\) then\n        return \\(\\widehat{A}_{k}\\)\n    else\n        Compute the Schur decomposition of \\(\\widehat{\\mathbf{A}}=\\mathbf{U T U}^{\\top}\\), where \\(\\left|T_{11}\\right| \\leq\\left|T_{22}\\right| \\leq \\ldots\\)\n        Solve \\(\\widehat{\\mathbf{A}}_{11} \\mathbf{X}-\\mathbf{X} \\widehat{\\mathbf{A}}_{22}+\\widehat{\\mathbf{A}}_{12}=\\mathbf{0}\\) for \\(\\mathbf{X}\\)\n        Let \\(\\mathbf{M}=\\left(\\begin{array}{cc}1 & \\mathbf{X} \\\\ 0 & 1\\end{array}\\right)\\) and \\(\\mathbf{M}^{-1}=\\left(\\begin{array}{cc}1 & -\\mathbf{X} \\\\ 0 & 1\\end{array}\\right)\\)\n        Let \\(\\boldsymbol{\\Gamma}=\\left(\\begin{array}{ll}\n1 & 0\n\\end{array}\\right)\\)\n        Let \\(\\widehat{\\mathbf{A}}_{+}=\\boldsymbol{\\Gamma} \\mathbf{M}^{-1} \\mathbf{U}^{\\top} \\widehat{\\mathbf{A}} \\mathbf{U} \\mathbf{M} \\boldsymbol{\\Gamma}^{\\top}\\)\n        Let \\(\\widehat{\\boldsymbol{\\alpha}}_{+}=\\boldsymbol{\\Gamma} \\mathbf{M}^{\\top} \\mathbf{U}^{\\top} \\widehat{\\boldsymbol{\\alpha}}\\)\n        Let \\(\\widehat{\\boldsymbol{\\beta}}_{+}=\\boldsymbol{\\Gamma} \\mathbf{M}^{-1} \\mathbf{U}^{\\top} \\widehat{\\boldsymbol{\\beta}}\\)\n        Let \\(\\widehat{A}_{k}=\\left\\langle\\widehat{\\boldsymbol{\\alpha}}_{+}, \\widehat{\\mathbf{A}}_{+}, \\widehat{\\boldsymbol{\\beta}}_{+}\\right\\rangle\\)\n        return \\(\\widehat{A}_{k}\\)\n```\n\nTheorem 23 (Roth (1952)). Let $\\mathbf{T}$ be the matrix defined in Equation 31. The matrix $\\mathbf{X}$ is a solution of the equation $\\widehat{\\mathbf{A}}_{+} \\mathbf{X}-\\mathbf{X} \\widehat{\\mathbf{A}}_{-}+\\widehat{\\mathbf{A}}_{12}=\\mathbf{0}$ if and only if the matrices\n\n$$\n\\mathbf{M}=\\left(\\begin{array}{ll}\n\\mathbf{1} & \\mathbf{X} \\\\\n\\mathbf{0} & \\mathbf{1}\n\\end{array}\\right), \\quad \\text { and } \\quad \\mathbf{M}^{-1}=\\left(\\begin{array}{cc}\n\\mathbf{1} & -\\mathbf{X} \\\\\n\\mathbf{0} & \\mathbf{1}\n\\end{array}\\right)\n$$\n\nsatisfy:\n\n$$\n\\mathbf{M}^{-1} \\mathbf{T} \\mathbf{M}=\\left(\\begin{array}{cc}\n\\widehat{\\mathbf{A}}_{+} & \\mathbf{0} \\\\\n\\mathbf{0} & \\widehat{\\mathbf{A}}_{-}\n\\end{array}\\right)\n$$\n\nwhere $\\mathbf{T}$ is the matrix defined in Equation 31.\nSetting $\\boldsymbol{\\Gamma}=\\left(\\begin{array}{ll}\\mathbf{1}_{k} & \\mathbf{0}\\end{array}\\right)$ we can now derive the rational component of the WFA:\n\n$$\n\\begin{aligned}\n& \\widehat{\\mathbf{A}}_{+}=\\boldsymbol{\\Gamma} \\mathbf{M}^{-1} \\mathbf{U}^{\\top} \\widehat{\\mathbf{A}} \\mathbf{U} \\mathbf{M} \\boldsymbol{\\Gamma}^{\\top} \\\\\n& \\widehat{\\boldsymbol{\\alpha}}_{+}=\\boldsymbol{\\Gamma} \\mathbf{M}^{\\top} \\mathbf{U}^{\\top} \\widehat{\\boldsymbol{\\alpha}} \\\\\n& \\widehat{\\boldsymbol{\\beta}}_{+}=\\boldsymbol{\\Gamma} \\mathbf{M}^{-1} \\mathbf{U}^{\\top} \\widehat{\\boldsymbol{\\beta}}\n\\end{aligned}\n$$\n\nThe algorithm BlockDiagonalize corresponds to the implementation of this procedure, and Step 2 can be performed using the Bartels-Stewart algorithm (Bartels \\& Stewart 1972).\n\n# 5.1 Computational Cost \n\nThe running time of BlockDiagonalize with input a WFA $\\widehat{A}$ with $(n-r)$ states is thus in $O\\left((n-r)^{3}\\right)$, where $r$ is the multiplicity of the singular value considered. The running time of AAKapproximation for an input WFA $\\widehat{A}$ with $n$ states is in $O\\left((n-r)^{3}\\right)$. In particular, it is possible to analyze the cost associated to each step of the algorithms (Trefethen \\& Bau III 1997):"
    },
    {
      "markdown": "- The product of two $n \\times n$ matrices can be computed in time $O\\left(n^{3}\\right)$ using a standard iterative algorithm.\n- The inversion of a $n \\times n$ matrix can be computed in time $O\\left(n^{3}\\right)$ using Gauss-Jordan elimination.\n- The computation of the Schur decomposition of a $n \\times n$ matrix can be done with a two-step algorithm, where each step takes $O\\left(n^{3}\\right)$, using the Hessenberg form of the matrix.\n- The Bartels-Stewart algorithm applied to upper triangular matrices to find a matrix of size $m \\times n$ takes $O\\left(m n^{2}+n m^{2}\\right)$.\n\n\n# 6 Example \n\nWe consider the following weighted finite automaton with three states over a one-letter alphabet, represented in SVA form:\n\n$$\n\\mathbf{A}=\\left(\\begin{array}{ccc}\n0.579 & 0.461 & 0.046 \\\\\n-0.461 & -0.192 & 0.225 \\\\\n0.046 & -0.225 & -0.387\n\\end{array}\\right), \\quad \\boldsymbol{\\alpha}=\\left(\\begin{array}{c}\n1.650 \\\\\n-0.851 \\\\\n0.038\n\\end{array}\\right), \\quad \\boldsymbol{\\beta}=\\left(\\begin{array}{l}\n1.650 \\\\\n0.851 \\\\\n0.038\n\\end{array}\\right)\n$$\n\nThe objective is to find the WFA with two states solving the approximate minimization problem optimally.\n\nWe first note that $\\mathbf{A}$ has spectral radius strictly smaller than 1 , having eigenvalues:\n\n$$\n\\lambda_{1,2}=0.0162324 \\pm 0.0297233 i \\quad \\lambda_{3}=0.0324648\n$$\n\nTherefore, the assumptions listed Section 4.1 are satisfied, and we can apply Theorem 18. We compute the gramian matrices and obtain, according to the partition in Equation 17, the following matrix:\n\n$$\n\\mathbf{P}=\\mathbf{Q}=\\left(\\begin{array}{ccc}\n4.67 & 0 & 0 \\\\\n0 & 1.79 & 0 \\\\\n0 & 0 & 0.12\n\\end{array}\\right)\n$$\n\nso that $\\sigma_{2}^{2}=0.12$ and:\n\n$$\n\\boldsymbol{\\Sigma}=\\left(\\begin{array}{cc}\n4.67 & 0 \\\\\n0 & 1.79\n\\end{array}\\right)\n$$\n\nWe then proceed by partitioning $\\mathbf{A}, \\boldsymbol{\\alpha}$ and $\\boldsymbol{\\beta}$ and obtain:\n\n$$\n\\begin{aligned}\n& \\mathbf{A}_{11}=\\left(\\begin{array}{cc}\n0.579 & 0.461 \\\\\n-0.461 & -0.192\n\\end{array}\\right), \\quad \\mathbf{A}_{1,2}=\\binom{0.046}{0.225}, \\quad \\mathbf{A}_{2,1}^{\\top}=\\binom{0.046}{-0.225}, \\quad \\mathbf{A}_{22}=-0.387 \\\\\n& \\boldsymbol{\\alpha}_{1}=\\binom{1.650}{-0.851}, \\quad \\boldsymbol{\\beta}_{1}=\\binom{1.650}{0.851}, \\quad \\boldsymbol{\\alpha}_{2}=\\boldsymbol{\\beta}_{2}=0.038\n\\end{aligned}\n$$\n\nSince $\\boldsymbol{\\alpha}_{2} \\neq 0$, we can use Equation 19 to find the coefficients of the auxiliary WFA $\\widehat{A}=$ $\\langle\\widehat{\\boldsymbol{\\alpha}}, \\widehat{\\mathbf{A}}, \\widehat{\\boldsymbol{\\beta}}\\rangle$."
    },
    {
      "markdown": "We have:\n\n$$\n\\left\\{\\begin{array}{l}\n\\widehat{\\boldsymbol{\\beta}}=-\\widehat{\\mathbf{A}} \\mathbf{A}_{21}^{\\top}\\left(\\boldsymbol{\\beta}_{2}^{\\top}\\right)^{+} \\\\\n\\widehat{\\boldsymbol{\\alpha}}=\\widehat{\\mathbf{A}}^{\\top} \\mathbf{R} \\mathbf{A}_{12}\\left(\\boldsymbol{\\alpha}_{2}^{\\top}\\right)^{+} \\\\\n\\widehat{\\mathbf{A}}\\left(\\mathbf{A}_{11}^{\\top}-\\mathbf{A}_{21}^{\\top}\\left(\\boldsymbol{\\beta}_{2}^{\\top}\\right)^{+} \\boldsymbol{\\beta}_{1}^{\\top}\\right)=\\mathbf{1} \\\\\n\\left\\{\\begin{array}{l}\n\\widehat{\\boldsymbol{\\beta}}=-\\widehat{\\mathbf{A}}\\binom{0.046}{-0.225}(0.038)^{-1} \\\\\n\\widehat{\\boldsymbol{\\alpha}}=\\widehat{\\mathbf{A}}^{\\top}\\left(\\left(\\begin{array}{cc}\n0.12 & 0 \\\\\n0 & 0.12\n\\end{array}\\right)-\\left(\\begin{array}{cc}\n4.67 & 0 \\\\\n0 & 1.79\n\\end{array}\\right)^{2}\\right)\\binom{0.046}{0.225}(0.038)^{-1} \\\\\n\\widehat{\\mathbf{A}}\\left(\\left(\\begin{array}{cc}\n0.579 & 0.461 \\\\\n-0.461 & -0.192\n\\end{array}\\right)^{\\top}-\\binom{0.046}{-0.225}(0.038)^{-1}\\binom{1.650}{0.851}^{\\top}\\right)=\\mathbf{1}\n\\end{array}\\right.\n$$\n\nso we get:\n\n$$\n\\widehat{\\mathbf{A}}=\\left(\\begin{array}{cc}\n0.578 & 0.178 \\\\\n-1.221 & -0.169\n\\end{array}\\right), \\quad \\widehat{\\boldsymbol{\\alpha}}=\\binom{7.105}{-1.579}, \\quad \\widehat{\\boldsymbol{\\beta}}=\\binom{0.353}{0.474}\n$$\n\nNow, we want to extract the rational component in order to find the optimal approximation. To do so, we block-diagonalize the transition matrix $\\widehat{\\mathbf{A}}$ and look at the modulus of its eigenvalues. We have:\n\n$$\n\\lambda_{1,2}=0.204593 \\pm 0.278322 i\n$$\n\nAs we can see, both eigenvalues have modulus smaller than one. This means that the WFA $\\widehat{A}$ is exactly the optimal approximation of size two that we are looking for, and there aren't any components that need to be discarded. Following the notation introduced in the previous section, we have: $\\widehat{A}_{k}=\\left\\langle\\widehat{\\boldsymbol{\\alpha}}_{+}, \\widehat{\\mathbf{A}}_{+}, \\widehat{\\boldsymbol{\\beta}}_{+}\\right\\rangle=\\langle\\widehat{\\boldsymbol{\\alpha}}, \\widehat{\\mathbf{A}}, \\widehat{\\boldsymbol{\\beta}}\\rangle$.\n\n# 7 Related Work \n\nThe problem of minimizing automata has been an important subject of research since the 1950s. There is a remarkable algorithm due to Brzozowski (Brzozowski 1962, 1964) that reduces a DFA to a minimal one. However, its worst-case running time is exponential in the number of states. Despite this shortcoming, this algorithm has seen a resurgence recently, mainly because it can be generalized to new models, such as weighted automata (Droste et al. 2009). This line of algorithms is based on a new understanding of Brzozowski's algorithm from the point of view of duality (Bonchi, Bonsangue, Rutten \\& Silva 2012, Bonchi, Bonsangue, Boreale, Rutten \\& Silva 2012, Bonchi et al. 2014, Bezhanishvili et al. 2012) and extend readily to other settings. In the context of quantitative systems, like weighted or probabilistic automata, it becomes meaningful to investigate the approximate minimization problem. The study of this problem and of its applications are fairly recent, and only a few works have been published on the subject. A problem analogous to approximate minimization is addressed by Kulesza, Jiang, and Singh for the spectral algorithm. The authors provide a bound on the loss of the learned low-rank model in terms of the singular values that are discarded during training (Kulesza et al. 2015). In a previous work, the same group of authors connected spectral learning to the approximation problem of a small class of Hidden Markov models, bounding the error in terms of the total variation distance (Kulesza et al. 2014). Still in the context of Hidden Markov models, Kotsalis and Shamma provide bounds for the model reduction problem using"
    },
    {
      "markdown": "the spectral norm as a measure of the error (Kotsalis \\& Shamma 2015). We remark that the framework of Hidden Markov models is encompassed by weighted automata (Denis \\& Esposito 2008). Balle, Panangaden, and Precup are the first authors to formalize the approximate minimization problem for WFAs (Balle et al. 2015, 2019). The technique presented in their paper relies on the construction (and truncation) of the singular value automaton, a canonical expression for WFAs arising from the singular value decomposition of the corresponding Hankel matrix. Their method can be viewed as a generalization to multi-letter alphabets of the balanced realization approach from control theory (Antoulas 2005). The authors conclude their analysis by providing bounds on the approximation error in the $\\ell^{2}$ norm. The result is supported by strong theoretical guarantees and applies to a large class of WFAs. This method has later been extended to the setting of weighted tree automata in Balle \\& Rabusseau (2020). The main limitation of these approaches based on SVA truncation is that the approximation obtained is not optimal in any norm. We partially address this point in this work, where we obtain an algorithm for the optimal approximation in the spectral norm for the same class of WFAs considered by Balle, Panangaden, and Precup, but restricted to a one-letter alphabet. Part of this results where presented in (Balle et al. 2021). In Lacroce et al. (2021) we extend this results to the more general setting of black-box models trained for language modelling over one-letter alphabets. In Lacroce et al. (2022), Lacroce (2022) we analyze the problem of extending the method presented in this paper to the case of multi-letter alphabets.\n\nThe control theory community has largely studied approximate minimization in the context of linear time-invariant systems (Antoulas 2005). A parallel with these results can be drawn by noting that the impulse response of a discrete Single-Input-Single-Output SISO system can be parametrized as a WFA over a one-letter alphabet. Glover (1984) presents a state-space solution for the case of continuous Multi-Input-Multi-Output MIMO systems. His method led to a widespread application of these results, thanks to its computational and theoretical simplicity. This stems from the structure of the continuous Lyapunov equations. For discrete systems, though, the quadratic nature of the Lyapunov equations does not allow for a simple closed form formula for the state space solution (Chui \\& Chen 1997). Thus, most of the results for the discrete case work with a suboptimal version of the problem (Ball \\& Ran 1987, AlHussari et al. 1993, Ionescu \\& Oara 2001). A solution for the SISO case can be found using a polynomial approach, but it does not provide an explicit representation of the state space nor it generalizes to the MIMO setting. The first to actually extend Glover results is Gu, who provides an elegant solution for the MIMO discrete problem (Gu 2005). Glover and Gu's solutions rely on building an all-pass system, equivalent to the WFA $E$ in our case. Part of our contribution is the adaptation of some of the control theory tools to WFAs.\n\n# 8 Extensions and Future Work \n\nIn this section we examine possible extensions of our method by relaxing some of the hypothesis.\n\n### 8.1 Removing the Finite-Rank Assumption\n\nThe proof of Theorem 12 is constructive for any compact Hankel operator. In the setting of this paper, compactness is guaranteed, as the operator corresponding to an irredundant WFA has finite rank and is bounded. While boundness is necessary for compactness, the finite-rank hypothesis is not. Therefore, an interesting extension of this work is to investigate other classes of models by relaxing the finite-rank (or finite state) assumption. An example of models corresponding to infinite-rank Hankel matrices are Recurrent Neural Networks (RNNs) (Hochreiter \\& Schmidhuber 1997). Recently, particular attention has been given to the problem of extract-"
    },
    {
      "markdown": "ing, from an RNN, a weighted finite automaton (Ayache et al. 2018, Rabusseau et al. 2019, Weiss et al. 2019, Okudono et al. 2020, Eyraud \\& Ayache 2020, Theertha Suresh et al. 2019, Zhang et al. 2021). In this sense, the knowledge distillation task (Hinton et al. 2015) is very similar to an approximate minimization problem, since WFAs are a less expensive alternative to RNNs, while still being expressive and suited for sequence modelling and prediction (Denis \\& Esposito 2008, Cortes et al. 2004). In Lacroce et al. (2021), we investigated the use of AAK theory on black-box models trained for language modelling on sequential data. In particular, we showed that compactness is automatically respected by black boxes for language modelling, and proposed an algorithm for the one-letter setting, based on AAK theory. This particular extension of the method presented in this paper constitutes a first fundamental step towards developing provable approximation algorithms for black box models.\n\n# 8.2 Removing the Spectral Radius Assumption \n\nOne could consider a WFA over a one-letter alphabet with $\\rho(\\mathbf{A}) \\neq 1$, i.e. not necessarily irredundant. In this case, the method proposed in the previous sections can be extended and the quality of the approximation can be estimated, but the result is not optimal in the spectral norm. Once again, we draw inspiration from the control theory literature, where some theoretical work has been done to study an analogous approach for continuous time systems and their approximation error (Glover 1984).\n\nThe key idea is to block-diagonalize $\\mathbf{A}$ like we did in Section 4.3.3. This way, we obtain two components, $\\mathbf{A}_{+}$and $\\mathbf{A}_{-}$, with the property that $\\rho<1$ and $\\rho>1$, respectively. We tackle each component separately. The case of $A_{+}=\\left\\langle\\boldsymbol{\\alpha}_{+}, \\mathbf{A}_{+}, \\boldsymbol{\\beta}_{+}\\right\\rangle$, the component having $\\rho(\\mathbf{A})<1$, can be dealt with in the way presented in the previous sections. This means that we can find an optimal spectral-norm approximation of the desired size for $A_{+}$. Then, we can consider the second component, $A_{-}=\\left\\langle\\boldsymbol{\\alpha}_{-}, \\mathbf{A}_{-}, \\boldsymbol{\\beta}_{-}\\right\\rangle$. In this case, we apply the transformation\n\n$$\nz^{j-1} \\mapsto z^{-j} \\quad \\text { for } j \\geq 1\n$$\n\nto the symbol $\\phi^{\\prime}(z)$ associated to $A_{-}$. Then, the function\n\n$$\n\\phi^{\\prime}\\left(z^{-1}\\right)=\\sum_{k \\geq 0} \\boldsymbol{\\alpha}_{-}^{\\top} \\mathbf{A}_{-}^{k} z^{k} \\boldsymbol{\\beta}_{-}=\\boldsymbol{\\alpha}_{-}^{\\top}\\left(\\mathbf{1}-z \\mathbf{A}_{-}\\right)^{-1} \\boldsymbol{\\beta}_{-}\n$$\n\nis well defined, as the series converges for $z$ with small enough modulus. The use of this transformation allows us to obtain a function having poles only inside the unit disc, and to apply the method presented in this chapter. We remark that in this case an important choice to make is the size of the target approximation of $A_{-}$, as it can influence the quality of the result. Analyzing the effects of this parameter on the approximation error is an interesting direction for future work, both on the theoretical and experimental side.\n\n### 8.3 Removing the One-Letter Assumption\n\nThe most pressing direction for future work is undoubtedly to extend our results to a multiletter setting. The work of Adamyan, Arov and Krein provides us with a powerful theory connecting sequences to the study of complex functions. Unfortunately, this approach cannot be directly generalized to the multi-letter case, when $\\Sigma^{*}$ is a noncommutative monoid, as it requires to generalize standard harmonic analysis results to the non-abelian case. A recent line of work in multivariable operator theory has been centered around extending results of standard operator theory to the case of noncommutative operators defined on Fock spaces Frazho (1982), Bunce (1984), Arias \\& Popescu (1995), Popescu (1989a,b, 1992, 1993, 1995, 2003, 2006, 2010,"
    },
    {
      "markdown": "2013), Ball \\& Bolotnikov (2021), Jury et al. (2021). In particular, a noncommutative definition of Hankel operator, and a noncommutative version of the AAK theorem are presented in a recent work of Popescu Popescu (2003), but its proof is not constructive. Therefore, solving the approximate minimization problem for multi-letter alphabets using AAK theory comes with two distinct challenges:\n\n- Finding a noncommutative Hankel operator: given a WFA and its Hankel matrix, we need to find a way to reformulate the approximation problem using multivariable operators. In particular, we need to find a noncommutative analogue of the Hardy space and of the symbol.\n- Making AAK constructive: the proof of the noncommutative version of the AAK theorem does not provide us with an expression for the optimal approximation. An interesting direction would be to explore ways to extend the proof to a constructive one.\n\nIn Lacroce et al. (2022), we proposed a framework to associate a noncommutative Hankel operator (defined on an noncommutative version of the Hardy space) and a noncommutative rational function to the Hankel matrix computed by a model on sequential data, solving the first point listed above. In the one-letter case, obtaining the framework allowed us to reformulate the approximation problem in terms of functional analysis, and to solve it using the constructive proof of AAK theorem. In Lacroce (2022), we tried to address the question of whether or not the proof of the noncommutative AAK theorem can be made constructive. While we did not manage to provide a definitive answer, we laid out possible approaches that can be used to tackle the problem of making the proof of the noncommutative version of AAK theorem constructive.\n\n# 9 Conclusion \n\nIn this paper we applied the AAK theory for Hankel operators and complex functions to the framework of WFAs in order to construct the optimal approximation to an automaton given a bound on the size. We propose an algorithm to find the parameters of the best WFA approximation in the spectral norm, and derive bounds on the error. Our method applies to real irredundant WFAs defined over a one-letter alphabet. These alphabets have proven to be of independent interest when dealing with automata, as in this case the classes of regular and context-free languages collapse (Pighizzini 2015).\n\nWe think the spectral norm has desirable characteristics, making it a solid candidate for the approximate minimization task. For example, it can be minimized in polynomial time and a global minimum for the error can be computed accurately. Moreover, the fact that this norm is independent on the specific architecture or model considered, facilitate future applications of this method, as it can be used to compare different classes of models. Nonetheless, a limitation of this work is that we do not have a clear picture of how effective it is to use the spectral norm to evaluate the approximation of WFAs and black boxes. Concretely, we do not know how the spectral norm performs with respect to behavioral metrics, or other metrics coming from natural language processing (e.g., word error rate and normalized discounted cumulative gain). To some extent, this problem is a collateral effect of the size of the alphabet: the comparison between spectral norm and other kind of norms is possible only in the multi-letter setting. Obtaining algorithms for the multi-letter case will thus open the possibility of evaluating the quality of the spectral norm.\n\nWhile the one-letter setting is certainly restricted, we believe that this work constitutes a first fundamental step in the direction of optimal approximation. Furthermore, the use of AAK techniques has proven to be very fruitful in related areas like control theory; we think"
    },
    {
      "markdown": "that automata theory can also benefit from it. The use of such methods can help deepen the understanding of the behaviour of rational functions. This paper highlights and strengthens the interesting connections between functional analysis, automata theory and control theory, unifying tools from different domains in one formalism.\n\n# Acknowledgments \n\nThis research has been supported by NSERC Canada (C. Lacroce, P. Panangaden) and Canada CIFAR AI chairs program (G. Rabusseau). The authors would like to thank Tianyu Li, Harsh Satija and Alessandro Sordoni for feedback on earlier drafts of this work, Gheorghe Comanici and Robert Robere for a detailed review, Florence Clerc for help with the submission, and Maxime Wabartha for fruitful discussions and comments on proofs."
    },
    {
      "markdown": "# References \n\nAdamyan, V. M., Arov, D. Z. \\& Krein, M. G. (1971), 'Analytic Properties of Schmidt Pairs for a Hankel Operator and the Generalized Schur-Takagi problem', Mathematics of The Ussr sbornik 15, 31-73.\n\nAl-Hussari, M., Jaimoukha, I. \\& Limebeer, D. (1993), A Descriptor Approach for the Solution of the One-Block Distance Problem, in 'In Proceedings of the IFAC World Congress'.\n\nAntoulas, A. C. (2005), Approximation of Large-Scale Dynamical Systems, SIAM.\nArias, A. \\& Popescu, G. (1995), 'Factorization and Reflexivity on Fock Spaces', Integral equations and operator theory 23(3), 268-286.\n\nAyache, S., Eyraud, R. \\& Goudian, N. (2018), Explaining Black Boxes on Sequential Data Using Weighted Automata, in 'Proceedings of the 14th International Conference on Grammatical Inference, ICGI 2018, Wrocław, Poland, September 5-7, 2018', Vol. 93 of Proceedings of Machine Learning Research, PMLR, pp. 81-103.\nURL: http://proceedings.mlr.press/v93/ayache19a.html\nBailly, R., Denis, F. \\& Ralaivola, L. (2009), Grammatical inference as a principal component analysis problem, in 'Proceedings of the 26th Annual International Conference on Machine Learning', ICML '09, Association for Computing Machinery, New York, NY, USA, pp. 33-$-40$.\nURL: https://doi.org/10.1145/1553374.1553379\nBall, J. A. \\& Bolotnikov, V. (2021), Noncommutative Function-Theoretic Operator Theory and Applications, Cambridge Tracts in Mathematics, Cambridge University Press.\n\nBall, J. A. \\& Ran, A. C. (1987), 'Optimal Hankel norm model reductions and Wiener-Hopf factorization I: The canonical case', SIAM Journal on Control and Optimization 25(2), 362382 .\n\nBalle, B., Carreras, X., Luque, F. M. \\& Quattoni, A. (2014), 'Spectral Learning of Weighted Automata - A Forward-Backward Perspective', Mach. Learn. 96(1-2), 33-63.\nURL: https://doi.org/10.1007/s10994-013-5416-x\nBalle, B., Gourdeau, P. \\& Panangaden, P. (2017), Bisimulation Metrics and Norms for Weighted Finite Automata, in I. Chatzigiannakis, P. Indyk, F. Kuhn \\& A. Muscholl, eds, '44th International Colloquium on Automata, Languages, and Programming, ICALP 2017, July 10-14, 2017, Warsaw, Poland', Vol. 80 of LIPIcs, Schloss Dagstuhl - Leibniz-Zentrum für Informatik, pp. 103:1-103:14.\nURL: https://doi.org/10.4230/LIPIcs.ICALP.2017.103\nBalle, B., Gourdeau, P. \\& Panangaden, P. (2022), 'Bisimulation metrics and norms for realweighted automata', Inf. Comput. 282, 104649.\nURL: https://doi.org/10.1016/j.ic.2020.104649\nBalle, B., Hamilton, W. L. \\& Pineau, J. (2014), Methods of Moments for Learning Stochastic Languages: Unified Presentation and Empirical Comparison, in 'Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014', Vol. 32 of JMLR Workshop and Conference Proceedings, JMLR.org, pp. 1386-1394.\nURL: http://proceedings.mlr.press/v32/balle14.html"
    },
    {
      "markdown": "Balle, B., Lacroce, C., Panangaden, P., Precup, D. \\& Rabusseau, G. (2021), Optimal SpectralNorm Approximate Minimization of Weighted Finite Automata, in N. Bansal, E. Merelli \\& J. Worrell, eds, '48th International Colloquium on Automata, Languages, and Programming, ICALP 2021, July 12-16, 2021, Glasgow, Scotland (Virtual Conference)', Vol. 198 of LIPIcs, Schloss Dagstuhl - Leibniz-Zentrum für Informatik, pp. 118:1-118:20.\nURL: https://doi.org/10.4230/LIPIcs.ICALP.2021.118\nBalle, B., Panangaden, P. \\& Precup, D. (2015), A Canonical Form for Weighted Automata and Applications to Approximate Minimization, in '30th Annual ACM/IEEE Symposium on Logic in Computer Science, LICS 2015, Kyoto, Japan, July 6-10, 2015', IEEE Computer Society, pp. 701-712.\nURL: https://doi.org/10.1109/LICS.2015.70\nBalle, B., Panangaden, P. \\& Precup, D. (2019), 'Singular Value Automata and Approximate Minimization', Math. Struct. Comput. Sci. 29(9), 1444-1478.\nURL: https://doi.org/10.1017/S0960129519000094\nBalle, B. \\& Rabusseau, G. (2020), 'Approximate Minimization of Weighted Tree Automata', Information and Computation p. 104654.\nURL: https://www.sciencedirect.com/science/article/pii/S0890540120301425\nBartels, R. H. \\& Stewart, G. W. (1972), 'Solution of the matrix equation ax $+\\mathrm{xb}=\\mathrm{c}[\\mathrm{f} 4]$ ', Commun. ACM 15(9), 820-826.\n\nBerstel, J. \\& Reutenauer, C. (2011), Noncommutative rational series with applications, Vol. 137, Cambridge University Press.\n\nBezhanishvili, N., Kupke, C. \\& Panangaden, P. (2012), Minimization via duality, in 'Logic, Language, Information and Computation - 19th International Workshop, WoLLIC 2012, Buenos Aires, Argentina, September 3-6, 2012. Proceedings', Vol. 7456 of Lecture Notes in Computer Science, Springer, pp. 191-205.\n\nBonchi, F., Bonsangue, M., Boreale, M., Rutten, J. \\& Silva, A. (2012), 'A coalgebraic perspective on linear weighted automata', Information and Computation 211, 77-105.\n\nBonchi, F., Bonsangue, M. M., Hansen, H. H., Panangaden, P., Rutten, J. \\& Silva, A. (2014), 'Algebra-coalgebra duality in Brzozowski's minimization algorithm', ACM Transactions on Computational Logic .\n\nBonchi, F., Bonsangue, M., Rutten, J. \\& Silva, A. (2012), Brzozowski's algorithm (co)algebraically, in R. Constable \\& A. Silva, eds, 'Logics and Program Semantics: Essays Dedicated to Dexter Kozen', Vol. 7230 of Lecture Notes In Computer Science, Springer-Verlag, pp. 12-23.\n\nBrzozowski, J. A. (1962), Canonical regular expressions and minimal state graphs for definite events, in J. Fox, ed., 'Proceedings of the Symposium on Mathematical Theory of Automata', number 12 in 'MRI Symposia Series', Polytechnic Press of the Polytechnic Institute of Brooklyn, pp. 529-561. Book appeared in 1963.\n\nBrzozowski, J. A. (1964), 'Derivatives of regular expressions', J. ACM 11(4), 481-494.\nBunce, J. W. (1984), 'Models for n-tuples of noncommuting operators', Journal of Functional Analysis 57(1), 21-30.\nURL: https://www.sciencedirect.com/science/article/pii/0022123684900983"
    },
    {
      "markdown": "Carlyle, J. \\& Paz, A. (1971), 'Realizations by stochastic finite automata', Journal of Computer and System Sciences 5(1), 26-40.\n\nChui, C. K. \\& Chen, G. (1997), Discrete $H^{\\infty}$ Optimization With Applications in Signal Processing and Control Systems, Springer-Verlag.\n\nCortes, C., Haffner, P. \\& Mohri, M. (2004), 'Rational Kernels: Theory and Algorithms', Journal of Machine Learning Research (JMLR) 5, 1035-1062.\nURL: http://www.cs.nyu.edu/ mohri/postscript/jmlr.pdf\nDenis, F. \\& Esposito, Y. (2008), 'On Rational Stochastic Languages', Fundamenta Informaticae 86(1, 2), 41-77.\n\nDroste, M., Kuich, W. \\& Vogler, H. (2009), Handbook of weighted automata, Springer Science \\& Business Media.\n\nEckart, C. \\& Young, G. (1936), 'The approximation of one matrix by another of lower rank', Psychometrika 1, 211-218.\nURL: https://doi.org/10.1007/BF02288367\nEyraud, R. \\& Ayache, S. (2020), 'Distillation of Weighted Automata from Recurrent Neural Networks Using a Spectral Approach', CoRR abs/2009.13101.\nURL: https://arxiv.org/abs/2009.13101\nFliess, M. (1974), 'Matrice de Hankel', Journal de Mathématique Pures et Appliquées 5, 197222 .\n\nFrazho, A. E. (1982), 'Models for noncommuting operators', Journal of Functional Analysis 48(1), 1-11.\nURL: https://www.sciencedirect.com/science/article/pii/002212368290057X\nGlover, K. (1984), 'All Optimal Hankel-Nnorm Approximations of Linear Multivariable Systems and their $\\mathcal{L}^{\\infty}$-Error Bounds', International Journal of Control 39(6), 1115-1193.\nURL: https://doi.org/10.1080/00207178408933239\nGu, G. (2005), 'All optimal Hankel-norm approximations and their error bounds in discretetime', International Journal of Control 78(6), 408-423.\n\nHinton, G., Vinyals, O. \\& Dean, J. (2015), 'Distilling the knowledge in a neural network', arXiv preprint arXiv:1503.02531 .\n\nHochreiter, S. \\& Schmidhuber, J. (1997), 'Long Short-Term Memory', Neural Computation 9(8), 1735-1780.\n\nHsu, D., Kakade, S. M. \\& Zhang, T. (2012), 'A spectral algorithm for learning hidden markov models', J. Comput. Syst. Sci. 78(5), 1460-1480.\nURL: https://doi.org/10.1016/j.jcss.2011.12.025\nIonescu, V. \\& Oara, C. (2001), The four-block Adamjan-Arov-Kein problem for discrete-time systems, in 'Linear Algebra and its Application', Elsevier, pp. 95-119.\n\nJury, M., Martin, R. \\& Shamovich, E. (2021), 'Non-commutative rational functions in the full fock space', Transactions of the American Mathematical Society ."
    },
    {
      "markdown": "Kotsalis, G. \\& Shamma, J. S. (2015), Limits of performance for the model reduction problem of hidden markov models, in '2015 54th IEEE Conference on Decision and Control (CDC)', pp. $4674-4679$.\n\nKronecker, L. (1881), 'Zur Theorie der Elimination einer Variablen aus zwei algebraischen Gleichungen', Montasber. Königl. Preussischen Acad Wies pp. 535 - 600.\n\nKulesza, A., Jiang, N. \\& Singh, S. (2015), Low-Rank Spectral Learning with Weighted Loss Functions, in 'Artificial Intelligence and Statistics', PMLR, pp. 517-525.\n\nKulesza, A., Rao, N. R. \\& Singh, S. (2014), Low-Rank Spectral Learning, in S. Kaski \\& J. Corander, eds, 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics', Vol. 33 of Proceedings of Machine Learning Research, PMLR, Reykjavik, Iceland, pp. $522-530$.\nURL: http://proceedings.mlr.press/v33/kulesza14.html\nLacroce, C. (2022), 'The approximate minimization problem of weighted finite automata and applications to language modelling: an approach based on adamyan-arov-krein theory', McGill University .\n\nLacroce, C., Panangaden, P. \\& Rabusseau, G. (2021), Extracting weighted automata for approximate minimization in language modelling, in J. Chandlee, R. Eyraud, J. Heinz, A. Jardine \\& M. van Zaanen, eds, 'Proceedings of the Fifteenth International Conference on Grammatical Inference', Vol. 153 of Proceedings of Machine Learning Research, PMLR, pp. 92-112.\nURL: https://proceedings.mlr.press/v153/lacroce21a.html\nLacroce, C., Panangaden, P. \\& Rabusseau, G. (2022), 'Towards an AAK theory approach to approximate minimization in the multi-letter case', CoRR abs/2206.00172.\nURL: https://doi.org/10.48550/arXiv.2206.00172\nLyapunov, A. M. (1950), 'The general problem of the stability of motion [in russian]', Gostekhizdat, Moscow .\n\nNehari, Z. (1957), 'On Bounded Bilinear Forms', Annals of Mathematics 65(1), 153-162.\nNikol'Skii, N. K. (2002), Operators, Functions and Systems: An Easy Reading, Vol. 92 of Mathematical Surveys and Monographs, American Mathematical Society.\n\nOkudono, T., Waga, M., Sekiyama, T. \\& Hasuo, I. (2020), Weighted Automata Extraction from Recurrent Neural Networks via Regression on State Spaces, in 'The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020', AAAI Press, pp. 5306-5314.\nURL: https://aaai.org/ojs/index.php/AAAI/article/view/5977\nOstrowski, A. \\& Schneider, H. (1962), 'Some Theorems on the Inertia of General Matrices', J. Math. Anal. Appl 4(1), 72-84.\n\nPeller, V. (2012), Hankel Operators and their Applications, Springer Science \\& Business Media.\nPighizzini, G. (2015), 'Investigations on Automata and Languages Over a Unary Alphabet', Int. J. Found. Comput. Sci. 26, 827-850."
    },
    {
      "markdown": "Popescu, G. (1989a), 'Isometric Dilations for Infinite Sequences of Noncommuting Operators', Transactions of the American Mathematical Society 316(2), 523-536.\n\nPopescu, G. (1989b), 'Models for infinite sequences of noncommuting operators', Acta Sci. Math 53, 355-285.\n\nPopescu, G. (1992), 'On Intertwining Dilations for Sequences of Noncommuting Operators', Journal of mathematical analysis and applications 167(2), 382-402.\n\nPopescu, G. (1993), Noncommutative dilation theory on Fock spaces, PhD thesis, Texas A\\&M University.\n\nPopescu, G. (1995), 'Multi-Analytic Operators on Fock Spaces', Mathematische Annalen 303(1), 31-46.\n\nPopescu, G. (2003), 'Multivariable Nehari Problem and Interpolation', Journal of Functional Analysis 200, 536-581.\nURL: https://doi.org/10.1016/S0022-1236(03)00078-8\nPopescu, G. (2006), 'Free holomorphic functions on the unit ball of b(h)n', Journal of Functional Analysis 241(1), 268-333.\nURL: https://www.sciencedirect.com/science/article/pii/S0022123606003028\nPopescu, G. (2010), Operator theory on noncommutative domains, American Mathematical Soc.\nPopescu, G. (2013), 'Noncommutative Multivariable Operator Theory', Integral Equations and Operator Theory 75(1), 87-133.\n\nRabusseau, G., Li, T. \\& Precup, D. (2019), Connecting Weighted Automata and Recurrent Neural Networks through Spectral Learning, in K. Chaudhuri \\& M. Sugiyama, eds, 'The 22nd International Conference on Artificial Intelligence and Statistics, AISTATS 2019, 16-18 April 2019, Naha, Okinawa, Japan', Vol. 89 of Proceedings of Machine Learning Research, PMLR, pp. 1630-1639.\nURL: http://proceedings.mlr.press/v89/rabusseau19a.html\nRoth, W. E. (1952), 'The equations ax - yb = c and ax - xb = c in matrices', Proceedings of the American Mathematical Society 3(3), 392-396.\n\nTheertha Suresh, A., Roark, B., Riley, M. \\& Schogol, V. (2019), 'Approximating Probabilistic Models as Weighted Finite Automata', arXiv e-prints pp. arXiv-1905.\n\nTrefethen, L. N. \\& Bau III, D. (1997), Numerical linear algebra, Vol. 50, Siam.\nWeiss, G., Goldberg, Y. \\& Yahav, E. (2019), Learning Deterministic Weighted Automata with Queries and Counterexamples, in 'Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada', pp. 8558-8569.\nURL: https://proceedings.neurips.cc/paper/2019/hash/d3f93e7766e8e1b7ef66dfdd9a8be93bAbstract.html\n\nWimmer, H. (1973), 'On the Ostrowski-Schneider Inertia Theorem', Journal of Mathematical Analysis and Applications 41(1), 164-169.\nURL: https://doi.org/10.1016/0022-247X(73)90190-X"
    },
    {
      "markdown": "Zhang, X., Du, X., Xie, X., Ma, L., Liu, Y. \\& Sun, M. (2021), Decision-guided weighted automata extraction from recurrent neural networks, in 'Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021', AAAI Press, pp. 1169911707.\n\nURL: https://ojs.aaai.org/index.php/AAAI/article/view/17391\nZhu, K. (1990), Operator Theory in Function Spaces, Vol. 138, American Mathematical Society."
    },
    {
      "markdown": "# A Technical Results \n\nSingular functions We show how to compute the functions corresponding to a $\\sigma_{k}$-Schmidt pair, and that their quotient is indeed unimodular.\n\nTheorem 24. Let $\\sigma_{k}$ be a singular number of the Hankel operator $H$. The singular functions associated with the $\\sigma_{k}$-Schmidt pair $\\left\\{\\boldsymbol{\\xi}_{k}, \\boldsymbol{\\eta}_{k}\\right\\}$ of $H$ are:\n\n$$\n\\begin{aligned}\n& \\xi_{k}^{+}(z)=\\sigma_{k}^{-1 / 2} \\boldsymbol{\\beta}^{\\top}(\\mathbf{1}-z \\mathbf{A})^{-1} \\mathbf{e}_{k} \\\\\n& \\eta_{k}^{-}(z)=\\sigma_{k}^{-1 / 2} \\boldsymbol{\\alpha}^{\\top}\\left(z \\mathbf{1}-\\mathbf{A}^{\\top}\\right)^{-1} \\mathbf{e}_{k}\n\\end{aligned}\n$$\n\nIf $\\psi$ is the best approximation to the symbol, then $\\sigma_{k}^{-1} e$ has modulus 1 almost everywhere on the unit circle (i.e. it is unimodular).\n\nProof. Let $\\mathbf{F}$ and $\\mathbf{B}$ be the forward and backward matrices, respectively, with $\\mathbf{H}=\\mathbf{F B}^{\\top}$, $\\mathbf{P}=\\mathbf{F}^{\\top} \\mathbf{F}, \\mathbf{Q}=\\mathbf{B}^{\\top} \\mathbf{B}$. We consider the $\\sigma_{k}$-Schmidt pair $\\left\\{\\boldsymbol{\\xi}_{k}, \\boldsymbol{\\eta}_{k}\\right\\}$. By definition, $\\mathbf{H}^{\\top} \\mathbf{H} \\boldsymbol{\\xi}_{k}=\\sigma_{k}^{2} \\boldsymbol{\\xi}_{k}$. By rewriting in terms of the FB factorization, we obtain:\n\n$$\n\\begin{aligned}\n& \\mathbf{H}^{\\top} \\mathbf{H} \\boldsymbol{\\xi}_{k}=\\sigma_{k}^{2} \\boldsymbol{\\xi}_{k} \\\\\n& \\mathbf{B F}^{\\top} \\mathbf{F B}^{\\top} \\boldsymbol{\\xi}_{k}=\\sigma_{k}^{2} \\boldsymbol{\\xi}_{k} \\\\\n& \\mathbf{B P B}^{\\top} \\boldsymbol{\\xi}_{k}=\\sigma_{k}^{2} \\boldsymbol{\\xi}_{k} \\\\\n& \\mathbf{B P e}_{k}=\\sigma_{k}^{2} \\boldsymbol{\\xi}_{k}\n\\end{aligned}\n$$\n\nwhere in the last step we set $\\mathbf{e}_{k}=\\mathbf{B}^{\\top} \\boldsymbol{\\xi}_{k}$, to reduce the SVD problem of $\\mathbf{H}$ to the one of $\\mathbf{Q P}$. Note that, since $\\mathbf{P}$ and $\\mathbf{Q}$ are diagonal, $\\mathbf{e}_{k}$ is the $k$-th coordinate vector $(0, \\ldots, 0,1,0, \\ldots, 0)^{\\top}$. Since $\\mathbf{e}_{k}$ is an eigenvector of $\\mathbf{Q P}$ for $\\sigma_{k}^{2}$, we get:\n\n$$\n\\begin{aligned}\n& \\mathbf{B Q}^{-1} \\mathbf{Q P e}_{k}=\\sigma_{k}^{2} \\boldsymbol{\\xi}_{k} \\\\\n& \\mathbf{B Q}^{-1} \\mathbf{e}_{k}=\\boldsymbol{\\xi}_{k}\n\\end{aligned}\n$$\n\nMoreover, $\\mathbf{H}$ is symmetric, so we have that the singular vectors $\\boldsymbol{\\eta}_{k}$ and $\\boldsymbol{\\xi}_{k}$ have the same coordinates up to the sign of the corresponding eigenvalues. We obtain:\n\n$$\n\\begin{aligned}\n& \\xi_{k}^{+}(z)=\\sum_{j=0}^{\\infty} \\sigma_{k}^{-1 / 2} \\boldsymbol{\\beta}^{\\top} \\mathbf{A}^{j} \\mathbf{e}_{k} z^{j}=\\sigma_{k}^{-1 / 2} \\boldsymbol{\\beta}^{\\top}(\\mathbf{1}-z \\mathbf{A})^{-1} \\mathbf{e}_{k} \\\\\n& \\eta_{k}^{-}(z)=\\sum_{j=0}^{\\infty} \\sigma_{k}^{-1 / 2} \\boldsymbol{\\alpha}^{\\top} \\mathbf{A}^{j \\top} \\mathbf{e}_{k} z^{-j-1}=\\sigma_{k}^{-1 / 2} \\boldsymbol{\\alpha}^{\\top}\\left(z \\mathbf{1}-\\mathbf{A}^{\\top}\\right)^{-1} \\mathbf{e}_{k}\n\\end{aligned}\n$$\n\nwhere the singular functions have been computed following Equation 8. If $r$ is the multiplicity of $\\sigma_{k}$, from Corollary 14 we get the following fundamental equation:\n\n$$\n(\\phi-\\psi) \\boldsymbol{\\beta}^{\\top}(\\mathbf{1}-z \\mathbf{A})^{-1} \\mathbf{V}=\\sigma_{k} \\boldsymbol{\\alpha}^{\\top}\\left(z \\mathbf{1}-\\mathbf{A}^{\\top}\\right)^{-1} \\mathbf{V}\n$$\n\nwhere $\\mathbf{V}=\\left(\\begin{array}{ll}\\mathbf{0} & \\mathbf{1}_{r}\\end{array}\\right)^{\\top}$ is a $n \\times r$ matrix. Consequently, we obtain the function:\n\n$$\n\\sigma_{k}^{-1} e=\\frac{\\boldsymbol{\\alpha}^{\\top}\\left(z \\mathbf{1}-\\mathbf{A}^{\\top}\\right)^{-1} \\mathbf{V}}{\\boldsymbol{\\beta}^{\\top}(\\mathbf{1}-z \\mathbf{A})^{-1} \\mathbf{V}}\n$$\n\nwhich is unimodular, since $\\boldsymbol{\\alpha}_{i}=\\operatorname{sgn}\\left(\\lambda_{i}\\right) \\boldsymbol{\\beta}_{i}$, and $\\mathbf{A}=\\operatorname{sgn}\\left(\\lambda_{i}\\right) \\mathbf{A}^{\\top}$."
    },
    {
      "markdown": "Proof of Theorem 17. In order to prove Theorem 17 we need an auxiliary lemma (Chui \\& Chen 1997, Lemma 6.1). These are the analogous of some control theory results, rephrased in terms of WFAs. The original theorem and lemma, together with the corresponding proofs, can be found in Chui \\& Chen (1997). Hence, we only provide a sketch of the proofs.\nLemma 25 (Chui \\& Chen (1997)). Let $E=\\left\\langle\\boldsymbol{\\alpha}_{e}, \\mathbf{A}_{e}, \\boldsymbol{\\beta}_{e}\\right\\rangle$ be a minimal WFA. Let $e(z)=$ $\\boldsymbol{\\alpha}_{e}^{\\top}\\left(z \\mathbf{1}-\\mathbf{A}_{e}\\right)^{-1} \\boldsymbol{\\beta}_{e}-C$, if $\\sigma_{k}^{-1} e(z)$ is unimodular, then there exist a unique invertible symmetric matrix $\\mathbf{T}$ satisfying:\n(a) $\\mathbf{A}_{e}^{\\top} \\mathbf{T} \\boldsymbol{\\beta}_{e}=\\boldsymbol{\\alpha}_{e} C$\n(b) $\\sigma_{k}^{2} \\boldsymbol{\\alpha}_{e}^{\\top} \\mathbf{T}^{-1} \\mathbf{A}_{e}^{\\top}=C \\boldsymbol{\\beta}_{e}^{\\top}$\n(c) $\\mathbf{A}_{e}^{\\top} \\mathbf{T} \\mathbf{A}_{e}-C^{-1} \\mathbf{A}_{e}^{\\top} \\mathbf{T} \\boldsymbol{\\beta}_{e} \\boldsymbol{\\alpha}_{e}^{\\top}=\\mathbf{T}$\n\nProof. Since $\\sigma_{k}^{-1} e(z)$ is unimodular, we have that:\n\n$$\ne(z) e^{*}\\left(\\bar{z}^{-1}\\right)=\\sigma_{k}^{2} \\mathbf{1}\n$$\n\nwhere we denote with $e^{*}$ the adjoint function. From the equation above, we obtain:\n\n$$\n\\begin{aligned}\ne^{*}\\left(\\bar{z}^{-1}\\right) & =\\sigma_{k}^{2} e^{-1}(z)=\\sigma_{k}^{2}\\left(-C+\\boldsymbol{\\alpha}_{e}^{\\top}\\left(z \\mathbf{1}-\\mathbf{A}_{e}\\right)^{-1} \\boldsymbol{\\beta}_{e}\\right)^{-1} \\\\\n& =-\\sigma_{k}^{2} C^{-1}-\\sigma_{k}^{2} C^{-1} \\boldsymbol{\\alpha}_{e}^{\\top}\\left(\\left(z \\mathbf{1}-\\left(\\mathbf{A}_{e}+C^{-1} \\boldsymbol{\\beta}_{e} \\boldsymbol{\\alpha}_{e}\\right)\\right)^{-1} \\boldsymbol{\\beta}_{e} C^{-1}\\right.\n\\end{aligned}\n$$\n\nwhere we used the matrix inversion lemma. On the other hand we have:\n\n$$\n\\begin{aligned}\ne^{*}\\left(\\bar{z}^{-1}\\right) & =-C+\\boldsymbol{\\beta}_{e}^{\\top}\\left(z^{-1} \\mathbf{1}-\\mathbf{A}_{e}^{\\top}\\right)^{-1} \\boldsymbol{\\alpha}_{e} \\\\\n& =-C+\\boldsymbol{\\beta}_{e}^{\\top}\\left(-\\mathbf{A}_{e}^{-\\top}\\left(\\mathbf{1}-z \\mathbf{A}_{e}^{\\top}\\right)+\\mathbf{A}_{e}^{-\\top}\\right)\\left(\\mathbf{1}-z \\mathbf{A}_{e}^{\\top}\\right)^{-1} \\boldsymbol{\\alpha}_{e} \\\\\n& =-\\left(C-\\boldsymbol{\\beta}_{e}^{\\top} \\mathbf{A}_{e}^{-\\top} \\boldsymbol{\\alpha}_{e}\\right)-\\boldsymbol{\\beta}_{e}^{\\top} \\mathbf{A}_{e}^{-\\top}\\left(z \\mathbf{1}-\\mathbf{A}_{e}^{-\\top}\\right)^{-1} \\mathbf{A}_{e}^{-\\top} \\boldsymbol{\\alpha}_{e}\n\\end{aligned}\n$$\n\nwhere we used again the matrix inversion lemma before grouping the terms. If the quantities in Equation 50 and Equation 53 have to be equal, we need their constant term to be the same. Then, we want the $\\mathcal{H}_{-}^{\\infty}$-components to correspond, so we consider the corresponding Hankel matrices. It is easy to see that we can once again associate the coefficients of these complex functions to the parameters of a WFA. From the minimality of $E$ we obtain:\n\n$$\n\\left\\{\\begin{array}{l}\n\\sigma_{k}^{2} C^{-1} \\boldsymbol{\\alpha}_{e}^{\\top}=\\boldsymbol{\\beta}_{e}^{\\top} \\mathbf{A}_{e}^{-\\top} \\mathbf{T} \\\\\n\\mathbf{A}_{e}+C^{-1} \\boldsymbol{\\beta}_{e} \\boldsymbol{\\alpha}_{e}=\\mathbf{T}^{-1} \\mathbf{A}_{e}^{-\\top} \\mathbf{T} \\\\\n\\boldsymbol{\\beta}_{e} C^{-1}=\\mathbf{T}^{-1} \\mathbf{A}_{e}^{-\\top} \\boldsymbol{\\alpha}_{e}\n\\end{array}\\right.\n$$\n\nwhere $\\mathbf{T}$ is an invertible matrix (Balle, Carreras, Luque \\& Quattoni 2014). This system is equivalent to:\n\n$$\n\\left\\{\\begin{array}{l}\n\\sigma_{k}^{2} \\boldsymbol{\\alpha}_{e}^{\\top} \\mathbf{T}^{-1} \\mathbf{A}_{e}^{\\top}=C \\boldsymbol{\\beta}_{e}^{\\top} \\\\\n\\mathbf{A}_{e}^{\\top} \\mathbf{T} \\mathbf{A}_{e}-C^{-1} \\mathbf{A}_{e}^{\\top} \\mathbf{T} \\boldsymbol{\\beta}_{e} \\boldsymbol{\\alpha}_{e}^{\\top}=\\mathbf{T} \\\\\n\\mathbf{A}_{e}^{\\top} \\mathbf{T} \\boldsymbol{\\beta}_{e}=\\boldsymbol{\\alpha}_{e} C\n\\end{array}\\right.\n$$\n\nTo conclude the proof it remains to check that $\\mathbf{T}$ is symmetric, and this can be done by direct computations.\n\nProof of Theorem 17. This proof follows easily from Lemma 25 by setting $\\mathbf{P}=-\\sigma_{k}^{2} \\mathbf{T}^{-1}$ and $\\mathbf{Q}=-\\mathbf{T}$. We obtain point $(c)$ by direct multiplication. Then, we substitute the last equation in 55 into the second one, and we obtain:\n\n$$\n\\mathbf{A}_{e}^{\\top} \\mathbf{T} \\mathbf{A}_{e}-\\boldsymbol{\\alpha}_{e} \\boldsymbol{\\alpha}_{e}^{\\top}=\\mathbf{T}\n$$\n\nwhich verifies point $(b)$ with $\\mathbf{Q}=-\\mathbf{T}$. Point $(a)$ can be obtained analogously combining the first and second equations in 55 ."
    }
  ],
  "usage_info": {
    "pages_processed": 32,
    "doc_size_bytes": 610292
  },
  "_metadata": {
    "model_version": "0.0.0",
    "llm_model": "mistral-ocr-2505",
    "query_data": {
      "pdf": "/Users/satyaortiz-gagne/CODE/paperext/data/cache/fulltext/b272d520a0ebe901a55f22cb8a27c02b/fulltext.pdf"
    },
    "model_id": "parsepdf"
  }
}