{
  "pages": [
    {
      "markdown": "# Training Language Models to Self-Correct via Reinforcement Learning \n\nAviral Kumar ${ }^{*+1}$, Vincent Zhuang ${ }^{*+1}$, Rishabh Agarwal ${ }^{*}, \\mathrm{Yi} \\mathrm{Su}^{*}, \\mathrm{JJ}$ Co-Reyes ${ }^{1}$, Avi Singh ${ }^{1}$, Kate Baumli ${ }^{1}$, Shariq Iqbal ${ }^{1}$, Colton Bishop ${ }^{1}$, Rebecca Roelofs ${ }^{1}$, Lei M Zhang ${ }^{1}$, Kay McKinney ${ }^{1}$, Disha Shrivastava ${ }^{1}$, Cosmin Paduraru ${ }^{1}$, George Tucker ${ }^{1}$, Doina Precup ${ }^{1}$, Feryal Behbahani ${ }^{\\dagger, 1}$ and Aleksandra Faust ${ }^{\\dagger, 1}$<br>${ }^{1}$ Google DeepMind, ${ }^{*}$ Equal Contribution, ${ }^{*}$ Randomly ordered via coin flip, ${ }^{1}$ Jointly supervised.\n\nSelf-correction is a highly desirable capability of large language models (LLMs), yet it has consistently been found to be largely ineffective in modern LLMs. Current methods for training self-correction typically depend on either multiple models, a more advanced model, or additional forms of supervision. To address these shortcomings, we develop a multi-turn online reinforcement learning (RL) approach, SCoRe, that significantly improves an LLM's self-correction ability using entirely self-generated data. To build SCoRe, we first show that variants of supervised fine-tuning (SFT) on offline model-generated correction traces are often insufficient for instilling self-correction behavior. In particular, we observe that training via SFT falls prey to either a distribution mismatch between mistakes made by the data-collection policy and the model's own responses, or to behavior collapse, where learning implicitly prefers only a certain mode of correction behavior that is often not effective at self-correction on test problems. SCoRe addresses these challenges by training under the model's own distribution of self-generated correction traces and using appropriate regularization to steer the learning process into learning a self-correction behavior that is effective at test time as opposed to fitting high-reward responses for a given prompt. This regularization process includes an initial phase of multi-turn RL on a base model to generate a policy initialization that is less susceptible to collapse, followed by using a reward bonus to amplify self-correction. With Gemini 1.0 Pro and 1.5 Flash models, we find that SCoRe achieves state-of-the-art self-correction performance, improving the base models' self-correction by $15.6 \\%$ and $9.1 \\%$ respectively on MATH and HumanEval.\n\n## 1. Introduction\n\nLarge language models (LLMs) are a useful tool for reasoning in scientific domains such as math and coding (Lozhkov et al., 2024; Shao et al., 2024; Team, 2024). An aspirational property of LLMs in such settings is their ability to implement meta-strategies or algorithms that use test-time computation to generate improved responses. However, modern LLMs do not implement such strategies reliably. For instance, consider a problem that requires models to detect and revise (or \"self-correct\") their own responses in order to eventually arrive at the best possible final response. This self-correction capability has been shown to be severely lacking in current LLMs, especially in the absence of external input (also called intrinsic self-correction) (Huang et al., 2023; Kamoi et al., 2024).\nTo make progress towards teaching LLMs to implement meta-strategies for challenging inputs, we study a special instance of training LLMs to perform self-correction to fix their mistakes \"on-the-fly\". This should be possible: on many queries where current LLMs fail, they possess the underlying \"knowledge\" needed to arrive at the correct response but are unable to correctly elicit and draw inferences about their own knowledge when needed (Yang et al., 2024). For example, strong LLMs can often successfully complete a sub-part of a math proof when prompted with the remainder, but may not be able to complete it from scratch. In a similar vein, leveraging their previous responses should, in principle, enable LLMs to"
    },
    {
      "markdown": "![img-0.jpeg](img-0.jpeg)\n\nFigure 1 | Left: SCoRe achieves state-of-the-art self-correction performance on MATH; Right: SCoRe inference-time scaling: spending samples on sequential self-correction becomes more effective than only on parallel direct samples (Section 6.2).\nimprove their subsequent ones. Despite this, self-correction has remained elusive, highlighting the need to go beyond existing training paradigms.\n\nHow can we imbue LLMs with self-correction abilities? Prior attempts for self-correcting LLMs either rely on prompt-engineering (Kim et al., 2023; Madaan et al., 2023) or on fine-tuning models specifically for self-correction. While the former approaches often fail to perform meaningful intrinsic self-correction, fine-tuning approaches require running multiple models during inference, such as a separate refinement model (Havrilla et al., 2024b; Welleck et al., 2023), or rely on \"teacher\" supervision to guide the process of self-correction (Qu et al., 2024). With the use of separate models of teacher supervision, self-correction does not necessarily outperform parallel, independent attempts. We develop an approach that is effective at self-correction without these requirements. Our approach, Self-Correction via Reinforcement Learning (SCoRe), trains only a single model that can both produce a response to a problem and also correct errors without any oracle feedback.\n\nTo develop SCoRe, we start by analyzing the shortcomings of SFT-based approaches (e.g., STaR (Zelikman et al., 2022)) and naïve RL that optimizes final response correctness for teaching self-correction. We find that such approaches fall prey to either: (1) distribution shift, where the trained model is able to correct errors made by the base model that generated the data, but these gains do not transfer to self-correction under the learned model's own mistakes; or (2) behavior collapse, where the learning progress simply learns to produce the best first-attempt response followed by superficial or no modifications in the second attempt. To address these issues, SCoRe trains for self-correction directly via on-policy, multi-turn RL. To prevent behavior collapse, $\\operatorname{SCoRe}$ employs two-stage training: in the first stage, it produces an initialization that is less susceptible to behavior collapse by training to correct second-attempt responses while constraining the first-turn distribution to be close to the base model; followed by training on both attempts to maximize reward in the second stage. Crucially, the second stage of multi-turn RL employs a reward shaping term that rewards \"progress\" towards self-correction as opposed to the correctness of the final response.\n\nOur main contribution is $\\operatorname{SCoRe}$, a multi-turn RL approach for teaching LLMs how to correct their own mistakes. To the best of our knowledge, $\\operatorname{SCoRe}$ is the first approach to attain significantly positive intrinsic self-correction: relative to base Gemini models, our method attains an absolute $\\mathbf{1 5 . 6 \\%}$ gain on self-correction for reasoning problems from MATH (Hendrycks et al., 2021) and an absolute $\\mathbf{9 . 1 \\%}$ gain"
    },
    {
      "markdown": "on coding problems from HumanEval (Chen et al., 2021). We additionally motivate the design of $\\operatorname{SCoRe}$ by extensively studying the failure modes of SFT and standard RL approaches, which broadly indicate that reinforcement learning plays an essential role in self-trained self-correction.\n\n# 2. Related Work \n\nPrior works study self-correction for LLMs under a variety of assumptions and problem settings. The most prominent settings include problems where external input tokens from an environment are available, such as agentic tasks (Liu et al., 2023), code repair (Jain et al., 2024), and tool use (Chen et al., 2023). While self-correction with external feedback is possible with strong models (Pan et al., 2023), even they struggle in the substantially harder setting with no external input (intrinsic self-correction) (Huang et al., 2023; Kamoi et al., 2024). Prior work that attempts to amplify intrinsic correction abilities is largely based on prompting and fine-tuning.\n\nPrompting for intrinsic self-correction. Recent work demonstrates that naïvely prompting LLMs for self-correction can degrade performance (Huang et al., 2023; Qu et al., 2024; Tyen et al., 2024; Zheng et al., 2024). These results contradict prior work (Kim et al., 2023; Madaan et al., 2023; Shinn et al., 2023) and largely stem from mismatched assumptions about the setting (Kamoi et al., 2024). For example, Kim et al. (2023); Shinn et al. (2023) use ground-truth answers during self-correction that may not generally be available; Madaan et al. (2023) use weak prompts for initial responses, thereby overestimating the total improvement possible. Therefore, there is no major work showing successful intrinsic self-correction via prompting alone. In the context of code self-repair, Olausson et al. (2023) show that even when strong models are prompted with some form of partial feedback, e.g., test-cases but not the desired outcomes, they are unable to correct their mistakes.\n\nFine-tuning for intrinsic self-correction. Several works that go beyond prompting rely on fine-tuning with demonstrations of revisions, e.g. obtaining revisions directly from human annotators (Saunders et al., 2022) or stronger models (Qu et al., 2024; Ye et al., 2023). Our work aims to train for self-correction entirely without the use of larger models or humans, when the learner itself is asked to generate its own training data. Similar to these prior works, we assume access to a reward function for evaluating modelgenerated outputs (Akyürek et al., 2023; Welleck et al., 2023; Zhang et al., 2024). Perhaps the closest to us from this set is Qu et al. (2024), which utilizes an iterative STaR-like approach self-correction. While this work largely uses oracle teacher supervision, their preliminary results from training for self-correction only show minor improvements over five turns, consistent with the results we see for STaR. We show that $\\operatorname{SCoRe}$ attains substantially better results. Other approaches train separate models for performing correction (e.g., GLoRE (Havrilla et al., 2024b), Self-Correction (Welleck et al., 2023), Akyürek et al. (2023); Paul et al. (2023). While such approaches can be convenient, they require system design for serving multiple models at deployment.\n\nMulti-turn RL for LLMs. Prior work at the intersection of LLMs and multi-turn RL builds machinery for optimizing rewards with value-based (Farebrother et al., 2024; Shani et al., 2024; Snell et al., 2022; Zhou et al., 2024), policy-based (Shao et al., 2024; Xiong et al., 2024), and model-based (Hong et al., 2024) approaches. We do not focus on building machinery for RL (we use the approach of Ahmadian et al. (2024)), but rather train for self-correction as an RL problem."
    },
    {
      "markdown": "# MATH Example 1: arithmetic Self-Correction \n\nProblem. If $n \\equiv 2(\\bmod 7)$, then find the remainder when $(n+2)(n+4)(n+6)$ is divided by 7 .\nSCoRe turn 1 solution (incorrect). Since $n \\equiv 2(\\bmod 7)$, we can write $n=7 k+2$ for some integer $k$. Substituting, we get $(n+2)(n+4)(n+6)=(7 k+4)(7 k+6)(7 k+8) \\equiv 4 \\cdot 6 \\cdot 8(\\bmod 7) \\equiv 192(\\bmod 7) \\equiv 1(\\bmod 7)$. Final Answer: The final answer is 1. I hope it is correct.\n\nSelf-correction instruction. There might be an error in the solution above because of lack of understanding of the question. Please correct the error, if any, and rewrite the solution.\nSCoRe turn 2 solution (correct). Since $n \\equiv 2(\\bmod 7)$, we can write $n=7 k+2$ for some integer $k$. Substituting, we get $(n+2)(n+4)(n+6)=(7 k+4)(7 k+6)(7 k+8) \\equiv 4 \\cdot 6 \\cdot 8(\\bmod 7) \\equiv 4 \\cdot 6 \\cdot 1(\\bmod 7) \\equiv 24(\\bmod 7) \\equiv 3(\\bmod 7)$. Final Answer: The final answer is 3 . I hope it is correct.\n\n## MATH Example 2: Reasoning Self-Correction\n\nProblem. Let $f(x)=\\left\\lfloor\\left(-\\frac{5}{8}\\right)^{x}\\right\\rfloor$ be a function that is defined for all values of $x$ in $[0, \\infty)$ such that $f(x)$ is a real number. How many distinct values exist in the range of $f(x)$ ?\nSCoRe turn 1 solution (incorrect). As $x$ increases, the value of $\\left(-\\frac{5}{8}\\right)^{x}$ decreases. Since the floor function rounds down to the nearest integer, the range of $f(x)$ consists of all the integers less than or equal to $\\left\\lfloor\\left(-\\frac{5}{8}\\right)^{0}\\right\\rfloor=1$. Therefore, the range of $f(x)$ is $\\{1,0,-1,-2, \\ldots\\}$, and there are $\\boxed{\\infty}$ distinct values in the range. Final Answer: The final answer is $\\infty$. I hope it is correct.\n\nSelf-correction instruction. There might be an error in the solution above because of lack of understanding of the question. Please correct the error, if any, and rewrite the solution.\nSCoRe turn 2 solution (correct). As $x$ increases from 0 to $\\infty$, the value of $\\left(-\\frac{5}{8}\\right)^{x}$ oscillates between positive and negative values, but\nits absolute value decreases. The floor function rounds down to the nearest integer, so the range of $f(x)$ consists of the integers less than or equal to $\\left\\lfloor\\left(-\\frac{5}{8}\\right)^{0}\\right\\rfloor=1$. Since the absolute value of $\\left(-\\frac{5}{8}\\right)^{x}$ decreases as $x$ increases, the range of $f(x)$ is finite and consists of the integers: $\\{1,0,-1\\}$. Final Answer: The final answer is 3 . I hope it is correct.\n\nFigure 2 | Two example traces of self-correction. In the upper example, SCoRe is able to correct an arithmetic mistake it makes in turn 1. In the lower example, the model is able to correct a reasoning error.\n\n## 3. Preliminaries and Problem Setup\n\nOur goal is to develop an approach for training LLMs to improve their own predictions entirely on selfgenerated data. As discussed so far, we situate ourselves in the intrinsic self-correction setting (Huang et al., 2023), where models attempt to correct their initial responses without any external feedback. Concretely, given a dataset $\\mathcal{D}=\\left\\{\\left(\\boldsymbol{x}_{i}, \\boldsymbol{y}_{i}^{*}\\right)\\right\\}_{i=1}^{N}$ of problems $\\boldsymbol{x}_{i}$ and responses $\\boldsymbol{y}_{i}^{*}$, we will train an LLM policy $\\pi_{\\theta}\\left(\\cdot \\mid\\left[\\boldsymbol{x}, \\hat{\\boldsymbol{y}}_{1: l}, p_{1: l}\\right]\\right)$ that, given the problem $\\boldsymbol{x}$, previous $l$ model attempts $\\hat{\\boldsymbol{y}}_{1: l}$ at the problem, and auxiliary instructions $p_{1: l}$ (e.g., instruction to find a mistake and improve the response), solves the problem $\\boldsymbol{x}$ as correctly as possible. This formalism is akin to the multi-turn MDP in Qu et al. (2024). We also assume access to an oracle reward $\\widehat{r}\\left(\\boldsymbol{y}, \\boldsymbol{y}^{*}\\right)$, such as an answer checker (Uesato et al., 2022), that evaluates the correctness of response $\\boldsymbol{y}$ by comparing it with the oracle response $\\boldsymbol{y}^{*}$. Critically, we do not assume access to this oracle at test-time; instead, the model must deduce whether there was a mistake and correct it if necessary, as is often the case in e.g. mathematical reasoning problems. Unlike the setup of Qu et al. (2024), we also do not run majority voting for most of our main results. Two example traces of self-correction are given in Figure 2, and our problem setting is depicted pictorially in Figure 3."
    },
    {
      "markdown": "![img-1.jpeg](img-1.jpeg)\n\nFigure 3 | The problem setting of self-correction. SCoRe trains a model to not just produce the best possible response, but instead aims to train the model to produce the best final response in the final attempt. In the second turn, extra input in the form of an instruction asking the model to correct itself or model-generated may be provided.\n\nWe aim to find an LLM policy $\\pi(\\square \\mid \\circ)$ mapping input tokens $\\circ$ to output tokens $\\square$ that maximizes the correctness reward obtained from the verifier at the end of $l+1$ turns $(l=1)$. Formally:\n\n$$\n\\max _{\\pi_{\\theta}} \\mathcal{E}_{\\boldsymbol{x}, \\boldsymbol{y}^{*}-\\mathcal{D}, \\hat{\\boldsymbol{y}}_{l+1}-\\pi_{\\theta}\\left(\\cdot \\mid\\left[\\boldsymbol{x}, \\hat{\\boldsymbol{y}}_{1: l}, \\boldsymbol{P} 1: l\\right]\\right)}\\left[\\sum_{i=1}^{l+1} \\tilde{r}\\left(\\hat{\\boldsymbol{y}}_{i}, \\boldsymbol{y}^{*}\\right)\\right]\n$$\n\nCrucially, note that unlike standard SFT or prevalent RL fine-tuning workflows, which train the policy $\\pi$ to directly produce $\\boldsymbol{y}^{*}$ (or any other $\\boldsymbol{y}$ wih $\\tilde{\\boldsymbol{r}}\\left(\\boldsymbol{y}, \\boldsymbol{y}^{*}\\right)=1$ ), Equation 1 trains $\\pi$ over multiple attempts simultaneously, where intermediate turns are supervised indirectly to maximize the sum.\n\nBase RL fine-tuning approach we use. We use a REINFORCE policy gradient training approach with a KL-divergence penalty against a fixed model (Ahmadian et al., 2024), which is widely used in RL fine-tuning of LLMs, primarily in the setting of single-turn RLHF. Formally, these methods train the policy $\\pi_{\\theta}(\\cdot \\mid \\boldsymbol{x})$ to optimize the following, where $\\pi_{\\text {ref }}$ is a reference policy.\n\n$$\n\\max _{\\theta} \\mathbb{E}_{\\boldsymbol{x}_{t}, \\boldsymbol{y}_{t}-\\pi_{\\theta}\\left(\\cdot \\mid \\boldsymbol{x}_{t}\\right)}\\left[\\tilde{\\boldsymbol{r}}\\left(\\boldsymbol{y}_{t}, \\boldsymbol{y}^{*}\\right)-\\beta_{1} D_{K L}\\left(\\pi_{\\theta}\\left(\\cdot \\mid \\boldsymbol{x}_{t}\\right) \\mid \\mid \\pi_{\\mathrm{ref}}\\left(\\cdot \\mid \\boldsymbol{x}_{t}\\right)\\right)\\right]\n$$\n\nMetrics. To measure self-correction performance (we consider $l=2$ in this paper), we report and analyze the following metrics: (1) Accuracy@t1: the model's accuracy at the first attempt; (2) Accuracy@t2: the model's accuracy at the second attempt, (3) $\\Delta(\\mathbf{t 1}, \\mathbf{t 2})$ : the net improvement in model accuracy between the first and second attempts, which measures the efficacy of self-correction, (4) $\\Delta^{\\mathrm{i} \\rightarrow \\mathrm{c}}(\\mathbf{t 1}, \\mathbf{t 2})$ : the fraction of problems that are incorrect in the first attempt but become correct at the second attempt, which measures how many new problems can self-correction solve; and (5) $\\Delta^{\\mathrm{c} \\rightarrow \\mathrm{i}}(\\mathbf{t 1}, \\mathbf{t 2})$ : the fraction of problems that are correct in the first attempt but become incorrect at the second attempt, which measures how well the model understands what makes a response correct.\n\n# 4. SFT on Self-Generated Data is Insufficient for Self-Correction \n\nA natural approach for training self-correction is to utilize some form of supervised fine-tuning on data collected from a base model. Variants of this approach have been shown to scale well on single-turn reasoning problems (Singh et al., 2023; Zelikman et al., 2022). In this section, we assess the empirical"
    },
    {
      "markdown": "Table $1 \\mid$ Self-correction performance after training on $\\mathcal{D}_{\\text {STaR }}$ and $\\mathcal{D}_{\\text {SFT }}$. We find that the gap between the second and first attempts $(\\Delta(\\mathrm{t} 1, \\mathrm{t} 2))$ is either negative or small. Both approaches erroneously modify a correct response to be incorrect, i.e., reflected in a high $\\Delta^{\\mathrm{c} \\rightarrow \\mathrm{i}}(\\mathrm{t} 1, \\mathrm{t} 2)$ and a low $\\Delta^{\\mathrm{i} \\rightarrow \\mathrm{c}}(\\mathrm{r} 1, \\mathrm{r} 2)$.\n\n| Method | Accuracy@t1 | Accuracy@t2 | $\\Delta(\\mathbf{t} \\mathbf{1}, \\mathbf{t} \\mathbf{2})$ | $\\Delta^{\\mathrm{i} \\rightarrow \\mathrm{c}}(\\mathbf{t} \\mathbf{1}, \\mathbf{t} \\mathbf{2})$ | $\\Delta^{\\mathrm{c} \\rightarrow \\mathrm{i}}(\\mathbf{t} \\mathbf{1}, \\mathbf{t} \\mathbf{2})$ |\n| :-- | :--: | :--: | :--: | :--: | :--: |\n| Base model | $52.6 \\%$ | $41.4 \\%$ | $-11.2 \\%$ | $4.6 \\%$ | $15.8 \\%$ |\n| STaR $\\mathcal{D}_{\\text {StaR }}$ | $55.4 \\%$ | $41.2 \\%$ | $-14.2 \\%$ | $5.4 \\%$ | $19.6 \\%$ |\n| STaR $\\mathcal{D}_{\\text {StaR }}^{*}$ | $53.6 \\%$ | $54.0 \\%$ | $0.4 \\%$ | $2.6 \\%$ | $2.2 \\%$ |\n| Pair-SFT $\\mathcal{D}_{\\text {SFT }}$ | $52.4 \\%$ | $54.2 \\%$ | $1.8 \\%$ | $5.4 \\%$ | $3.6 \\%$ |\n| Pair-SFT $\\mathcal{D}_{\\text {SFT }}^{*}$ | $55.0 \\%$ | $55.0 \\%$ | $0 \\%$ | $0 \\%$ | $0 \\%$ |\n\nefficacy of two such approaches for self-correction: STaR (Zelikman et al., 2022), and a version of Welleck et al. (2023) that trains only one model.\n\nWe ultimately find that although these methods improve self-correction over the base model, they fail to achieve substantially positive self-correction ( $\\Delta(\\mathbf{t} \\mathbf{1}, \\mathbf{t} \\mathbf{2})$ ). By probing these models, we observe two main failure modes: (1) a collapse to non-correcting behavior, where the models learn to produce a good response on the first attempt and only make minor (or no) modifications in the second attempt, and (2) an inability of offline methods to be robust to distribution shift in the first-attempt responses.\n\nAnalysis setup: methods and dataset construction. We prompt Gemini 1.5 Flash to obtain a large number of two-turn self-correction traces on MATH (Hendrycks et al., 2021). The STaR approach filters these trajectories to retain only those that successfully revise incorrect responses and runs SFT on the resulting dataset. Another approach is to use base model data from above to construct \"synthetic\" repair traces by pairing incorrect responses with correct ones (Welleck et al., 2023). We study a variant of this method that we call Pair-SFT, which does not train a separate corrector model and does not augment this initial dataset with multi-turn traces. Formally, we denote the datasets for STaR and Pair-SFT as $\\mathcal{D}_{\\text {STaR }}$ and $\\mathcal{D}_{\\text {SFT }}$ respectively. We run 3 iterations for STaR following the protocol in Singh et al. (2024), and only one iteration for Pair-SFT, following the protocol in Welleck et al. (2023) and other standard workflows on SFT.\n\nMain empirical findings. We present the self-correction results before and after fine-tuning on $\\mathcal{D}_{\\text {STaR }}$ and $\\mathcal{D}_{\\text {SFT }}$ in Table 1. We find that although $\\Delta(\\mathbf{t} \\mathbf{1}, \\mathbf{t} \\mathbf{2})$ is substantially higher for Pair-SFT relative to the base model, there is only little benefit to self-correction ( $1.8 \\%$ gain). This gain is of a similar order to findings from Qu et al. (2024). By studying $\\Delta^{\\mathrm{i} \\rightarrow \\mathrm{c}}$ and $\\Delta^{\\mathrm{c} \\rightarrow \\mathrm{i}}$, we find that SFT mainly reduces the number of correct problems that are mistakenly changed to incorrect in the second attempt, but does not significantly increase the fraction of incorrect first attempts that are corrected. This result is consistent with prior works on intrinsic self-correction that have found negligible or negative $\\Delta(\\mathbf{t} \\mathbf{1}, \\mathbf{t} \\mathbf{2})$ values.\n\nWe also find that unlike Pair-SFT, training on $\\mathcal{D}_{\\text {STaR }}$ does not reduce $\\Delta^{\\mathrm{c} \\rightarrow \\mathrm{i}}$, indicating that the STaR policy does not have a clear understanding of when to make modifications and when not to. Observing this, we also trained on an extended version of $\\mathcal{D}_{\\text {STaR }}^{*}$ (and $\\mathcal{D}_{\\text {SFT }}^{*}$ ), which additionally contains tuples with both correct responses. We would expect the addition of such \"correct-to-correct\" data to prevent the model from erroneously revising a correct response. As shown in Table 1, the inclusion of this data helps STaR substantially but only results in $0.4 \\%$ change in $\\Delta(\\mathbf{t} \\mathbf{1}, \\mathbf{t} \\mathbf{2})$. On the other hand, for SFT, inclusion of this"
    },
    {
      "markdown": "![img-2.jpeg](img-2.jpeg)\n(a) Histograms of edit distance ratios on MATH 500.\n(c) Pair-SFT edit distance ratios.\n\nFigure 4 | Edit distance between first-attempt and second-attempt responses from fine-tuned models, our approach (SCoRe) and the base model. While training on self-generated error correction traces learns to not make major edits primarily, SFT learns to make some edits but is still quite conservative.\ndata overly biases the model against changing its answer.\nDiving deeper: analyzing self-correction behavior. To further understand how these STaR and SFT models edit their responses, we measured their edit distance ratios, defined as the edit distance between the responses normalized by the total length of both the responses. As shown in Figure 4a, while the base model sometimes makes substantially large edits to the original response, models fine-tuned on $\\mathcal{D}_{\\text {STaR }}$ and $\\mathcal{D}_{\\text {SFT }}$ are overly conservative and often make no edits at all. This is akin to a form of behavior collapse: training to maximize likelihoods on off-policy revision traces does not teach the desired correction \"behavior\", even though it improves first-attempt accuracy. Similar observations of LLMs ignoring nuanced be-\n![img-3.jpeg](img-3.jpeg)\n\nFigure 5 | Self-correction performance on different sets of firstattempt responses: (a) \"fixed\": first response is sampled from the initial model, (b) \"self-generated\": first response is generated by the learner itself. Throughout training, the correction rate on fixed responses increases for both train and validation problems, but degrades substantially on self-generated responses. This indicates that training on a fixed offline dataset of correction traces suffers from distribution shift.\nhaviors (e.g., producing a mistake in a response and then correcting it in subsequent steps) have been observed in Ye et al. (2024).\n\nWe also compared the distributions of edit distance ratios on training and test-time self-correction traces in Figures 4b/4c. While STaR produces qualitatively similar edit distance ratios on both the train and validation sets, we still observe some discrepancies between the train and validation edit distance ratios for SFT, implying that Pair-SFT is not very effective at generalizing to new problems from the same"
    },
    {
      "markdown": "![img-4.jpeg](img-4.jpeg)\n(a) Training accuracy curves. When training with standard multiturn RL, the responses at both the attempts become tightly coupled together, leading to poor coverage for subsequent iterations and worse learning progress. Stage I in $\\boldsymbol{S C o R e}$ is explicitly designed to alleviate this and achieves much higher $\\Delta(\\mathrm{t} 1, \\mathrm{t} 2)$, leading to increased exploration and better final performance.\n(b) Frequency in which the learner proposes a different answer in the second turn. Without explicitly modifying the policy initialization as in $\\boldsymbol{S C o R e}$, the policy quickly learns to often not change its answer, leading to poor exploration. Stage I in SCoRe prevents this issue, and learns non-collapsed behavior in Stage II.\n\nFigure 6 | Behavior collapse in standard multi-turn RL for training self-correction. These results indicate that some explicit approach to avoid collapse is necessary, i.e. Stage I in $\\boldsymbol{S C o R e}$.\ndistribution. We visualized this by plotting the self-correction performance of the SFT model on a fixed set of first attempts and self-generated first attempts in Figure 5. We observe vastly different behaviors between static and self-generated first-attempt distributions: while the model is able to optimize training correction accuracy and also slightly improves on first attempts appearing in the validation set (distributed i.i.d. to the training distribution), its self-correction accuracy degrades. Hence, distribution shift is a significant challenge for offline methods such as Pair-SFT.\n\n# Takeaways. Insufficiency of SFT \n\nSFT-based methods suffer from two distinct failures when learning self-correction: (1) distribution shift, and (2) behavior collapse. Training on on-policy data can fix (1), but not (2).\n\n## 5. SCoRe: Self-Correction via Multi-Turn Reinforcement Learning\n\nThe above results highlight that an effective approach for training LLMs to self-correct entirely via self-generated data must address both distribution shift and behavior collapse. Utilizing on-policy RL is a natural way to address distribution shift, and our method will do so by extending Equation 2 to multiple turns under the hierarchical framework of Zhou et al. (2024). However, is behavior collapse an issue for standard multi-turn RL? And if not, how can we address it?\n\nTo answer these questions, we run standard multi-turn RL training to optimize Equation 1 only on $\\left(x_{2}, y_{2}\\right)$ pairs. Since this objective maximizes the second-attempt performance of the model without training the first attempt, we expect the self-correction $\\Delta(\\mathrm{t} 1, \\mathrm{t} 2)$ of the model to increase. However, as shown in Figure 6, while the performance of each attempt improves with training, their difference $\\Delta(\\mathrm{t} 1, \\mathrm{t} 2)$ does not. In other words, standard multi-turn RL converges to a state that is overly biased against changing its response, resulting in no self-correction ability and a similar behavior collapse as what we saw with STaR.\n\nWhy does RL still suffer from collapse? There are at least two equally good solutions when optimizing a policy with RL on the training data: (i) learning to improve from the first to the second attempt, or (ii)"
    },
    {
      "markdown": "![img-5.jpeg](img-5.jpeg)\n\nFigure 7 | An overview of our approach (SCoRe). SCoRe trains a model in two stages: Stage I: instead of running SFT (which produces pathological amplification of biases) to initialize RL training, we train a good initialization that can produce high-reward responses in the second-attempt while mimicking the base model's initial response at the first attempt. Stage II: jointly optimizing both attempts, where the latter uses a shaped reward to incentivize the discovery of the self-correction strategy instead of the simple strategy of producing the best first response followed by making any minor edits to it in the second attempt.\nlearning to produce the best first-attempt response, followed by no correction in the second attempt. Of course only the former strategy generalizes to new problems, but an overparameterized LLM may not necessarily learn strategy (i) instead of (ii), since both of these strategies can be equally optimal on the training set. Abstractly, learning the \"meta strategy\" of self-correction during training is difficult unless the \"direct\" strategy that optimizes reward appears less viable on the training data. Conceptually, this is similar to the memorization challenge in meta-learning (Yin et al., 2019), which suggests that when provided with mutually exclusive tasks, meta-learning is likely to recover the supervised learning solution (without using context from the few shots) that directly predicts the output. Here, this is analogous to not self-correcting past attempts, directly producing an answer.\n\nMethod overview. Although a good self-correcting policy should maximize both accuracy@t1 and accuracy@t2, we saw that standard RL leads to a collapse to non-correcting behavior. Hence, our key insight in SCoRe is that we must more explicitly encourage self-correction behavior, which we accomplish via a two-stage approach. The first stage (Stage I) serves the role of initialization where we train the model to decouple its behavior across the two attempts by attempting to optimize second-attempt accuracy while explicitly constraining the distribution of first attempts to the base model. From here, Stage II then jointly optimizes the reward of both attempts. To ensure that Stage II does not collapse to the \"direct\" solution, we bias the reward to reinforce self-correction progress.\n\n# 5.1. Stage I: Training an Initialization that Decouples Attempts \n\nThe goal of Stage I of SCoRe is to obtain an initialization by improving the base model's coverage over second attempts given the first attempt, so that subsequent training for self-correction is less prone to behavior collapse. While this would typically be done via SFT, our results in Section 4 show that SFT itself suffers from collapse. Therefore, we use RL in this stage to decouple the two attempts. To do so, we explicitly fine-tune the base model to produce high-reward responses at the second attempt, while forcing the model to not change its first attempt by constraining it to be close to the base model using a KL-divergence. While this may appear sub-optimal - a first attempt with fewer mistakes should lead to a better second attempt - but as we will show, this stage is critical in reducing the base model's bias towards simply coupling the first and second-attempt distributions, thus avoiding behavior collapse when"
    },
    {
      "markdown": "actual multi-turn RL is run. Formally, the objective is:\n\n$$\n\\max _{\\theta} \\mathbb{E}_{\\boldsymbol{x}_{1}, \\boldsymbol{y}_{1} \\sim \\pi_{\\theta}(\\cdot \\mid \\boldsymbol{x}), \\boldsymbol{y}_{2} \\sim \\pi_{\\theta}(\\cdot)\\left[\\boldsymbol{x}_{1}, \\boldsymbol{p}_{1}\\right]}\\left[\\widehat{\\boldsymbol{r}}\\left(\\boldsymbol{y}_{2}, \\boldsymbol{y}^{*}\\right)-\\beta_{2} D_{K L}\\left(\\pi_{\\theta}(\\cdot) \\mid \\boldsymbol{x}_{1}\\right) \\| \\pi_{\\operatorname{ref}}\\left(\\cdot \\mid \\boldsymbol{x}_{1}\\right)\\right]\n$$\n\nwhere $\\beta_{2}$ is a hyperparameter designed to enforce a strict KL penalty only on the first attempt to avoid shifting of the first-turn responses (denoted by the term in blue). Note that we still utilize the default KL-divergence penalty from Equation 2, but with a relatively small weight and is omitted from Equation 3 for clarity. Indeed, we show that compared to standard multi-turn RL, Stage I is more effective at decoupling the two responses (Figure 6b) and leads to better Stage II performance.\n\n# 5.2. Stage II: Multi-Turn RL with Reward Shaping \n\nThe second stage of $\\boldsymbol{S C o R e}$ is initialized from Stage I and now jointly optimizes the performance of both attempts. Concretely, Stage II trains the policy $\\pi_{\\theta}(\\cdot \\mid \\cdot)$ using the following objective:\n\n$$\n\\max _{\\theta} \\mathbb{E}_{\\boldsymbol{x}_{1}, \\boldsymbol{y}_{1} \\sim \\pi_{\\theta}(\\cdot \\mid \\boldsymbol{x}), \\boldsymbol{y}_{2} \\sim \\pi_{\\theta}(\\cdot)\\left[\\boldsymbol{x}_{1}, \\boldsymbol{p}_{1}\\right]}\\left[\\sum_{i=1}^{2} \\widehat{\\boldsymbol{r}}\\left(\\boldsymbol{y}_{i}, \\boldsymbol{y}^{*}\\right)-\\beta_{1} D_{K L}\\left(\\pi_{\\theta}\\left(\\cdot \\mid \\boldsymbol{x}_{i}\\right) \\| \\pi_{\\operatorname{ref}}\\left(\\cdot \\mid \\boldsymbol{x}_{i}\\right)\\right)\\right]\n$$\n\nwhere $\\boldsymbol{x}_{i}, i \\in\\{1,2\\}$ corresponds to the set of input tokens passed as context to the model.\nReward shaping to prevent behavior collapse. In principle, optimizing Equation 4 can also produce a solution that couples responses. This is because we still aim to maximize ground-truth rewards at both attempts. To prevent the learning process from collapsing to a non self-correcting solution in Stage II, we need to bias the learning problem towards self-correction. We implement this via reward shaping: by rewarding transitions that make \"progress\" towards learning the desired self-correction behavior. Concretely, given an two-turn rollout sampled from the policy $\\tau=\\left\\{\\boldsymbol{x}_{1}, \\hat{\\boldsymbol{y}}_{1}, \\widehat{\\boldsymbol{r}}\\left(\\boldsymbol{y}_{1}, \\boldsymbol{y}^{*}\\right), \\boldsymbol{x}_{2}, \\hat{\\boldsymbol{y}}_{2}, \\widehat{\\boldsymbol{r}}\\left(\\boldsymbol{y}_{2}, \\boldsymbol{y}^{*}\\right)\\right\\}$, we modify the reward $\\widehat{r}\\left(\\boldsymbol{y}_{2}, \\boldsymbol{y}^{*}\\right)$ in Equation 4, at the second attempt with a bonus $\\widehat{b}\\left(\\boldsymbol{y}_{2} \\mid \\boldsymbol{y}_{1}, \\boldsymbol{y}^{*}\\right):=$ $\\alpha \\cdot\\left(\\widehat{r}\\left(\\boldsymbol{y}_{2}, \\boldsymbol{y}^{*}\\right)-\\widehat{r}\\left(\\boldsymbol{y}_{1}, \\boldsymbol{y}^{*}\\right)\\right)$, where $\\alpha$ is a positive constant multiplier, ideally larger than 1.0. Adding this bonus to the second attempt measures a notion of progress by only emphasizing transitions that flip the correctness of the response and assigns a heavy negative penalty to transitions that change a correct response to incorrect in the second attempt. Thus, the addition of this bonus should regularize the training process from collapsing on to the \"direct\" solution that also appears optimal on the training set but does not learn self-correction.\n\n### 5.3. Putting it Together and Implementation Details\n\nOur approach is illustrated pictorially in Figure 7. We detail all hyperparameters used in Appendix B. In practice, one can also use an adaptive $\\beta_{2}$ that attempts to balance the magnitudes of the first-attempt KL regularization and the second-attempt policy objective. In some of our experiments, we also choose to amplify the coverage of states used for on-policy RL by incorporating first-attempt solutions obtained by repeatedly sampling the base model as offline prompts in RL. We find that incorporating this data, especially in Stage II - where the first-turn policy may have drifted further from that of the base model can have substantial benefits especially when attempting to learn from limited data.\n\n## Takeaways and Implications\n\nThe core insight behind $\\boldsymbol{S C o R e}$ is that we must make it more attractive to learn a more nuanced algorithmic strategy (i.e., self-correction) instead of collapsing to a degenerate behavior mode. To avoid distribution shift, this must be done on self-generated online data."
    },
    {
      "markdown": "Table 2 | Performance of SCoRe on MATH. SCoRe not only attains a higher accuracy at both attempts, but also provides the most positive self-correction performance $\\Delta(\\mathbf{t 1}, \\mathbf{t 2})$.\n\n| Approach | Acc.@t1 | Acc.@t2 | $\\Delta(\\mathbf{t 1}, \\mathbf{t 2})$ | $\\Delta^{\\mathrm{i} \\rightarrow \\mathrm{c}}(\\mathbf{t 1}, \\mathbf{t 2})$ | $\\Delta^{\\mathrm{c} \\rightarrow \\mathrm{i}}(\\mathbf{t 1}, \\mathbf{t 2})$ |\n| :-- | :--: | :--: | :--: | :--: | :--: |\n| Base model | $52.6 \\%$ | $41.4 \\%$ | $-11.2 \\%$ | $4.6 \\%$ | $15.8 \\%$ |\n| Self-Refine (Madaan et al., 2023) | $52.8 \\%$ | $51.8 \\%$ | $-1.0 \\%$ | $3.2 \\%$ | $4.2 \\%$ |\n| STaR w/ $\\mathcal{D}_{\\text {StaR }}^{*}$ (Zelikman et al., 2022) | $53.6 \\%$ | $54.0 \\%$ | $0.4 \\%$ | $2.6 \\%$ | $2.2 \\%$ |\n| Pair-SFT w/ $\\mathcal{D}_{\\text {SFT }}$ (Welleck et al., 2023) | $52.4 \\%$ | $54.2 \\%$ | $1.8 \\%$ | $5.4 \\%$ | $3.6 \\%$ |\n| SCoRe (Ours) | $\\mathbf{6 0 . 0 \\%}$ | $\\mathbf{6 4 . 4 \\%}$ | $\\mathbf{4 . 4 \\%}$ | $\\mathbf{5 . 8 \\%}$ | $\\mathbf{1 . 4 \\%}$ |\n\n# 6. Experimental Evaluation \n\nThe goal of our experiments is to demonstrate the efficacy and justify the design of SCoRe in training LLMs how to self-correct by only training on their own data. To this end, we perform a comparative evaluation of $\\operatorname{SCoRe}$ against prior methods that also use self-generated data to train for self-correction, and run several ablation studies on two representative reasoning tasks where error correction is crucial.\n\nTasks. We mainly focus on reasoning problems in math and coding: (a) math problem solving on MATH (Hendrycks et al., 2021), and (b) code generation on MBPP (Austin et al., 2021) and HumanEval (Chen et al., 2021). We use the following train-test splits in our experiments: (1) MATH: following Lightman et al. (2023), we augment the MATH training set with 4500 problems from the test set, and report results on the remaining 500 problems (MATH500); and (2) Code generation: we train on MBPP and report results on HumanEval, which does not expose test cases to the model. For all tasks, we use binary rewards during training, indicating whether the model's answer matches the ground truth one (for MATH) or passes all test cases (for coding).\n\nEvaluation protocol and metrics. We report the self-correction accuracy on a number of tasks with two sequential attempts at the problem, i.e., one round of self-correction. For code generation, following the evaluation protocol of Ni et al. (2024), we also report results on MBPP-R, an offline repair task that requires correcting incorrect first-attempt programs generated from PaLM 2.\n\nModels. For all of our experiments on coding problems, we fine-tune Gemini 1.0 Pro and for MATH, we fine-tune Gemini 1.5 Flash. For all evaluations, we use greedy decoding (i.e. temperature 0 ), except for inference-compute scaling in Section 6.2 where we set temperature to be 0.7 . For all training methods, we attempted to use a fixed budget of model samples and gradient updates, and do not vary hyperparameters such as learning rate and batch size between runs. For all RL runs, we selected checkpoints with the highest training reward, although a small held-out validation set of problems can also be used.\n\nEvaluation prompts. We use zero-shot CoT prompting for evaluation on MATH, zero-shot prompting for evaluation on HumanEval, and the canonical three-shot prompt for first-attempt training samples on MBPP (Austin et al., 2021). At the second attempt, we utilize an instruction that does not reveal the correctness of the previous answer, but asks the model to attempt to deduce whether a mistake exists in its first attempt response, and if so, potentially rewrite its response. Our full prompts and self-correction instructions can be found in Appendix C.\n\nBaselines \\& comparisons. We compare SCoRe to relevant prior approaches based on prompting or those that fine-tune only a single model for both solving the task and for revising responses, and only use self-generated data. Specifically, we compare to Self-Refine (Madaan et al., 2023), a representative"
    },
    {
      "markdown": "Table 3 | Performance of SCoRe on HumanEval. SCoRe attains the highest self-correction performance (Accuracy@t2, $\\Delta(\\mathbf{t 1}$, $\\mathbf{t 2})$ ), and also outperforms other methods at offline correction (MBPP-R).\n\n| Method | MBPP-R | Acc.@t1 | Acc.@t2 | $\\Delta(\\mathbf{t 1}, \\mathbf{t 2})$ | $\\Delta^{\\mathrm{i} \\rightarrow \\mathrm{c}}(\\mathbf{t 1}, \\mathbf{t 2})$ | $\\Delta^{\\mathrm{c} \\rightarrow \\mathrm{j}}(\\mathbf{t 1}, \\mathbf{t 2})$ |\n| :-- | :--: | :--: | :--: | :--: | :--: | :--: |\n| Base model | $47.3 \\%$ | $53.7 \\%$ | $56.7 \\%$ | $3.0 \\%$ | $7.9 \\%$ | $4.9 \\%$ |\n| Self-Refine | $30.7 \\%$ | $53.7 \\%$ | $52.5 \\%$ | $-1.2 \\%$ | $9.8 \\%$ | $11.0 \\%$ |\n| Pair-SFT | $59.8 \\%$ | $56.1 \\%$ | $54.3 \\%$ | $-1.8 \\%$ | $4.3 \\%$ | $6.1 \\%$ |\n| SCoRe (Ours) | $\\mathbf{6 0 . 6 \\%}$ | $52.4 \\%$ | $\\mathbf{6 4 . 6 \\%}$ | $\\mathbf{1 2 . 2 \\%}$ | $\\mathbf{1 5 . 2 \\%}$ | $\\mathbf{3 . 0 \\%}$ |\n\nprompting-based approach to elicit self-correction behaviors from a model, akin to Reflexion (Shinn et al., 2023). Among the fine-tuning based approaches, we compare to Pair-SFT based on the approach from Welleck et al. (2023), and multi-turn STaR (Singh et al., 2023; Zelikman et al., 2022) that fine-tune the model by maximizing log-likelihood respectively on synthetically-paired repair traces (Pair-SFT) and successful repair traces (STaR).\n\n# 6.1. Benchmark Results \n\nMATH. Our results are shown in Table 2, as well as in Figure 1. SCoRe exhibits substantially stronger performance on both direct and self-correction accuracies relative to baselines. Notably, the intrinsic self-correction gain $\\Delta(\\mathbf{t 1}, \\mathbf{t 2})$ of $4.4 \\%$ is the first significantly positive delta, despite having fewer incorrect problems to correct by virtue of its higher Accuracy@t1. Relative to the base model Gemini 1.5 Flash, SCoRe improves $\\Delta(\\mathbf{t 1}, \\mathbf{t 2})$ by $15.6 \\%$, and Accuracy@t2 by $23.0 \\%$, and over the next best prior approach, Pair-SFT, by $10.2 \\%$ and $2.6 \\%$ respectively. By observing the frequency of problems that change from incorrect in the first attempt to correct in the second attempt and vice versa, we see that $\\operatorname{SCoRe}$ both improves the rate at which it fixes incorrect answers ( $14.5 \\%$, compared to $9.5 \\%$ for base) and reduces the proportion of correct answers it changes ( $15.8 \\%$ to $1.4 \\%$ ).\n\nCode generation. Our results for the code generation task are shown in Table 3. Generally, we find that SCoRe achieves both improved self-correction and offline repair performance. For MBPP-R (Ni et al., 2024), we find that SCoRe improves the base model from $47.3 \\%$ to $60.6 \\%$, which is comparable to the gap between GPT-3.5 (43\\%) and GPT-4 (63.2\\%). Despite only training on MBPP, we find that SCoRe is especially effective at generalizing to HumanEval, achieving a $\\mathbf{1 2 . 2 \\%}$ intrinsic self-correction delta, or $9 \\%$ higher than the base model. By contrast, Pair-SFT works nearly as well on the static repair task MBPP-R, but actually degrades the base model when evaluated in the self-correction setting, thus underscoring the importances of on-policy sampling for self-correction.\n\n### 6.2. Inference-Compute Scaling with Self-Correction\n\nNext, we investigate if SCoRe can be used in conjunction with inference-time compute scaling strategies. To do so, we evaluate self-consistency decoding (Wang et al., 2022), where we sample a diverse set of solutions, and then select the most consistent answer among these solutions. Typically, the default strategy is to sample all solutions in parallel to perform majority voting. However, we show in Figure 1 (right) that instead of sampling $2 K$ solutions in parallel, it is more compute-efficient to sample $K$ solutions in parallel, then perform one round of self-correction on each solution. With 32 solution budget per problem, parallel sampling shows a $7.4 \\%$ accuracy gain, while combining it with sequential sampling using self-correction yields a $10.5 \\%$ improvement."
    },
    {
      "markdown": "Table 4 | Ablation studies to understand the impact of various components in SCoRe. Observe that while single-turn training is effective at optimizing the first-attempt accuracy of the model, it leads to degradation in the second attempt. The performance improvements without Stage I or without reward shaping in $\\boldsymbol{S C o R e}$ are small when measured by the difference in accuracy over the two attempts. Utilizing STaR generally leads to worse performance even when it is run from an effective Stage I checkpoint.\n\n| Method | Accuracy@t1 | Accuracy@t2 | $\\Delta(\\mathrm{t} 1, \\mathrm{t} 2)$ |\n| :-- | :--: | :--: | :--: |\n| SCoRe (Ours) | $60.0 \\%$ | $\\mathbf{6 4 . 4 \\%}$ | $\\mathbf{4 . 4 \\%}$ |\n| w/o multi-turn training | $\\mathbf{6 1 . 8 \\%}$ | $59.4 \\%$ | $-2.4 \\%$ |\n| w/o Stage I | $59.2 \\%$ | $61.4 \\%$ | $2.2 \\%$ |\n| w/o reward shaping | $60.0 \\%$ | $62.6 \\%$ | $2.6 \\%$ |\n| w/ STaR instead of REINFORCE Stage II | $56.2 \\%$ | $58.4 \\%$ | $2.2 \\%$ |\n\n# 6.3. Ablation Studies: Understanding the Impact of SCoRe Components \n\nFinally, we also present a number of ablation studies to understand the importance of various components in SCoRe. We perform these ablations on the MATH dataset. Concretely, we aim to answer the following questions: (1) the importance of multi-turn training: Can RL trained to maximize single-turn performance achieve better accuracy@t1 or accuracy@t2?; (2) the importance of multi-stage training: How essential is Stage I to SCoRe? In other words, why not run Stage II directly?; (3) the impact of reward shaping. How would removing the reward shaping terms affect performance of SCoRe in Stage II, assuming Stage I was done identically?; (4) the importance of on-policy RL: What if we replaced REINFORCE in Stage II with STaR?.\n\nThe results of all of these ablation experiments are shown in Table 4. As expected, single-turn training improves turn 1 performance, but has negative $\\Delta(\\mathbf{t 1}, \\mathbf{t 2})$. As shown in Figure 6, Stage I is critical to SCoRe; without it, the model achieves $2 \\%$ lower $\\Delta(\\mathbf{t 1}, \\mathbf{t 2})$ and $3 \\%$ lower accuracy@t2. Similarly, we find that removing reward shaping also hurts performance, indicating that the RL objectives in both stages play a significant role in teaching the self-correction behavior. We also find that replacing REINFORCE with STaR in Stage II results in significantly lower absolute performance with no visible improvements in self-improvement performance, which contrasts with the findings in Havrilla et al. (2024a) that STaR and on-policy RL have similar convergence rates for single-turn RL. This suggests that leveraging on-policy samples is especially critical in the self-correction setting, which presents a multi-turn problem that admits potentially spurious solutions.\n\n## 7. Discussion, Limitations, and Conclusion\n\nIn this work, we investigated how to imbue LLMs with self-correction behavior that enables them to correct their own responses on the fly. To accomplish this, we proposed SCoRe, a multi-turn RL approach, and demonstrated through extensive evaluations that it is one of the first methods to attain significantly positive intrinsic self-correction performance. To do so, we rigorously analyzed the behavior of various SFT approaches and identified failure modes in which the model learns a non-correcting strategy (e.g. learning to make no edits; behavior collapse) or falls prey to distribution shift. SCoRe trains a self-correcting strategy by utilizing a two-stage design and reward shaping, both of which help preventing behavior collapse into not learning effective self-corrective behavior. SCoRe has limitations that also provide avenues for future work. We did not train SCoRe for more than one round of iterative self-correction due"
    },
    {
      "markdown": "to infrastructural reasons, which means that subsequent rounds may not be as effective as the first. Future work should train with more than two attempts via RL, which is already a common and effective practice to obtain effective self-correction behavior over more than two rounds with SFT (Qu et al., 2024; Snell et al., 2024). Unifying Stages I and II would also be interesting, since it would alleviate the limitation of running multiple runs. Finally, our results suggest that learning meta-strategies (e.g., self-correction) might require going beyond standard LLM fine-tuning (Section 4), and incorporate regularization (e.g., progress reward).\n\n# Acknowledgements \n\nThe authors would like to thank Satinder Baveja, Kalesha Bullard, Gheorghe Comanici, Claire Cui, Valentin Dalibard, Angelos Filos, Yang Gao, Zoubin Ghahramani, Izzeddin Gur, Raia Hadsell, Clara Huiyi Hu, Melvin Johnson, Mina Khan, Balaji Lakshminarayanan, Yiran Mao, Hussain Masoom, Junhyuk Oh, Jordi Orbay, David Silver, and Yury Sulsky for helpful discussions, feedback, and sponsorship. We thank Amrith Setlur, Yuxiao Qu, Charlie Snell, Tianhe Yu, and Xinyang (Young) Geng for helpful discussions and feedback on an earlier version of the paper.\n\n## Author Contributions\n\nAK and VZ led the paper, with substantial technical contributions from RA and YS. VZ led the experimentation in the final paper with AK, with support from RA and YS. AK and RA conceived the initial idea with advice and discussions from DS, FB, AF, JDC, AS, and GT. JDC, YS, AS, RA, and AK iterated on the methodology. The development of the final method was done by AK and VZ, with inputs from RA and FB. VZ led the infrastructure development, while RA, YS, CP, SI, KB, DS, and LMZ contributed to the infrastructure. AK, RA, FB, AF, DP, GT advised on the overall direction. AK and VZ wrote the manuscript, with input from all co-authors. KM provided program management. FB, and AF co-supervised the project.\n\n## References\n\nA. Ahmadian, C. Cremer, M. Gallé, M. Fadaee, J. Kreutzer, A. Üstün, and S. Hooker. Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms. arXiv preprint arXiv:2402.14740, 2024.\nA. F. Akyürek, E. Akyürek, A. Madaan, A. Kalyan, P. Clark, D. Wijaya, and N. Tandon. Rl4f: Generating natural language feedback with reinforcement learning for repairing model outputs. arXiv preprint arXiv:2305.08844, 2023.\nJ. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry, Q. Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.\nM. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. D. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.\nX. Chen, M. Lin, N. Schärli, and D. Zhou. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128, 2023."
    },
    {
      "markdown": "J. Farebrother, J. Orbay, Q. Vuong, A. A. Taiga, Y. Chebotar, T. Xiao, A. Irpan, S. Levine, P. S. Castro, A. Faust, et al. Stop regressing: Training value functions via classification for scalable deep rl. arXiv preprint arXiv:2403.03950, 2024.\nA. Havrilla, Y. Du, S. C. Raparthy, C. Nalmpantis, J. Dwivedi-Yu, M. Zhuravinskyi, E. Hambro, S. Sukhbaatar, and R. Raileanu. Teaching large language models to reason with reinforcement learning. arXiv preprint arXiv:2403.04642, 2024a.\nA. Havrilla, S. Raparthy, C. Nalmpantis, J. Dwivedi-Yu, M. Zhuravinskyi, E. Hambro, and R. Railneau. Glore: When, where, and how to improve llm reasoning via global and local refinements. arXiv preprint arXiv:2402.10963, 2024b.\nD. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021.\nJ. Hong, N. Lee, and J. Thorne. Reference-free monolithic preference optimization with odds ratio. arXiv preprint arXiv:2403.07691, 2024.\nJ. Huang, X. Chen, S. Mishra, H. S. Zheng, A. W. Yu, X. Song, and D. Zhou. Large language models cannot self-correct reasoning yet. arXiv preprint arXiv:2310.01798, 2023.\nN. Jain, K. Han, A. Gu, W.-D. Li, F. Yan, T. Zhang, S. Wang, A. Solar-Lezama, K. Sen, and I. Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024.\nR. Kamoi, Y. Zhang, N. Zhang, J. Han, and R. Zhang. When can llms actually correct their own mistakes? a critical survey of self-correction of llms. arXiv preprint arXiv:2406.01297, 2024.\nG. Kim, P. Baldi, and S. McAleer. Language models can solve computer tasks. arXiv preprint arXiv:2303.17491, 2023.\nH. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever, and K. Cobbe. Let's verify step by step. arXiv preprint arXiv:2305.20050, 2023.\nX. Liu, H. Yu, H. Zhang, Y. Xu, X. Lei, H. Lai, Y. Gu, H. Ding, K. Men, K. Yang, et al. Agentbench: Evaluating llms as agents. arXiv preprint arXiv:2308.03688, 2023.\nA. Lozhkov, R. Li, L. B. Allal, F. Cassano, J. Lamy-Poirier, N. Tazi, A. Tang, D. Pykhtar, J. Liu, Y. Wei, et al. Starcoder 2 and the stack v2: The next generation. arXiv preprint arXiv:2402.19173, 2024.\nA. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye, Y. Yang, et al. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023.\nA. Ni, M. Allamanis, A. Cohan, Y. Deng, K. Shi, C. Sutton, and P. Yin. Next: Teaching large language models to reason about code execution. arXiv preprint arXiv:2404.14662, 2024.\nT. X. Olausson, J. P. Inala, C. Wang, J. Gao, and A. Solar-Lezama. Is self-repair a silver bullet for code generation? In The Twelfth International Conference on Learning Representations, 2023."
    },
    {
      "markdown": "L. Pan, M. Saxon, W. Xu, D. Nathani, X. Wang, and W. Y. Wang. Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies. arXiv preprint arXiv:2308.03188, 2023.\nD. Paul, M. Ismayilzada, M. Peyrard, B. Borges, A. Bosselut, R. West, and B. Faltings. Refiner: Reasoning feedback on intermediate representations. arXiv preprint arXiv:2304.01904, 2023.\nY. Qu, T. Zhang, N. Garg, and A. Kumar. Recursive introspection: Teaching language model agents how to self-improve. arXiv preprint arXiv:2407.18219, 2024.\nW. Saunders, C. Yeh, J. Wu, S. Bills, L. Ouyang, J. Ward, and J. Leike. Self-critiquing models for assisting human evaluators. arXiv preprint arXiv:2206.05802, 2022.\nL. Shani, A. Rosenberg, A. Cassel, O. Lang, D. Calandriello, A. Zipori, H. Noga, O. Keller, B. Piot, I. Szpektor, et al. Multi-turn reinforcement learning from preference human feedback. arXiv preprint arXiv:2405.14655, 2024.\nZ. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, M. Zhang, Y. Li, Y. Wu, and D. Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.\nN. Shinn, B. Labash, and A. Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection. arXiv preprint arXiv:2303.11366, 2023.\nA. Singh, J. D. Co-Reyes, R. Agarwal, A. Anand, P. Patil, P. J. Liu, J. Harrison, J. Lee, K. Xu, A. Parisi, et al. Beyond human data: Scaling self-training for problem-solving with language models. arXiv preprint arXiv:2312.06585, 2023.\nA. Singh, J. D. Co-Reyes, R. Agarwal, A. Anand, P. Patil, X. Garcia, P. J. Liu, J. Harrison, J. Lee, K. Xu, A. Parisi, A. Kumar, A. Alemi, A. Rizkowsky, A. Nova, B. Adlam, B. Bohnet, G. Elsayed, H. Sedghi, I. Mordatch, I. Simpson, I. Gur, J. Snoek, J. Pennington, J. Hron, K. Kenealy, K. Swersky, K. Mahajan, L. Culp, L. Xiao, M. L. Bileschi, N. Constant, R. Novak, R. Liu, T. Warkentin, Y. Qian, Y. Bansal, E. Dyer, B. Neyshabur, J. Sohl-Dickstein, and N. Fiedel. Beyond human data: Scaling self-training for problem-solving with language models, 2024.\nC. Snell, I. Kostrikov, Y. Su, M. Yang, and S. Levine. Offline rl for natural language generation with implicit language q learning. arXiv preprint arXiv:2206.11871, 2022.\nC. Snell, J. Lee, K. Xu, and A. Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024.\nC. Team. Codegemma: Open code models based on gemma. arXiv preprint arXiv:2406.11409, 2024.\nG. Tyen, H. Mansoor, V. Cărbune, Y. P. Chen, and T. Mak. Llms cannot find reasoning errors, but can correct them given the error location. In Findings of the Association for Computational Linguistics ACL 2024, pages 13894-13908, 2024.\nJ. Uesato, N. Kushman, R. Kumar, F. Song, N. Siegel, L. Wang, A. Creswell, G. Irving, and I. Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022."
    },
    {
      "markdown": "X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.\nS. Welleck, X. Lu, P. West, F. Brahman, T. Shen, D. Khashabi, and Y. Choi. Generating sequences by learning to self-correct. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=hH36JeQZDa0.\nW. Xiong, C. Shi, J. Shen, A. Rosenberg, Z. Qin, D. Calandriello, M. Khalman, R. Joshi, B. Piot, M. Saleh, et al. Building math agents with multi-turn iterative preference learning. arXiv preprint arXiv:2409.02392, 2024.\nS. Yang, E. Gribovskaya, N. Kassner, M. Geva, and S. Riedel. Do large language models latently perform multi-hop reasoning? arXiv preprint arXiv:2402.16837, 2024.\nS. Ye, Y. Jo, D. Kim, S. Kim, H. Hwang, and M. Seo. Selfee: Iterative self-revising llm empowered by self-feedback generation. Blog post, 2023.\nT. Ye, Z. Xu, Y. Li, and Z. Allen-Zhu. Physics of language models: Part 2.2, how to learn from mistakes on grade-school math problems, 2024. URL https://arxiv.org/abs/2408.16293.\nM. Yin, G. Tucker, M. Zhou, S. Levine, and C. Finn. Meta-learning without memorization. arXiv preprint arXiv:1912.03820, 2019.\nE. Zelikman, Y. Wu, J. Mu, and N. Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:15476-15488, 2022.\nY. Zhang, M. Khalifa, L. Logeswaran, J. Kim, M. Lee, H. Lee, and L. Wang. Small language models need strong verifiers to self-correct reasoning. arXiv preprint arXiv:2404.17140, 2024.\nH. S. Zheng, S. Mishra, H. Zhang, X. Chen, M. Chen, A. Nova, L. Hou, H.-T. Cheng, Q. V. Le, E. H. Chi, et al. Natural plan: Benchmarking llms on natural language planning. arXiv preprint arXiv:2406.04520, 2024.\nY. Zhou, A. Zanette, J. Pan, S. Levine, and A. Kumar. Archer: Training language model agents via hierarchical multi-turn rl. arXiv preprint arXiv:2402.19446, 2024."
    },
    {
      "markdown": "# Appendices \n\n## A. Additional Experiments\n\n## A.1. Scaling to Multiple Attempts\n\n![img-6.jpeg](img-6.jpeg)\n\nFigure 8 | Performance of the base model, Pair-SFT, and SCoRe over 10 attempts on MATH.\nWe investigate the performance of various models when asked to iteratively self-correct over multiple attempts, despite only being trained over two attempts (or not at all, in the case of the base model). As shown in Figure 8, we find that the performance of the base Gemini 1.5 Flash model is quite noisy, but never surpasses that of the first attempt. Similarly, Pair-SFT does not improve past the second attempt. By contrast, the performance of $\\boldsymbol{S C o R e}$ increases slightly past two turns, although it does plateau likely because the distribution over responses shifts quickly as more revision attempts are performed. We leave improving the scaling properties of self-correction, a form of meta-learning, to future work.\n\n## A.2. Reward Function Design\n\n![img-7.jpeg](img-7.jpeg)\n\nFigure $9 \\mid$ Impact of discount factor of $\\gamma$ on standard multi-turn RL training.\nIn all of our experiments, we used only the instantaneous reward in our policy gradient objective, which is equivalent to returns with discount factor $\\gamma=0$. We additionally investigated whether leveraging"
    },
    {
      "markdown": "$\\gamma>0$, in conjunction with reward shaping, can elicit self-correction paper. As presented in Figure 9, we find that with $\\gamma=0.8$ and $\\alpha=1.0$, multi-turn RL still suffers from the same non-correcting behavior collapse as the standard multi-turn RL approach.\n\n# B. Additional Experiment Details \n\nTable $5 \\mid$ Hyperparameters for SCoRe on MATH (left) and MBPP (right)\n\n| Hyperparameter | Value |\n| :-- | :-- |\n| Base model | Gemini 1.5 Flash |\n| Optimizer | Adam |\n| Learning rate | $5 \\mathrm{e}-6$ |\n| Training steps | 3000 |\n| Batch size | 512 |\n| Sampling temperature | 1.0 |\n| $\\alpha$ | 10 |\n| $\\beta_{1}$ | 0.01 |\n| $\\beta_{2}$ | 0.1 |\n\n\n| Hyperparameter | Value |\n| :-- | :-- |\n| Base model | Gemini 1.0 Pro |\n| Optimizer | Adam |\n| Learning rate | $1 \\mathrm{e}-5$ |\n| Training steps | 1500 |\n| Batch size | 128 |\n| Sampling temperature | 1.0 |\n| $\\alpha$ | 10 |\n| $\\beta_{1}$ | 0.01 |\n| $\\beta_{2}$ | 0.25 |\n\nWe include the hyperparameters used for training $\\boldsymbol{S C o R e}$ in Table 5.\n\n## C. Prompts\n\n## MATH Zero-shot Prompt\n\nYou are a math expert. When you respond, respond only with the Solution of the final Problem, thinking step by step. At the end of the Solution, when you give your final answer, write it in the form \"Final Answer: The final answer is \\$answer\\$. I hope it is correct.\"\n\n## MATH Self-Correction Instruction\n\nThere might be an error in the solution above because of lack of understanding of the question. Please correct the error, if any, and rewrite the solution. Only output the final solution! At the end of the Solution, when you give your final answer, write it in the form \"Final Answer: The final answer is \\$answer\\$. I hope it is correct.\"\n\n## MBPP 3-shot Prompt\n\nYou are an expert Python programmer, and here is your task: Write a function to find the similar elements from the given two tuple lists. Your code should pass these tests:\nassert similar_elements $((3,4,5,6),(5,7,4,10))==(4,5)$\nassert similar_elements $((1,2,3,4),(5,4,3,7))==(3,4)$\nassert similar_elements $((11,12,14,13),(17,15,14,13))==(13,14)$\n[BEGIN]"
    },
    {
      "markdown": "```\ndef similar_elements(test_tup1, test_tup2):\n    res = tuple(set(test_tup1) & set(test_tup2))\n    return (res)\n[DONE]\nYou are an expert Python programmer, and here is your task: Write a python function to identify non-prime\nnumbers. Your code should pass these tests:\nassert is_not_prime(2) == False\nassert is_not_prime(10) == True\nassert is_not_prime(35) == True\n[BEGIN]\nimport math\ndef is_not_prime(n):\n    result = False\n    for i in range(2,int(math.sqrt(n)) + 1):\n        if n%\n            result = True\n    return result\n[DONE]\nYou are an expert Python programmer, and here is your task: Write a function to find the largest integers from a\ngiven list of numbers using heap queue algorithm. Your code should pass these tests:\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],3) ==[85, 75, 65]\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],2) ==[85, 75]\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],5) ==[85, 75, 65, 58, 35]\n[BEGIN]\nimport heapq as hq\ndef heap_queue_largest(nums,n):\n    largest_nums = hq.nlargest(n, nums)\n    return largest_nums\n[DONE]\n```\n\n\n# MBPP/HumanEval Self-Correction Instruction \n\n\\# There might be an error in the code above because of lack of understanding of the question. Please correct the error, if any, and rewrite the solution. Only output the final correct Python program!\n\n## D. Qualitative Analysis of SCoRe\n\nWe also perform a qualitative investigation into how $\\boldsymbol{S C o R e}$ addresses the self-repair shortcomings of base LLMs, and provide several examples in Appendix E. We find that $\\boldsymbol{S C o R e}$ is able to refine its own responses in a variety of manners - rewriting the entire solution when necessary, or reproducing the"
    },
    {
      "markdown": "correct parts of the solution, while revising the incorrect ones. For the latter, we interestingly find that $\\boldsymbol{S C o R e}$ is especially adept at revising its computational mistakes, and even demonstrates a bias towards showing more steps in certain computations and manipulations in order to increase its probability of producing a correct answer. We additionally observe that the model learns to occasionally self-correct within a turn, e.g. MATH example 4.\n\n# E. Examples of SCoRe Self-Correction \n\n## HumanEval Example 1\n\n## Problem:\n\ndef cycpattern_check(a, b):\n${ }^{n m}$ You are given 2 words. You need to return True if the second word or any of its rotations is a substring in the first word\ncycpattern_check(\"abcd\",\"abd\") => False\ncycpattern_check(\"hello\",\"ell\") => True\ncycpattern_check(\"whassup\",\"psus\") => False\ncycpattern_check(\"abab\",\"baa\") => True\ncycpattern_check(\"efef\",\"eeff\") => False\ncycpattern_check(\"himenss\",\"simen\") => True\n${ }^{m n}$\n\n## Turn 1 solution (incorrect):\n\ndef cycpattern_check(a, b):\n${ }^{m n}$ You are given 2 words. You need to return True if the second word or any of its rotations is a substring in the first word\ncycpattern_check(\"abcd\",\"abd\") => False\ncycpattern_check(\"hello\",\"ell\") => True\ncycpattern_check(\"whassup\",\"psus\") => False\ncycpattern_check(\"abab\",\"baa\") => True\ncycpattern_check(\"efef\",\"eeff\") => False\ncycpattern_check(\"himenss\",\"simen\") => True\n${ }^{m n}$\nfor i in range(len(b)):\nif a.find(b[i:])!=-1:\nreturn True\nreturn False\nTurn 2 solution (correct):\ndef cycpattern_check(a, b):\n${ }^{m n}$ You are given 2 words. You need to return True if the second word or any of its rotations is a substring in the first word\ncycpattern_check(\"abcd\",\"abd\") => False\ncycpattern_check(\"hello\",\"ell\") => True\ncycpattern_check(\"whassup\",\"psus\") => False\ncycpattern_check(\"abab\",\"baa\") => True\ncycpattern_check(\"efef\",\"eeff\") => False\ncycpattern_check(\"himenss\",\"simen\") => True"
    },
    {
      "markdown": "```\n    min\nfor i in range(len(b)):\n    rotated_b = b[i:] + b[:i]\n    if a.find(rotated_b) != -1:\n        return True\nreturn False\n```\n\n\n# HumanEval Example 2 \n\n## Problem:\n\ndef even_odd_count(num):\n${ }^{\\text {mnn }}$ Given an integer. return a tuple that has the number of even and odd digits respectively.\n\n## Example:\n\neven_odd_count $(-12)==>(1,1)$\neven_odd_count(123) $==>(1,2)$\n$\\mathrm{mnn}$\nTurn 1 solution (incorrect):\ndef even_odd_count(num):\n${ }^{\\text {mnn }}$ Given an integer. return a tuple that has the number of even and odd digits respectively.\n\n## Example:\n\neven_odd_count $(-12)==>(1,1)$\neven_odd_count(123) $==>(1,2)$\n$\\mathrm{mnn}$\neven_count $=0$\nodd_count $=0$\nfor digit in str(num):\nint_digit $=$ int(digit)\nif int_digit $\\%$\neven_count $+=1$\nelse:\nodd_count $+=1$\nreturn even_count, odd_count\nTurn 2 solution (correct):\ndef even_odd_count(num):\n${ }^{\\text {mnn }}$ Given an integer. return a tuple that has the number of even and odd digits respectively.\n\n## Example:\n\neven_odd_count $(-12)==>(1,1)$\neven_odd_count(123) $==>(1,2)$\n$\\mathrm{mnn}$\neven_count $=0$\nodd_count $=0$"
    },
    {
      "markdown": "for digit in $\\operatorname{str}(\\operatorname{abs}$ (num)):\nint_digit $=$ int(digit)\nif int_digit $\\%$\neven_count $+=1$\nelse:\nodd_count $+=1$\nreturn even_count, odd_count"
    },
    {
      "markdown": "# MATH Example 1 \n\nProblem. Let $A$ be a matrix such that\n\nFind $A\\left(\\begin{array}{l}3 \\\\ 1 \\\\ 0\\end{array}\\right)=\\left(\\begin{array}{c}2 \\\\ 4 \\\\ -3\\end{array}\\right) \\quad$ and $\\quad A\\left(\\begin{array}{c}-5 \\\\ 2 \\\\ 2\\end{array}\\right)=\\left(\\begin{array}{c}0 \\\\ -5 \\\\ -5\\end{array}\\right)$.\nSCoRe turn 1 solution (incorrect). We can write\n\n$$\n\\left(\\begin{array}{c}\n-13 \\\\\n3 \\\\\n4\n\\end{array}\\right)=a\\left(\\begin{array}{l}\n3 \\\\\n1 \\\\\n0\n\\end{array}\\right)+b\\left(\\begin{array}{c}\n-5 \\\\\n2 \\\\\n2\n\\end{array}\\right)\n$$\n\nfor some scalars $a$ and $b$. Solving the system of equations\n\n$$\n\\begin{aligned}\n3 a-5 b & =-13 \\\\\na+2 b & =3 \\\\\n2 b & =4\n\\end{aligned}\n$$\n\nwe find $a=-1$ and $b=2$. Therefore,\n\n$$\nA\\left(\\begin{array}{c}\n-13 \\\\\n3 \\\\\n4\n\\end{array}\\right)=A\\left(-1\\left(\\begin{array}{l}\n3 \\\\\n1 \\\\\n0\n\\end{array}\\right)+2\\left(\\begin{array}{c}\n-5 \\\\\n2 \\\\\n2\n\\end{array}\\right)\\right)=-1 A\\left(\\begin{array}{l}\n3 \\\\\n1 \\\\\n0\n\\end{array}\\right)+2 A\\left(\\begin{array}{c}\n-5 \\\\\n2 \\\\\n2\n\\end{array}\\right)=-1\\left(\\begin{array}{c}\n2 \\\\\n4 \\\\\n-3\n\\end{array}\\right)+2\\left(\\begin{array}{c}\n0 \\\\\n-5 \\\\\n-5\n\\end{array}\\right)=\\left[\\begin{array}{c}\n-2 \\\\\n-14 \\\\\n-13\n\\end{array}\\right]\n$$\n\nFinal Answer: The final answer is $\\left(\\begin{array}{c}-2 \\\\ -14 \\\\ -13\\end{array}\\right)$. I hope it is correct.\nSCoRe turn 2 solution (correct). We can write\n\n$$\n\\left(\\begin{array}{c}\n-13 \\\\\n3 \\\\\n4\n\\end{array}\\right)=a\\left(\\begin{array}{l}\n3 \\\\\n1 \\\\\n0\n\\end{array}\\right)+b\\left(\\begin{array}{c}\n-5 \\\\\n2 \\\\\n2\n\\end{array}\\right)\n$$\n\nfor some scalars $a$ and $b$. This gives us the system of equations:\n\n$$\n\\begin{aligned}\n3 a-5 b & =-13 \\\\\na+2 b & =3 \\\\\n2 b & =4\n\\end{aligned}\n$$\n\nSolving the third equation, we get $b=2$. Substituting into the second equation, we get $a=-1$. We can check that these values of $a$ and $b$ also satisfy the first equation.\nTherefore,\n$A\\left(\\begin{array}{c}-13 \\\\ 3 \\\\ 4\\end{array}\\right)=A\\left(-1\\left(\\begin{array}{l}3 \\\\ 1 \\\\ 0\\end{array}\\right)+2\\left(\\begin{array}{c}-5 \\\\ 2 \\\\ 2\\end{array}\\right)\\right)=-1 A\\left(\\begin{array}{l}3 \\\\ 1 \\\\ 0\\end{array}\\right)+2 A\\left(\\begin{array}{c}-5 \\\\ 2 \\\\ 2\\end{array}\\right)=-1\\left(\\begin{array}{c}2 \\\\ 4 \\\\ -3\\end{array}\\right)+2\\left(\\begin{array}{c}0 \\\\ -5 \\\\ -5\\end{array}\\right)=\\left(\\begin{array}{c}-2 \\\\ -4 \\\\ 3\\end{array}\\right)+\\left(\\begin{array}{c}0 \\\\ -10 \\\\ -10\\end{array}\\right)=\\left[\\begin{array}{c}-2 \\\\ -14 \\\\ -7\\end{array}\\right]$.\nFinal Answer: The final answer is $\\left(\\begin{array}{c}-2 \\\\ -14 \\\\ -7\\end{array}\\right)$. I hope it is correct."
    },
    {
      "markdown": "# MATH Example 2 \n\nProblem. Simplify $\\frac{1}{5} \\cdot \\frac{8}{7} \\div \\frac{12}{20}$.\nSCoRe turn 1 solution (incorrect). First, we can simplify the division by multiplying by the reciprocal of the divisor:\n\n$$\n\\frac{1}{5} \\cdot \\frac{8}{7} \\div \\frac{12}{20}=\\frac{1}{5} \\cdot \\frac{8}{7} \\cdot \\frac{20}{12}\n$$\n\nNext, we can simplify by canceling common factors:\n\n$$\n\\frac{1}{5} \\cdot \\frac{8}{7} \\cdot \\frac{20}{12}=\\frac{1 \\cdot 8 \\cdot 20}{5 \\cdot 7 \\cdot 12}=\\frac{1 \\cdot 2 \\cdot 4 \\cdot 5 \\cdot 4}{5 \\cdot 7 \\cdot 2 \\cdot 6}=\\frac{4 \\cdot 4}{7 \\cdot 6}=\\frac{2 \\cdot 2 \\cdot 2 \\cdot 2}{7 \\cdot 2 \\cdot 3}=\\frac{2 \\cdot 2}{7 \\cdot 3}\n$$\n\nFinally, we can multiply the remaining factors:\n\n$$\n\\frac{2 \\cdot 2}{7 \\cdot 3}=\\frac{4}{21}\n$$\n\nFinal Answer: The final answer is $\\frac{4}{21}$. I hope it is correct.\nSCoRe turn 2 solution (correct).\n\n$$\n\\frac{1}{5} \\cdot \\frac{8}{7} \\div \\frac{12}{20}=\\frac{1}{5} \\cdot \\frac{8}{7} \\cdot \\frac{20}{12}=\\frac{1 \\cdot 8 \\cdot 20}{5 \\cdot 7 \\cdot 12}=\\frac{1 \\cdot 8 \\cdot 4 \\cdot 5}{5 \\cdot 7 \\cdot 4 \\cdot 3}=\\frac{1 \\cdot 8}{7 \\cdot 3}=\\frac{8}{21}\n$$\n\nFinal Answer: The final answer is $\\frac{8}{21}$. I hope it is correct.\n\n## MATH Example 5\n\nProblem. Suppose the polynomial\n\n$$\nf(x)=a_{n} x^{n}+a_{n-1} x^{n-1}+\\cdots+a_{2} x^{2}+a_{1} x+a_{0}\n$$\n\nhas integer coefficients, and its roots are distinct integers.\nGiven that $a_{n}=2$ and $a_{0}=66$, what is the least possible value of $\\left|a_{n-1}\\right|$ ?\nSCoRe turn 1 solution (incorrect). Let the roots of $f(x)$ be $r_{1}, r_{2}, \\ldots, r_{n}$. Then by Vieta's formulas,\n\n$$\nr_{1}+r_{2}+\\cdots+r_{n}=-\\frac{a_{n-1}}{a_{n}}=-\\frac{a_{n-1}}{2}\n$$\n\nSince the roots are distinct integers, $r_{1}+r_{2}+\\cdots+r_{n}$ is an integer. Thus, $a_{n-1}$ must be even. Also,\n\n$$\nr_{1} r_{2} \\ldots r_{n}=\\frac{(-1)^{n} a_{0}}{a_{n}}=\\frac{(-1)^{n} \\cdot 66}{2}=(-1)^{n} \\cdot 33\n$$\n\nSince the roots are integers, $r_{1} r_{2} \\ldots r_{n}$ is an integer. Thus, $n$ must be odd.\nSince $n$ is odd, the sum of the roots is an integer, and $a_{n-1}$ is even, the least possible value of $\\left|a_{n-1}\\right|$ is 2 . Final Answer: The final answer is 2 . I hope it is correct.\n\nSCoRe turn 2 solution (correct). Let the roots of $f(x)$ be $r_{1}, r_{2}, \\ldots, r_{n}$. Then by Vieta's formulas,\n\n$$\nr_{1}+r_{2}+\\cdots+r_{n}=-\\frac{a_{n-1}}{a_{n}}=-\\frac{a_{n-1}}{2}\n$$\n\nSince the roots are distinct integers, $r_{1}+r_{2}+\\cdots+r_{n}$ is an integer. Thus, $a_{n-1}$ must be even."
    },
    {
      "markdown": "Also,\n\n$$\nr_{1} r_{2} \\ldots r_{n}=\\frac{(-1)^{n} a_{0}}{a_{n}}=\\frac{(-1)^{n} \\cdot 66}{2}=(-1)^{n} \\cdot 33\n$$\n\nSince the roots are integers, $r_{1} r_{2} \\ldots r_{n}$ is an integer. Since 33 has a factor of 11 , one of the roots must be a multiple of 11 . Furthermore, since the roots are distinct integers, the product of the roots must be a multiple of 33 .\nWe want to minimize $\\left|a_{n-1}\\right|$, which is the absolute value of the sum of the roots. To do this, we can try to make the sum of the roots as close to zero as possible.\nSince the product of the roots is $\\pm 33$, we can consider the following cases:\n\n* **Case 1:** $n=3$ and the roots are $-11,1$, and 3 . In this case, the sum of the roots is -7 , and $\\left|a_{n-1}\\right|=14$.\n** Case 2:** $n=5$ and the roots are $-11,-3,-1,1$, and 3 . In this case, the sum of the roots is -11 , and $\\left|a_{n-1}\\right|=22$.\n***Case 3:** $n=7$ and the roots are $-11,-3,-1,1,3,5$, and 7 . In this case, the sum of the roots is -11 , and $\\left|a_{n-1}\\right|=22$.\nWe can continue this pattern, but we notice that the absolute value of the sum of the roots keeps increasing. Therefore, the least possible value of $\\left|a_{n-1}\\right|$ is 14 .\nFinal Answer: The final answer is 14. I hope it is correct.\n\n\n# MATH Example 4 \n\nProblem. Find the minimum value of\n\n$$\n\\frac{(x+5)(x+2)}{x+1}\n$$\n\nfor $x>0$.\nSCoRe turn 1 solution (incorrect). We can write\n\n$$\n\\begin{aligned}\n\\frac{(x+5)(x+2)}{x+1} & =\\frac{x^{2}+7 x+10}{x+1} \\\\\n& =\\frac{\\left(x^{2}+7 x+10\\right)-\\left(x^{2}+x\\right)}{x+1} \\\\\n& =\\frac{6 x+10}{x+1} \\\\\n& =\\frac{(6 x+6)+4}{x+1} \\\\\n& =6+\\frac{4}{x+1}\n\\end{aligned}\n$$\n\nSince $x>0, x+1>1$, so $\\frac{4}{x+1}<4$. Therefore,\n\n$$\n\\frac{(x+5)(x+2)}{x+1}<6+4=10\n$$\n\nEquality occurs when $x=0$, so the minimum value is 6 .\nFinal Answer: The final answer is 6 . I hope it is correct."
    },
    {
      "markdown": "SCoRe turn 2 solution (correct). We can write\n\n$$\n\\begin{aligned}\n\\frac{(x+5)(x+2)}{x+1} & =\\frac{x^{2}+7 x+10}{x+1} \\\\\n& =\\frac{\\left(x^{2}+x\\right)+(6 x+6)+4}{x+1} \\\\\n& =\\frac{x(x+1)+6(x+1)+4}{x+1} \\\\\n& =\\frac{(x+1)(x+6)+4}{x+1} \\\\\n& =x+6+\\frac{4}{x+1}\n\\end{aligned}\n$$\n\nSince $x>0, x+1>1$, so $\\frac{4}{x+1}<4$. Therefore,\n\n$$\n\\frac{(x+5)(x+2)}{x+1}>x+6\n$$\n\nSince $x>0, x+6>6$. Thus, the minimum value is greater than 6 .\nFurthermore, as $x$ increases, the term $\\frac{4}{x+1}$ decreases, so the expression $\\frac{(x+5)(x+2)}{x+1}$ decreases. Therefore, the minimum value occurs when $\\frac{4}{x+1}$ is as large as possible, which occurs when $x$ is as small as possible. Since $x>0$, the minimum value is\n\n$$\n6+\\frac{4}{0+1}=10\n$$\n\nEquality occurs when $x=0$, but this contradicts the condition $x>0$. Therefore, the minimum value does not exist.\nWe made an error in assuming that the minimum value occurs when $\\frac{4}{x+1}$ is as large as possible. Instead, we should minimize the entire expression. Since $x>0$, the term $\\frac{4}{x+1}$ is always positive. Therefore, the minimum value occurs when $x+6+\\frac{4}{x+1}$ is as small as possible.\nBy AM-GM,\n\n$$\nx+6+\\frac{4}{x+1}=(x+1)+\\frac{4}{x+1}+5 \\geq 2 \\sqrt{(x+1) \\cdot \\frac{4}{x+1}}+5=9\n$$\n\nEquality occurs when $(x+1)=\\frac{4}{x+1}$, so $x^{2}+2 x+1=4$, which means $x^{2}+2 x-3=0$. This factors as $(x+3)(x-1)=0$, so $x=1$ (since $x>0$ ).\nTherefore, the minimum value is 9 .\nFinal Answer: The final answer is 9 . I hope it is correct.\n\n# MATH Example 5 \n\nProblem. Four points, $A, B, C$, and $D$, are chosen randomly and independently on the circumference of a circle. What is the probability that segments $A B$ and $C D$ intersect?\n\nSCoRe turn 1 solution (incorrect). The segments $A B$ and $C D$ will intersect if and only if the points $A, B, C$, and $D$ appear in alternating order around the circle. There are 4 ! ways to order the points, and 2 ways to divide the points into two pairs. Therefore, the probability that the segments intersect is $\\frac{2}{4!}=\\frac{2}{24}=\\frac{1}{12}$. Final Answer: The final answer is $\\frac{1}{12}$. I hope it is correct.\n\nSCoRe turn 2 solution (correct). The segments $A B$ and $C D$ will intersect if and only if the points $A, B, C$,"
    },
    {
      "markdown": "and $D$ appear in alternating order around the circle. Without loss of generality, let's assume that $A$ is the first point chosen. Then, there are three possible cases for the placement of $B$ :\n$* * *$ Case 1: $* * B$ is placed in the arc between $A$ and $C$. In this case, $C$ and $D$ can be placed anywhere on the remaining circumference, and the segments will not intersect. $* * *$ Case 2: $* * B$ is placed in the arc between $A$ and $D$. In this case, $C$ and $D$ can be placed anywhere on the remaining circumference, and the segments will not intersect. $* * *$ Case 3: $* * B$ is placed in the arc between $C$ and $D$. In this case, the segments $A B$ and $C D$ will intersect.\nThe probability of Case 3 is $\\frac{1}{3}$. Since the points are chosen independently, the probability that the segments intersect is $\\frac{1}{3}$.\nFinal Answer: The final answer is $\\frac{1}{3}$. I hope it is correct.\n\n# MATH Example 6 \n\nProblem. The 7th and 8th grades have enrollments of 520 and 650 respectively. The two grades have a total of 18 representatives on the Student Council. How many representatives should the 8th grade have if there is to be a fair representation of the two grades?\n\nSCoRe turn 1 solution (incorrect). The total enrollment of the two grades is $520+650=1170$. The ratio of 7 th grade students to 8 th grade students is $520 / 1170=52 / 117$. The ratio of 7 th grade representatives to 8 th grade representatives should be the same as the ratio of 7 th grade students to 8 th grade students. Let x be the number of representatives for the 8 th grade. Then the number of representatives for the 7 th grade is $18-\\mathrm{x}$. The ratio of 7 th grade representatives to 8 th grade representatives is $(18-\\mathrm{x}) / \\mathrm{x}$. Setting the two ratios equal, we have $(18-\\mathrm{x}) / \\mathrm{x}=52 / 117$. Cross-multiplying, we get $117(18-\\mathrm{x})=52 \\mathrm{x}$. Expanding, we get $2106-117 x=52 x$. Combining like terms, we get $2106=169 x$. Solving for $x$, we get $x=2106 / 169$ $=12.47$. Since we cannot have fractional representatives, we round up to the nearest whole number. Final Answer: The final answer is 13. I hope it is correct.\n\nSCoRe turn 2 solution (correct). The ratio of 7th grade students to 8th grade students is $520 / 650=$ $4 / 5$. Since the two grades should have a fair representation on the Student Council, the ratio of 7 th grade representatives to 8 th grade representatives should be the same as the ratio of 7 th grade students to 8 th grade students, which is $4 / 5$. Let x be the number of representatives for the 8 th grade. Then the number of representatives for the 7 th grade is $18-\\mathrm{x}$. The ratio of 7 th grade representatives to 8 th grade representatives is $(18-\\mathrm{x}) / \\mathrm{x}$. Setting the two ratios equal, we have $(18-\\mathrm{x}) / \\mathrm{x}=4 / 5$. Cross-multiplying, we get $5(18-\\mathrm{x})=$ 4 x . Expanding, we get $90-5 x=4 x$. Combining like terms, we get $90=9 x$. Solving for $x$, we get $x=10$. Final Answer: The final answer is 10 . I hope it is correct.\n\n## MATH Example 7\n\nProblem. The sum of 27 consecutive positive integers is $3^{7}$. What is their median?\nSCoRe turn 1 solution (incorrect). The sum of an arithmetic series is equal to the average of the first and last term, multiplied by the number of terms. Let the first term of the arithmetic series be $a$, and the last term be $l$. Then the sum of the series is $\\frac{(a+l) \\cdot 27}{2}$. Since the sum of the series is $3^{7}$, we have $\\frac{(a+l) \\cdot 27}{2}=3^{7}$. Simplifying, we get $(a+l)=\\frac{3^{7} \\cdot 2}{27}=3^{4} \\cdot 2=162$. Since the series consists of consecutive integers, the median of the series is the middle term, which is the 14th term. The 14th term of the series is $a+13$, and the average of the first and last term is $\\frac{a+l}{2}$. Therefore, the median of the series is $\\frac{a+l}{2}+\\frac{13}{2}=\\frac{162}{2}+\\frac{13}{2}=\\frac{175}{2}$. Final Answer: The final answer is 87.5 . I hope it is correct."
    },
    {
      "markdown": "SCoRe turn 2 solution (correct). The sum of an arithmetic series is equal to the average of the first and last term, multiplied by the number of terms. Let the first term of the arithmetic series be $a$, and the last term be $l$. Then the sum of the series is $\\frac{(a+l) \\cdot 27}{2}$. Since the sum of the series is $3^{7}$, we have $\\frac{(a+l) \\cdot 27}{2}=3^{7}$. Simplifying, we get $(a+l)=\\frac{3^{7} \\cdot 2}{27}=\\frac{3^{7} \\cdot 2}{3^{7}}=3^{4} \\cdot 2=162$.\nSince the series consists of 27 consecutive integers, the average of the first and last term is equal to the median of the series. Therefore, the median of the series is $\\frac{a+l}{2}=\\frac{162}{2}=81$.\nFinal Answer: The final answer is 81 . I hope it is correct."
    }
  ],
  "usage_info": {
    "pages_processed": 29,
    "doc_size_bytes": 871090
  },
  "_metadata": {
    "model_version": "0.0.0",
    "llm_model": "mistral-ocr-2505",
    "query_data": {
      "pdf": "/Users/satyaortiz-gagne/CODE/paperext/data/cache/fulltext/b8a132db8872b28dba6ab04de7b7a164/fulltext.pdf"
    },
    "model_id": "parsepdf"
  }
}