{
  "pages": [
    {
      "markdown": "# Robust Policy Learning over Multiple Uncertainty Sets \n\nAnnie Xie ${ }^{1}$ Shagun Sodhani ${ }^{2}$ Chelsea Finn ${ }^{1}$ Joelle Pineau ${ }^{2}$ Amy Zhang ${ }^{2}$\n\n\n#### Abstract\n\nReinforcement learning (RL) agents need to be robust to variations in safety-critical environments. While system identification methods provide a way to infer the variation from online experience, they can fail in settings where fast identification is not possible. Another dominant approach is robust RL which produces a policy that can handle worst-case scenarios, but these methods are generally designed to achieve robustness to a single uncertainty set that must be specified at train time. Towards a more general solution, we formulate the multi-set robustness problem to learn a policy robust to different perturbation sets. We then design an algorithm that enjoys the benefits of both system identification and robust RL: it reduces uncertainty where possible given a few interactions, but can still act robustly with respect to the remaining uncertainty. On a diverse set of control tasks, our approach demonstrates improved worstcase performance on new environments compared to prior methods based on system identification and on robust RL alone.\n\n\n## 1. Introduction\n\nUncertainty is a prevalent challenge in most realistic reinforcement learning (RL) settings. Our work studies the uncertainty that arises when an agent is transferred to a new environment, after training on similar tasks related through a common set of underlying parameters, often referred to as the context. In safety-critical settings, we often care about the agent's worst-case performance on the distribution of plausible environments.\n\nRobust RL is one of the primary approaches to this problem as it aims to learn a policy that performs well under worst-case perturbations to the context (Rajeswaran et al., 2016; Pinto et al., 2017; Mankowitz et al., 2019; Tessler et al., 2019; Vinitsky et al., 2020; Abraham et al., 2020).\n\n[^0]![img-0.jpeg](img-0.jpeg)\n\nFigure 1. Two peg-insertion tasks. The first (green) presents high uncertainty in the peg size, while the second (orange) has high uncertainty in the controller gain.\n\nHowever, these solutions require a prior uncertainty set over the context for the test-time environment to learn the robust policy for this set at training time. Building in this prior ahead of time can limit the flexibility of the resulting policy: a large uncertainty set produces an overly conservative policy that can potentially underperform in all environments, but a small uncertainty set can fail to represent the target environment (Mozian et al., 2020).\n\nWe, therefore, formulate and study the multi-set robustness problem (illustrated in Fig. 2) whose goal is to learn a policy with strong worst-case performance on new uncertainty sets. Since the optimal robust policy varies across different perturbation sets, we incorporate the uncertainty set as contextual information to the agent and learn a generalized set-conditioned policy.\n\nHowever, naively contextualizing existing robust methods with the uncertainty set can still be sub-optimal as these methods do not reduce uncertainty over the context. In particular, the parameters that make up the context can sometimes be quickly identified, given a history of interactions. For example, consider a robot inserting a peg in one of the boxes in Fig. 1. The box closer to the robot only fits smaller pegs, while the box to the left can accommodate all sizes. Hence, the optimal policy should select the closer box for smaller pegs and the faraway box for larger ones. While the size of the peg cannot be estimated without additional trial and error, the strength of the robot's actions, on the other hand, can be identified after taking a handful of actions. Performing online system identification to reduce uncertainty over this parameter can allow the agent to solve the task\n\n\n[^0]:    ${ }^{1}$ Stanford University ${ }^{2}$ Facebook AI Research. Correspondence to: Annie Xie <anniexie@stanford.edu $>$.\n\n    Preliminary work."
    },
    {
      "markdown": "![img-1.jpeg](img-1.jpeg)\n\nFigure 2. An illustration of the robust, multi-task, and multi-set robust RL setups. Robust RL learns a policy for a single uncertainty set, while multi-task RL optimizes a policy to solve a collection of tasks. Finally, multi-set robust RL aims to learn a policy that performs well with respect to a collection of uncertainty sets.\nmore effectively. Thus, we propose to enhance existing robust RL solutions by introducing uncertainty set-awareness and system identification capabilities.\n\nTo this end, we formulate the multi-set robustness problem to learn a policy that is robust to multiple uncertainty sets. We then propose a framework that consists of a probabilistic system identification model and our multi-set robust policy, which we condition on the uncertainty set inferred by the model. We call our approach System Identification and Risk-Sensitive Adaptation (SIRSA). We compare SIRSA to prior methods based on robust RL and on system identification on a suite of continuous control tasks, including the 7-DoF peg insertion task in Fig. 1, and find substantial improvements in worst-case performance on new environments. We also find that the policy learned with SIRSA can transfer to environments with misspecified priors and with non-stationary dynamics.\n\n## 2. Related Work\n\nOur work is at the intersection of robust control, Bayesian RL, and multi-task and meta-RL, which we review below.\n\nRobust and risk-sensitive RL. The robust Markov decision process is a worst-case formulation of the RL problem with uncertainty in the transition probabilities, but can only be tractably solved in the tabular case (Morimoto \\& Doya, 2000; Nilim \\& El Ghaoui, 2005; Iyengar, 2005; Lim et al., 2013). Subsequent formulations treat the uncertainty as perturbations from a parameterized adversary, which can occur in the transition dynamics (Pinto et al., 2017; Mankowitz et al., 2019; Tessler et al., 2019; Vinitsky et al., 2020), the reward function (Lin et al., 2020; Zahavy et al., 2020), or the underlying parameters of the environment (Rajeswaran et al., 2016; Abraham et al., 2020; Mehta et al., 2020). Our work formulates a new robust control problem: robustness to a distribution over uncertainty sets. These uncertainty sets characterize uncertainty over a set of unobserved environment parameters.\n\nWorst-case solutions can be overly pessimistic, prompting the adoption of a different risk metric, the conditional value-at-risk (CVaR), which allows control over the level of risk\nsensitivity through the hyperparameter $\\alpha$ (Rockafellar et al., 2000). In RL, the CVaR objective can be optimized by sampling (Tamar et al., 2015), a distributional critic (Tang et al., 2019), or an ensemble of environment models (Mordatch et al., 2015; Rajeswaran et al., 2016). We implement a sampling-based approximation to the CVaR objective, using a learned multi-task critic.\n\nBayesian RL and system identification. Another way to handle uncertainty in RL is with the Bayes-adaptive MDP (BAMDP) (Duff, 2002; Ross et al., 2007) (see Ghavamzadeh et al. (2016) for a review). As the agent accumulates experience, we can refine its uncertainty estimates about the environment, and adapt the policy to either the most likely MDP (Yu et al., 2017; 2019), a sample from the posterior over MDPs (Rakelly et al., 2019), or the full belief distribution (Brunskill, 2012; Guez et al., 2012; 2013; Lee et al., 2019; Zintgraf et al., 2020; Abraham et al., 2020; Mozian et al., 2020). Our work combines robust and Bayesian methods by deriving an uncertainty set from the belief and acting according to a risk-sensitive RL objective. Another different method at this intersection is RAMCP (Sharma et al., 2019), which robustly plans under misspecified prior beliefs in the Bayes-adaptive MDP. A key difference in our work is that we aim to generalize to new prior beliefs that describe novel environments. Furthermore, our experiments show that our framework can also handle misspecified priors.\n\nWhile the BAMDP assumes the latent context is never observed, including at train time, we relax this assumption in our setting and access the context for each training environment to train a probabilistic system identification model via supervised learning. At test time, the model infers an uncertainty set over the true context from a partial trajectory. Unlike prior work in system identification for transfer (Yu et al., 2017; 2019; Kumar et al., 2021), we address the identifiability issues in systems where different contexts cannot be distinguished, and optimize a risk-sensitive objective to act robustly with respect to the non-identifiable parameters.\n\nMulti-task and meta-RL. The multi-task RL setting aims to transfer knowledge between related tasks by learning the set of tasks together (Parisotto et al., 2016; Teh et al., 2017; Hausman et al., 2018; Yang et al., 2020; Yu et al., 2020; Sodhani et al., 2021). Meta-RL is a related setting whose goal is to rapidly adapt to new tasks (Finn et al., 2017; Rothfuss et al., 2019; Nagabandi et al., 2019; Song et al., 2020). Notably, context-based meta-RL algorithms extract information about new tasks from a few interactions (Duan et al., 2016; Wang et al., 2016; Perez et al., 2018; Rakelly et al., 2019; Zintgraf et al., 2019; Lee et al., 2020). Our method, which falls into this category, conditions the agent on the inferred uncertainty set.\n\nBayesian meta-RL. Prior work in Bayesian meta-RL propose algorithms that train a policy conditioned on the posterior"
    },
    {
      "markdown": "distribution (or belief) over the inferred context, allowing the agent to reason about task uncertainty (Humplik et al., 2019; Zintgraf et al., 2020; Zhang et al., 2021). However, our policy optimizes a risk-sensitive objective, rather than the expectation of the return over the belief. While Bayesian meta-RL agents balance exploration and exploitation in a new task based on the uncertainty, our work focuses less on exploration in a new task. Instead, we aim to design an agent that can robustly solve a new task under safety-critical conditions.\n\nRobust meta-RL. Prior work has also studied robust meta-RL but under different setups and objectives, including robustness against adversarial reward functions under a learned model (Lin et al., 2020) and robustness by learning diverse behaviors within a single MDP (Kumar et al., 2020; Zahavy et al., 2020). The objective of our work is to robustly adapt to new environments by training in a set of related MDPs. Most closely related is CARL (Zhang et al., 2020), which prepares the agent for safety-critical few-shot adaptation through pre-training on related source environments. CARL captures uncertainty through a probabilistic dynamics model, fine-tunes the model with new data collected in adaptation episodes with the target environment, and generates risk-sensitive plans with respect to the fine-tuned model. In contrast to CARL, which relies on multiple rounds of trial-and-error for adaptation, the robust RL setting typically evaluates zero-shot performance on new environments. Without the opportunity to test different behaviors for adaptation, new challenges with system identifiability arise.\n\n## 3. Problem Setup\n\nWe first introduce notation in the standard RL setting in Sec. 3.1 and the robust contextual MDP in Sec. 3.2. Then, we formalize the multi-set robustness objective in Sec. 3.3.\n\n### 3.1. Preliminaries\n\nA Markov decision process (MDP) or task is a tuple $\\mathcal{M}=$ $\\langle\\mathcal{S}, \\mathcal{A}, p, r, \\rho, \\gamma\\rangle$ where $\\mathcal{S}$ is the state space, $\\mathcal{A}$ is the action space, $p$ is the state transition probability, $r$ is the reward function, $\\rho$ is the initial state distribution, and $\\gamma \\in[0,1)$ is the discount factor. The goal in standard RL is to learn a policy $\\pi$ that maximizes the expected sum of rewards $\\mathcal{J}(\\pi):=\\mathbb{E}_{\\pi, p}\\left[G_{\\pi}\\right]$ where $G_{\\pi}=\\sum_{t=0}^{\\infty} \\gamma^{t} r\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)$.\nWe also consider the contextual Markov decision process (CMDP) (Hallak et al., 2015) which, like the standard MDP, is equipped with a state space $\\mathcal{S}$ and action space $\\mathcal{A}$. It additionally has a context space $\\mathcal{C}$ and function $\\mathcal{M}$ that maps any context $c \\in \\mathcal{C}$ to an MDP $\\mathcal{M}(c)=\\left\\langle\\mathcal{S}, \\mathcal{A}, p^{c}, r^{c}, \\rho, \\gamma\\right\\rangle$, where $p^{c}$ and $r^{c}$ are parameterized by $c .{ }^{1}$\n\n[^0]\n### 3.2. Robust Contextual Markov Decision Process\n\nThe exact context $c$ describing the task is often unknown, especially when the agent is transferred to an entirely new task. Instead, there may be a prior belief $b(c)$ over the context $c$, from which we can derive an uncertainty set.\n\nFormally, we define the robust contextual Markov decision process (R-CMDP), which extends the CMDP with an initial uncertainty set $\\Xi \\subseteq \\mathcal{C}$ over the contexts. We also assume a distribution $p(\\Xi)$ from which we can draw samples. This uncertainty set is given at the beginning of an episode and can be interpreted as a prior over the true context. We focus on parameterized uncertainty sets, and $\\Xi$ refers to the parameters that define the set, e.g., the center and radius for ball sets. Then, one way to acquire a robust policy is to optimize the worst-case objective with respect to the uncertainty set $\\mathcal{J}_{\\min }(\\pi):=\\min _{c \\in \\Xi} \\mathbb{E}_{\\pi, p^{c}}\\left[G_{\\pi}^{c}\\right]$ where $G_{\\pi}^{c}=\\sum_{t=0}^{\\infty} \\gamma^{t} r^{c}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)$. In this work, we optimize a softer version of this worst-case objective: the conditional value-at-risk (CVaR) (Tamar et al., 2015; Rajeswaran et al., 2016; Tang et al., 2019).\n\nThe CVaR objective is defined over the random variable $G_{\\pi}^{\\Xi}$ of returns induced by the uniform distribution over contexts in the uncertainty set $\\Xi$. First, the value-at-risk is given by the $\\alpha$-quantile of the return distribution\n\n$$\n\\operatorname{VaR}_{\\alpha}\\left(G_{\\pi}^{\\Xi}\\right):=\\max \\left\\{y \\mid P\\left(G_{\\pi}^{\\Xi} \\leq y\\right) \\leq \\alpha\\right\\}\n$$\n\nThen, denoting $\\mathcal{P}(c)$ as the uniform distribution over the set $\\left\\{c \\in \\Xi \\mid G_{\\pi}^{c} \\leq \\operatorname{VaR}_{\\alpha}\\left(G_{\\pi}^{\\Xi}\\right)\\right\\}$, the CVaR objective is\n\n$$\n\\mathcal{J}_{\\pi}^{\\mathrm{CVaR}_{\\alpha}}(\\Xi):=\\mathbb{E}_{\\pi, c \\sim \\mathcal{P}(c)}\\left[G_{\\pi}^{c}\\right]\n$$\n\nthe expected return over the lower $\\alpha$-percentile subset of the uncertainty set. When $\\alpha=1$, the objective is the average over the perturbation set, and when $\\alpha \\rightarrow 0$, the objective becomes the max-min objective.\n\n### 3.3. Multi-Set Robustness\n\nOptimizing a robust policy with respect to a new uncertainty set can be costly for each new policy. Hence, we aim to learn a single policy that can be robust to several different uncertainty sets. We do so by leveraging the multi-task RL setting to optimize a policy that can generalize to and provide good worst-case performance with respect to new uncertainty sets.\n\nIn particular, the learner has access to $M$ training tasks $\\left\\{\\mathcal{M}_{i}\\right\\}_{i=1}^{M}$ that are parameterized by $M$ different observed contexts $\\left\\{c_{i}\\right\\}_{i=1}^{M}$. The goal of our setting is to learn a set-conditioned policy $\\pi(\\mathbf{a} \\mid \\mathbf{s}, \\Xi)$ that maximizes the worstcase expected return with respect to all uncertainty sets from the distribution $p(\\Xi)$. That is, we want to optimize $\\mathbb{E}_{\\Xi \\sim p(\\Xi)}\\left[\\mathcal{J}_{\\pi}^{\\mathrm{CVaR}_{\\alpha}}(\\Xi)\\right]$.\n\n\n[^0]:    ${ }^{1}$ We refer to contexts and tasks interchangeably."
    },
    {
      "markdown": "## 4. System Identification and its Challenges\n\nRather than behaving invariantly to different contexts as in robust RL, another approach is to condition the policy on the context or a distribution over the context. System identification methods based on this idea train a predictive model to produce either a point estimate of or a posterior distribution over the context, given a history $h_{t}=\\left(\\mathbf{s}_{0}, \\mathbf{a}_{0}, r_{0}, \\ldots, \\mathbf{s}_{t-1}, \\mathbf{a}_{t-1}, r_{t-1}\\right)$. This approach has demonstrated strong generalization performance, including transfer from simulation to the real world (Yu et al., 2017; 2019; Kumar et al., 2021).\n\nHowever, as discussed by Dorfman \\& Tamar, many systems are often determined by parameters that are not easily identifiable from a limited amount of interaction. Recall our peg-insertion example from Fig. 1: the size of the peg cannot be determined within a single trial, but critically, the robot has to take this parameter into account to select a box to insert the peg into. In these low-data regimes, the system identification model can fail to accurately distinguish between multiple MDP contexts. Formally, the context $c$ is non-identifiable from a dataset $h$ if there is a set of other contexts $\\mathcal{C}^{\\prime} \\subseteq \\mathcal{C}$ that can also explain the data.\nDefinition 4.1 (Context non-identifiability). Let $P^{c, \\pi}\\left(h_{: t}\\right)$ denote the probability distribution over histories at time-step $t$ under the MDP $\\mathcal{M}(c)$ and policy $\\pi$. Then, the context $c$ is non-identifiable from the dataset $h_{: t}$ collected by policy $\\pi$ if there exists a subset $\\mathcal{C}^{\\prime} \\neq\\{c\\}$ such that $P^{c, \\pi}\\left(h_{: t}\\right)=$ $P^{c^{\\prime}, \\pi}\\left(h_{: t}\\right)$ for all $c \\in \\mathcal{C}^{\\prime}$.\n\nAs an aside, context non-identifiability can also be viewed as posterior collapse (Wang \\& Cunningham, 2020). In our setting, posterior collapse occurs when the prior and posterior belief distributions are equal, i.e., $b^{\\prime}(c \\mid b, h)=b(c)$. Hence, one proxy measure of context identifiability is the entropy of the belief distribution, i.e., higher entropy indicates lower identifiability of the context. This problem is further exacerbated when knowledge of the context is critical to the task at hand, i.e., confusing it with a different context can lead to a large drop in performance.\nDefinition 4.2 (Critical contexts). Denote the optimal context-dependent policy by $\\pi^{*}(c):=\\arg \\max _{\\pi} \\mathbb{E}_{\\pi, c}\\left[G_{\\pi}^{c}\\right]$. Consider the set of contexts $\\mathcal{C}^{\\prime}$ for which $P^{c, \\pi^{*}(c)}(h)=$ $P^{c^{\\prime}, \\pi^{*}(c)}(h)$ holds for all $c^{\\prime} \\in \\mathcal{C}^{\\prime}$. The worst-case gap for a context-dependent policy evaluated in the MDP with context $c$ is $D(c)=\\max _{c^{\\prime} \\in \\mathcal{C}^{\\prime}} G_{\\pi^{*}(c)}^{c}-G_{\\pi^{*}\\left(c^{\\prime}\\right)}^{c}$.\n\nIt becomes clear when there is uncertainty around a critical context $c$, the gap can be significant. Hence, to be robust to the worst case of the non-identifiable set, we define our objective to minimize the worst-case gap: $\\min _{\\pi} \\max _{c \\in \\mathcal{C}^{\\prime}} G_{\\pi^{*}(c)}^{c}-G_{\\pi}^{c}$. In the next section, we introduce our algorithm which re-estimates the uncertainty set while taking actions that are robust at each time-step.\n\n## 5. Risk-Sensitive Adaptation via System Identification and Multi-Set Robustness\n\nTo address the challenges associated with non-identifiable systems, we propose a simple framework that consists of a probabilistic system identification model and a family of risk-sensitive policies $\\pi(\\mathbf{a} \\mid \\mathbf{s}, \\Xi)$ conditioned on the uncertainty set inferred by the model. The resulting algorithm combines the benefits of system identification and risksensitive RL as it reduces the model uncertainty where possible while behaving cautiously with respect to the irreducible uncertainty. Our overall approach, which we call System Identification and Risk-Sensitive Adaptation (SIRSA), is illustrated in Fig. 3, with each component detailed below.\n\n### 5.1. Probabilistic System Identification\n\nTo capture the epistemic uncertainty of an unknown environment at test time, we train an ensemble of models to predict the context $c$ that parameterizes the environment's dynamics and reward function. Recall that at train time, the agent observes the context of each training task $\\left\\{c_{i}\\right\\}_{i=1}^{M}$, and for each task, collects a dataset of transitions $\\left\\{\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}, r_{t}, \\mathbf{s}_{t}^{\\prime}\\right)\\right\\}_{t}$.\n\nWe learn an ensemble of $B$ different models, where each model $f_{i}$ maps an initial uncertainty set $\\Xi$ and a history $h$ of $H$ transitions to a context. In this work, the uncertainty set is an $\\ell_{1}$-ball with its own center $\\mu$ and width $\\sigma$. Then, each model $f_{i}$ has parameters $\\psi_{i}$ that are trained with the mean squared error on the predicted context:\n\n$$\n\\mathcal{L}_{\\psi_{1: B}}=\\mathbb{E}_{(\\mu, \\sigma, h, c) \\sim \\mathcal{D}, j \\sim \\operatorname{Unif}(B)}\\left[\\left(f_{\\psi_{j}}(\\mu, \\sigma, h)-c\\right)^{2}\\right]\n$$\n\nwhere the initial uncertainty set $\\Xi=(\\mu, \\sigma)$, given by the environment to the learner, offers an initial guess of the true context. We define the parameters of the posterior uncertainty set $\\Xi^{\\prime}=\\left(\\mu^{\\prime}, \\sigma^{\\prime}\\right)$ as the mean and standard deviation of the ensemble:\n\n$$\n\\mu^{\\prime}=\\mu\\left(\\left\\{f_{\\psi_{j}}(\\Xi, h)\\right\\}_{j=1}^{B}\\right), \\sigma^{\\prime}=\\sigma\\left(\\left\\{f_{\\psi_{j}}(\\Xi, h)\\right\\}_{j=1}^{B}\\right)\n$$\n\nwhere $\\mu(\\cdot)$ and $\\sigma(\\cdot)$ compute the mean and standard deviation, respectively. At inference time, we recursively update the uncertainty set by using the set inferred from the previous time-step as the prior. That is, the perturbation set $\\Xi_{t}$ at time-step $t$ has parameters $\\mu_{t}=\\mu\\left(\\left\\{f_{\\psi_{j}}\\left(\\mu_{t-1}, \\sigma_{t-1}, h\\right)\\right\\}_{j: 1 \\ldots B}\\right)$ and $\\sigma_{t}=$ $\\sigma\\left(\\left\\{f_{\\psi_{j}}\\left(\\mu_{t-1}, \\sigma_{t-1}, h\\right)\\right\\}_{j: 1 \\ldots B}\\right)$. Next, we describe how we optimize a risk-sensitive policy to act robustly with respect to the inferred uncertainty set.\n\n### 5.2. Risk-Sensitive Policy Optimization\n\nThe CVaR objective is computed by finding, from the set of environments defined by the uncertainty set $\\Xi$, the $\\alpha$ quantile that the policy performs worst in. While the dis-"
    },
    {
      "markdown": "![img-2.jpeg](img-2.jpeg)\n\nFigure 3. Our framework combines system identification with risk-sensitive RL to robustly adapt to new environments. First, the algorithm updates the uncertainty set over the context with the agent's recent history $h$. We then optimize a risk-sensitive policy over the returns within this uncertainty set.\ntribution of $G_{\\pi}^{\\Xi}$ is unknown, we can approximate its CVaR through a context-conditioned critic $Q_{\\theta}(\\mathbf{s}, \\mathbf{a}, c)$. That is,\n\n$$\n\\mathbb{E}_{c \\sim \\mathcal{P}(c)}\\left[G_{\\pi}^{c}\\right] \\approx \\mathbb{E}_{c \\sim \\mathcal{P}(c)}\\left[\\mathbb{E}_{\\mathbf{s} \\sim \\mathcal{D}, \\mathbf{a} \\sim \\pi(\\cdot \\mid \\mathbf{s})}\\left[Q_{\\theta}(\\mathbf{s}, \\mathbf{a}, c)\\right]\\right]\n$$\n\nwhere $\\mathcal{P}(c)$ is the uniform distribution over the set $\\{c \\in$ $\\left.\\Xi \\mid G_{\\pi}^{c} \\leq \\operatorname{VaR}_{\\alpha}\\left(G_{\\pi}^{\\Xi}\\right)\\right\\}$.\nWe can form a Monte-Carlo estimate of the CVaR as follows. Let $\\tilde{c}_{1}, \\ldots, \\tilde{c}_{N}$ be $N$ samples drawn i.i.d. from the uncertainty set $\\Xi$, and $Q_{1}, \\ldots, Q_{N}$ be their corresponding Q-values, i.e., $Q_{i}=Q_{\\theta}(\\mathbf{s}, \\mathbf{a}, \\tilde{c}_{i})$. After sorting the contexts in ascending order based on their Q-values, $\\tilde{c}_{[1]}, \\ldots, \\tilde{c}_{[N]}$, the empirical $\\alpha$-quantile is simply $Q_{\\theta}\\left(\\mathbf{s}, \\mathbf{a}, \\tilde{c}_{[[\\alpha N]]}\\right)$, and, the empirical CVaR approximation is\n\n$$\n\\frac{1}{[\\alpha N]} \\sum_{i=1}^{[\\alpha N]} Q_{\\theta}\\left(\\mathbf{s}, \\mathbf{a}, \\tilde{c}_{[i]}\\right)\n$$\n\nTo update the policy $\\pi_{\\phi}$, we can compute $\\nabla_{\\phi} J_{\\pi}^{\\mathrm{CVaR}_{\\alpha}}$, the gradient of the approximated CVaR with respect to the policy parameters $\\phi$, with\n\n$$\n\\mathbb{E}_{\\mathbf{s} \\sim \\mathcal{D}}\\left[\\sum_{i=1}^{[\\alpha N]} \\frac{\\nabla_{\\mathbf{a}} Q_{\\theta}\\left(\\mathbf{s}, \\mathbf{a}, \\tilde{c}_{[i]}\\right)\\left\\lvert\\, \\mathbf{a} \\sim \\pi_{\\phi}\\right.}{[\\alpha N]} \\nabla_{\\phi} \\pi_{\\phi}(\\mathbf{a} \\mid \\mathbf{s}, \\Xi)\\right]\n$$\n\nWe construct our CVaR actor on top of Soft Actor-Critic (SAC) (Haarnoja et al., 2018). Our algorithm, which we call System Identification and Risk-Sensitive Adaptation (SIRSA), is summarized in Alg. 1. We begin by training the actor and critic with the losses $\\mathcal{J}_{\\pi}$ and $\\mathcal{J}_{Q}$ defined in SAC, and the ensemble of models with the loss defined in Eqn. 1. After $T_{\\text {threshold }}$ iterations, we update the actor $\\pi_{\\phi}$ based on the CVaR objective defined in Eqn. 3 instead of $\\mathcal{J}_{\\pi}$. Full implementation details can be found in Appendix A.\n\n## 6. Experiments\n\nWe design several experiments to understand the effectiveness of our proposed approach compared to system identification and robust RL approaches in unseen environments. Specifically, we seek to answer the following questions: ${ }^{2}$\n\n[^0]Algorithm 1 System Identification and Risk-Sensitive Adaptation (SIRSA)\nInput: CVaR level $\\alpha$, threshold $T_{\\text {threshold }}$\nInitialize replay buffers for each training task $\\mathcal{D}[c]$\nfor $i=1,2, \\ldots$ do\nSample training environment from set of environments\nInitialize history $h_{0}=\\left\\{\\mathbf{s}_{0}\\right\\}$\nfor each environment step do\nTake action $\\mathbf{a}_{t} \\sim \\pi_{\\phi}\\left(\\cdot \\mid \\mathbf{s}, \\Xi_{t}\\right)$\nUpdate history $h_{t} \\leftarrow h_{t-1} \\cup\\left\\{\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}, r_{t}, \\mathbf{s}_{t}^{\\prime}\\right)\\right\\}$\nUpdate uncertainty set $\\Xi_{t+1}$ according to Eqn. 2 end for\nUpdate replay buffer $\\mathcal{D}[c] \\leftarrow \\mathcal{D}[c] \\cup h_{T}$\nfor each gradient step do\nSample batch from replay buffers $\\bigcup_{c} \\mathcal{D}[c]$\nUpdate critic parameters $\\theta$ with $\\nabla_{\\theta} \\mathcal{J}_{Q}$\nif $i<T_{\\text {threshold }}$ then\nUpdate actor parameters $\\phi$ with $\\nabla_{\\phi} J_{\\pi}$\nelse\nUpdate actor parameters $\\phi$ with $\\nabla_{\\phi} J_{\\pi}^{\\mathrm{CVaR}_{\\alpha}}$ end if\nUpdate ensemble parameters $\\psi_{k}$ with $\\nabla_{\\psi_{k}} \\mathcal{L}_{\\psi_{k}}$ end for\nend for\n\n1. How does our method SIRSA compare to standard system identification and robust RL in terms of worst-case performance on new uncertainty sets?\n2. Can SIRSA generalize to new test-time scenarios such as misspecified priors and non-stationary dynamics?\n3. How does SIRSA respond to varying $\\alpha$-levels of risk sensitivity?\n\n### 6.1. Experimental Setup\n\nBaselines. First, we consider multi-task RL baselines that train a context-conditioned policy $\\pi(\\mathbf{a} \\mid \\mathbf{s}, c)$.\n\n- Context-conditioned policy ensemble. At test time, $N^{\\text {ens }}$ contexts $\\left\\{c^{i}\\right\\}_{i=1}^{N^{\\text {ens }}}$ are sampled from the initial uncertainty set to create an ensemble of policies $\\pi^{\\text {ens }}(\\cdot \\mid \\mathbf{s})=$ $\\sum_{i=1}^{N^{\\text {ens }}} \\pi(\\mathbf{a} \\mid \\mathbf{s}, c^{i}) / N^{\\text {ens }}$. We use $N^{\\text {ens }}=5$.\n- Context-conditioned policy with true context (oracle).\n\n\n[^0]:    ${ }^{2}$ Code and videos of our results are on our webpage: https : //sites.google.com/view/sirsa-public/home."
    },
    {
      "markdown": "An oracle with access to the ground-truth context at test time, given as input to the context-conditioned policy.\n\nWe also compare to the system ID ablation of SIRSA, which optimizes the expected return rather than the CVaR:\n\n- Set-conditioned policy with system identification (Yu et al., 2017). Along with a set-conditioned policy $\\pi(\\mathbf{a} \\mid \\mathbf{s}, \\Xi)$, this baseline trains a system identification model that maps the history of last $H$ states and actions to a belief over the context. The belief inferred by the model is given to the policy.\n\nFinally, we compare to existing robust/risk-sensitive RL methods. Like our approach, each of these methods controls the risk level through the CVaR hyperparameter $\\alpha$. We run each algorithm with $\\alpha \\in\\{0.25,0.5,0.75,1.0\\}$, and report the results for the most performant policy in this section. In Appendix C.2, we report the full results for each value of $\\alpha$.\n\n- EPOpt (Rajeswaran et al., 2016). A domain randomization method that optimizes the CVaR objective by training on the $\\alpha$-worst percentile of all training environments.\n- Multi-Set EPOpt. We design a stronger variant of EPOpt by training a multi-set robust policy $\\pi(\\mathbf{a} \\mid \\mathbf{s}, \\Xi)$ on the $\\alpha$ worst percentile of environments in each set $\\Xi \\subseteq \\mathcal{C}$.\n- Worst Cases Policy Gradients (WCPG) (Tang et al., 2019). This comparison trains a family of $\\alpha$ conditional policies $\\pi(\\mathbf{a} \\mid \\mathbf{s}, \\alpha)$ with varying levels of risk sensitivity. In order to approximate the CVaR across different $\\alpha$ levels, the future return generated by policy $\\pi$ is modeled as a Gaussian distribution and approximated by a distributional critic, allowing the CVaR to be computed in closed form. During training, we sample $\\alpha$ uniformly from $[0,1]$. At inference time, we evaluate the policy at the $\\alpha$-levels $\\{0.25,0.5,0.75,1.0\\}$. Like EPOpt, this comparison trains on the entire range of contexts as its uncertainty set.\n- Multi-Set WCPG (Tang et al., 2019). We design a stronger variant of WCPG by training a multi-set robust policy $\\pi(\\mathbf{a} \\mid \\mathbf{s}, \\alpha, \\Xi)$ with WCPG.\n\nEnvironments. We design several environments to evaluate our approach, and in each, vary one or more parameters that affect the dynamics and/or reward function. All methods can access the true context of each training environment. However, at inference time, only an initial uncertainty set is provided, and none of the methods (with the exception of the context-conditioned oracle) have access to the true parameter values. In Table 1, we tabulate the ranges for the different parameters, and describe the environments below:\n\n- Point mass navigation. A point mass has to navigate around a roundabout with uncertainty in the size of the roundabout and the precise velocity. We additionally design two variants: Point mass (obstacle) where only the obstacle size is uncertain and Point mass (velocity) where only the velocity of the agent is uncertain.\n\n| Environment | Uncertain Params. | Range |\n| :--: | :--: | :--: |\n| Point mass | Obstacle size Velocity | $\\begin{aligned} & {[0.025,0.075]} \\\\ & {[0.06,0.1]} \\end{aligned}$ |\n|  | Torso mass Leg mass Leg failure (x4) | $\\begin{aligned} & \\hline-0.2,0.2] \\\\ & -0.2,0.2] \\\\ & {[0.0,1.0]} \\end{aligned}$ |\n| Half-cheetah | Torso mass Joint friction Joint failure (x6) | $\\begin{aligned} & \\hline-0.5,0.5] \\\\ & {[0.1,0.9]} \\\\ & {[0.0,1.0]} \\end{aligned}$ |\n| Peg insertion | Step size <br> Peg size | $\\begin{aligned} & {[0.5,1.5]} \\\\ & {[0.0125,0.0225]} \\end{aligned}$ |\n\nTable 1. Range of parameter values $\\mathcal{C}$ in our environments.\n\n- Minitaur (Tan et al., 2018). A simulated 8-DoF minitaur robot with uncertainty in the mass and leg failure rate.\n- Half-cheetah (Brockman et al., 2016; Vinitsky et al., 2020). Modified OpenAI Gym environment with uncertainty in the mass, joint friction, and joint failure rate.\n- Peg insertion (Zhao et al., 2020; Schoettler et al., 2020). A simulated 7-DoF Sawyer robot arm is to insert a peg into one of the boxes (see Fig. 1). The uncertainty is in the position controller's step size and the size of the peg.\n\nFull descriptions of each environment are in Appendix B.\nEvaluation metrics. To evaluate each method, we construct test-time uncertainty sets centered around new contexts not seen during training. We then evaluate a policy's performance by, first, uniformly sampling $K$ context vectors from each set, then, rolling out the policy in each of the $K$ sampled environments. We are in particular interested in the worst-case performance, approximated by the minimum return of the $K$ rollouts, and additionally report the averagecase performance, approximated by the mean return of the $K$ rollouts. In all experiments, we use $K=50$.\n\n### 6.2. Robustness to New Uncertainty Sets\n\nPoint mass. We first seek to better understand the strengths and weaknesses of prior methods in the Point mass (obstacle) and Point mass (velocity) environments. The former represents a parameter that is difficult to precisely identify since it would require making contact with the obstacle to infer its size. In contrast, the latter parameter can be exactly estimated given a single time-step. We compare System ID to Multi-Set EPOpt, which acts as the representative of robust RL methods. In Table 2, we compare the worst-case performance of the two methods across 20 test uncertainty sets, and find that firstly the identification error of the obstacle size is indeed higher than that of the velocity parameter, confirming our intuition. In the Point mass (obstacle) domain, Set-EPOpt outperforms System ID as the uncertain parameter cannot be exactly identified without incurring a penalty. In the Point mass (velocity) domain, we see the reverse result: the System ID method correctly adapts to the"
    },
    {
      "markdown": "|  |  | Method |  |\n| :-- | :--: | :--: | :--: |\n| Uncertain Param. | ID Error | System ID | Set-EPOpt |\n| Obstacle size | $0.071 \\pm 0.002$ | $37.7 \\pm 0.4$ | $\\mathbf{3 9 . 4} \\pm \\mathbf{0 . 5}$ |\n| Velocity | $0.035 \\pm 0.000$ | $\\mathbf{3 7 . 9} \\pm \\mathbf{0 . 0}$ | $37.3 \\pm 0.1$ |\n\nTable 2. Worst-case performance of system ID and Set-EPOpt when (1) the obstacle size and when (2) the velocity is uncertain.\npredicted context, whereas Set-EPOpt acts conservatively without precise identification of the parameter. The trajectories taken by these policies are visualized in Appendix C.1.\n\nHigh-dimensional domains. In Table 3, we present the results in the remaining domains. In terms of the worstcase performance (see the \"Sample Min\" column), many of the studied baselines perform competitively against each other. System ID, which optimizes for the expectation of the return over its inferred belief, attains strong averagecase returns as a result (see the \"Sample Mean\" column). However, there is no single baseline that outperforms the rest in all settings in terms of worst-case returns, the primary metric we are interested in. On the other hand, SIRSA, which inherits from both system identification and robust RL algorithms, consistently achieves high worst-case returns across the different environments. Interestingly, all methods demonstrate similarly strong performance in the Minitaur domain, which suggests that context awareness is not as critical in this domain.\n\nIn general, the multi-set robust RL baselines demonstrate better worst-case as well as average-case performance than their single-set counterparts. Without access to a more informative prior, the latter group of policies is trained to act robustly to the maximal uncertainty set: the union over all of the tasks seen during training. As a result, their behavior can be overly conservative.\n\nMaximum initial uncertainty. We now evaluate the policies learned by each method on the maximum uncertainty set in the Half-Cheetah domain, defined as the range of parameters $\\mathcal{C}$ in Table 1. This also represents the evaluation setup of standard single-set robust RL. We report the results in Table 4. Notably, the single-set and multi-set robust RL methods perform comparably, where Set-EPOpt even outperforms EPOpt, indicating that multi-set robust policies can generalize even to the maximum uncertainty set $\\mathcal{C}$.\n\n### 6.3. Generalization under Misspecification\n\nNon-stationarity. Most real-world environments are dynamic. For example, a robot's mass can vary over time as it carries different payloads. Does training for multi-set robustness facilitate generalization to non-stationary environments at test time? Intuitively, if the non-stationary parameters remain contained within a finite range, we can capture the changing environment with a single uncertainty set.\n\n| Task | Method | Sample Min | Sample Mean |\n| :--: | :--: | :--: | :--: |\n| Point mass | Ensemble | $36.8 \\pm 0.3$ | $\\mathbf{4 1 . 7} \\pm \\mathbf{0 . 2}$ |\n|  | System ID | $\\mathbf{3 7 . 6} \\pm \\mathbf{0 . 3}$ | $\\mathbf{4 1 . 5} \\pm \\mathbf{0 . 2}$ |\n|  | EPOpt | $36.3 \\pm 0.6$ | $40.5 \\pm 0.4$ |\n|  | Set-EPOpt | $37.1 \\pm 0.5$ | $40.7 \\pm 0.4$ |\n|  | WCPG | $34.8 \\pm 0.6$ | $39.3 \\pm 0.5$ |\n|  | Set-WCPG | $34.7 \\pm 0.7$ | $39.0 \\pm 0.5$ |\n|  | SIRSA (Ours) | $\\mathbf{3 7 . 9} \\pm \\mathbf{0 . 2}$ | $\\mathbf{4 1 . 7} \\pm \\mathbf{0 . 1}$ |\n|  | Oracle | $38.6 \\pm 0.3$ | $42.6 \\pm 0.1$ |\n| Minitaur | Ensemble | $\\mathbf{1 7 8 . 0} \\pm \\mathbf{7 . 1}$ | $\\mathbf{2 1 2 . 5} \\pm \\mathbf{3 . 6}$ |\n|  | System ID | $\\mathbf{1 7 4 . 0} \\pm \\mathbf{1 0 . 8}$ | $\\mathbf{2 1 1 . 8} \\pm \\mathbf{2 . 9}$ |\n|  | EPOpt | $\\mathbf{1 7 2 . 2} \\pm \\mathbf{7 . 3}$ | $199.9 \\pm 5.5$ |\n|  | Set-EPOpt | $\\mathbf{1 8 3 . 1} \\pm \\mathbf{7 . 5}$ | $\\mathbf{2 1 6 . 7} \\pm \\mathbf{3 . 7}$ |\n|  | WCPG | $\\mathbf{1 6 5 . 5} \\pm \\mathbf{1 7 . 7}$ | $\\mathbf{1 9 3 . 7} \\pm \\mathbf{1 9 . 6}$ |\n|  | Set-WCPG | $\\mathbf{1 7 4 . 5} \\pm \\mathbf{1 0 . 0}$ | $206.9 \\pm 4.7$ |\n|  | SIRSA (Ours) | $\\mathbf{1 8 7 . 8} \\pm \\mathbf{7 . 6}$ | $\\mathbf{2 1 4 . 3} \\pm \\mathbf{2 . 5}$ |\n|  | Oracle | $172.2 \\pm 4.1$ | $208.0 \\pm 3.0$ |\n| Half-cheetaft | Ensemble | $\\mathbf{3 9 8 8} \\pm \\mathbf{7 5}$ | $4714 \\pm 38$ |\n|  | System ID | $3774 \\pm 318$ | $4393 \\pm 260$ |\n|  | EPOpt | $2272 \\pm 218$ | $2717 \\pm 324$ |\n|  | Set-EPOpt | $3806 \\pm 224$ | $4477 \\pm 231$ |\n|  | WCPG | $3747 \\pm 229$ | $4305 \\pm 314$ |\n|  | Set-WCPG | $3871 \\pm 207$ | $4433 \\pm 253$ |\n|  | SIRSA (Ours) | $\\mathbf{4 1 4 6} \\pm \\mathbf{1 1 2}$ | $\\mathbf{4 8 7 2} \\pm \\mathbf{7 4}$ |\n|  | Oracle | $4246 \\pm 59$ | $4851 \\pm 38$ |\n| Peg insertion | Ensemble | $45.2 \\pm 3.0$ | $92.9 \\pm 1.8$ |\n|  | System ID | $73.7 \\pm 4.3$ | $101.3 \\pm 4.0$ |\n|  | EPOpt | $43.2 \\pm 5.4$ | $75.1 \\pm 8.2$ |\n|  | Set-EPOpt | $70.6 \\pm 3.6$ | $96.9 \\pm 2.7$ |\n|  | WCPG | $33.8 \\pm 7.2$ | $63.2 \\pm 6.9$ |\n|  | Set-WCPG | $68.3 \\pm 4.6$ | $92.0 \\pm 4.4$ |\n|  | SIRSA (Ours) | $\\mathbf{8 3 . 4} \\pm \\mathbf{4 . 5}$ | $\\mathbf{1 0 9 . 5} \\pm \\mathbf{2 . 6}$ |\n|  | Oracle | $78.2 \\pm 3.6$ | $122.0 \\pm 1.5$ |\n\nTable 3. We evaluate each policy on 20 uncertainty sets at test time by drawing samples from the set and evaluating the policy's performance on each sampled environment. We report the minimum and mean performance over the samples as an approximation to the average-case and worst-case performance on the perturbation set. The means and standard errors are computed over 10 policies for each method trained with random seeds. We bold the highest results that are more than a standard error higher than others.\n\nIn this experiment, we design a non-stationary variant of the Half-Cheetah environment. Specifically, we sample an uncertainty set at the beginning of the episode, and at every 50 timesteps, we set the parameters of the environment to a new context sampled from the initial uncertainty set. The episode is terminated after 500 timesteps. We report the results aggregated over 10 different rollouts, each corresponding to a different initial uncertainty set, in Table 4. The best-performing policy is that learned by SIRSA, indicating that adaptive risk sensitivity to a given uncertainty set is a reasonable solution to overcome non-stationarity. In Fig. 4 (left), we plot the reward attained by the agent over the 500 timesteps of one of the rollouts. Here, the policy learned with SIRSA attains higher rewards at each timestep."
    },
    {
      "markdown": "| Method | Max Uncertainty <br> Set Return | Non-stationary <br> Env Return |\n| :-- | :--: | :--: |\n| Ensemble | $3619 \\pm 95$ | $4698 \\pm 31$ |\n| System ID | $3608 \\pm 468$ | $4277 \\pm 341$ |\n| EPOpt | $1516 \\pm 253$ | - |\n| Set-EPOpt | $3360 \\pm 498$ | $4336 \\pm 247$ |\n| WCPG | $3598 \\pm 339$ | - |\n| Set-WCPG | $3055 \\pm 456$ | $3950 \\pm 486$ |\n| SIRSA (Ours) | $\\mathbf{4 2 8 1} \\pm \\mathbf{7 3}$ | $\\mathbf{4 9 1 3} \\pm \\mathbf{6 5}$ |\n| Oracle | $4203 \\pm 91$ | $4580 \\pm 63$ |\n\nTable 4. Middle: Mean and standard error of the return over 10 random seeds on the maximum uncertainty set in the Half-Cheetah domain. Right: Mean and standard error of the return over 10 random seeds on a non-stationary Half-Cheetah environment, wherein the unobserved context changes every 50 timesteps.\n![img-3.jpeg](img-3.jpeg)\n\nFigure 4. Left: Reward versus time in the Half-Cheetah environment with non-stationary parameters. The solid lines represent mean and the shaded regions represent the standard error over 10 policies trained with different random seeds. Right: Performance on increasingly misspecified priors. Each data point is averaged over 10 policies trained with different random seeds.\n\nInitial uncertainty set. So far, we provided an initial uncertainty set $\\Xi_{0}=\\left(\\mu_{0}, \\sigma_{0}\\right)$ that correctly informs the agent about the environment. How robust is the agent when this prior is misspecified, i.e., when the initial perturbation set does not contain the true environment? In this experiment, we provide intentionally misspecified sets to the agent at test time. For each set $\\Xi_{0}$, we sample contexts of the form $\\mu_{0}+w \\sigma_{0}$, where $w \\in\\{-(1+r), 1+r\\}^{d}, d$ is the number of context variables, and $r$ varies between $0.25,0.50,0.75$, and 1.00 . We evaluate on the corresponding environments, and plot the average over these samples as a function of $r$ in Fig. 4 (right). The multi-set robust RL methods, Set-EPOpt and Set-WCPG, as well as the Ensemble baseline tend to drop in performance faster as the environment deviates more from the prior uncertainty set. In contrast, SIRSA and System ID are capable of identifying contexts outside of the initial uncertainty set and degrade more gracefully.\n\n### 6.4. Sensitivity Analysis\n\nOur algorithm introduces several hyperparameters that determine the computation cost and the robustness of the policy. We study the CVaR hyperparameter $\\alpha$ below, and study the\neffect of the number of CVaR samples $N$ and the effect of the system identification ensemble size $B$ in Appendix C.3.\n\nCVaR level $\\alpha$. Lower levels of $\\alpha$ in principle leads to a more robust policy. However, too low levels of $\\alpha$ can harm performance as the actor becomes too conservative. In terms of the computational cost, however, the algorithm computes $\\lfloor\\alpha N\\rfloor$ gradients to approximate the CVaR gradient, is therefore more efficient with smaller $\\alpha$ 's. In Fig. 5, we plot the average and worst-case performance of SIRSA with $\\alpha \\in\\{0.25,0.5,0.75,1.0\\}$ in Peg Insertion, and find that the best performing value of $\\alpha$ is 0.5 , which strikes a good balance in risk sensitivity and computation cost.\n\n## 7. Discussion\n\nMany robust RL solutions require a prior uncertainty set over the unobserved parameters of the test-time environment to learn a robust policy for this set at training time. To alleviate the need to build in this prior, we introduced and studied the multi-set robustness problem to facilitate generalization to new uncertainty sets. We further recognized the potential sub-optimality of memoryless robust RL methods in systems with parameters that can be identified from a history of interactions, and designed a framework that combines probabilistic system identification with the multi-set robust RL objective. Our method improves upon existing methods on a range of challenging control domains in terms of worst-case performance on new uncertainty sets.\n\nWhile we believe the multi-set robustness problem represents a more general and useful framing of robustness to variations, there is also a number of interesting future directions. For example, SIRSA currently assumes the contexts that underlie the training tasks are observed to train an ensemble of predictive models via supervised learning. To remove this assumption, one can leverage tools from unsupervised representation learning to learn a representation of the true context. Another question is whether there are robustness benefits when the agent explicitly seeks exploratory actions that minimize its uncertainty over the parameters. In our experiments, we designed two types of parameters: non-identifiable parameters whose uncertainty cannot be reduced at all and identifiable parameters whose uncertainty can be reduced within a single timestep. In settings where identifiable parameters require coordinated sequences of actions to reduce uncertainty over, the agent needs to be able to balance exploration, exploitation, and robustness."
    },
    {
      "markdown": "## References\n\nAbraham, I., Handa, A., Ratliff, N., Lowrey, K., Murphey, T. D., and Fox, D. Model-based generalization under parameter uncertainty using path integral control. IEEE Robotics and Automation Letters, 5(2):2864-2871, 2020.\n\nBrockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba, W. Openai gym. arXiv preprint arXiv:1606.01540, 2016.\n\nBrunskill, E. Bayes-optimal reinforcement learning for discrete uncertainty domains. In Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems-Volume 3, pp. 1385-1386, 2012.\n\nChen, X., Wang, C., Zhou, Z., and Ross, K. Randomized ensembled double q-learning: Learning fast without a model. International Conference on Learning Representations (ICLR), 2021.\n\nDorfman, R. and Tamar, A. Offline meta reinforcement learning - identifiability challenges and effective data collection strategies. Neural Information Processing Systems (NeurIPS), 2021.\n\nDuan, Y., Schulman, J., Chen, X., Bartlett, P. L., Sutskever, I., and Abbeel, P. RL2: Fast reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016.\n\nDuff, M. O. Optimal Learning: Computational procedures for Bayes-adaptive Markov decision processes. University of Massachusetts Amherst, 2002.\n\nFinn, C., Abbeel, P., and Levine, S. Model-agnostic metalearning for fast adaptation of deep networks. International Conference on Machine Learning (ICML), 2017.\n\nGhavamzadeh, M., Mannor, S., Pineau, J., and Tamar, A. Bayesian reinforcement learning: A survey. arXiv preprint arXiv:1609.04436, 2016.\n\nGuez, A., Silver, D., and Dayan, P. Efficient bayes-adaptive reinforcement learning using sample-based search. Neural Information Processing Systems (NeurIPS), 2012.\n\nGuez, A., Silver, D., and Dayan, P. Scalable and efficient bayes-adaptive reinforcement learning based on montecarlo tree search. Journal of Artificial Intelligence Research, 48:841-883, 2013.\n\nHaarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning, pp. 1861-1870. PMLR, 2018.\n\nHallak, A., Di Castro, D., and Mannor, S. Contextual markov decision processes. arXiv preprint arXiv:1502.02259, 2015.\n\nHausman, K., Springenberg, J. T., Wang, Z., Heess, N., and Riedmiller, M. Learning an embedding space for transferable robot skills. International Conference on Learning Representations (ICLR), 2018.\n\nHumplik, J., Galashov, A., Hasenclever, L., Ortega, P. A., Teh, Y. W., and Heess, N. Meta reinforcement learning as task inference. arXiv preprint arXiv:1905.06424, 2019.\n\nIyengar, G. N. Robust dynamic programming. Mathematics of Operations Research, 30(2):257-280, 2005.\n\nKumar, A., Fu, Z., Pathak, D., and Malik, J. Rma: Rapid motor adaptation for legged robots. Robotics: Science and Systems (RSS), 2021.\n\nKumar, S., Kumar, A., Levine, S., and Finn, C. One solution is not all you need: Few-shot extrapolation via structured maxent rl. Neural Information Processing Systems (NeurIPS), 33, 2020.\n\nLee, A. X., Nagabandi, A., Abbeel, P., and Levine, S. Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model. Neural Information Processing Systems (NeurIPS), 2020.\n\nLee, G., Hou, B., Mandalika, A., Lee, J., Choudhury, S., and Srinivasa, S. S. Bayesian policy optimization for model uncertainty. International Conference on Learning Representations (ICLR), 2019.\n\nLim, S. H., Xu, H., and Mannor, S. Reinforcement learning in robust markov decision processes. Neural Information Processing Systems (NeurIPS), 26:701-709, 2013.\n\nLin, Z., Thomas, G., Yang, G., and Ma, T. Model-based adversarial meta-reinforcement learning. In Neural Information Processing Systems (NeurIPS), 2020.\n\nMankowitz, D. J., Levine, N., Jeong, R., Abdolmaleki, A., Springenberg, J. T., Shi, Y., Kay, J., Hester, T., Mann, T., and Riedmiller, M. Robust reinforcement learning for continuous control with model misspecification. In International Conference on Learning Representations (ICLR), 2019.\n\nMehta, B., Diaz, M., Golemo, F., Pal, C. J., and Paull, L. Active domain randomization. In Conference on Robot Learning (CoRL), pp. 1162-1176. PMLR, 2020.\n\nMordatch, I., Lowrey, K., and Todorov, E. Ensemble-cio: Full-body dynamic motion planning that transfers to physical humanoids. In 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 5307-5314. IEEE, 2015."
    },
    {
      "markdown": "Morimoto, J. and Doya, K. Robust reinforcement learning. Neural Information Processing Systems (NeurIPS), pp. 1061-1067, 2000.\n\nMozian, M., Higuera, J. C. G., Meger, D., and Dudek, G. Learning domain randomization distributions for training robust locomotion policies. In 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 6112-6117. IEEE, 2020.\n\nNagabandi, A., Clavera, I., Liu, S., Fearing, R. S., Abbeel, P., Levine, S., and Finn, C. Learning to adapt in dynamic, real-world environments through meta-reinforcement learning. International Conference on Learning Representations (ICLR), 2019.\n\nNilim, A. and El Ghaoui, L. Robust control of markov decision processes with uncertain transition matrices. Operations Research, 53(5):780-798, 2005.\n\nParisotto, E., Ba, L. J., and Salakhutdinov, R. Actor-mimic: Deep multitask and transfer reinforcement learning. International Conference on Learning Representations (ICLR), 2016.\n\nPerez, C. F., Such, F. P., and Karaletsos, T. Efficient transfer learning and online adaptation with latent variable models for continuous control. arXiv preprint arXiv:1812.03399, 2018.\n\nPinto, L., Davidson, J., Sukthankar, R., and Gupta, A. Robust adversarial reinforcement learning. In International Conference on Machine Learning (ICML), pp. 28172826. PMLR, 2017.\n\nRajeswaran, A., Ghotra, S., Ravindran, B., and Levine, S. Epopt: Learning robust neural network policies using model ensembles. In International Conference on Learning Representations (ICLR), 2016.\n\nRakelly, K., Zhou, A., Quillen, D., Finn, C., and Levine, S. Efficient off-policy meta-reinforcement learning via probabilistic context variables. International Conference on Machine Learning (ICML), 2019.\n\nRockafellar, R. T., Uryasev, S., et al. Optimization of conditional value-at-risk. Journal of risk, 2:21-42, 2000.\n\nRoss, S., Chaib-draa, B., and Pineau, J. Bayes-adaptive pomdps. Neural Information Processing Systems (NeurIPS), pp. 1225-1232, 2007.\n\nRothfuss, J., Lee, D., Clavera, I., Asfour, T., and Abbeel, P. Promp: Proximal meta-policy search. International Conference on Learning Representations (ICLR), 2019.\n\nSchoettler, G., Nair, A., Ojea, J. A., Levine, S., and Solowjow, E. Meta-reinforcement learning for robotic industrial insertion tasks. In 2020 IEEE/RSJ International\n\nConference on Intelligent Robots and Systems (IROS), pp. 9728-9735. IEEE, 2020.\n\nSharma, A., Harrison, J., Tsao, M., and Pavone, M. Robust and adaptive planning under model uncertainty. In Proceedings of the International Conference on Automated Planning and Scheduling, volume 29, pp. 410-418, 2019.\n\nSodhani, S., Zhang, A., and Pineau, J. Multi-task reinforcement learning with context-based representations. International Conference on Machine Learning (ICML), 2021.\n\nSong, X., Yang, Y., Choromanski, K., Caluwaerts, K., Gao, W., Finn, C., and Tan, J. Rapidly adaptable legged robots via evolutionary meta-learning. In 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 3769-3776. IEEE, 2020.\n\nTamar, A., Glassner, Y., and Mannor, S. Optimizing the cvar via sampling. In Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015.\n\nTan, J., Zhang, T., Coumans, E., Iscen, A., Bai, Y., Hafner, D., Bohez, S., and Vanhoucke, V. Sim-to-real: Learning agile locomotion for quadruped robots. Robotics: Science and Systems (RSS), 2018.\n\nTang, Y. C., Zhang, J., and Salakhutdinov, R. Worst cases policy gradients. Conference on Robot Learning (CoRL), 2019.\n\nTeh, Y. W., Bapst, V., Czarnecki, W. M., Quan, J., Kirkpatrick, J., Hadsell, R., Heess, N., and Pascanu, R. Distral: Robust multitask reinforcement learning. Neural Information Processing Systems (NeurIPS), 2017.\n\nTessler, C., Efroni, Y., and Mannor, S. Action robust reinforcement learning and applications in continuous control. In International Conference on Machine Learning (ICML), pp. 6215-6224. PMLR, 2019.\n\nVinitsky, E., Du, Y., Parvate, K., Jang, K., Abbeel, P., and Bayen, A. Robust reinforcement learning using adversarial populations. arXiv preprint arXiv:2008.01825, 2020.\n\nWang, J. X., Kurth-Nelson, Z., Tirumala, D., Soyer, H., Leibo, J. Z., Munos, R., Blundell, C., Kumaran, D., and Botvinick, M. Learning to reinforcement learn. arXiv preprint arXiv:1611.05763, 2016.\n\nWang, Y. and Cunningham, J. P. Posterior collapse and latent variable non-identifiability. In Third Symposium on Advances in Approximate Bayesian Inference, 2020.\n\nYang, R., Xu, H., Wu, Y., and Wang, X. Multi-task reinforcement learning with soft modularization. Neural Information Processing Systems (NeurIPS), 2020."
    },
    {
      "markdown": "Yu, T., Kumar, S., Gupta, A., Levine, S., Hausman, K., and Finn, C. Gradient surgery for multi-task learning. Neural Information Processing Systems (NeurIPS), 2020.\n\nYu, W., Tan, J., Liu, C. K., and Turk, G. Preparing for the unknown: Learning a universal policy with online system identification. Robotics: Science and Systems (RSS), 2017.\n\nYu, W., Liu, C. K., and Turk, G. Policy transfer with strategy optimization. International Conference on Learning Representations (ICLR), 2019.\n\nZahavy, T., Barreto, A., Mankowitz, D. J., Hou, S., O’Donoghue, B., Kemaev, I., and Singh, S. Discovering a set of policies for the worst case reward. In International Conference on Learning Representations (ICLR), 2020.\n\nZhang, J., Cheung, B., Finn, C., Levine, S., and Jayaraman, D. Cautious adaptation for reinforcement learning in safety-critical settings. In International Conference on Machine Learning, pp. 11055-11065. PMLR, 2020.\n\nZhang, J., Wang, J., Hu, H., Chen, T., Chen, Y., Fan, C., and Zhang, C. Metacure: Meta reinforcement learning with empowerment-driven exploration. In International Conference on Machine Learning, pp. 12600-12610. PMLR, 2021.\n\nZhao, T. Z., Nagabandi, A., Rakelly, K., Finn, C., and Levine, S. Meld: Meta-reinforcement learning from images via latent state models. Conference on Robot Learning (CoRL), 2020.\n\nZintgraf, L., Shiarlis, K., Igl, M., Schulze, S., Gal, Y., Hofmann, K., and Whiteson, S. Varibad: A very good method for bayes-adaptive deep rl via meta-learning. International Conference on Learning Representations (ICLR), 2020.\n\nZintgraf, L. M., Shiarlis, K., Kurin, V., Hofmann, K., and Whiteson, S. Fast context adaptation via meta-learning. International Conference on Machine Learning (ICML), 2019."
    },
    {
      "markdown": "## Appendix\n\n## A. Implementation Details\n\nBelow, we provide details of the implementation of our algorithm SIRSA and the baselines.\n\n## A.1. SIRSA (OURS)\n\nThe System ID baseline shares the same implementation details and hyperparameters as SIRSA, except it does not implement the CVaR objective.\n\nSystem identification model. We train an ensemble of $B=4$ models, which are MLPs with 2 fully-connected layers of size 64 in the Point Mass domain; 2 fully-connected laters of size 256 in all other domains. Each model takes a $\\left(\\mathbf{s}, \\mathbf{a}, \\mathbf{s}^{\\prime}\\right)$ tuple, outputs a prediction for the context, and is trained with the MSE of the predicted and true context.\n\nPolicy and critic networks. The policy and critic networks are MLPs with 2 fully-connected layers of size 64 in the Point Mass domain; 2 fully-connected layers of size 256 in all other domains.\n\nIn the Minitaur domain, training the critic was somewhat unstable. We therefore implement REDQ (Chen et al., 2021), which has been found to stabilize and accelerate learning. It trains an ensemble of $M$ critic networks. To compute the Qvalues, REDQ randomly subsamples 2 of the critic networks and take their minimum. In our Minitaur experiments, we use $M=8$ for our method as well as all comparisons.\n\nCVaR approximation. In our experiments, we use $N=50$ CVaR samples to approximate the gradient of the CVaR.\n\nTraining phases. Before updating the policy with the CVaR objective, we first train the actor and critic networks with the SAC objectives:\n\n$$\n\\mathcal{J}_{Q}=\\mathbb{E}_{(\\mathbf{s}, \\mathbf{a}) \\sim \\mathcal{D}}\\left[\\frac{1}{2}\\left(Q_{\\theta}(\\mathbf{s}, \\mathbf{a})-\\hat{Q}(\\mathbf{s}, \\mathbf{a})\\right)^{2}\\right]\n$$\n\nwith\n\n$$\n\\hat{Q}(\\mathbf{s}, \\mathbf{a})=r(\\mathbf{s}, \\mathbf{a})+\\gamma \\mathbb{E}_{s^{\\prime} \\sim p}\\left[V_{\\psi}\\left(s^{\\prime}\\right)\\right]\n$$\n\nwhere $V_{\\psi}$ is a target network, whose weights are an exponentially moving average of the value function weights. Then, after $T_{\\text {threshold }}$ iterations, we optimize policy with the CVaR objective defined in Eqn. 3 instead.\n\nIn Point Mass, we optimize the SAC objectives for 25 K iterations then optimize the CVaR for another 25 K iterations, for a total of 50 K training iterations. In the Minitaur and Peg Insertion domains, we pre-train for 150 K iterations then optimize CVaR for 150 K iterations for a total of 300 K . In Half-Cheetah, the pre-training is 2.5 M , and the the CVaR optimization is 0.5 M long, for a total of 3 M steps.\n\n## A.2. EPOPT (RAJESWARAN ET AL., 2016)\n\nPolicy and critic networks. The policy and critic networks are MLPs with 2 fully-connected layers of size 64 in the Point Mass domain, and with 2 fully-connected layers of size 256 in all other domains. These networks are trained with the SAC objectives. To sample a batch of size $D$ that they train on, we first sample $D / \\alpha$ s-a-s' tuples from the replay buffer. Then, we sort the samples by the return of the trajectory they came from, and return the lowest $D$ tuples.\n\n## A.3. WCPG (TANG ET AL., 2019)\n\nPolicy and critic networks. The policy and critic networks are MLPs with 2 fully-connected layers of size 64 in the Point Mass domain; 2 fully-connected layers of size 256 in all other domains. These networks are trained with SAC.\n\nQ-variance network. WCPG requires an estimate of the variance of $Q(\\mathbf{s}, \\mathbf{a}, \\Xi)$. We train an MLP with 2 fullyconnected layers of size 256 via the MSE to predict the variance of $Q(\\mathbf{s}, \\mathbf{a}, \\Xi)$. To generate the target variance this network regresses to, we generate a Monte-Carlo approximation: we sample 50 contexts $\\left\\{c_{i}\\right\\}_{i: 1 \\ldots 50}$ from $\\Xi$, evaluate them with our context-conditioned critic $Q_{\\theta}\\left(\\mathbf{s}, \\mathbf{a}, c_{i}\\right)$, and compute their sample variance.\n\nCVaR approximation. WCPG assumes that the Q-values follow a Gaussian distribution, and can therefore compute the CVaR of the uncertainty set $\\Xi=(\\mu, \\sigma)$ in closed form as follows:\n\n$$\nQ(\\mathbf{s}, \\mathbf{a}, \\mu)-(\\phi(\\alpha) / \\Phi(\\alpha)) \\sqrt{\\text { Q-Var }(\\mathbf{s}, \\mathbf{a}, \\Xi)}\n$$\n\nwhere $\\mathrm{Q}-\\operatorname{Var}(\\mathbf{s}, \\mathbf{a}, \\Xi)$ represents the output of the Qvariance network, $\\phi(\\cdot)$ is the standard normal distribution, and $\\Phi(\\cdot)$ is its CDF:\n\n$$\n\\Psi(x)=\\frac{1}{2}(1+\\operatorname{erf}(x / \\sqrt{2}))\n$$\n\n## B. Environment Details\n\nIn this section, we provide details of each of the four experimental domains. The domains are visualized in Fig. 6.\n\n## B.1. Point MASS\n\nIn this environment, the agent is a point mass particle and must go around the roundabout to the goal on its other side. The $x$-velocity of the agent is fixed within a task but its precise value is unknown. The size of the roundabout $r$ is also unknown. Each episode is 50 timesteps long, and the state consists of the $x y$-position of the agent and whether the agent is on top of the roundabout. There is one action input which controls the change in $y$-position. The reward function is defined as\n\n$$\nr_{t}=1-\\mathbb{1}\\left(x_{t}^{2}+y_{t}^{2}<r^{2}\\right)-8\\left|y_{t}\\right|\n$$"
    },
    {
      "markdown": "![img-4.jpeg](img-4.jpeg)\n\nFigure 6. The domains in our evaluation. Point Mass: In this domain, there is uncertainty in the size of the obstacle (in lavender). The blue concentric circle represents the worst-case obstacle size within the uncertainty set. The agent must navigate around the obstacle. Its path is highlighted in pink-fuschia. Minitaur: The uncertainty lies in the mass of the robot and the failure rate of one of its legs. Half-Cheetah: The uncertainty lies in the mass of the agent, the friction of its joints, and failure rate of one of its joints. Peg Insertion: This domain has uncertainty over the size of the peg and the step size of the robot's actions.\n\nHence, the agent is encouraged to take a tight turn around the roundabout without colliding with it. We train on 20 different uncertainty sets, with 3 sampled contexts from each set, for a total of 60 different contexts.\n\n## B.2. MinitAUR\n\nThis environment simulates an 8-DoF minitaur robot whose objective is to walk forward as quickly as possible. The mass of the robot's base and mass of the legs vary across tasks but are unknown. In each task, there is also a probability of failure $p_{\\text {fail }}$ for one of the four legs, which is also unknown. Each episode terminates when the robot falls or after 500 timesteps. At each timestep, the action input to one leg is dropped with probability $p_{\\text {fail }}$. The agent's state consists of the robot's roll, roll rate, pitch, pitch rate, and the angles of each of the eight motors. We also append the history of last five actions, which the reward function depends on. The reward function is defined as\n\n$$\nr_{t}=v_{t}-0.01\\left\\|\\mathbf{a}_{t}-2 \\mathbf{a}_{t-1}+\\mathbf{a}_{t-2}\\right\\|\n$$\n\nwhere $v_{t}$ is the observed velocity of the robot. We train on 80 different uncertainty sets, with 10 sampled contexts from each set, for a total of 800 different contexts.\n\n## B.3. Half-CheEtaH\n\nThis environment modifies the Half-Cheetah task from OpenAI Gym (Brockman et al., 2016). The agent's objective is to run forward as quickly as possible, starting from rest. The mass of the agent and the friction of the joints vary across tasks and are unknown. Like in the Minitaur environment, there is a probability of failure $p_{\\text {fail }}$ for one of the six joints, which varies across tasks. Each episode lasts 500 timesteps. At each timestep, the action input to one of the joints is dropped with probability $p_{\\text {fail }}$. The agent's state consists of the velocity of the agent's center of mass and angular velocity of each of its six joints, and actions correspond to torques applied to each joint. The reward function is\n\n$$\nr_{t}=v_{t}-0.05\\left\\|\\mathbf{a}_{t}\\right\\|\n$$\n\nwhere $v_{t}$ is the observed velocity of the agent. We train on 80 different uncertainty sets, with 10 sampled contexts from each set, for a total of 800 different contexts.\n\n## B.4. SAWYER Peg InSERTIon\n\nIn this modified peg-insertion task (Zhao et al., 2020), a 7-DoF Sawyer robot arm needs to insert the peg attached to its end-effector into one of the two boxes in as few timesteps as possible. Across tasks, the scaling of the joint position controller and size of the peg vary and are unknown. The first box is closer to the initial position of the robot, but it also has a smaller hole. The second box on the other hand is farther from the initial position, and has a larger hole that will allow the agent to always successfully insert the peg into. Each episode lasts 50 timesteps, which is only enough time to try one of the boxes. The agent's state consists of the robot's joint angles, joint velocities, and end-effector pose. The reward at each time-step is 1 if the peg is successfully inserted into one of the boxes and 0 otherwise.\n\nSince this is a sparse-reward task, we first pre-train an agent on the dense rewards for 300 K environment steps with Soft Actor-Critic and save its replay buffer. We then initialize training of our method and of all comparisons with the restored replay buffer. We train on 80 different uncertainty sets, with 10 sampled contexts from each set, for a total of 800 different contexts.\n\n## C. Experimental Results\n\n## C.1. Point Mass Visualizations\n\nIn Table 5, we visualize the trajectories of policies learned by System ID, Set-EPOpt, and SIRSA $(\\alpha=0.25)$ in the Point Mass domain. The unfilled blue circle marks the maximum obstacle size of the particular uncertainty set, while the true obstacle is shaded in lavender. The agent always starts to the left of the obstacle and must reach the right side. We mark the trajectory in orange. The trajectories taken by System ID (top row) tend to be overly optimistic and make"
    },
    {
      "markdown": "![img-5.jpeg](img-5.jpeg)\n\nTable 5. Visualizations of the trajectories taken by policies learned with System ID, Set-EPOpt, and SIRSA in the Point Mass domain. The maximum obstacle size of this particular uncertainty set is demarcated by the unfilled blue circle, while the true obstacle in the environment is shaded in purple. The trajectory taken by the agent (from left to right) is in orange.\ncontact with the obstacle, incurring penalty. Meanwhile, the trajectories taken by Set-EPOpt (middle row) are always along the maximum obstacle of the uncertainty set, which can be overly conservative, e.g., in the third task (third column), when the true obstacle is smaller. Finally, SIRSA (bottom row) strikes a balance between the two. Specifically, in the first task (first column), the agent initially makes contact with the obstacle but corrects its trajectory thereafter. In the subsequent two tasks, the agent is not too conservative but avoids the obstacle most of the time.\n\n## C.2. PERFORMANCE FOR DIFFERENT CVAR $\\alpha$ 's\n\nIn Tables 6 and 7, we report the full results for the $\\alpha$ dependent methods, EPOpt, Set-EPOpt, WCPG, Set-WCPG, and SIRSA (Ours), for $\\alpha$ values in $\\{0.25,0.50,0.75,1.00\\}$.\n\n## C.3. SENSitivity ANALYSIS\n\nNumber of CVaR samples $N$. Increasing the number of Monte-Carlo samples we use to approximate the CVaR objective can improve the estimate of the function. However, it is also more costly since it requires computing $\\lfloor\\alpha N\\rfloor$ gradients. In Fig. 7 (left), we plot the average and worstcase performance of SIRSA for $N \\in\\{25,50,100,200\\}$ in the Peg Insertion domain. From these results, we conclude that there is no significant benefit to increasing the CVaR samples beyond 50 .\n\nNumber of ensemble models $B$. Increasing the ensemble size can potentially lead to improved estimates of the posterior belief distribution, but requires training more model parameters. In Fig. 7 (right), we plot the average and worstcase performance for $B \\in\\{4,8,16\\}$ in the Peg Insertion domain. Overall, the performance of SIRSA is agnostic to\n![img-6.jpeg](img-6.jpeg)\n\nFigure 7. Left: Performance of SIRSA with different amounts of CVaR samples. Right: Performance of SIRSA with different ensemble sizes. Both plots depict the means and standard errors over 10 random seeds.\nthe ensemble size in this domain."
    },
    {
      "markdown": "| Task | Method | $\\alpha$ | Min | Mean |\n| :--: | :--: | :--: | :--: | :--: |\n| Point mass | EPOpt | 0.25 | $34.3 \\pm 0.6$ | $38.7 \\pm 0.6$ |\n|  |  | 0.50 | $35.2 \\pm 0.6$ | $39.3 \\pm 0.4$ |\n|  |  | 0.75 | $36.1 \\pm 0.6$ | $39.7 \\pm 0.4$ |\n|  |  | 1.00 | $36.3 \\pm 0.6$ | $40.5 \\pm 0.4$ |\n|  | Set-EPOpt | 0.25 | $34.4 \\pm 0.5$ | $38.5 \\pm 0.4$ |\n|  |  | 0.50 | $35.3 \\pm 0.6$ | $39.2 \\pm 0.5$ |\n|  |  | 0.75 | $36.1 \\pm 0.5$ | $40.1 \\pm 0.3$ |\n|  |  | 1.00 | $37.1 \\pm 0.5$ | $40.7 \\pm 0.4$ |\n|  | WCPG | 0.25 | $34.8 \\pm 0.6$ | $39.3 \\pm 0.5$ |\n|  |  | 0.50 | $34.7 \\pm 0.6$ | $39.3 \\pm 0.5$ |\n|  |  | 0.75 | $34.6 \\pm 0.6$ | $39.3 \\pm 0.5$ |\n|  |  | 1.00 | $34.4 \\pm 0.6$ | $39.3 \\pm 0.5$ |\n|  | Set-WCPG | 0.25 | $34.7 \\pm 0.7$ | $39.0 \\pm 0.5$ |\n|  |  | 0.50 | $34.6 \\pm 0.7$ | $39.1 \\pm 0.5$ |\n|  |  | 0.75 | $34.6 \\pm 0.7$ | $39.2 \\pm 0.5$ |\n|  |  | 1.00 | $34.6 \\pm 0.7$ | $39.2 \\pm 0.6$ |\n|  | SIRSA (Ours) | 0.25 | $37.9 \\pm 0.2$ | $41.8 \\pm 0.1$ |\n|  |  | 0.50 | $37.9 \\pm 0.2$ | $41.7 \\pm 0.1$ |\n|  |  | 0.75 | $37.5 \\pm 0.2$ | $41.9 \\pm 0.1$ |\n|  |  | 1.00 | $37.4 \\pm 0.3$ | $41.3 \\pm 0.3$ |\n|  | EPOpt | 0.25 | $131.8 \\pm 10.7$ | $168.7 \\pm 7.0$ |\n|  |  | 0.50 | $160.1 \\pm 8.4$ | $192.5 \\pm 5.2$ |\n|  |  | 0.75 | $171.3 \\pm 7.4$ | $196.6 \\pm 4.4$ |\n|  |  | 1.00 | $172.2 \\pm 7.3$ | $199.9 \\pm 5.5$ |\n|  | Set-EPOpt | 0.25 | $99.3 \\pm 8.0$ | $141.2 \\pm 6.2$ |\n|  |  | 0.50 | $180.3 \\pm 8.7$ | $211.5 \\pm 3.6$ |\n|  |  | 0.75 | $181.7 \\pm 6.7$ | $213.9 \\pm 2.4$ |\n|  |  | 1.00 | $183.1 \\pm 7.5$ | $216.7 \\pm 3.7$ |\n|  | WCPG | 0.25 | $163.0 \\pm 17.4$ | $191.5 \\pm 19.1$ |\n|  |  | 0.50 | $163.5 \\pm 17.6$ | $193.1 \\pm 19.3$ |\n|  |  | 0.75 | $163.3 \\pm 18.1$ | $193.6 \\pm 19.5$ |\n|  |  | 1.00 | $165.5 \\pm 17.7$ | $193.7 \\pm 19.6$ |\n|  | Set-WCPG | 0.25 | $173.1 \\pm 9.2$ | $204.7 \\pm 5.5$ |\n|  |  | 0.50 | $173.4 \\pm 9.1$ | $205.7 \\pm 5.3$ |\n|  |  | 0.75 | $173.1 \\pm 9.3$ | $206.7 \\pm 4.9$ |\n|  |  | 1.00 | $174.5 \\pm 10.0$ | $206.9 \\pm 4.7$ |\n|  | SIRSA (Ours) | 0.25 | $169.1 \\pm 9.0$ | $205.4 \\pm 3.8$ |\n|  |  | 0.50 | $167.1 \\pm 9.3$ | $199.6 \\pm 3.5$ |\n|  |  | 0.75 | $149.6 \\pm 11.5$ | $192.7 \\pm 5.0$ |\n|  |  | 1.00 | $187.8 \\pm 7.6$ | $214.3 \\pm 2.5$ |\n\nTable 6. Evaluation on 20 new uncertainty sets. Means and standard errors are computed over 10 random seeds for each method.\n\n| Task | Method | $\\alpha$ | Min | Mean |\n| :--: | :--: | :--: | :--: | :--: |\n| Half-Cheetah | EPOpt | 0.25 | $1292 \\pm 95$ | $1478 \\pm 125$ |\n|  |  | 0.50 | $1434 \\pm 165$ | $1809 \\pm 214$ |\n|  |  | 0.75 | $1863 \\pm 172$ | $2192 \\pm 252$ |\n|  |  | 1.00 | $2272 \\pm 218$ | $2718 \\pm 325$ |\n|  | Set-EPOpt | 0.25 | $2148 \\pm 42$ | $2454 \\pm 47$ |\n|  |  | 0.50 | $2845 \\pm 29$ | $3274 \\pm 93$ |\n|  |  | 0.75 | $3246 \\pm 206$ | $3703 \\pm 171$ |\n|  |  | 1.00 | $3811 \\pm 224$ | $4474 \\pm 232$ |\n|  | WCPG | 0.25 | $3703 \\pm 256$ | $4264 \\pm 334$ |\n|  |  | 0.50 | $3724 \\pm 263$ | $4279 \\pm 328$ |\n|  |  | 0.75 | $3688 \\pm 231$ | $4291 \\pm 321$ |\n|  |  | 1.00 | $3747 \\pm 229$ | $4304 \\pm 316$ |\n|  | Set-WCPG | 0.25 | $3418 \\pm 435$ | $3973 \\pm 480$ |\n|  |  | 0.50 | $3476 \\pm 445$ | $3985 \\pm 476$ |\n|  |  | 0.75 | $3823 \\pm 200$ | $4420 \\pm 256$ |\n|  |  | 1.00 | $3873 \\pm 207$ | $4432 \\pm 253$ |\n|  | SIRSA (Ours) | 0.25 | $3742 \\pm 209$ | $4288 \\pm 228$ |\n|  |  | 0.50 | $4126 \\pm 72$ | $4806 \\pm 75$ |\n|  |  | 0.75 | $3583 \\pm 298$ | $4103 \\pm 280$ |\n|  |  | 1.00 | $4146 \\pm 112$ | $4872 \\pm 73$ |\n| Peg Insertion | EPOpt | 0.25 | $0.0 \\pm 0.0$ | $0.0 \\pm 0.0$ |\n|  |  | 0.50 | $11.4 \\pm 7.6$ | $17.7 \\pm 11.2$ |\n|  |  | 0.75 | $43.2 \\pm 5.4$ | $75.1 \\pm 8.2$ |\n|  |  | 1.00 | $41.3 \\pm 5.3$ | $93.7 \\pm 4.9$ |\n|  | Set-EPOpt | 0.25 | $22.1 \\pm 8.4$ | $33.3 \\pm 11.2$ |\n|  |  | 0.50 | $57.8 \\pm 7.1$ | $78.0 \\pm 9.5$ |\n|  |  | 0.75 | $70.6 \\pm 3.6$ | $96.9 \\pm 2.7$ |\n|  |  | 1.00 | $67.4 \\pm 4.7$ | $77.8 \\pm 8.3$ |\n|  | WCPG | 0.25 | $20.2 \\pm 5.4$ | $41.7 \\pm 8.7$ |\n|  |  | 0.50 | $28.8 \\pm 6.3$ | $57.3 \\pm 8.0$ |\n|  |  | 0.75 | $33.8 \\pm 7.2$ | $63.2 \\pm 6.9$ |\n|  |  | 1.00 | $31.8 \\pm 5.9$ | $64.1 \\pm 7.3$ |\n|  | Set-WCPG | 0.25 | $56.6 \\pm 4.5$ | $80.4 \\pm 4.7$ |\n|  |  | 0.50 | $64.0 \\pm 4.2$ | $86.5 \\pm 4.2$ |\n|  |  | 0.75 | $66.4 \\pm 4.0$ | $89.4 \\pm 3.9$ |\n|  |  | 1.00 | $68.3 \\pm 4.6$ | $92.0 \\pm 4.4$ |\n|  | SIRSA (Ours) | 0.25 | $81.1 \\pm 3.3$ | $106.0 \\pm 1.9$ |\n|  |  | 0.50 | $83.4 \\pm 4.5$ | $109.5 \\pm 2.6$ |\n|  |  | 0.75 | $76.2 \\pm 4.4$ | $102.1 \\pm 3.4$ |\n|  |  | 1.00 | $73.5 \\pm 4.7$ | $103.1 \\pm 2.3$ |\n\nTable 7. Evaluation on 20 new uncertainty sets. Means and standard errors are computed over 10 random seeds for each method."
    }
  ],
  "usage_info": {
    "pages_processed": 15,
    "doc_size_bytes": 1955717
  },
  "_metadata": {
    "model_version": "0.0.0",
    "llm_model": "mistral-ocr-2505",
    "query_data": {
      "pdf": "/Users/satyaortiz-gagne/CODE/paperext/data/cache/fulltext/c636f4eccfae3b0f1cf2be53d641f062/fulltext.pdf"
    },
    "model_id": "parsepdf"
  }
}