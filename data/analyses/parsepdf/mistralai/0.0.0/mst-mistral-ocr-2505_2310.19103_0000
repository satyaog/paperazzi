{
  "pages": [
    {
      "markdown": "# Proving Linear Mode Connectivity of Neural Networks via Optimal Transport \n\nDamien Ferbach ${ }^{1,3}$ Baptiste Goujaud ${ }^{2}$ Gauthier Gidel ${ }^{1, \\dagger}$ Aymeric Dieuleveut ${ }^{2}$ Mila, Université de Montréal ${ }^{1}$ CMAP, Ecole Polytechnique, IPP ${ }^{2}$ ENS Paris, PSL ${ }^{3}$\n\n\n#### Abstract\n\nThe energy landscape of high-dimensional non-convex optimization problems is crucial to understanding the effectiveness of modern deep neural network architectures. Recent works have experimentally shown that two different solutions found after two runs of a stochastic training are often connected by very simple continuous paths (e.g., linear) modulo a permutation of the weights. In this paper, we provide a framework theoretically explaining this empirical observation. Based on convergence rates in Wasserstein distance of empirical measures, we show that, with high probability, two wide enough two-layer neural networks trained with stochastic gradient descent are linearly connected. Additionally, we express upper and lower bounds on the width of each layer of two deep neural networks with independent neuron weights to be linearly connected. Finally, we empirically demonstrate the validity of our approach by showing how the dimension of the support of the weight distribution of neurons, which dictates Wasserstein convergence rates is correlated with linear mode connectivity.\n\n\n## 1 INTRODUCTION AND RELATED WORK\n\nTraining deep neural networks on complex tasks is a high-dimensional, non-convex optimization problem. While stochastic gradient-based methods (i.e., SGD and its derivatives) have proven highly efficient in find-\n\n## $\\dagger$ Canada CIFAR AI Chair\n\nProceedings of the $27^{\\text {th }}$ International Conference on Artificial Intelligence and Statistics (AISTATS) 2024, Valencia, Spain. PMLR: Volume 238. Copyright 2024 by the author(s).\ning a local minimum with low test error, the loss landscape of deep neural networks (DNNs) still contains numerous open questions. In particular, Goodfellow et al. [2014] try to find ways to connect two local minima reached by two independent runs of the same stochastic algorithm with different initialization and data orders. This problem has applications in diverse domains such as model averaging [Izmailov et al., 2018, Rame et al., 2022, Wortsman et al., 2022], loss landscape study [Gotmare et al., 2018, Vlaar and Frankle, 2022, Lucas et al., 2021], adversarial robustness [Zhao et al., 2020] or generalization theory [Pittorino et al., 2022, Juneja et al., 2022, Lubana et al., 2023].\n\nAn answer to this question is the mode connectivity phenomenon. It suggests the existence of a continuous low-loss path connecting all the local minima found by a given optimization procedure. The mode connectivity phenomenon has extensively been studied in the literature [Goodfellow et al., 2014, Keskar et al., 2016, Sagun et al., 2017, Venturi et al., 2019, Neyshabur et al., 2020, Tatro et al., 2020, Yunis et al., 2022, Zhou et al., 2023b] and non-linear connecting paths have been evidenced for DNNs trained on MNIST and CIFAR10 by Freeman and Bruna [2016], Garipov et al. [2018], Draxler et al. [2018].\n(Linear) mode connectivity. Formally, let $A:=$ $\\hat{f}\\left(., \\theta_{A}\\right)$ and $B:=\\hat{f}\\left(., \\theta_{B}\\right)$ two neural networks sharing a common architecture $\\hat{f}$. They are parametrized by $\\theta_{A}$ and $\\theta_{B}$ after training those networks on a data distribution $P$ with loss $\\mathcal{L}$, i.e. by minimizing $\\mathcal{E}(\\theta):=$ $\\mathbb{E}_{(x, y) \\sim P}[\\mathcal{L}(\\hat{f}(x, \\theta), y)]$ over $\\theta$. Let $p$ be a continuous path connecting $\\theta_{A}$ and $\\theta_{B}$, i.e. a continuous function defined on $[0,1]$ with $p(0)=\\theta_{A}$ and $p(1)=\\theta_{B}$. Frankle et al. [2020] initially identified the problem of linear mode connectivity and defined the error barrier height [Frankle et al., 2020, Entezari et al., 2021] of $p$ as $\\sup _{t \\in[0,1]} \\mathcal{E}(p(t))-\\left((1-t) \\mathcal{E}\\left(\\theta_{A}\\right)+t \\mathcal{E}\\left(\\theta_{B}\\right)\\right)$. The two found solutions $\\theta_{A}$ and $\\theta_{B}$ are said to be mode connected if there is a continuous path with zero error barrier height connecting them. Furthermore if $p$ is linear, that is $p(t)=(1-t) \\theta_{A}+t \\theta_{B}, \\theta_{A}$ and $\\theta_{B}$ are"
    },
    {
      "markdown": "![img-0.jpeg](img-0.jpeg)\n\nFigure 1: Permuting the neurons in the hidden layer of network $B$ to align them on network $A$\nsaid to be linearly mode connected (LMC).\n\nPermutation invariance. Recently, Singh and Jaggi [2020], Ainsworth et al. [2022] highlighted the fact that the units in a hidden layer of a given model can be permuted while preserving the network's functionality. Figure 1 shows how one can permute the hidden layer of a two-layer network to match a different target network without changing the source function. From now on, we will understand LMC modulo permutation invariance, i.e. two networks $A, B$ are said to be linear mode connected whenever there exists a permutation of neurons in each hidden layer of network $B$ such that the linear path in parameter space between network $A$ and $B$ permuted has low loss.\n\nLinear mode connectivity up to permutation. Singh and Jaggi [2020] proposed to use optimal transport (OT) theory to find soft alignment providing a \"good match\" (in a certain sense) between the neurons of two trained DNNs. Furthermore, the authors propose ways to fusion the aligned networks together in a federated learning context with local-steps. Ainsworth et al. [2022] further experimentally studied linear mode connectivity between two pre-aligned networks. The authors first align network B's weights on the weights of network A before connecting both of them by a linear path in the parameter space. They notably achieved zero-loss barrier for two trained Resnets with SGD on CIFAR10. Moreover, their experiments strongly suggest that the error barrier on a linear path gets smaller for wider networks, with a detrimental effect of big depth.\n\nPrior theoretical explanations. A recent work by Kuditipudi et al. [2019] shows that dropout stable networks (i.e. networks that are functionally stable to the action of randomly setting a fraction of their weights and normalizing the others) exhibit mode connectivity. Shevchenko and Mondelli [2020] use a mean field viewpoint to show that wide two-layer neural networks trained with SGD are dropout stable and hence show\n(non-linear) mode connectivity for two-layer neural networks in the mean field regime (i.e. one single wide hidden layer). Finally Entezari et al. [2021] show that two-layer neural networks exhibit linear mode connectivity up to permutation at initialization for parameters initialized following uniform independent distribution properly scaled. They highlight that this result could be extended to networks trained in the Neural Tangent Kernel regime where parameters stay close to initialization [Jacot et al., 2018].\n\nContributions. This paper aims at building theoretical foundations on the phenomenon of linear mode connectivity up to permutation. More precisely, we theoretically prove this phenomenon arises naturally on multi-layer perceptrons (MLPs), which goes beyond two-layer networks on which theoretical works focused so far. We also provide a new efficient way to find the right permutation to apply on the units of a neural network's layer. The paper is organized as follow:\n\n- In Section 3, we focus on two-layer neural networks in the mean field regime. While Shevchenko and Mondelli [2020] proved non-linear mode connectivity in this setting; we go further by proving linear mode connectivity up to permutation. Moreover, we provide an upper bound on the minimal width of the hidden layer to guarantee linear mode connectivity.\n- In Section 4, we use general OT theory to exhibit tight asymptotics on the minimal width of a multilayer perceptron (MLP) to ensure LMC.\n- In Section 5, we apply our general results to networks with parameters following sub-Gaussian distribution. Our result holds for deep networks, generalizing the result of Entezari et al. [2021] with better bounds. We shed light on the dependence in the dimension of the underlying distributions of the weights in each layer and explain how it connects with previous empirical observations [Ainsworth et al., 2022]. Using a model of approximately low dimensional weight distribution as a proxy of sparse feature learning, we yield more realistic bounds on the architectures of DNNs to ensure linear mode connectivity. We therefore, show why LMC is possible after training and how it depends on the complexity of the task. Finally we unify our framework with dropout stability.\n- In Section 6, we validate our theoretical framework by showing how the implicit dimension of the weight distribution is correlated with linear mode connectivity for MLPs trained on MNIST with SGD and propose a new weight matching method."
    },
    {
      "markdown": "## 2 PRELIMINARIES AND NOTATIONS\n\nNotations. Let two multilayer perceptrons (MLP) $A$ and $B$ with the same depth $L+1$ ( $L$ hidden layers), an input dimension $m_{0}$, intermediate widths $m_{1}, \\ldots, m_{L}$ and an output dimension $m_{L+1}$. Given $2(L+1)$ weights matrices $W_{A, B}^{1, \\ldots, L+1}$, and a non-linearity $\\sigma$, we define the neural network function of network A by $\\hat{f}_{A}$ (respectively $\\left.\\hat{f}_{B}\\right): \\forall x \\in \\mathbb{R}^{m_{0}}$,\n\n$$\n\\hat{f}_{A}(x):=\\hat{f}\\left(x ; \\theta_{A}\\right):=W_{A}^{L+1} \\sigma\\left(W_{A}^{L} \\ldots \\sigma\\left(W_{A}^{1} x\\right)\\right)\n$$\n\nTo $W_{A}^{\\ell} \\in \\mathcal{M}_{m_{\\ell}, m_{\\ell-1}}(\\mathbb{R})$ we associate $\\tilde{\\mu}_{A, \\ell}$ the empirical measure of its rows $\\left[W_{A}^{i}\\right]_{i:} \\in \\mathbb{R}^{m_{\\ell-1}}$ : $\\frac{1}{m_{\\ell}} \\sum_{i=1}^{m_{\\ell}} \\delta_{\\left[W_{A}^{i}\\right]_{i:}}$ which belongs to the space of probability measures $\\mathcal{P}_{1}\\left(\\mathbb{R}^{m_{\\ell-1}}\\right)$, where $\\left[W_{A}^{\\ell}\\right]_{i:}$ is the $i$ th row of the matrix and $\\delta$ denotes the Dirac measure. Note that $\\left[W_{A}^{\\ell}\\right]_{i:}$ is also the weights vector of the $i$-th neuron of the layer $\\ell$ of network $A$. Given an equi-partition ${ }^{1} \\mathcal{I}^{\\ell-1}=\\left\\{I_{1}^{\\ell-1}, \\ldots I_{\\tilde{m}_{\\ell-1}}^{\\ell-1}\\right\\}$ of $\\left[m_{\\ell-1}\\right]$ we denote $W_{A}^{\\chi_{\\ell-1}} \\in \\mathcal{M}_{m_{\\ell}, \\tilde{m}_{\\ell-1}}(\\mathbb{R})$ the matrix issued from $W_{A}^{\\ell}$ where we have summed the columns being in the same set of the partition $\\mathcal{I}^{\\ell-1}$. In that case $\\tilde{\\mu}_{A}^{\\chi_{\\ell-1}} \\in \\mathcal{P}_{1}\\left(\\mathbb{R}^{\\tilde{m}_{\\ell}}\\right)$ denotes the associate empirical measure of its rows.\nDenote $\\phi_{A}^{\\ell}(x):=\\sigma\\left(W_{A}^{\\ell} \\ldots \\sigma\\left(W_{A}^{1} x\\right)\\right)$ (respectively $\\phi_{B}^{\\ell}$ ) the activations of neurons at layer $\\ell$ of network $A$ on input $x$. The data $x$ follows a distribution $P$ in $\\mathbb{R}^{m_{0}}$.\nGiven permutations matrices $\\Pi_{\\ell} \\in \\mathcal{S}_{m_{\\ell}},{ }^{2} \\ell=1, \\ldots, L$ of each hidden layer of network $B$, the weight matrix at layer $\\ell$ of the permuted network $B$ is $\\tilde{W}_{B}^{\\ell}:=$ $\\Pi_{\\ell} W_{B}^{\\ell} \\Pi_{\\ell-1}^{T}$ and its new activation vector is $\\tilde{\\phi}_{B}^{\\ell}(x):=$ $\\Pi_{\\ell} \\phi_{B}^{\\ell}(x)$. Finally, $\\forall t \\in[0,1]$ we define $M_{t}$ the convex combination of network $A$ and $B$ permuted, with weights matrices $t W_{A}^{\\ell}+(1-t) \\tilde{W}_{B}^{\\ell}$ and $\\phi_{M_{t}}^{\\ell}$ its activations at layer $\\ell$.\nPreliminaries. We consider networks $A$ and $B$ to be independently chosen from the same distribution $Q$ on parameters. This is coherent with considering two networks initialized independently or trained independently with the same optimization procedure (\\$3). We additionally suppose the choice of $A$ and $B$ to be independent of the choice of $x \\sim P$, which is valid when evaluating models on test data not seen during training. We denote $\\mathbb{E}_{Q}, \\mathbb{E}_{P}, \\mathbb{E}_{P, Q}$ expectations with respect to the choice of the networks, the data, or both.\nTo show linear mode connectivity of networks $A$ and $B$\n\n[^0]we will show the existence of permutations $\\Pi_{1}, \\ldots, \\Pi_{L}$ of layers $1, \\ldots, L$ that align the neurons of network $B$ on the closest neurons weights of network $A$ at the same layer as shown in Figure 1. In other words, we want to find permutations that minimize for each layer $\\ell \\in[L]$ the norm $\\left\\|W_{A}^{\\ell}-\\Pi_{\\ell} W_{B}^{\\ell} \\Pi_{\\ell-1}^{T}\\right\\|_{2}$. Recursively on $\\ell$, we solve the following optimization problem:\n\n$$\n\\begin{aligned}\n\\Pi_{\\ell} & =\\underset{\\Pi \\in \\mathcal{S}_{m_{\\ell}}}{\\arg \\min }\\left\\|W_{A}^{\\ell}-\\Pi W_{B}^{\\ell} \\Pi_{\\ell-1}^{T}\\right\\|_{2}^{2} \\\\\n& =\\underset{\\pi \\in \\mathcal{S}_{m_{\\ell}}}{\\arg \\min } \\frac{1}{m_{\\ell}} \\sum_{i=1}^{m_{\\ell}}\\left\\|\\left[W_{A}^{\\ell}\\right]_{i:}-\\left[W_{B}^{\\ell} \\Pi_{\\ell-1}^{T}\\right]_{\\pi_{i:}}\\right\\|_{2}^{2}\n\\end{aligned}\n$$\n\nFor each layer, the problem can be cast as finding a pairing of weights neurons $\\left[W_{A}^{\\ell}\\right]_{i:}$ and $\\left[W_{B}^{\\ell} \\Pi_{\\ell-1}^{T}\\right]_{\\pi_{i:}}$ to minimize the sum of their Euclidean distances. It is known as the Monge problem is the optimal transport literature Peyré et al. [2019]. More precisely Equation (2) can be formulated as finding an optimal transport plan corresponding to the Wasserstein distance between the empirical measures of the rows of $W_{A}^{\\ell}$ and $W_{B}^{\\ell} \\Pi_{\\ell-1}^{T}$. We provide more details about this connection between Equation (2) and optimal transport in Appendix B.2. In the following, the $p$-Wasserstein distance will be denoted $\\mathcal{W}_{p}(\\cdot, \\cdot)$ and defined with the underlying distance $\\|\\cdot\\|_{2}$ unless expressed otherwise.\nBy controlling the cost in Equation (2) at every layer, we show that the permuted s of networks $A$ and $B$ are approximately equal. Linearly interpolating both networks will therefore keep activations of all hidden layers unchanged except the last layer which acts as a linear function of the interpolation parameter $t \\in[0,1]$.\n\n## 3 LMC FOR TWO-LAYER NNs IN THE MEAN FIELD REGIME\n\nWe will first study linear mode connectivity between a pair of two-layer neural networks independently trained with SGD for the same number of steps.\n\n### 3.1 Background on the Mean Field Regime\n\nWe will use some notations from Mei et al. [2019] and consider a two-layer neural network,\n\n$$\n\\hat{f}_{N}(x ; \\theta)=\\frac{1}{N} \\sum_{i=1}^{N} \\sigma_{*}\\left(x ; \\theta_{i}\\right)\n$$\n\nparametrized by $\\theta_{i}=\\left(a_{i}, w_{i}\\right) \\in \\mathbb{R} \\times \\mathbb{R}^{d}$ and where $\\sigma_{*}\\left(x ; \\theta_{i}\\right)=a_{i} \\sigma\\left(w_{i} x\\right)$. The parameters evolve as to minimize the following regularized cost $R_{N}(\\theta)=\\mathbb{E}_{(x, y) \\sim P}\\left[\\left(y-\\hat{f}_{N}(x ; \\theta)\\right)^{2}\\right]+\\lambda\\|\\theta\\|_{2}^{2}$. Define noisy regularized stochastic gradient descent (or noiseless regularization-free when $\\lambda=0, \\tau=0$ ) with\n\n\n[^0]:    ${ }^{1}$ All subsets have the same number of elements\n    ${ }^{2}$ We use interchangeably $\\mathcal{S}_{m}$ to denote the space of permutations of $\\{1, \\ldots, m\\}$ and the corresponding space of permutations matrices. Given $\\pi \\in \\mathcal{S}_{m}$ its corresponding permutation matrix $\\Pi$ is defined as $\\Pi_{i j}=1 \\Longleftrightarrow \\pi(i)=j$."
    },
    {
      "markdown": "step size $s_{k}$, and i.i.d. Gaussian noise $g^{k} \\sim \\mathcal{N}\\left(0, I_{d}\\right)$ :\n\n$$\n\\begin{aligned}\n& \\theta_{i}^{k+1}=\\left(1-2 \\lambda s_{k}\\right) \\theta_{i}^{k} \\\\\n& +2 s_{k}\\left(y_{k}-\\hat{f}_{N}\\left(x_{k} ; \\theta^{k}\\right)\\right) \\nabla_{\\theta} \\sigma_{*}\\left(x_{k} ; \\theta_{i}^{k}\\right)+\\sqrt{\\frac{2 s_{k} \\tau}{d}} g_{i}^{k}\n\\end{aligned}\n$$\n\nIt will be useful to consider $\\rho_{N}^{k}:=\\frac{1}{N} \\sum_{i=1}^{N} \\delta_{\\theta_{i}^{k}}$ the empirical distribution of the weights after $k$ SGD steps. Indeed some recent works [Chizat and Bach, 2018, Mei et al., 2018, 2019] have shown that when setting the width $N$ to be large and the step size $s_{k}$ to be small, the empirical distribution of weights during training remains close to an empirical measure drawn from the solution of a partial differential equation (PDE) we explicit in Appendix C.1. Especially, the parameters $\\left\\{\\theta_{i}^{k}, i \\in[N]\\right\\}$ evolve approximately independently.\n\n### 3.2 Proving LMC in the mean field setting\n\nDefine respectively the alignment of a neuron function on the data and the correlation between two neurons:\n\n$$\n\\begin{aligned}\n& V\\left(\\theta_{1}\\right):=a v(w):=-\\mathbb{E}_{P}\\left[y \\sigma_{*}\\left(x ; \\theta_{1}\\right)\\right] \\\\\n& U\\left(\\theta_{1}, \\theta_{2}\\right):=a_{1} a_{2} u\\left(w_{1}, w_{2}\\right):=\\mathbb{E}_{P}\\left[\\sigma_{*}\\left(x ; \\theta_{1}\\right) \\sigma_{*}\\left(x ; \\theta_{2}\\right)\\right]\n\\end{aligned}\n$$\n\nand we for $\\varepsilon>0$ fixed we note the step size\n\n$$\ns_{k}=\\varepsilon \\xi(k \\varepsilon)\n$$\n\nwhere $\\xi$ is a positive scaling function. The underlying training time up to step $k_{T}$ is defined as $T:=\\sum_{k=1}^{k_{T}} s_{k}$. We now state the standard assumptions to work on the mean field regime [Mei et al., 2019],\nAssumption 1. The function $t \\mapsto \\xi(t)$ is bounded Lipschitz. The non-linearity $\\sigma$ is bounded Lipschitz and the data distribution has a bounded support. The functions $w \\mapsto v(w)$ and $\\left(w_{1}, w_{2}\\right) \\mapsto u\\left(w_{1}, w_{2}\\right)$ are differentiable, with bounded and Lipschitz continuous gradient. The weights at initialization $\\theta_{i}^{0}$ are i.i.d. with distribution $\\rho_{0}$ which has bounded support.\n\nAssumption 1 imposes that the step size is of order $\\mathcal{O}(\\varepsilon)$ and its variations are of order $\\mathcal{O}\\left(\\varepsilon^{2}\\right)$. Constant step size $\\varepsilon$ will work. Bounded non-linearity include arctan and sigmoid but excludes ReLU. While it is a standard assumption in mean field theory ([Mei et al., 2018, 2019]), we mention in §C that this assumption can be relaxed by the weaker assumption that the nonlinearity stays small on some big enough compact set.\nThe second assumption is technical and only used for studying noisy regularized SGD.\nAssumption 2. $V, U$ are four times continuously differentiable and $\\nabla_{1}^{k} u\\left(\\theta_{1}, \\theta_{2}\\right)$ is uniformly bounded for $0 \\leq k \\leq 4$.\n\nThe following theorem states that two wide enough two-layer neural networks trained independently with\n\nSGD exhibit, with high probability, a linear connection of the prediction modulo permutations for all data.\n\nTheorem 3.1. Consider two two-layer neural networks as in Equation (3) trained with equation SGD with the same initialization over the weights independently and for the same underlying time $T$. Suppose Assumptions 1 and 2 to hold. Then $\\forall \\delta$, err, $\\exists N_{\\min }$ such that if $N \\geq$ $N_{\\min }, \\exists \\varepsilon_{\\max }(N)$ such that if $\\varepsilon \\leq \\varepsilon_{\\max }(N)$ in Equation (4), then with probability at least $1-\\delta$ over the training process, there exists a permutation of the second network's hidden layer such that for almost every $x \\sim P$ :\n\n$$\n\\begin{aligned}\n& \\left|t \\hat{f}_{N}\\left(x ; \\theta_{A}\\right)+(1-t) \\hat{f}_{N}\\left(x ; \\theta_{B}\\right)\\right. \\\\\n& \\left.-\\hat{f}_{N}\\left(x ; t \\theta_{A}+(1-t) \\hat{\\theta}_{B}\\right)\\right| \\leq \\operatorname{err}, \\quad \\forall t \\in[0,1]\n\\end{aligned}\n$$\n\nRemark. Assumption 2 is not used when studying noiseless regularization-free $\\operatorname{SGD}(\\lambda=0, \\tau=0)$.\n\nCorollary 3.2. Under assumptions of Theorem 3.1, $\\forall \\delta$, err $>0, \\exists N_{\\min }^{\\prime}, \\forall N \\geq$ $N_{\\min }^{\\prime}, \\exists \\varepsilon_{\\max }^{\\prime}(N), \\forall \\varepsilon \\leq \\varepsilon_{\\max }^{\\prime}(N)$ in Equation (4), then with probability at least $1-\\delta$ over the training process, there exists a permutation of the second network's hidden layer such that $\\forall t \\in[0,1]:$\n\n$$\n\\begin{aligned}\n& \\mathbb{E}_{P}\\left[\\left(\\hat{f}_{N}\\left(x ; t \\theta_{A}+(1-t) \\tilde{\\theta}_{B}\\right)-y\\right)^{2}\\right] \\leq \\operatorname{err} \\\\\n& +\\mathbb{E}_{P}\\left[t\\left(\\hat{f}_{N}\\left(x ; \\theta_{A}\\right)-y\\right)^{2}+(1-t)\\left(\\hat{f}_{N}\\left(x ; \\theta_{B}\\right)-y\\right)^{2}\\right]\n\\end{aligned}\n$$\n\nDiscussion. Two wide enough two-layer neural networks wide enough trained with SGD are therefore Linear Mode Connected with an upper bound on the error tolerance we explicit in Appendix C. We have extensively used the independence between weights in the mean field regime to apply OT bounds on convergence rates of empirical measures. To go beyond the two-layer case, we will need to make such an assumption on the distribution of weights. Note that this is true at initialization and after training for twolayer networks. Studying the independence of weights in the multi-layer case is a natural avenue for future work, already studied in $\\Uparrow$.\n\n## 4 GENERAL STRATEGY FOR MULTI-LAYER NETWORKS\n\nWe now build the foundations to study the case of multi-layer neural networks (see Equation (1))."
    },
    {
      "markdown": "We first write one formal property expressing the existence of permutations of neurons of network $B$ up to layer $\\ell$ such that the activations of network $A$, network $B$ permuted and the mean network $M_{t}$ are close up to layer $\\ell$. This property is trivially satisfied at the input layer. We then show that under two formal assumptions on the weights matrices of networks $A$ and $B$, this property still hold at layer $\\ell+1$.\n\n### 4.1 Formal Property at layer $\\ell$\n\nLet $\\varepsilon>0, m_{\\ell} \\geq \\tilde{m}_{l}$ and $m_{\\ell+1} \\geq \\tilde{m}_{\\ell+1}$. Assume $\\frac{m_{\\ell}}{\\tilde{m}_{t}}, \\frac{m_{\\ell+1}}{\\tilde{m}_{\\ell+1}} \\in \\mathbb{N}$ to simplify technical details but this hypothesis can easily be removed.\nProperty 1. There exists two constants $E_{\\ell}, E_{\\ell}$ such that given weight matrices up to layer $\\ell, W_{A . B}^{1, \\ell}, W_{B}^{1, \\ldots, \\ell}$ one can find $\\ell$ permutations $\\Pi_{1}, \\cdots, \\Pi_{\\ell}$ of the neurons in the hidden layers 1 to $\\ell$ of network $B$, an equi-partition $\\mathcal{I}^{\\ell}=\\left\\{I_{1}^{\\ell}, \\ldots, I_{\\tilde{m}_{\\ell}}^{\\ell}\\right\\}$, and a map $\\phi^{\\ell}(x) \\in \\mathbb{R}^{n}$ such that $\\forall k \\in\\left[\\tilde{m}_{\\ell}\\right], \\forall i, j \\in I_{k}^{\\ell}, \\phi_{i}^{\\ell}(x)=\\phi_{j}^{\\ell}(x)$ such that:\n\n$$\n\\begin{aligned}\n& \\mathbb{E}_{P, Q}\\left\\|\\phi^{\\ell}(x)\\right\\|_{2}^{2} \\leq E_{\\ell} m_{\\ell} \\\\\n& \\mathbb{E}_{P, Q}\\left\\|\\phi_{A}^{\\ell}(x)-\\phi^{\\ell}(x)\\right\\|_{2}^{2} \\leq E_{\\ell} m_{\\ell} \\\\\n& \\mathbb{E}_{P, Q}\\left\\|\\tilde{\\phi}_{B}^{\\ell}(x)-\\phi^{\\ell}(x)\\right\\|_{2}^{2} \\leq E_{\\ell} m_{\\ell} \\\\\n& \\mathbb{E}_{P, Q}\\left\\|\\phi_{M_{\\ell}}^{\\ell}(x)-\\phi^{\\ell}(x)\\right\\|_{2}^{2} \\leq E_{\\ell} m_{\\ell}, \\quad \\forall t \\in[0,1]\n\\end{aligned}\n$$\n\nThis property not only requires proximity between activations $\\phi_{A}^{\\ell}(x), \\phi_{B}^{\\ell}(x)$ at layer $\\ell$ but requires the existence of a vector $\\phi^{\\ell}(x)$ whose coefficients in the same groups of the partition $\\mathcal{I}^{\\ell}$ are equal, and therefore lives in a $\\tilde{m}_{\\ell}$. It bounds the size of the function space available at layer $\\ell$ and hence allows to use an effective width $\\tilde{m}_{\\ell}$ independent of the real width $m_{\\ell}$, which can be much larger. It is crucial in order to show LMC for neural networks of constant width across layers. The introduction of such a map $\\phi^{\\ell}(x)$ is non trivial and is an important contribution since it allows to extend results of Entezari et al. [2021] beyond two layers.\n\n### 4.2 Assumptions on the weight distribution\n\nWe now make an assumption on the empirical distribution of the weights $\\tilde{\\mu}_{A, \\ell+1}$ at layer $\\ell+1$ of $W_{A}^{\\ell+1}$.\nAssumption 3. There exists an integer $\\tilde{m}_{\\ell+1}$ such that for all equi-partiton $\\mathcal{I}^{\\ell}$ of $\\left[m_{\\ell}\\right]$ with $\\tilde{m}_{\\ell}$ sub-sets, there exists a random empirical measure $\\hat{\\mu}_{\\tilde{m}_{\\ell+1}}$ independent of $A$ and $B$ composed of $\\tilde{m}_{\\ell+1}$ vectors in $\\mathbb{R}^{m_{\\ell}}$, such that $\\mathbb{E}_{Q}\\left[\\mathcal{W}_{2}^{2}\\left(\\tilde{\\mu}_{A, \\ell+1}^{\\mathcal{I}^{\\ell}}, \\tilde{\\mu}_{\\tilde{m}_{\\ell+1}}^{\\mathcal{I}^{\\ell}}\\right)\\right] \\leq C_{1}$.\n\nThis assumption requires that the empirical distribution with $m_{\\ell+1}$ points of the neurons' weights of network $A$ at layer $\\ell+1$ can be approximated by an empirical measure with a smaller $\\tilde{m}_{\\ell+1}$ number of points.\n\nNote that it implies proximity in Wasserstein distance between $\\tilde{\\mu}_{A}^{\\mathcal{I}^{\\ell}}$ and $\\tilde{\\mu}_{B}^{\\mathcal{I}^{\\ell}}$ by a triangular inequality.\nWe finally assume some central limit behavior when summing the errors made for each neuron of layer $\\ell$.\nAssumption 4. There exists a constant $C_{2}$ such that $\\forall X \\in \\mathbb{R}^{m_{l}}$ we have:\n\n$$\n\\begin{aligned}\n\\max \\left\\{\\mathbb{E}_{Q}\\left[\\left\\|W_{A}^{\\ell+1} X\\right\\|_{2}^{2}\\right], \\mathbb{E}_{Q}\\left[\\left\\|W_{\\tilde{m}_{l+1}} X\\right\\|_{2}^{2}\\right]\\right\\} \\\\\n\\leq C_{2} \\frac{m_{\\ell+1}}{m_{\\ell}}\\|X\\|_{2}^{2}\n\\end{aligned}\n$$\n\nFinally, we consider the following assumption on the non-linearity, verified for example by pointwise ReLU.\nAssumption 5. $\\sigma$ is pointwise, 1-Lipschitz, $\\sigma(0)=0$.\n\n### 4.3 Propagating Property 1 to layer $\\ell+1$\n\nWe state now how Property 1 propagates throughout the layers using Assumptions 3 to 5 with new parameters $E_{\\ell+1}, E_{\\ell+1}$. We give a proof in Appendix B.6.\n\nLemma 4.1. Let $\\ell \\in\\{0, \\cdots, L-1\\}$ and suppose Property 1 to hold at layer $\\ell$ and Assumptions 3 to 5 to hold, then Property 1 still holds at the next layer with $\\tilde{m}_{\\ell+1}$ given in Assumption 3 and\n\n$$\n\\begin{aligned}\n& E_{\\ell+1}=C_{2} E_{\\ell} \\\\\n& E_{\\ell+1}=2 C_{2} E_{\\ell}+2 C_{1} \\tilde{m}_{\\ell} E_{\\ell}\n\\end{aligned}\n$$\n\n## 5 LMC FOR RANDOM MULTI-LAYER NNs\n\nWe will make the following assumption on the empirical distribution of neurons weights $\\tilde{\\mu}_{A, \\ell}, \\tilde{\\mu}_{B, \\ell}$ of $W_{A}^{\\ell}, W_{B}^{\\ell}$ at layer $\\ell$.\nAssumption 6 (Independence of neurons weights). $\\tilde{\\mu}_{A, \\ell}, \\tilde{\\mu}_{B, \\ell}$ correspond to two i.i.d drawings of vectors with distribution $\\mu_{\\ell}$ i.e., $\\tilde{\\mu}_{A, \\ell}, \\tilde{\\mu}_{B, \\ell}$ have the law of $\\frac{1}{m_{\\ell}} \\sum_{i=1}^{m_{\\ell}} \\delta_{x_{i}}$ where $x_{i} \\sim \\mu_{\\ell}$ i.i.d.\n\nAssumption 6 is verified for example at initialization but more generally when weights do not depend too much one of each other. This case still holds for wide two-layer neural networks trained with SGD and is at the heart of the proof of Theorem 3.1.\n\n### 5.1 Showing LMC for multilayer MLPs under Gaussian distribution\n\nWe first examine the case $\\mu_{\\ell}=\\mathcal{N}\\left(0, \\frac{I_{m_{\\ell-1}}}{m_{\\ell-1}}\\right)$. We moreover assume that the input data distribution has bounded second moment: $\\mathbb{E}_{P}\\left[\\|x\\|_{2}^{2}\\right] \\leq m_{0}$."
    },
    {
      "markdown": "Our strategy detailed in Appendix B. 7 consists in showing that wide enough such networks will satisfy Assumptions 3 and 4 with well controlled constants $C_{1}, C_{2}$. We can then apply Lemma 4.1 successively $L$ times to get the following lemma:\n\nLemma 5.1. Under normal initialization of the weights, given $\\varepsilon>0$, if $m_{0} \\geq 5$, there exists minimal widths $\\tilde{m}_{1}, \\ldots, \\tilde{m}_{L}$ such that if $m_{1} \\geq$ $\\tilde{m}_{1}, \\ldots, m_{L} \\geq \\tilde{m}_{L}$, Property 1 is verified at the last hidden layer $L$ for $\\underline{E}_{L}=1, E_{L}=\\varepsilon^{2}$. Moreover, $\\forall \\ell \\in[L], \\exists T_{\\ell}$ which does only depend on $L, \\ell$ such that one can define recursively $\\tilde{m}_{\\ell}$ as $\\tilde{m}_{0}=m_{0}$ and\n\n$$\n\\tilde{m}_{\\ell}=\\tilde{\\mathcal{O}}\\left(\\frac{T_{\\ell}}{\\varepsilon}\\right)^{\\tilde{m}_{\\ell-1}}\n$$\n\nDiscussion. The hypothesis $m_{0} \\geq 5$ is technical and could be relaxed at the price of slightly changing the bound on $\\tilde{m}_{1}$. Lemma 5.1 shows that given two random networks whose widths $m_{\\ell}$ is larger than $\\tilde{m}_{\\ell}$, we can permute neurons of the second one such that their activations at layer $\\ell$ are both close to the one of the networks on a linear path in parameter's space.\nAs $\\varepsilon$ goes to 0 , the width of the layer $\\ell+1$ must scale at least as $\\left(\\frac{1}{\\varepsilon}\\right)^{\\tilde{m}_{\\ell-1}}$. This is a fundamental bound due to the convergence rate in Wasserstein distance of empirical measures. It imposes a recursive exponential growth in the width needed with respect to depth. This condition appears excessive as compared to the typical width of neural networks used in practice. We highlight here that Ainsworth et al. [2022] empirically demonstrates that networks at initialization do not exhibit LMC and that the loss barrier is erased only after a sufficient number of SGD steps.\n\n### 5.2 Showing Linear Mode Connectivity\n\nWe make the following assumption on the loss function to show LMC from Lemma 5.1.\nAssumption 7. $\\forall y \\in \\mathbb{R}^{m_{L+1}}$, the loss $\\mathcal{L}(\\cdot, y)$ is convex and 1-Lipschitz.\n\nWe finally prove the following bound on the loss of the mean network $M_{t}$ in Appendix B.8:\n\nTheorem 5.2. Under normal initialization of the weights, for $m_{1} \\geq \\tilde{m}_{1}, \\cdots, m_{L} \\geq \\tilde{m}_{L}$ as defined in Lemma 5.1, $m_{0} \\geq 5$, and under Assumption 7 we know that $\\forall t \\in[0,1]$, with $Q$-probability at least $1-\\delta_{Q}$, there exists permutations of hidden layers $1, \\ldots, L$ of network $B$ that are independent\nof $t$, such that:\n\n$$\n\\begin{aligned}\n\\mathbb{E}_{P}\\left[\\mathcal{L}\\right. & \\left(\\hat{f}_{M_{t}}(x), y\\right)] \\leq t \\mathbb{E}_{P}\\left[\\mathcal{L}\\left(\\hat{f}_{A}(x), y\\right)\\right]+ \\\\\n& (1-t) \\mathbb{E}_{P}\\left[\\mathcal{L}\\left(\\hat{f}_{B}(x), y\\right)\\right]+\\frac{4 \\sqrt{m_{L+1}} \\delta_{Q}}{2} \\varepsilon\n\\end{aligned}\n$$\n\nDiscussion. The minimal width at layer $\\ell$ needed for Theorem 5.2 is recursively $\\tilde{m}_{l} \\sim \\varepsilon^{-\\tilde{m}_{l-1}}$. Applied to randomly initialized two-layer networks, we need a hidden layer's dimension of $\\varepsilon^{-m_{0}}$ as opposed to Entezari et al. [2021] which prove a bound of $\\varepsilon^{-\\left(2 m_{0}+4\\right)}$.\n\n### 5.3 Tightness of the bound dependency with respect to the error tolerance\n\nWe discuss here the tightness of the minimal width $\\tilde{m}_{\\ell}$ we require in Lemma 5.1 with respect to the error tolerance $\\varepsilon$. The recursive exponential growth of the width in the form $\\tilde{m}_{\\ell} \\sim\\left(\\frac{1}{\\varepsilon}\\right)^{\\tilde{m}_{\\ell-1}}$ is a consequence of the convergence rate of Wasserstein distance of empirical measures in dimension $\\tilde{m}_{\\ell-1}$ at the rate $1 / \\tilde{m}_{\\ell-1}$. Theorem 5.3 provides a corresponding lower bound which shows that this recursive exponential growth is tight at the precise rate $\\left(\\frac{1}{\\varepsilon}\\right)^{\\tilde{m}_{\\ell-1}}$ (just take $n=$ $\\tilde{m}_{\\ell-1}, m=\\tilde{m}_{\\ell}, \\mu=\\mu_{\\ell}, x=\\phi_{A}^{\\ell-1}(x), W_{A, B}=W_{A, B}^{\\ell}$ ). A proof is given in Appendix B.11.\n\nTheorem 5.3. Let $n \\geq 1, x \\sim P \\in \\mathcal{P}_{1}\\left(\\mathbb{R}^{n}\\right)$ and $\\mu \\in \\mathcal{P}\\left(\\mathbb{R}^{n}\\right)$ such that $\\frac{\\partial \\mu}{\\partial \\text { Let }} \\leq F_{1}$. Suppose $\\Sigma=$ $\\mathbb{E}\\left[x x^{T}\\right]$ is full rank $n$. Let $m \\geq 1$ and $W_{A}, W_{B} \\in$ $\\mathcal{M}_{m, n}(\\mathbb{R})$ whose rows are drawn i.i.d. from $\\mu$. Then, there exists $F_{0}$ such that\n\n$$\n\\mathbb{E}_{W_{A}, W_{B}}\\left[\\min _{\\Pi \\in \\mathcal{S}_{m}} \\mathbb{E}_{P}\\left\\|\\left(W_{A}-\\Pi W_{B}\\right) x\\right\\|_{2}^{2}\\right] \\geq F_{0}\\left(\\frac{1}{m}\\right)^{2 / n}\n$$\n\nRemark 1. Using an effective width $\\tilde{m}_{\\ell-1}$ smaller and independent of the real width $m_{\\ell-1}$ allows to show LMC for networks of constant hidden width $m_{1}=m_{2}=\\ldots=m_{L}$ as soon as they verify $m_{1} \\geq \\tilde{m}_{1}, \\ldots, m_{L} \\geq \\tilde{m}_{L}$ where $\\tilde{m}_{1}, \\ldots, \\tilde{m}_{L}$ are defined in Lemma 5.1. Without this trick, we need a recursive exponential growth of the real width $m_{\\ell} \\sim\\left(\\frac{1}{\\varepsilon}\\right)^{m_{\\ell-1}}$.\nRemark 2. Motivated by the fact that feature learning may concentrate the weight distribution on low dimensional sub-space, we could extend our proofs to the case where the underlying weight distribution has a support with smaller dimension to get recursive bounds no longer at rate $\\tilde{m}_{\\ell-1}$ but at a smaller one. Note this is unlikely to happen as we expect the matrix of weight vectors of a given layer to be full rank. Therefore, we study in the next section the case when"
    },
    {
      "markdown": "this matrix is approximately low rank, or equivalently when the weight distribution is concentrated around a low dimensional approximated support.\n\n### 5.4 Approximately low dimensional supported measures\n\nFor the sake of clarity, assume from now on that the layer $\\ell-1$ of network $A$ has been permuted such that for $\\mathcal{I}^{\\ell-1}=\\left\\{I_{1}^{\\ell-1}, \\ldots, I_{\\tilde{m}_{\\ell-1}}^{\\ell-1}\\right\\}$ (given in Property 1) we have $I_{1}^{\\ell-1}=\\left\\{1, \\ldots, p_{\\ell-1}\\right\\}, \\ldots, I_{\\tilde{m}_{\\ell-1}}^{\\ell-1}=$ $\\left\\{m_{\\ell-1}-p_{\\ell-1}+1, \\ldots, m_{\\ell-1}\\right\\}$ with $p_{\\ell-1}=m_{\\ell-1} / \\tilde{m}_{\\ell-1}\\}$. This assumption is mild since we can always consider a permuted version of network $A$ without changing the problem.\n\nMotivated by the discussion in Appendix B.9.1 we consider the model where the weights at layer $\\ell$ are initialized i.i.d. multivariate Gaussian $\\mu_{\\ell}=\\mathcal{N}\\left(0, \\Sigma^{\\ell-1}\\right)$ with\n\n$$\n\\Sigma^{\\ell-1}:=\\operatorname{Diag}\\left(\\lambda_{1}^{\\ell} I_{p_{\\ell-1}}, \\lambda_{2}^{\\ell} I_{p_{\\ell-1}}, \\ldots, \\lambda_{\\tilde{m}_{\\ell-1}}^{\\ell} I_{p_{\\ell-1}}\\right)\n$$\n\nwith $\\frac{1}{m_{\\ell-1}} \\frac{\\tilde{m}_{\\ell-1}}{k_{\\ell-1}} \\geq \\lambda_{1}^{\\ell} \\geq \\lambda_{2}^{\\ell} \\geq \\ldots \\geq \\lambda_{\\tilde{m}_{\\ell-1}}^{\\ell}$ with $k_{\\ell-1} \\leq \\tilde{m}_{\\ell-1}$ an approximate dimension of the support of the underlying weights distribution. Note that to balance the low dimensionality of the weights distribution, we have replaced the upper-bound on the eigenvalues $\\frac{1}{\\tilde{m}_{\\ell-1}}$ by the greater value $\\frac{1}{m_{\\ell-1}} \\frac{\\tilde{m}_{\\ell-1}}{k_{\\ell-1}}$ to avoid vanishing activations when $\\ell$ grows which would have made our result vacuous.\n\nThe following assumption states that the weights distribution $\\mu_{\\ell}^{\\ell \\ell-1}$ at layer $\\ell$ considered in $\\mathcal{P}_{1}\\left(\\mathbb{R}^{\\tilde{m}_{\\ell-1}}\\right)$ (with the operation explicited in Section 2) is approximately of dimension $k_{\\ell-1}=e \\tilde{m}_{\\ell-1}$. The approximation becomes more correct as $\\eta \\rightarrow 0$.\nAssumption 8 (Approximately low-dimensionality). $\\exists \\eta, e \\in(0,1), \\forall \\ell \\in[L], \\frac{\\sqrt{\\sum_{j=k_{l-1}}^{m_{\\ell-1}} \\lambda_{j}^{\\ell}}}{4 \\sqrt{\\sum_{j=1}^{k_{\\ell-1}} \\lambda_{j}^{\\ell}}} \\leq \\eta, \\frac{k_{\\ell-1}}{\\tilde{m}_{\\ell-1}}=e$\n\nTheorem 5.4. Under Assumptions 7 and 8, given $\\varepsilon>0$, if $e m_{0} \\geq 5$ there exists minimal widths $\\tilde{m}_{1}, \\ldots, \\tilde{m}_{L}$ such that if $\\eta^{-k_{0}} \\geq m_{1} \\geq$ $\\tilde{m}_{1}, \\ldots, \\eta^{-k_{L-1}} \\geq m_{L} \\geq \\tilde{m}_{L}$, Property 1 is verified at the last hidden layer $L$ for $E_{L}=1, E_{L}=$ $\\varepsilon^{2}$. Moreover, $\\forall \\ell \\in[L], \\exists T_{j}^{\\prime}$ which does only depend on $L, e, \\ell$, such that one can define recursively $\\tilde{m}_{\\ell}$ as\n\n$$\n\\tilde{m}_{\\ell}=\\tilde{\\mathcal{O}}\\left(\\frac{T_{j}^{\\prime}}{\\varepsilon}\\right)^{k_{\\ell-1}}=\\tilde{\\mathcal{O}}\\left(\\frac{T_{j}^{\\prime}}{\\varepsilon}\\right)^{e \\tilde{m}_{\\ell-1}}\n$$\n\nwhere $\\tilde{m}_{0}=m_{0}$. Moreover there exists permutations of hidden layers $1, \\ldots, L$ of network $B$ s.t.\n\n$$\n\\begin{aligned}\n& \\forall t \\in[0,1], \\text { with } Q \\text {-probability at least } 1-\\delta_{Q} \\text { : } \\\\\n& \\mathbb{E}_{P}\\left[\\mathcal{L}\\left(\\hat{f}_{M_{t}}(x), y\\right)\\right] \\leq t \\mathbb{E}_{P}\\left[\\mathcal{L}\\left(\\hat{f}_{A}(x), y\\right)\\right] \\\\\n& \\quad+(1-t) \\mathbb{E}_{P}\\left[\\mathcal{L}\\left(\\hat{f}_{B}(x), y\\right)\\right]+\\frac{4 \\sqrt{m_{L-1}}}{\\sqrt{e \\delta_{Q}^{2}}} \\varepsilon\n\\end{aligned}\n$$\n\nDiscussion. We give a proof in Appendix B.10. For $\\eta$ small enough, the distribution of weights is approximately lower dimensional. It yields faster convergence rates until $m$ becomes exponentially big in $\\eta$. This prevents the previous recursive exponential growth of width with respect to depth, though asymptotically, we recover the same rates as in Theorem 5.2. The smaller $e$, the lower dimensional are the distributions, and the less the width needs to grow when $\\varepsilon \\rightarrow 0$. The problem in that model is that the constant $T_{i}^{\\prime}$ explodes if $e \\rightarrow 0$, which prevents using a model with fixed $k_{\\ell}$ across the layers for the weight distribution. We want to highlight here that the proof can be extended to such a case, but we need to assume that the constant $C_{2}$ is bounded and not depending on $e$ across the layers in Lemma 4.1 (recall that with our proof, we had $\\left.C_{2}=\\frac{1}{e}\\right)$. This assumption seems coherent because the average activations don't explode across layers in the model. Assuming this, the bound we obtain for $\\tilde{m}_{\\ell}$ in Theorem 5.4 is completely independent on $\\tilde{m}_{\\ell-1}$, and there is no recursive exponential growth in the width needed across the layers. We give a more explicit discussion in Appendix B.12.\n\n### 5.5 LMC for sub-Gaussian distributions\n\nStill under the setting of Assumption 6 assume that the underlying distribution $\\mu_{\\ell}$ verifies for each layer $\\ell \\in[L+1]$ : if $X \\sim \\mu_{\\ell}$ then, $\\forall j \\neq k \\in\\left[m_{l-1}\\right], X_{j} \\amalg X_{k}$. Moreover $\\forall i \\in\\left[\\tilde{m}_{\\ell-1}\\right], \\forall j, k \\in I_{i}^{\\ell-1}$,\n\n$$\n\\mathbb{E}\\left[X_{j}^{2}\\right]=\\mathbb{E}\\left[X_{k}^{2}\\right]=\\lambda_{i}^{\\ell}\n$$\n\nFinally suppose the variables are sub-Gaussian i.e., $\\exists K>0, \\forall i \\in\\left[\\tilde{m}_{\\ell-1}\\right], \\forall j \\in I_{i}^{\\ell-1}, \\forall c>0$,\n\n$$\n\\mathbb{P}\\left(\\left|X_{j}\\right| \\geq c\\right) \\leq 2 \\exp \\left(-\\frac{c^{2}}{K \\lambda_{i}^{\\ell}}\\right)\n$$\n\nWe explain in Appendix B. 13 why both Theorem 5.2 (in the case $\\lambda_{1}^{\\ell}=\\ldots=\\lambda_{\\tilde{m}_{\\ell-1}}^{\\ell}=1 / m_{\\ell-1}$ ) and Theorem 5.4 hold with mild modifications in the constants. It extends our previous result considerably to LMC for any large enough networks whose weights are i.i.d. and whose underlying distribution has a sub-Gaussian tail (for example uniform distribution)."
    },
    {
      "markdown": "### 5.6 Link with dropout stability\n\nIn Appendix B.14, we build a first step towards unifying our study with the dropout stability viewpoint [Kuditipudi et al., 2019, Shevchenko and Mondelli, 2020] by showing in a simplified setting how networks become dropout stable in the same asymptotics on the width as the one needed in our Theorem 5.2.\n\n## 6 EXPERIMENTS\n\nOur previous study shows the influence of the dimension of the underlying weight distribution on LMC effectiveness. Based on this insight we develop a new weight matching method at the crossroads between previous naive weight matching (WM) and activation matching (AM) methods [Ainsworth et al., 2022]. Given $n$ training points $x_{i}, i \\in[n]$, denote $Z_{A}^{\\ell} \\in \\mathcal{M}_{m_{\\ell}, n}(\\mathbb{R})$ (respectively $Z_{B}^{\\ell}$ ) the activations $\\phi_{A}^{\\ell}\\left(x_{i}\\right)$ for the $n$ data points $x_{i}$. Further denote $\\Sigma_{A}^{\\ell}:=\\frac{1}{n} Z_{A}^{\\ell}\\left[Z_{A}^{\\ell}\\right]^{T} \\approx \\mathbb{E}_{P}\\left[\\phi_{A}^{\\ell}(x)\\left[\\phi_{A}^{\\ell}(x)\\right]^{T}\\right]$. We aim at finding for each layer $\\ell$ the optimal permutation $\\Pi$ minimizing the cost (respectively for naive WM, our new WM method and AM):\n\n$$\n\\begin{aligned}\n& \\min _{\\Pi \\in S_{m_{\\ell}}}\\left\\|W_{A}^{\\ell}-\\Pi W_{B}^{\\ell} \\Pi_{\\ell-1}^{T}\\right\\|_{2}^{2} \\\\\n& \\min _{\\Pi \\in S_{m_{\\ell}}}\\left\\|W_{A}^{\\ell}-\\Pi W_{B}^{\\ell} \\Pi_{\\ell-1}^{T}\\right\\|_{2, \\Sigma_{A}^{\\ell-1}}^{2} \\\\\n& \\min _{\\Pi \\in S_{m_{\\ell}}}\\left\\|Z_{A}^{\\ell}-\\Pi Z_{B}^{\\ell}\\right\\|_{2}^{2}\n\\end{aligned}\n$$\n\nwhere $\\|\\cdot\\|_{2, \\Sigma_{A}^{\\ell-1}}$ is the norm ${ }^{3}$ induced by the scalar product $(X, Y) \\mapsto \\operatorname{tr}\\left(X \\Sigma_{A}^{\\ell-1} Y^{T}\\right)$. We both theoretically support the gain of our method in Theorem D. 2 and empirically verify that this method constantly and substantially outperforms naive Weight Matching across different learning rates when training with SGD.\n\nWe train a three hidden layer MLP of width 512 on MNIST with learning rates varying between $10^{-4}$ and $10^{-1}$ across 4 runs. We plot on Figure 2b the approximate dimension of the considered covariance matrix for each matching method: $W_{A}^{\\ell}\\left[W_{A}^{\\ell}\\right]^{T}$ for WM (naive), $W_{A}^{\\ell} \\Sigma_{A}^{\\ell-1}\\left[W_{A}^{\\ell}\\right]^{T}$ for WM (ours) and $\\Sigma_{A}^{\\ell}$ for AM (see §D.2). Our code is available at https: //github.com/damienferbach/OT_LMC/tree/main.\n\n[^0]![img-1.jpeg](img-1.jpeg)\n(a) Mean test loss of the trained networks $A$ and $B$ and error barrier on the linear path $M_{t}, t \\in[0,1]$ across different learning rate values for each matching problem.\n![img-2.jpeg](img-2.jpeg)\n(b) Approximate dimension $\\operatorname{Dim}(S):=\\operatorname{tr}(S)^{2} / \\operatorname{tr}\\left(S^{2}\\right)$ of the matrices considered in the matching problems at each layer.\n\nFigure 2: Statistics of the average network $M$ over the linear path between networks $A$ and $B$ using respectively weight matching (blue), weight matching using covariance of activations and activations (green), and activation matching (orange)\n\nWe see on Figure 2 the detrimental effect of high approximate dimension on LMC effectiveness, therefore validating our theoretical approach. Note that for a learning rate of $10^{-1}$ the correlation is less clear but a trend is visible on decreasing dimension for naive WM as it performs better (and increasing dimension for AM and our WM method as it performs comparatively less well). An alternative would be to use a proxy taking the diameter of the distributions into account (and not only the dimension of their support). Finally, experiments on Adam lead to less clear results that we did not report as more experimental investigation is needed. In particular, understanding the impact of the optimizer on the independence of weights during training is crucial, as it is a central assumption\n\n\n[^0]:    ${ }^{3}$ Semi-norm in full generality (if $\\Sigma_{A}^{\\ell-1}$ is not full rank)"
    },
    {
      "markdown": "in our study.\n\n## 7 DISCUSSION\n\nOptimal transport serves as a good framework to study linear mode connectivity of neural networks. This paper uses convergence rates of empirical measures in Wasserstein distance to upper bound the test error of the linear combination of two networks in weight space modulo permutation symmetries. Our main assumption is the independence of all neuron's weight vectors inside a given layer. This assumption is trivially true at initialization but remains valid for wide two-layers networks trained with SGD. We experimentally demonstrate the correlation between the dimension of the underlying weight distribution with LMC effectiveness and design a new weight matching method that significantly outperforms existing ones. A natural direction for future work is to focus on the behaviour of the weights distribution inside each layer of DNNs and their independence. Moreover, extending our results to only assuming approximate independence of weights is a natural direction as it seems a more realistic setting.\n\n## Acknowledgements\n\nThe work of B. Goujaud and A. Dieuleveut is partially supported by ANR-19-CHIA-0002-01/chaire SCAI, and Hi!Paris. This work was partly funded by the French government under management of Agence Nationale de la Recherche as part of the \"Investissements d'avenir\" program, reference ANR-19-P3IA0001 (PRAIRIE 3IA Institute). D. Ferbach acknowledges a stipend from ENS Paris as fonctionnaire stagiaire. This work was partly done during an internship of D. Ferbach at Ecole Polytechnique.\n\n## References\n\nL. Adilova, A. Fischer, and M. Jaggi. Layerwise linear mode connectivity. arXiv preprint arXiv:2307.06966, 2023. 12\nS. K. Ainsworth, J. Hayase, and S. Srinivasa. Git rebasin: Merging models modulo permutation symmetries. arXiv preprint arXiv:2209.04836, 2022. 2, $6,8,38$\nA. K. Akash, S. Li, and N. G. Trillos. Wasserstein barycenter-based model fusion and linear mode connectivity of neural networks. arXiv preprint arXiv:2210.06671, 2022. 12\nL. Chizat and F. Bach. On the global convergence of gradient descent for over-parameterized models using optimal transport. Advances in neural information processing systems, 31, 2018. 4, 29\nF. Draxler, K. Veschgini, M. Salmhofer, and F. Hamprecht. Essentially no barriers in neural network energy landscape. In J. Dy and A. Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 1309-1318. PMLR, 10-15 Jul 2018. URL https://proceedings.mlr. press/v80/draxler18a.html. 1\nR. Entezari, H. Sedghi, O. Saukh, and B. Neyshabur. The role of permutation invariance in linear mode connectivity of neural networks. arXiv preprint arXiv:2110.06296, 2021. 1, 2, 5, 6\nD. Ferbach, C. Tsirigotis, G. Gidel, and A. Bose. A general framework for proving the equivariant strong lottery ticket hypothesis. arXiv preprint arXiv:2206.04270, 2022. 12\nJ. Frankle and M. Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635, 2018. 12\nJ. Frankle, G. K. Dziugaite, D. Roy, and M. Carbin. Linear mode connectivity and the lottery ticket hypothesis. In International Conference on Machine Learning, pages 3259-3269. PMLR, 2020. 1, 12\nC. D. Freeman and J. Bruna. Topology and geometry of half-rectified network optimization. arXiv preprint arXiv:1611.01540, 2016. 1\nT. Garipov, P. Izmailov, D. Podoprikhin, D. P. Vetrov, and A. G. Wilson. Loss surfaces, mode connectivity, and fast ensembling of dnns. Advances in neural information processing systems, 31, 2018. 1\nI. J. Goodfellow, O. Vinyals, and A. M. Saxe. Qualitatively characterizing neural network optimization problems. arXiv preprint arXiv:1412.6544, 2014. 1\nA. Gotmare, N. S. Keskar, C. Xiong, and R. Socher. Using mode connectivity for loss landscape analysis. arXiv preprint arXiv:1806.06977, 2018. 1\nP. Izmailov, D. Podoprikhin, T. Garipov, D. Vetrov, and A. G. Wilson. Averaging weights leads to wider optima and better generalization. arXiv preprint arXiv:1803.05407, 2018. 1\nA. Jacot, F. Gabriel, and C. Hongler. Neural tangent kernel: Convergence and generalization in neural networks. Advances in neural information processing systems, 31, 2018. 2\nJ. Juneja, R. Bansal, K. Cho, J. Sedoc, and N. Saphra. Linear connectivity reveals generalization strategies. arXiv preprint arXiv:2205.12411, 2022. 1\nN. S. Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and P. T. P. Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836, 2016. 1"
    },
    {
      "markdown": "R. Kuditipudi, X. Wang, H. Lee, Y. Zhang, Z. Li, W. Hu, R. Ge, and S. Arora. Explaining landscape connectivity of low-cost solutions for multilayer nets. Advances in neural information processing systems, 32, 2019. 2, 8, 28\nE. S. Lubana, E. J. Bigelow, R. P. Dick, D. Krueger, and H. Tanaka. Mechanistic mode connectivity. In International Conference on Machine Learning, pages 22965-23004. PMLR, 2023. 1, 12\nJ. Lucas, J. Bae, M. R. Zhang, S. Fort, R. Zemel, and R. Grosse. Analyzing monotonic linear interpolation in neural network loss landscapes. arXiv preprint arXiv:2104.11044, 2021. 1\nE. Malach, G. Yehudai, S. Shalev-Schwartz, and O. Shamir. Proving the lottery ticket hypothesis: Pruning is all you need. In International Conference on Machine Learning, pages 6682-6691. PMLR, 2020. 12\nS. Mei, A. Montanari, and P.-M. Nguyen. A mean field view of the landscape of two-layer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665-E7671, 2018. 4, 29, 37\nS. Mei, T. Misiakiewicz, and A. Montanari. Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit. In Conference on Learning Theory, pages 2388-2464. PMLR, 2019. 3, 4, 29, 30, $31,32,33,34,35,37$\nS. I. Mirzadeh, M. Farajtabar, D. Gorur, R. Pascanu, and H. Ghasemzadeh. Linear mode connectivity in multitask and continual learning. arXiv preprint arXiv:2010.04495, 2020. 12\nB. Neyshabur, H. Sedghi, and C. Zhang. What is being transferred in transfer learning? Advances in neural information processing systems, 33:512-523, 2020. 1\nP.-M. Nguyen and H. T. Pham. A rigorous framework for the mean field limit of multilayer neural networks. Mathematical Statistics and Learning, 6 (3):201-357, 2023. 37\nA. Pensia, S. Rajput, A. Nagle, H. Vishwakarma, and D. Papailiopoulos. Optimal lottery tickets via subset sum: Logarithmic over-parameterization is sufficient. Advances in neural information processing systems, 33:2599-2610, 2020. 12\nG. Peyré, M. Cuturi, et al. Computational optimal transport: With applications to data science. Foundations and Trends ${ }^{\\circledR}$ in Machine Learning, 11(5-6): 355-607, 2019. 3, 12, 13\nJ. M. Phillips. Chernoff-hoeffding inequality and applications. arXiv preprint arXiv:1209.6396, 2012. 13\nF. Pittorino, A. Ferraro, G. Perugini, C. Feinauer, C. Baldassi, and R. Zecchina. Deep networks on\ntoroids: removing symmetries reveals the structure of flat regions in the landscape geometry. In International Conference on Machine Learning, pages 17759-17781. PMLR, 2022. 1\nY. Qin, C. Qian, J. Yi, W. Chen, Y. Lin, X. Han, Z. Liu, M. Sun, and J. Zhou. Exploring mode connectivity for pre-trained language models. arXiv preprint arXiv:2210.14102, 2022. 12\nA. Rame, M. Kirchmeyer, T. Rahier, A. Rakotomamonjy, P. Gallinari, and M. Cord. Diverse weight averaging for out-of-distribution generalization. Advances in Neural Information Processing Systems, 35:10821-10836, 2022. 1\nL. Sagun, U. Evci, V. U. Guney, Y. Dauphin, and L. Bottou. Empirical analysis of the hessian of over-parametrized neural networks. arXiv preprint arXiv:1706.04454, 2017. 1\nA. Shevchenko and M. Mondelli. Landscape connectivity and dropout stability of sgd solutions for overparameterized neural networks. In International Conference on Machine Learning, pages 8773-8784. PMLR, 2020. 2, 8, 28\nS. P. Singh and M. Jaggi. Model fusion via optimal transport. Advances in Neural Information Processing Systems, 33:22045-22055, 2020. 2, 12\nN. Tatro, P.-Y. Chen, P. Das, I. Melnyk, P. Sattigeri, and R. Lai. Optimizing mode connectivity via neuron alignment. Advances in Neural Information Processing Systems, 33:15300-15311, 2020. 1\nL. Venturi, A. S. Bandeira, and J. Bruna. Spurious valleys in one-hidden-layer neural network optimization landscapes. Journal of Machine Learning Research, 20:133, 2019. 1\nC. Villani et al. Optimal transport: old and new, volume 338. Springer, 2009. 12, 14\nT. J. Vlaar and J. Frankle. What can linear interpolation of neural network loss landscapes tell us? In International Conference on Machine Learning, pages 22325-22341. PMLR, 2022. 1\nH. Wang, M. Yurochkin, Y. Sun, D. Papailiopoulos, and Y. Khazaeni. Federated learning with matched averaging. arXiv preprint arXiv:2002.06440, 2020. 12\nJ. Weed and F. Bach. Sharp asymptotic and finitesample rates of convergence of empirical measures in wasserstein distance. Advances in Neural Information Processing Systems, 2019. 14, 15, 16, 26, 32, 33,35\nM. Wortsman, G. Ilharco, S. Y. Gadre, R. Roelofs, R. Gontijo-Lopes, A. S. Morcos, H. Namkoong, A. Farhadi, Y. Carmon, S. Kornblith, et al. Model soups: averaging weights of multiple fine-tuned"
    },
    {
      "markdown": "models improves accuracy without increasing inference time. In International Conference on Machine Learning, pages 23965-23998. PMLR, 2022. 1\nY. Wu. Packing, covering, and consequences on minimax risk. http://www.stat.yale.edu/ yw562/ teaching/598/lec14.pdf, 2016. [Online; accessed October-10-2023]. 14\nD. Yunis, K. K. Patel, P. H. P. Savarese, G. Vardi, J. Frankle, M. Walter, K. Livescu, and M. Maire. On convexity and linear mode connectivity in neural networks. In OPT 2022: Optimization for Machine Learning (NeurIPS 2022 Workshop), 2022. 1\nM. Yurochkin, M. Agarwal, S. Ghosh, K. Greenewald, N. Hoang, and Y. Khazaeni. Bayesian nonparametric federated learning of neural networks. In International conference on machine learning, pages 7252-7261. PMLR, 2019. 12\nP. Zhao, P.-Y. Chen, P. Das, K. N. Ramamurthy, and X. Lin. Bridging mode connectivity in loss landscapes and adversarial robustness. arXiv preprint arXiv:2005.00060, 2020.1\nT. Zhou, J. Zhang, and D. H. Tsang. Mode connectivity and data heterogeneity of federated learning. arXiv preprint arXiv:2309.16923, 2023a. 12\nZ. Zhou, Y. Yang, X. Yang, J. Yan, and W. Hu. Going beyond linear mode connectivity: The layerwise linear feature connectivity. arXiv preprint arXiv:2307.08286, 2023b. 1, 12"
    },
    {
      "markdown": "# A EXTENDED RELATED WORK \n\nFrankle et al. [2020] were the ones to coin the term Linear Mode Connectivity and the first to recognize the importance of this structure. Notably, they studied this structure through its connections with pruning and the lottery ticket hypothesis [Frankle and Carbin, 2018, Malach et al., 2020, Pensia et al., 2020, Ferbach et al., 2022].\nIt is worth noting that some recent works in the literature study linear mode connectivity specifically in a layerwise manner, especially due to the permutation symmetries we described in section 1. For example, Zhou et al. [2023b], Adilova et al. [2023] both study the effect of layer-wise averaging when connecting two deep neural networks. This is aligned with our theoretical study since we recursively align deep networks layer after layer.\nThe mode connectivity framework has been used as a tool to better understand the similarity between two models or the effect of a training procedure on the trained model. For example, Lubana et al. [2023] introduces mechanistic similarity to quantify how two models react to the same alteration of the data in latent space. They show relations between mode connectivity and mechanistic similarity. Especially they prove that if two models cannot be linear mode connected, then they are mechanistically dissimilar. Moreover, Mirzadeh et al. [2020] use the mode connectivity framework to study whether continual and sequential learning (two different training procedure for multitask learning) are converging to a similar solution. Finally, Qin et al. [2022] explores mode connectivity of pretrained langage models, especially how hyper-parameters affect mode connectivity and how mode connectivity evolves during training.\nComputational optimal transport has been leveraged by Singh and Jaggi [2020], Akash et al. [2022] to find paths between two models in parameter space. The latter formulates the model fusion problem as a Wasserstein barycenter problem.\nPast works have studied mode connectivity through the lens of model averaging with applications in federated learning [Yurochkin et al., 2019, Wang et al., 2020]. Zhou et al. [2023a] studies the effect of data heterogeneity in federated learning on mode connectivity of global modes.\n\n## B PROOFS AND DETAILS ABOUT OPTIMAL TRANSPORT THEORY FOR LMC\n\n## B. 1 Background on optimal transport and convexity lemma\n\nOptimal transport is a mathematical framework that aims at quantifying distances between distributions. We refer to the books Villani et al. [2009] and Peyré et al. [2019] (focused on computational aspects and applications to data science) for an extensive overview of this topic.\nDefinition B. 1 (Wasserstein distance [Villani et al., 2009]). Let $(\\mathcal{X}, d)$ a Polish metric space, $p \\in[1, \\infty)$. $\\forall \\mu, \\nu \\in \\mathcal{P}_{1}(\\mathcal{X})$, define the $p$-Wasserstein distance between $\\mu$ and $\\nu$ by:\n\n$$\n\\mathcal{W}_{p}(\\mu, \\nu)=\\left(\\inf _{\\pi \\in \\Pi(\\mu, \\nu)} \\int_{\\mathcal{X}^{2}} d^{p}(x, y) d \\pi(x, y)\\right)^{1 / \\mu}\n$$\n\nRecall that $\\Pi(\\mu, \\nu)$ denotes the set of coupling between $\\mu$ and $\\nu$, i.e.,\n\n$$\n\\pi \\in \\Pi(\\mu, \\nu) \\Leftrightarrow \\pi \\in \\mathcal{P}_{1}\\left(\\mathcal{X}^{2}\\right) \\text { with marginals } \\mu, \\nu\n$$\n\nThis defines a distance and especially it satisfies the triangular inequality. Moreover we state a Jensen type inequality proved in Villani et al. [2009]:\nLemma B. 2 (Convexity of the optimal cost (Theorem 4.8 in Villani et al. [2009])). Let $d: \\mathbb{R}^{n} \\times \\mathbb{R}^{n} \\rightarrow \\mathbb{R}_{+} a$ distance and for $p \\in[1, \\infty), \\mathcal{W}_{p}: \\mathcal{P}_{1}\\left(\\mathbb{R}^{n}\\right)^{2} \\rightarrow \\mathbb{R}_{+}$its associate Wasserstein distance. Let $(\\Theta, \\lambda)$ be a probability space and $\\mu_{\\theta}, \\nu_{\\theta}$ two measurable functions on that space taking values in $\\mathcal{P}_{1}\\left(\\mathbb{R}^{n}\\right)$. Then,\n\n$$\n\\mathcal{W}_{p}^{p}\\left(\\int_{\\Theta} \\mu_{\\theta} \\lambda(d \\theta), \\int_{\\Theta} \\nu_{\\theta} \\lambda(d \\theta)\\right) \\leq \\int_{\\Theta} \\mathcal{W}_{p}^{p}\\left(\\mu_{\\theta}, \\nu_{\\theta}\\right) \\lambda(d \\theta)\n$$\n\nProof. To apply Theorem 4.8 in Villani et al. [2009], we just need to notice that $d^{p}(\\cdot, \\cdot)$ is continuous, $d^{p}(\\cdot, \\cdot) \\geq 0$ and the associated optimal cost functional is $\\mathcal{W}_{p}^{p}(\\cdot, \\cdot)$."
    },
    {
      "markdown": "# B. 2 Validity of the OT viewpoint: Birkhoff's theorem \n\nWe have motivated in Section 2 the following minimization problem (Equation (2)):\n\n$$\n\\begin{aligned}\n\\Pi_{\\ell} & =\\underset{\\Pi \\in \\mathcal{S}_{m_{\\ell}}}{\\arg \\min }\\left\\|W_{A}^{\\ell}-\\Pi W_{B}^{\\ell} \\Pi_{\\ell-1}^{T}\\right\\|_{2}^{2} \\\\\n& =\\underset{\\pi \\in \\mathcal{S}_{m_{\\ell}}}{\\arg \\min } \\frac{1}{m_{\\ell}} \\sum_{i=1}^{m_{\\ell}}\\left\\|\\left[W_{A}^{\\ell}\\right]_{i:}-\\left[W_{B}^{\\ell} \\Pi_{\\ell-1}^{T}\\right]_{\\pi_{i:}}\\right\\|_{2}^{2}\n\\end{aligned}\n$$\n\nSince LMC effectiveness will be related to the effectiveness of this optimization problem, we want to quantify the minimization error:\n\n$$\n\\min _{\\Pi \\in \\mathcal{S}_{m_{\\ell}}}\\left\\|W_{A}^{\\ell}-\\Pi W_{B}^{\\ell} \\Pi_{\\ell-1}^{T}\\right\\|_{2}^{2}\n$$\n\nWe highlight the similarity with previous Definition B.1. The main difference being that the latter minimizes the cost among all couplings $\\pi \\in \\Pi(\\mu, \\nu)$ between the two distributions, especially transport plans that can split mass. Permutation must be on the other hand seen as Monge maps, i.e. deterministic maps. This ambiguity is solved with the following lemma:\nLemma B. 3 (Proposition 2.1 in Peyré et al. [2019]). Let $m, n$ integers and $x_{1}, \\ldots, x_{m}, y_{1}, \\ldots, y_{m}, 2 m$ points of $\\mathbb{R}^{n}$. Let $\\tilde{\\mu}_{m}=\\frac{1}{m} \\sum_{i=1}^{m} \\delta_{x_{i}}, \\tilde{\\nu}_{m}=\\frac{1}{m} \\sum_{i=1}^{m} \\delta_{y_{i}}$ their associated empirical measures. Consider $d: \\mathbb{R}^{n} \\times \\mathbb{R}^{n} \\rightarrow \\mathbb{R}_{+} a$ distance function and for $p \\in[1, \\infty), \\mathcal{W}_{p}$ its associated Wasserstein distance. Then one has:\n\n$$\n\\mathcal{W}_{p}:=\\inf _{\\pi \\in \\Pi\\left(\\tilde{\\mu}_{m}, \\tilde{\\nu}_{m}\\right)}\\left(\\int_{\\mathbb{R}^{n} \\times \\mathbb{R}^{n}} d(x, y)^{p} d \\pi(x, y)\\right)^{\\frac{1}{p}}=\\min _{\\pi \\in \\mathcal{S}_{m}}\\left(\\frac{1}{m} \\sum_{i=1}^{m} d\\left(x_{i}, y_{\\sigma_{i}}\\right)^{p}\\right)^{\\frac{1}{p}}\n$$\n\nProof. $\\Pi\\left(\\tilde{\\mu}_{m}, \\tilde{\\nu}_{m}\\right)$ is the convex envelope of its extremal points which are described by permutations by Birkhoff's theorem. Moreover,\n\n$$\n\\pi \\in \\mathcal{P}_{1}\\left(\\mathbb{R}^{n} \\times \\mathbb{R}^{n}\\right) \\rightarrow \\int d(x, y)^{p} d \\pi(x, y)\n$$\n\nis linear and therefore its infimum is attained on one extremal point of $\\Pi\\left(\\tilde{\\mu}_{m}, \\tilde{\\nu}_{m}\\right)$\nThis lemma implies equality between the Wasserstein distance between two empirical measures and the minimum cost over the set of permutations. We can therefore restrict our study to permutations while still using tools from general optimal transport theory and convergence rates of empirical measures in Wasserstein distance.\n\n## B. 3 Technical lemmas\n\nThe following lemma will be very useful in the following and shows that Binomial variables are concentrated around their expectation.\nLemma B. 4 (Hoeffding inequality for Binomial variables). Let $\\mathcal{B}_{\\frac{p}{2}, m} \\sim \\mathcal{B}\\left(\\frac{p}{2}, m\\right)$ a binomial variable with $p \\in$ $[0,1]$. Then,\n\n$$\n\\mathbb{P}\\left(\\frac{\\mathcal{B}_{\\frac{p}{2}, m}}{m} \\geq p\\right) \\leq \\exp \\left(-\\frac{p^{2} m}{2}\\right)\n$$\n\nProof: This is a simple application of Hoeffding concentration inequality [Phillips, 2012] since a Binomial variable is a sum of independent Bernoulli variables.\nLemma B.5. Let $a, b>0$ such that $a+b=1$ and let $\\mu_{1}, \\mu_{2}, \\nu_{1}, \\nu_{2} \\in \\mathcal{P}_{1}(\\mathcal{X})$ with $(\\mathcal{X}, d)$ a Polish space. Then, $\\forall p \\in[1, \\infty):$\n\n$$\n\\mathcal{W}_{p}^{p}\\left(a \\mu_{1}+b \\mu_{2}, a \\nu_{1}+b \\nu_{2}\\right) \\leq a \\mathcal{W}_{p}^{p}\\left(\\mu_{1}, \\nu_{1}\\right)+b \\mathcal{W}_{p}^{p}\\left(\\mu_{2}, \\nu_{2}\\right)\n$$"
    },
    {
      "markdown": "Proof. Just apply Lemma B. 2 with $\\mu_{\\Theta}, \\nu_{\\Theta}$ such that $\\mathbb{P}\\left(\\left(\\mu_{\\Theta}, \\nu_{\\Theta}\\right)=\\left(\\mu_{1}, \\nu_{1}\\right)\\right)=a$ and $\\mathbb{P}\\left(\\left(\\mu_{\\Theta}, \\nu_{\\Theta}\\right)=\\left(\\mu_{2}, \\nu_{2}\\right)\\right)=$ $b$.\n\nLemma B. 6 (Hölder, Remark 6.6 in Villani et al. [2009]). Let $(\\mathcal{X}, d)$ a Polish space, let $p, q \\in[1, \\infty]$ such that $p \\leq q$ :\n\n$$\n\\forall \\mu, \\nu \\in \\mathcal{P}_{1}(\\mathcal{X}), \\mathcal{W}_{p} \\leq \\mathcal{W}_{q}\n$$\n\nProof. Just apply Hölder's inequality.\n\n# B. 4 Definitions and technical lemma on packing numbers \n\nLet $n \\in \\mathbb{N}^{*}, S \\subset \\mathbb{R}^{n}$ and $d(\\cdot, \\cdot)$ a distance of $\\mathbb{R}^{n}$. We recall the definitions of packing numbers and covering numbers as well as two lemmas stated and proved in Wu [2016]:\nDefinition B. 7 ( $\\varepsilon$-covering number [Weed and Bach, 2019]). The $\\varepsilon$-covering of a set $S$ denoted $\\mathcal{N}_{\\varepsilon}(S)$ is the minimum number $m$ of closed balls $B_{1}, \\ldots, B_{m}$ of radius $\\varepsilon$ such that $S \\subset \\bigcup_{i=1}^{m} B_{i}$.\nDefinition B. 8 ( $\\varepsilon$-packing number). The $\\varepsilon$-packing number of a set $S$ denoted $\\mathcal{P}_{\\varepsilon}(S)$ is the maximum number $m$ of distinct points $\\theta_{1}, \\ldots, \\theta_{m} \\in S$ such that $\\forall i \\neq j,\\left\\|\\theta_{i}-\\theta_{j}\\right\\|>\\varepsilon$.\nLemma B.9. If the distance $d(\\cdot, \\cdot)$ comes from a norm, $(d(x, y)=\\|x-y\\|)$, denoting Leb $_{n}$ the Lebesgue measure in $\\mathbb{R}^{n}$ we have: $\\forall S \\subset \\mathbb{R}^{n}, \\forall \\varepsilon>0$,\n\n$$\n\\mathcal{N}_{\\varepsilon}(S) \\leq \\frac{\\operatorname{Leb}_{n}(S+\\mathcal{B}(0, \\varepsilon / 2))}{\\operatorname{Leb}_{n}(\\mathcal{B}(0, \\varepsilon / 2))}\n$$\n\nProof. We prove first $\\mathcal{N}_{\\varepsilon}(S) \\leq \\mathcal{P}_{\\varepsilon}(S)$. Indeed considering $m=\\mathcal{P}_{\\varepsilon}(S)$ and $\\theta_{1}, \\ldots, \\theta_{m}$ associated, we know by definition of the $\\varepsilon$-packing number that $\\forall \\theta \\in S, \\exists i \\in[m]$ such that $\\left\\|\\theta_{i}-\\theta\\right\\| \\leq \\varepsilon$. This shows that $S \\subset \\bigcup_{i=1}^{m} \\mathcal{B}\\left(\\theta_{i}, \\varepsilon\\right)$\nNow, on the other hand, we know that all balls $\\mathcal{B}\\left(\\theta_{i}, \\frac{\\varepsilon}{2}\\right)$ are disjoint and $\\bigcup_{i=1}^{m} \\mathcal{B}\\left(\\theta_{i}, \\frac{\\varepsilon}{2}\\right) \\subset S+\\mathcal{B}(0, \\varepsilon / 2)$. This yields the result by a volume (i.e., Lebesgue measure) argument.\nLemma B.10. If the distance $d(\\cdot, \\cdot)$ comes from a norm, $(d(x, y)=\\|x-y\\|)$ we have: $\\forall S \\subset \\mathbb{R}^{n}, \\forall \\varepsilon>0$,\n\n$$\n\\mathcal{N}_{\\varepsilon}(S) \\geq\\left(\\frac{1}{\\varepsilon}\\right)^{n} \\frac{\\operatorname{Leb}_{n}(S)}{\\operatorname{Leb}_{n}(\\mathcal{B}(0,1))}\n$$\n\nProof. Notice that given a covering $\\bigcup_{i=1}^{\\mathcal{N}_{\\varepsilon}(S)} \\mathcal{B}\\left(x_{i}, \\varepsilon\\right) \\supset S$, by a volume argument we get\n\n$$\n\\operatorname{Leb}_{n}(S) \\leq \\varepsilon^{n} \\mathcal{N}_{\\varepsilon}(S) \\operatorname{Leb}_{n}(\\mathcal{B}(0,1))\n$$\n\nwhere we have used homogeneity of the norm.\n\n## B. 5 Lemmas on convergence rates of empirical measures\n\nThis section is devoted to proving convergence rates in Wasserstein distance of empirical measure towards the underlying distribution whose points are drawn. More precisely, given $\\mu \\in \\mathcal{P}_{1}\\left(\\mathbb{R}^{n}\\right)$ a probability measure on an euclidean space and $p \\in[1, \\infty)$, we will focus on bounding the quantity $\\mathbb{E}\\left[\\mathcal{W}_{p}^{p}\\left(\\hat{\\mu}_{m}, \\mu\\right)\\right]$ as $m$ grows, where $\\hat{\\mu}_{m}$ is a random empirical measure i.e., $\\hat{\\mu}_{m}=\\frac{1}{m} \\sum_{i=1}^{m} \\delta_{x_{i}}$ where $x_{i} \\stackrel{\\text { i.i.d. }}{\\sim} \\mu$.\nWe will first prove the following lemma:\nLemma B.11. Consider $X_{n} \\sim \\mathcal{N}\\left(0, \\frac{I_{n}}{n}\\right)$ a random variable whose law is parametrized by $n \\in \\mathbb{N}^{*}$. There exists a universal constant $D_{0}$ such that\n\n$$\n\\forall n \\in \\mathbb{N}^{*}, \\forall c>1, \\mathbb{E}\\left[\\left\\|X_{n}\\right\\|_{2}^{2} \\mid\\left\\|X_{n}\\right\\|_{2}>c\\right] \\leq D_{0} c^{2}\n$$\n\nJensen inequality implies:\n\n$$\n\\forall n \\in \\mathbb{N}^{*}, \\forall c>1, \\mathbb{E}\\left[\\left\\|X_{n}\\right\\|_{2} ;\\left|\\left\\|X_{n}\\right\\|_{2}>c\\right| \\leq \\sqrt{D_{0}} c\\right.\n$$"
    },
    {
      "markdown": "Proof. We write\n\n$$\n\\begin{aligned}\n\\mathbb{E}\\left[\\left\\|X_{n}\\right\\|_{2}^{2}\\left|\\left\\|X_{n}\\right\\|_{2}>c\\right]\\right. & =\\frac{\\int_{c}^{+\\infty} r^{2} r^{n-1} e^{-\\frac{r^{2} n}{2}} d r}{\\int_{c}^{+\\infty} r^{n-1} e^{-\\frac{r^{2} n}{2}} d r} \\\\\n& \\leq 4 c^{2}+\\sum_{m=2}^{\\infty} \\frac{\\int_{m c}^{(m+1) c} r^{2} r^{n-1} e^{-\\frac{r^{2} n}{2}} d r}{\\int_{c}^{2 c} r^{n-1} e^{-\\frac{r^{2} n}{2}} d r} \\\\\n& \\leq 4 c^{2}+\\sum_{m=2}^{\\infty} \\frac{\\int_{0}^{1}((m+1) c)^{n+1} e^{-\\frac{((m+t) c)^{2} n}{2}} c d t}{\\int_{0}^{1} c^{n-1} e^{-\\frac{((1+t) c)^{2} n}{2}} c d t}\n\\end{aligned}\n$$\n\nBut, $\\forall t \\in[0,1], \\forall m \\geq 2$,\n\n$$\n\\frac{((m+1) c)^{n+1} e^{-\\frac{((m+t) c)^{2} n}{2}}}{c^{n-1} e^{-\\frac{((1+t) c)^{2} n}{2}}} \\leq c^{2}(m+1)^{n+1} e^{-\\frac{c^{2} n}{2}\\left(m^{2}-1\\right)} \\leq c^{2}(2 m)^{2 n} e^{-\\frac{c^{2} n}{2}\\left(m^{2}-1\\right)} \\leq c^{2} e^{-n\\left(\\frac{c^{2}}{2}\\left(m^{2}-1\\right)-2 \\log (2 m)\\right)}\n$$\n\nIt is clear that uniformly in $n, \\sum_{m=2}^{\\infty} c^{2} e^{-n\\left(\\frac{c^{2}}{2}\\left(m^{2}-1\\right)-2 \\log (2 m)\\right)} \\underset{c \\rightarrow \\infty}{\\longrightarrow} 0$ which proves the lemma.\nWe will now prove a bound on the rate of convergence in Wasserstein distance of an empirical measure to the underlying distribution when this one has a bounded support. Denote $\\mathcal{B}_{2}^{b}(0, r)$ the euclidean ball centered around 0 of radius $r$ in dimension $k$.\nLemma B.12. Let $\\mu \\in \\mathcal{P}_{1}\\left(\\mathbb{R}^{n}\\right)$ be a measure whose support is included in $\\mathcal{B}_{2}^{n}\\left(0, \\frac{1}{12}\\right) \\subset \\mathbb{R}^{n}$ with $n \\geq 5$ Then, $\\forall m \\geq 1$ we have\n\n$$\n\\mathbb{E}\\left[\\mathcal{W}_{2}^{2}\\left(\\hat{\\mu}_{m}, \\mu\\right)\\right] \\leq D_{1}\\left(\\frac{1}{m}\\right)^{2 / n}\n$$\n\nWhere $D_{1}=27^{2}\\left(2+\\frac{1}{\\sqrt{3}-1}\\right)$\nProof. We know from Lemma B. 9 that when considering $\\|\\cdot\\|_{2}$ the distance for defining covering number, $\\forall \\varepsilon^{\\prime} \\leq \\frac{1}{6}$ :\n\n$$\n\\mathcal{N}_{\\varepsilon^{\\prime}}\\left(\\mathcal{B}_{2}^{n}(0,1 / 12)\\right)=\\left(\\frac{\\frac{1}{12}+\\frac{\\varepsilon^{\\prime}}{2}}{\\frac{\\varepsilon^{\\prime}}{2}}\\right)^{n} \\leq\\left(\\frac{\\frac{1}{6}+\\varepsilon^{\\prime}}{\\varepsilon^{\\prime}}\\right)^{n} \\leq\\left(3 \\varepsilon^{\\prime}\\right)^{-n}\n$$\n\nand therefore also when $\\varepsilon^{\\prime} \\leq \\frac{1}{27}$ Applying Proposition 15 from Weed and Bach [2019] we get that that since $\\operatorname{Supp}(\\mu) \\subset \\mathcal{B}_{2}^{n}\\left(0,1 / 12\\right) \\subset \\mathcal{B}_{2}^{n}(0,1 / 12)+\\mathcal{B}_{2}^{n}(0, \\varepsilon)$ for any $\\varepsilon>0$, if $n \\geq 5$,\n\n$$\n\\forall m \\geq 1, \\mathbb{E}\\left[\\mathcal{W}_{2}^{2}\\left(\\hat{\\mu}_{m}, \\mu\\right)\\right] \\leq 27^{2}\\left(2+\\frac{1}{\\sqrt{3}-1}\\right)\\left(\\frac{1}{m}\\right)^{2 / n}\n$$\n\nas well as if $n \\geq 3$,\n\n$$\n\\forall m \\geq 1, \\mathbb{E}\\left[\\mathcal{W}_{1}\\left(\\hat{\\mu}_{m}, \\mu\\right)\\right] \\leq 27\\left(2+\\frac{1}{\\sqrt{3}-1}\\right)\\left(\\frac{1}{m}\\right)^{1 / n}\n$$\n\nWe can prove the same kind of inequality when $\\mu$ concentrates mass around an approximately low dimensional set."
    },
    {
      "markdown": "Lemma B.13. Let $\\mu \\in \\mathcal{P}_{1}\\left(\\mathbb{R}^{n}\\right)$ be a measure whose support is included in $\\mathcal{B}_{2}^{b}(0,1 / 12) \\times \\mathcal{B}_{2}^{n-k}(0, r)$ with $k \\geq 5$. Then, $\\forall m \\leq(3 r)^{-k}$ we have\n\n$$\n\\mathbb{E}\\left[\\mathcal{W}_{2}^{2}\\left(\\bar{\\mu}_{m}, \\mu\\right)\\right] \\leq D_{1}\\left(\\frac{1}{m}\\right)^{2 / k}\n$$\n\nwhere $D_{1}=27^{2}\\left(2+\\frac{1}{\\sqrt{3}-1}\\right)$\n\nProof. This is the same proof as before, just notice that $\\operatorname{Supp}(\\mu) \\subset \\mathcal{B}_{2}^{k}(0,1 / 12) \\times\\{0\\}^{n-k}+\\mathcal{B}_{2}^{n}(0, r)$ and as before:\n\n$$\n\\mathcal{N}_{\\varepsilon^{\\prime}}\\left(\\mathcal{B}_{2}^{b}(0,1 / 12) \\times\\{0\\}^{n-k}\\right) \\leq \\mathcal{N}_{\\varepsilon^{\\prime}}\\left(\\mathcal{B}_{2}^{b}(0,1 / 12)\\right) \\leq\\left(3 \\varepsilon^{\\prime}\\right)^{-k}\n$$\n\nif $\\varepsilon^{\\prime} \\leq \\frac{1}{27}$.\n\nWe will now extend our results to unbounded variables very concentrated around bounded sets, beginning with multivariate normal random variable.\nLemma B.14. Consider a centered multivariate normal distribution $\\mu$ on $\\mathbb{R}^{n}$ with covariance matrix $\\operatorname{Diag}\\left(\\lambda_{1}, \\ldots, \\lambda_{n}\\right)$ where $\\frac{1}{n} \\geq \\lambda_{1} \\geq \\ldots \\geq \\lambda_{n} \\geq 0$. There exists two universal constants $D_{2}, E_{2}$ such that $\\forall n \\geq 5, \\forall m \\in \\mathbb{N}^{*}$, if $m \\geq E_{2}^{n}$ then,\n\n$$\n\\mathbb{E}\\left[\\mathcal{W}_{2}^{2}\\left(\\bar{\\mu}_{m}, \\mu\\right)\\right] \\leq \\frac{D_{2}}{n} \\log (m)\\left(\\frac{1}{m}\\right)^{2 / n}\n$$\n\nIn that case,\n\n$$\n\\mathbb{E}\\left[\\mathcal{W}_{1}\\left(\\bar{\\mu}_{m}, \\mu\\right)\\right] \\leq \\frac{\\sqrt{D_{2}}}{\\sqrt{n}} \\sqrt{\\log (m)}\\left(\\frac{1}{m}\\right)^{1 / n}\n$$\n\nProof. We will use previous Lemma B.12. The problem is that it applies only for a bounded distribution. Therefore we will have to bound the mass of a multivariate Gaussian outside of a euclidean ball. We will prove the lemma for $\\lambda_{1}=\\ldots=\\lambda_{n}=\\frac{1}{n}$ by noticing that it extends for smaller eigenvalues by rescaling the axis.\nLet $f \\in(0,1), c>0, X \\sim \\mu$.\nLemma 1 from Weed and Bach [2019] tells us:\n\n$$\n\\mathbb{P}\\left(\\|X\\|_{2}^{2} \\geq c^{2} \\sum_{i=1}^{n} \\lambda_{i}\\right) \\leq e^{-\\frac{c^{2}}{4}}\n$$\n\nNoticing $\\sum_{i=1}^{n} \\lambda_{i} \\leq 1$ and taking $c=2 \\sqrt{\\log \\left(\\frac{1}{f}\\right)}$ we get that:\n\n$$\n\\mathbb{P}\\left(\\|X\\|_{2}^{2} \\geq c^{2}\\right) \\leq f\n$$\n\nUsing Lemma B. 4 with $p=2 f$ we get that with probability at least $1-\\exp \\left(-2 f^{2} m\\right)$, a fraction at least $1-2 f$ of vectors $x_{i}$ lies in $\\mathcal{B}_{2}(0, c)$. We denote $H_{f}$ this event such that $\\mathbb{P}\\left(H_{f}\\right) \\geq 1-\\exp \\left(-2 f^{2} m\\right)$.\nFurther denote $I$ the corresponding set of indices for $x_{i}, \\bar{\\mu}_{m, I}=\\frac{\\sum_{i \\in I} \\delta_{x_{i}}}{|I|}$ and $\\bar{\\mu}_{m, I^{c}}=\\frac{\\sum_{i \\in I} \\delta_{x_{i}}}{|I^{c}|}$. Finally for a Borel set $U \\subset \\mathbb{R}^{n}$ denote $\\mu_{\\mid U}$ the renormalized restricted measure $\\mu$ on $U: \\mu_{\\mid U}=\\frac{1}{\\mu(U)} \\mu \\mathbb{1}_{U}$\n\nWe will now consider two cases:"
    },
    {
      "markdown": "1st case We consider the case $\\frac{|I|}{m} \\leq \\mu(\\mathcal{B}(0, c))$ and denote Case ${ }_{1}$ this set.\nIn that case, we can write using Lemma B.5:\n\n$$\n\\left\\{\\begin{array}{ll}\n\\mathcal{W}_{2}^{2}\\left(\\hat{\\mu}_{m}, \\mu\\right) & \\leq \\frac{|I|}{m} \\mathcal{W}_{2}^{2}\\left(\\hat{\\mu}_{m, I}, \\mu_{\\left|\\mathcal{B}_{2}(0, c)\\right\\rangle}\\right)+\\frac{\\left|I^{c}\\right|}{m} \\mathcal{W}_{2}^{2}\\left(\\hat{\\mu}_{m, I^{c}}, \\frac{\\mu_{\\left|\\mathcal{B}_{2}(0, c)\\right|}-\\frac{|I|}{m} \\mu_{\\left|\\mathcal{B}_{2}(0, c)\\right|}}{\\frac{|I|^{2}}{m}}\\right) \\\\\n\\mathcal{W}_{1}\\left(\\hat{\\mu}_{m}, \\mu\\right) & \\leq \\frac{|I|}{m} \\mathcal{W}_{1}\\left(\\hat{\\mu}_{m, I}, \\mu_{\\left|\\mathcal{B}_{2}(0, c)\\right\\rangle}\\right)+\\frac{\\left|I^{c}\\right|}{m} \\mathcal{W}_{1}\\left(\\hat{\\mu}_{m, I^{c}}, \\frac{\\mu_{\\left|\\mathcal{B}_{2}(0, c)\\right|}-\\frac{|I|}{m} \\mu_{\\left|\\mathcal{B}_{2}(0, c)\\right|}}{\\frac{|I|^{2}}{m}}\\right)\n\\end{array}\\right\\}\n$$\n\nBy previous Lemma B.12, we know the existence of $D_{1}$ such that if $n \\geq 5$ :\n\n$$\n\\mathbb{E}\\left[\\mathcal{W}_{2}^{2}\\left(\\hat{\\mu}_{m, I}, \\mu_{\\left|\\mathcal{B}_{2}(0, c)\\right\\rangle}\\right)\\right] \\leq D_{1}(12 c)^{2}\\left(\\frac{1}{|I|}\\right)^{2 / n} \\leq\\left(144 D_{1}\\right) c^{2}\\left(\\frac{1}{\\frac{|I|}{m} m}\\right)^{2 / n}\n$$\n\nTherefore we get since $\\frac{2}{n} \\leq 1$ and $\\frac{|I|}{n} \\leq 1$ :\n\n$$\n\\mathbb{E}\\left[\\frac{|I|}{m} \\mathcal{W}_{2}^{2}\\left(\\hat{\\mu}_{m, I}, \\mu_{\\left|\\mathcal{B}_{2}(0, c)\\right\\rangle}\\right)\\right] \\leq\\left(144 D_{1}\\right) c^{2}\\left(\\frac{1}{m}\\right)^{2 / n}\n$$\n\nWe know from Lemma B. 11 the existence of a universal constant $D_{0}$ such that: $\\forall c \\geq 1, \\mathbb{E}\\left[\\|X\\|_{2}^{2}\\|\\ X\\|_{2} \\geq c\\right] \\leq D_{0} c^{2}$. Using a triangular inequality\n\n$$\n\\mathbb{E}\\left[\\mathcal{W}_{2}^{2}\\left(\\hat{\\mu}_{m, I^{c}}, \\mu_{\\left|\\mathcal{B}_{2}(0, c)^{c}\\right\\rangle}\\right)\\right] \\leq 4 D_{0} c^{2}\n$$\n\nFinally, conditioned on the event $H_{f}$ and that we are in Case ${ }_{1}$, we get that if $c \\geq 1$ :\n\n$$\n\\left\\{\\begin{array}{l}\n\\mathbb{E}\\left[\\mathcal{W}_{2}^{2}\\left(\\hat{\\mu}_{m}, \\mu\\right) \\mid H_{f} \\cap \\text { Case }_{1}\\right] \\leq\\left(144 D_{1}\\right) c^{2}\\left(\\frac{1}{m}\\right)^{2 / n}+8 f D_{0} c^{2} \\\\\nc=2 \\sqrt{\\log \\left(\\frac{1}{f}\\right)}\n\\end{array}\\right.\n$$\n\n2nd case We consider the case $\\frac{|I|}{m}>\\mu\\left(\\mathcal{B}_{2}(0, c)\\right)$ and denote Case ${ }_{2}$ this set.\nIn that case, we denote $I^{\\prime} \\subset I$ taken randomly uniformly, such that $\\left|I^{\\prime}\\right|=\\max \\left\\{k \\geq 1, \\frac{k}{m} \\leq \\mu(\\mathcal{B}(0, c))\\right\\}$ and denote $\\hat{\\mu}_{m, I^{\\prime}}$ the renormalized empirical measure with points in $I^{\\prime}$ and $\\hat{\\mu}_{m, I \\cap I^{\\prime c}}$ the renormalized empirical measure with points in $I \\bigcap I^{\\prime c}$.\n\nWe can write:\n\n$$\n\\begin{aligned}\n\\mathcal{W}_{2}^{2}\\left(\\hat{\\mu}_{m}, \\mu\\right) & \\leq \\frac{\\left|I^{\\prime}\\right|}{m} \\mathcal{W}_{2}^{2}\\left(\\hat{\\mu}_{m, I^{\\prime}}, \\mu_{\\left|\\mathcal{B}_{2}(0, c)\\right\\rangle}\\right)+\\frac{|I|-\\left|I^{\\prime}\\right|}{m} \\mathcal{W}_{2}^{2}\\left(\\hat{\\mu}_{m, I \\cap I^{\\prime c}}, \\mu_{\\left|\\mathcal{B}_{2}(0, c)^{c}\\right\\rangle}\\right) \\\\\n& +\\left(\\mu\\left(\\mathcal{B}_{2}(0, c)\\right)-\\frac{\\left|I^{\\prime}\\right|}{m}\\right) \\mathcal{W}_{2}^{2}\\left(\\hat{\\mu}_{m, I \\cap I^{\\prime c}}, \\mu_{\\left|\\mathcal{B}_{2}(0, c)\\right\\rangle}\\right)+\\frac{\\left|I^{c}\\right|}{m} \\mathcal{W}_{2}^{2}\\left(\\hat{\\mu}_{m, I^{c}}, \\mu_{\\left|\\mathcal{B}_{2}(0, c)^{c}\\right\\rangle}\\right)\n\\end{aligned}\n$$\n\nProvided $f m>1$, we know that $\\frac{\\left|I^{\\prime}\\right|}{m} \\geq 1-3 f$.\nWe can repeat all the previous arguments and get that if $c \\geq 1$ and $m$ :\n\n$$\n\\left\\{\\begin{array}{l}\n\\mathbb{E}\\left[\\mathcal{W}_{2}^{2}\\left(\\hat{\\mu}_{m}, \\mu\\right) \\mid H_{f} \\cap \\text { Case }_{2}\\right] \\leq\\left(144 D_{1}\\right) c^{2}\\left(\\frac{1}{m}\\right)^{2 / n}+16 f D_{0} c^{2} \\\\\nc=2 \\sqrt{\\log \\left(\\frac{1}{f}\\right)}\n\\end{array}\\right.\n$$\n\nFinally,"
    },
    {
      "markdown": "$$\n\\mathbb{E}\\left[\\mathcal{W}_{2}^{2}\\left(\\hat{\\mu}_{m}, \\mu\\right)\\right] \\leq \\mathbb{P}\\left(H_{f}\\right) \\mathbb{E}\\left[\\mathcal{W}_{2}^{2}\\left(\\hat{\\mu}_{m}, \\mu\\right) \\mid H_{f}\\right]+\\left(1-\\mathbb{P}\\left(H_{f}\\right)\\right) \\mathbb{E}\\left[\\mathcal{W}_{2}^{2}\\left(\\hat{\\mu}_{m}, \\mu\\right) \\mid H_{f}^{c}\\right]\n$$\n\nWe can easily bound as before $\\mathbb{E}\\left[\\mathcal{W}_{2}^{2}\\left(\\hat{\\mu}_{m}, \\mu\\right) \\mid H_{f}^{c}\\right] \\leq 4 D_{0} c^{2}$\nwhich yields finally:\n\n$$\n\\mathbb{E}\\left[\\mathcal{W}_{2}^{2}\\left(\\hat{\\mu}_{m}, \\mu\\right)\\right] \\leq\\left(144 D_{1}\\right) 4 \\log \\left(\\frac{1}{f}\\right)\\left(\\frac{1}{m}\\right)^{2 / n}+16 f D_{0} 4 \\log \\left(\\frac{1}{f}\\right)+2 \\exp \\left(-f^{2} m\\right) 4 D_{0} 4 \\log \\left(\\frac{1}{f}\\right)\n$$\n\nTaking $f=\\left(\\frac{1}{m}\\right)^{2 / n}$ we get\n\n$$\n\\mathbb{E}\\left[\\mathcal{W}_{2}^{2}\\left(\\hat{\\mu}_{m}, \\mu\\right)\\right] \\leq \\frac{1152 D_{1}}{n} \\log (m)\\left(\\frac{1}{m}\\right)^{2 / n}+D_{0} \\frac{128}{n} \\log (m)\\left(\\frac{1}{m}\\right)^{2 / n}+\\frac{64}{n} \\exp \\left(-m^{1-4 / n}\\right) \\log (m)\n$$\n\nNote now that there exists a universal constant $C>0$ such that $\\forall n \\geq 5, \\forall m \\geq 1$ :\n\n$$\n\\exp \\left(-m^{1-4 / n}\\right) \\leq \\exp \\left(-m^{1 / 5}\\right) \\leq C\\left(\\frac{1}{m}\\right)^{2 / 5} \\leq C\\left(\\frac{1}{m}\\right)^{2 / n}\n$$\n\nFinally we get the existence of universal constants $D_{2}, E_{2}$ such that if $\\left(\\frac{1}{m}\\right)^{2 / n} \\leq \\frac{1}{E_{2}^{2}}$ (to ensure $c \\geq 1$ take for example $\\left.E_{2}=\\exp \\left(\\frac{1}{4}\\right)\\right)$,\n\n$$\n\\mathbb{E}\\left[\\mathcal{W}_{2}^{2}\\left(\\hat{\\mu}_{m}, \\mu\\right)\\right] \\leq \\frac{D_{2}}{n} \\log (m)\\left(\\frac{1}{m}\\right)^{2 / n}\n$$\n\nTo prove the second part of the lemma just apply Lemma B. 6 and Jensen inequality.\n\nWe can finally extend Lemma B. 13 to unbounded distributions as we just extended Lemma B. 12 to unbounded distributions.\nLemma B.15. Let $\\lambda_{1} \\geq \\ldots \\geq \\lambda_{n}$ and $\\mu=\\mathcal{N}\\left(0, \\operatorname{Diag}\\left(\\left(\\lambda_{i}\\right)_{i=1}^{n}\\right)\\right)$, with $k \\geq 5$. Suppose $1 \\geq \\sum_{i=1}^{k} \\lambda_{i}$. Denote $\\eta=\\frac{\\sqrt{\\sum_{i=1}^{n} \\lambda_{i}}}{\\frac{1}{4} \\sqrt{\\sum_{i=1}^{k} \\lambda_{i}}}$. We know the existence of two universal constants $D_{2}^{\\prime}, E_{2}^{\\prime}$ such that if $\\eta^{-k} \\geq m \\geq E_{2}^{\\prime k}$, then:\n\n$$\n\\mathbb{E}\\left[\\mathcal{W}_{2}^{2}\\left(\\hat{\\mu}_{m}, \\mu\\right)\\right] \\leq \\frac{D_{2}^{\\prime}}{k} \\log (m)\\left(\\frac{1}{m}\\right)^{2 / k}\n$$\n\nIn that case,\n\n$$\n\\mathbb{E}\\left[\\mathcal{W}_{1}\\left(\\hat{\\mu}_{m}, \\mu\\right)\\right] \\leq \\frac{\\sqrt{D_{2}^{\\prime}}}{\\sqrt{k}} \\sqrt{\\log (m)}\\left(\\frac{1}{m}\\right)^{1 / k}\n$$\n\nProof. We will follow the same steps as previously. Let $X \\sim \\mu$ and denote $X=\\left(X_{1}, \\ldots, X_{k}\\right) \\in \\mathbb{R}^{k}, \\bar{X}=$ $\\left(X_{k+1}, \\ldots, X_{n}\\right) \\in \\mathbb{R}^{n-k}$.\n\n$$\n\\begin{aligned}\n& \\mathbb{P}\\left(\\|\\bar{X}\\|_{2}^{2} \\geq c^{2} \\sum_{i=1}^{k} \\lambda_{i}\\right) \\leq e^{-\\frac{c^{2}}{4}} \\\\\n& \\mathbb{P}\\left(\\|\\bar{X}\\|_{2}^{2} \\geq c^{2} \\sum_{i=k+1}^{n} \\lambda_{i}\\right) \\leq e^{-\\frac{c^{2}}{4}}\n\\end{aligned}\n$$"
    },
    {
      "markdown": "Take $c=2 \\sqrt{\\log \\left(\\frac{2}{f}\\right)}$. Then, by the same arguments as before, using Lemma B. 4 and union bounds, with probability at least $1-2 \\exp \\left(-f^{2} m / 2\\right)$ a fraction at least $1-2 f$ of points $x_{i}$ are in $\\mathcal{B}_{c}:=\\mathcal{B}_{k}\\left(0, c \\sqrt{\\sum_{i=1}^{k} \\lambda_{i}}\\right) \\times$ $\\mathcal{B}_{n-k}\\left(0, c \\sqrt{\\sum_{i=k+1}^{n} \\lambda_{i}}\\right)$. We denote $H_{f}$ such an event and $I$ such a set of indices.\nBy using Lemma B.13, we know that in that case we can bound\n\n$$\n\\mathbb{E}\\left[\\mathcal{W}_{2}^{2}\\left(\\hat{\\mu}_{m, I}, \\mu_{\\mid \\mathcal{B}_{c}}\\right) \\mid H_{f}\\right] \\leq\\left(12 c \\sqrt{\\sum_{i=1}^{k} \\lambda_{i}}\\right)^{2} C_{1}\\left(\\frac{1}{m}\\right)^{2 / k} \\leq(12 c)^{2} C_{1}\\left(\\frac{1}{m}\\right)^{2 / k}\n$$\n\nif $m \\leq\\left(\\frac{3 \\sqrt{\\sum_{i=k+1}^{m} \\lambda_{i}}}{12 \\sqrt{\\sum_{i=1}^{k} \\lambda_{i}}}\\right)^{-k}$ where we have used $\\sum_{i=1}^{k} \\lambda_{i} \\leq 1$.\nMoreover, we know that\n\n$$\n\\begin{aligned}\n\\mathbb{E}\\left[\\|X\\|_{2}^{2} \\mid X \\notin \\mathcal{B}_{2}^{k}\\left(0, c \\sqrt{\\sum_{i=1}^{k} \\lambda_{i}}\\right) \\times \\mathcal{B}_{2}^{n-k}\\left(0, c \\sqrt{\\sum_{i=k+1}^{n} \\lambda_{i}}\\right)\\right] & \\leq \\mathbb{E}\\left[\\|X\\|_{2}^{2} \\mid X \\notin \\mathcal{B}_{2}^{k}\\left(0, c \\sqrt{\\sum_{i=1}^{k} \\lambda_{i}}\\right)\\right] \\\\\n& +\\mathbb{E}\\left[\\|\\bar{X}\\|_{2}^{2} \\mid \\bar{X} \\notin \\mathcal{B}_{2}^{n-k}\\left(0, c \\sqrt{\\sum_{i=k+1}^{n} \\lambda_{i}}\\right)\\right] \\\\\n& \\leq D_{0} c^{2} \\sum_{i=1}^{k} \\lambda_{i}+D_{0} c^{2} \\sum_{i=k+1}^{n} \\lambda_{i} \\\\\n& \\leq 2 D_{0} c^{2} \\sum_{i=1}^{k} \\lambda_{i} \\\\\n& \\leq 2 D_{0} c^{2}\n\\end{aligned}\n$$\n\nWe just need to differentiate the same two cases as in the proof of Lemma B. 14 to get that finally, if $m \\leq$ $\\left(\\frac{\\sqrt{\\sum_{i=k+1}^{m} \\lambda_{i}}}{4 \\sqrt{\\sum_{i=1}^{k} \\lambda_{i}}}\\right)^{-k}$ we can bound as before, with $c=2 \\sqrt{\\log \\left(\\frac{2}{f}\\right)}$ :\n\n$$\n\\mathbb{E}\\left[\\mathcal{W}_{2}^{2}\\left(\\hat{\\mu}_{m}, \\mu\\right)\\right] \\leq(12 c)^{2} C_{1}\\left(\\frac{1}{m}\\right)^{2 / k}+16 f * 4 C_{0} c^{2}+2 \\exp \\left(-\\frac{f^{2} m}{2}\\right) * 4 * 2 C_{0} c^{2}\n$$\n\nTaking $f=\\left(\\frac{1}{m}\\right)^{2 / k}$, we see as before the existence of $E_{2}^{\\prime}, D_{2}^{\\prime}$ such that if $E_{2}^{\\prime k} \\leq m \\leq \\eta^{-k}$ then,\n\n$$\n\\mathbb{E}\\left[\\mathcal{W}_{2}^{2}\\left(\\hat{\\mu}_{m}, \\mu\\right)\\right] \\leq \\frac{D_{2}^{\\prime}}{k} \\log (m)\\left(\\frac{1}{m}\\right)^{2 / k}\n$$\n\nWe choose $E_{2}^{\\prime}=\\sqrt{2 e^{-1 / 4}}$ such that $c>1 \\Leftrightarrow 2 \\sqrt{\\log \\left(\\frac{2}{f}\\right)}>1 \\Leftrightarrow 2 \\sqrt{\\log \\left(\\frac{2}{\\left(\\frac{1}{m}\\right)^{2 / k}}\\right)}>1 \\Leftrightarrow m>\\sqrt{2 e^{-1 / 4}}$\nFor the second part of the lemma, apply Lemma B. 6 and Jensen inequality.\n\n# B. 6 Proof of Lemma 4.1"
    },
    {
      "markdown": "Lemma 4.1. Let $\\ell \\in\\{0, \\cdots, L-1\\}$ and suppose Property 1 to hold at layer $\\ell$ and Assumptions 3 to 5 to hold, then Property 1 still holds at the next layer with $\\tilde{m}_{\\ell+1}$ given in Assumption 3 and\n\n$$\n\\begin{aligned}\n& E_{\\ell+1}=C_{2} E_{\\ell} \\\\\n& E_{\\ell+1}=2 C_{2} E_{\\ell}+2 C_{1} \\tilde{m}_{\\ell} E_{\\ell}\n\\end{aligned}\n$$\n\nProof. We know from Assumption 3 the existence of a random empirical measure with $\\tilde{m}_{l+1}$ points $\\tilde{\\mu}_{\\tilde{m}_{l+1}}$ such that $\\mathbb{E}\\left[\\mathcal{W}_{2}^{2}\\left(\\tilde{\\mu}_{A}^{2^{\\ell}}, \\tilde{\\mu}_{\\tilde{m}_{l+1}}^{2^{\\ell}}\\right)\\right] \\leq C_{1}$. Therefore by using Lemma B.3 and a carefully chosen permutation, we can consider the (random) matrix $W_{\\tilde{m}_{l+1}}$ associated to $W_{A}^{\\ell+1}$ such that $\\mathbb{E}\\left[\\left\\|W_{\\tilde{m}_{l+1}}^{\\mathcal{I}^{\\ell}}-W_{A}^{\\mathcal{I}^{\\ell}}\\right\\|_{2}^{2}\\right] \\leq C_{1} m_{\\ell+1}$. Note that since $W_{\\tilde{m}_{\\ell+1}}$ comes from an empirical measure with only $\\tilde{m}_{\\ell+1}$ points one can denote $\\mathcal{I}^{\\ell+1}=\\left\\{I_{1}^{\\ell+1}, \\ldots, I_{\\tilde{m}_{\\ell+1}}^{\\ell+1}\\right\\}$ the (random) equi-partition of $\\left[m_{\\ell+1}\\right]$ delimiting its equal rows. In the same way, since $A$ and $B$ have the same weights distribution, we can find a permutation $\\Pi_{\\ell+1}$ of the layer $\\ell+1$ of network $B$ such that denoting $\\tilde{W}_{B}^{\\ell+1}=\\Pi_{\\ell+1} W_{B}^{\\ell+1} \\Pi_{\\ell}^{T}$ and taking expectations over the choice of the weights matrices, we have $\\mathbb{E}\\left[\\left\\|\\tilde{W}_{B}^{\\ell+1}-\\right.\\right.$ $\\left.W_{\\tilde{m}_{\\ell+1}}\\right\\|_{2}^{2}\\left[\\leq m_{\\ell+1} C_{1} .\\right.$ Consider $W_{M_{t}}^{\\ell+1}=t W_{A}^{\\ell+1}+(1-t) \\tilde{W}_{B}^{\\ell+1}$ and denote $\\phi^{\\ell+1}(x)=\\sigma\\left(W_{\\tilde{m}_{\\ell+1}} \\phi^{\\ell}(x)\\right), \\phi_{A}^{\\ell+1}(x)=$ $\\sigma\\left(W_{A}^{\\ell+1} \\phi_{A}^{\\ell}(x)\\right), \\phi_{B}^{\\ell+1}(x)=\\sigma\\left(W_{B}^{\\ell+1} \\phi_{B}^{\\ell}(x)\\right), \\phi_{M_{t}}^{\\ell+1}(x)=\\sigma\\left(W_{M_{t}}^{\\ell+1} \\phi_{M_{t}}^{\\ell}(x)\\right)$. It is clear that $\\forall k \\in\\left[\\tilde{m}_{\\ell+1}\\right], \\forall i, j \\in$ $I_{k}^{\\ell+1}, \\phi_{i}^{\\ell+1}(x)=\\phi_{j}^{\\ell+1}(x)$ by definition of the choice of the equi-partition $\\mathcal{I}^{\\ell+1}$.\nWe will finally denote $\\phi^{\\prime \\ell+1}(x) \\in \\mathbb{R}^{\\tilde{m}_{\\ell+1}}, \\phi^{\\prime \\ell}(x) \\in \\mathbb{R}^{\\tilde{m}_{\\ell}}$ the vectors $\\phi^{\\ell+1}(x), \\phi^{\\ell}(x)$ where we have kept only one index in each of the elements of the partitions respectively $\\mathcal{I}^{\\ell+1}, \\mathcal{I}^{\\ell}$.\nMoreover using that the non-linearity is pointwise 1-Lipschitz and $\\sigma(0)=0$,\n\n$$\n\\mathbb{E}\\left[\\left\\|\\phi^{\\ell+1}(x)\\right\\|_{2}^{2}\\right] \\leq \\mathbb{E}_{P, Q}\\left[\\mathbb{E}\\left[\\left\\|W_{\\tilde{m}_{\\ell+1}} \\phi^{\\ell}(x)\\right\\|_{2}^{2}\\left|\\phi^{\\ell}(x)\\right|\\right] \\leq \\mathbb{E}_{P, Q}\\left[C_{2} \\frac{m_{\\ell+1}}{m_{\\ell}}\\left\\|\\phi^{\\ell}(x)\\right\\|_{2}^{2}\\right] \\leq m_{\\ell+1} C_{2} E_{\\ell}\\right.\n$$\n\nwhich yields $E_{\\ell+1}=C_{2} E_{\\ell}$ where we have used Assumption 4.\nThen\n\n$$\n\\begin{aligned}\n\\mathbb{E}_{P, Q}\\left[\\left\\|\\phi_{A}^{\\ell+1}(x)-\\phi^{\\ell+1}(x)\\right\\|_{2}^{2}\\right] & \\leq \\mathbb{E}_{P, Q}\\left[\\left\\|W_{A}^{\\ell+1} \\phi_{A}^{\\ell}(x)-W_{\\tilde{m}_{\\ell+1}} \\phi^{\\ell}(x)\\right\\|_{2}^{2}\\right. \\\\\n& \\leq 2 \\mathbb{E}_{P, Q}\\left[\\left\\|W_{A}^{\\ell+1}\\left(\\phi_{A}^{\\ell}(x)-\\phi^{\\ell}(x)\\right)\\right\\|_{2}^{2}+\\left\\|\\left(W_{A}^{\\ell+1}-W_{\\tilde{m}_{\\ell+1}}\\right) \\phi^{\\ell}\\right\\|_{2}^{2}\\right] \\\\\n& \\leq 2 m_{\\ell+1} C_{2} E_{\\ell}+2 \\mathbb{E}_{P, Q}\\left[\\left\\|\\left(W_{A}^{\\mathcal{I}^{\\ell}}-W_{\\tilde{m}}^{\\mathcal{I}^{\\ell}}\\right) \\phi^{\\prime \\ell}\\right\\|_{2}^{2}\\right] \\\\\n& \\leq 2 m_{\\ell+1} C_{2} E_{\\ell}+2 \\mathbb{E}_{P, Q}\\left[\\left\\|\\left(W_{A}^{\\mathcal{I}_{\\ell}}-W_{\\tilde{m}_{\\ell+1}}^{\\mathcal{I}_{\\ell}}\\right)\\right\\|_{2}^{2}\\right] \\mathbb{E}_{P, Q}\\left\\|\\phi^{\\prime \\ell}\\right\\|_{2}^{2}\\right] \\\\\n& \\leq 2 m_{\\ell+1} C_{2} E_{\\ell}+2 m_{\\ell+1} C_{1} \\frac{\\tilde{m}_{\\ell}}{m_{\\ell}} \\mathbb{E}_{P, Q}\\left\\|\\phi^{\\ell}(x)\\right\\|_{2}^{2} \\\\\n& \\leq 2 m_{\\ell+1} C_{2} E_{\\ell}+2 m_{\\ell+1} C_{1} \\tilde{m}_{\\ell} E_{\\ell}\n\\end{aligned}\n$$\n\nwhere we have used $\\left(W_{A}^{\\ell+1}-W_{\\tilde{m}_{\\ell+1}}\\right) \\phi^{\\ell}(x)=\\left(W_{A}^{\\mathcal{I}^{\\ell}}-W_{\\tilde{m}_{\\ell+1}}^{\\mathcal{I}^{\\ell}}\\right) \\phi^{\\prime \\ell}$ and $\\left\\|\\phi^{\\prime \\ell}(x)\\right\\|_{2}^{2}=\\frac{\\tilde{m}_{\\ell}}{m_{\\ell}}\\left\\|\\phi^{\\ell}(x)\\right\\|_{2}^{2}$.\nWe do the same computations for $\\mathbb{E}_{P, Q}\\left[\\left\\|\\phi_{B}^{\\ell+1}(x)-\\phi^{\\ell+1}(x)\\right\\|_{2}^{2}\\right]$.\nFinally,\n\n$$\n\\begin{aligned}\n\\mathbb{E}_{P, Q}\\left[\\left\\|\\phi_{M_{t}}^{\\ell+1}(x)-\\phi^{\\ell+1}(x)\\right\\|_{2}^{2}\\right] & \\leq t \\mathbb{E}_{P, Q}\\left[\\left\\|W_{A}^{\\ell+1} \\phi_{M_{t}}^{\\ell}(x)-W_{\\tilde{m}_{\\ell+1}} \\phi^{\\ell}(x)\\right\\|_{2}^{2}\\right]+(1-t) \\mathbb{E}_{P, Q}\\left[\\left\\|W_{B}^{\\ell+1} \\phi_{M_{t}}(x)-W_{\\tilde{m}_{\\ell+1}} \\phi^{\\ell}(x)\\right\\|_{2}^{2}\\right] \\\\\n& \\leq 2 C_{2} E_{\\ell} m_{\\ell+1}+2 C_{1} \\tilde{m}_{\\ell} E_{\\ell} m_{\\ell+1}\n\\end{aligned}\n$$\n\nwhere we have used convexity for the first inequality and then the same proof as above for both terms. This yields $E_{\\ell+1}=2 C_{2} E_{\\ell}+2 C_{1} \\tilde{m}_{\\ell} E_{\\ell}$."
    },
    {
      "markdown": "# B. 7 Proof of Lemma 5.1 \n\nLemma B. 16 (Version of Assumption 3 for normal distribution). Consider $\\mu_{\\ell} \\in \\mathcal{P}_{1}\\left(\\mathbb{R}^{m_{\\ell-1}}\\right)$ a multivariate Gaussian distribution with covariance matrix $\\Sigma^{\\ell}=\\operatorname{Diag}\\left(\\lambda_{1}^{\\ell} I_{p_{\\ell-1}}, \\ldots, \\lambda_{\\hat{m}_{\\ell}}^{\\ell} I_{p_{\\ell-1}}\\right)$ where $p_{\\ell-1}=\\frac{m_{\\ell-1}}{\\hat{m}_{\\ell-1}}$. Suppose that $\\frac{1}{m_{\\ell-1}} \\geq \\lambda_{1}^{\\ell}, \\geq \\ldots \\geq \\lambda_{\\hat{m}_{\\ell-1}}^{\\ell}$ with $\\hat{m}_{\\ell-1} \\geq 5$. Then, there exists two universal constants $D_{3}, E_{3}$ such that $\\forall \\hat{m}_{\\ell} \\geq E_{3}^{\\hat{m}_{\\ell-1}}$ there exists a random empirical measure $\\hat{\\mu}_{\\hat{m}}$ with only $\\hat{m}_{\\ell}$ points such that $\\forall m_{\\ell} \\geq \\hat{m}_{\\ell}$\n\n$$\n\\mathbb{E}\\left[\\mathcal{W}_{2}^{2}\\left(\\hat{\\mu}_{A}^{\\mathcal{T}^{\\ell-1}}, \\hat{\\mu}_{\\hat{m}_{\\ell}}^{\\mathcal{T}^{\\ell-1}}\\right)\\right] \\leq \\frac{D_{3}}{\\hat{m}_{\\ell-1}} \\log \\left(\\hat{m}_{\\ell}\\right)\\left(\\frac{1}{\\hat{m}_{\\ell}}\\right)^{2 / \\hat{m}_{\\ell-1}}\n$$\n\nIn that case:\n\n$$\n\\mathbb{E}\\left[\\mathcal{W}_{1}\\left(\\hat{\\mu}_{A}^{\\mathcal{T}^{\\ell-1}}, \\hat{\\mu}_{\\hat{m}_{\\ell}}^{\\mathcal{T}^{\\ell-1}}\\right)\\right] \\leq \\frac{\\sqrt{D_{3}}}{\\sqrt{\\hat{m}_{\\ell-1}}} \\sqrt{\\log \\left(\\hat{m}_{\\ell}\\right)}\\left(\\frac{1}{\\hat{m}_{\\ell}}\\right)^{1 / \\hat{m}_{\\ell-1}}\n$$\n\nProof. We know that the distribution on the rows of $W_{A}^{\\mathcal{T}^{\\ell-1}}$ in $\\mathbb{R}^{\\hat{m}_{\\ell-1}}$ is multivariate Gaussian with covariance matrix $\\frac{I_{\\hat{m}_{\\ell-1}}}{\\hat{m}_{\\ell-1}}$ since each parameters is obtained by summing the $p_{\\ell-1}$ corresponding parameters of the row of $W_{A}^{\\ell}$ which has covariance matrix $\\frac{I_{m_{\\ell-1}}}{m_{\\ell-1}}$ by hypothesis. Therefore using Lemma B.14, we know the existence of constants $D_{2}, E_{2}$ such that if $m_{\\ell} \\geq E_{2}^{\\hat{m}_{\\ell-1}}, \\mathbb{E}\\left[\\mathcal{W}_{2}^{2}\\left(\\hat{\\mu}_{A}^{\\mathcal{T}^{\\ell-1}}, \\mu_{\\ell}^{\\mathcal{T}^{\\ell-1}}\\right)\\right] \\leq \\frac{D_{2}}{\\hat{m}_{\\ell-1}} \\log \\left(m_{\\ell}\\right)\\left(\\frac{1}{m_{\\ell}}\\right)^{2 / \\hat{m}_{\\ell-1}}$.\nTherefore considering $\\hat{\\mu}_{\\hat{m}_{\\ell}}$ with the same law but only a fixed number $\\tilde{m}_{\\ell}$ of elements, we get for $m_{\\ell} \\geq \\tilde{m}_{\\ell}$ :\n\n$$\n\\begin{aligned}\n& \\mathbb{E}\\left[\\mathcal{W}_{2}^{2}\\left(\\hat{\\mu}_{A}^{\\mathcal{T}^{\\ell-1}}, \\mu_{\\ell}^{\\mathcal{T}^{\\ell-1}}\\right)\\right] \\leq \\frac{D_{2}}{\\hat{m}_{\\ell-1}} \\log \\left(m_{\\ell}\\right)\\left(\\frac{1}{m_{\\ell}}\\right)^{2 / \\hat{m}_{\\ell-1}} \\leq \\frac{D_{2}}{\\hat{m}_{\\ell-1}} \\log \\left(\\tilde{m}_{\\ell}\\right)\\left(\\frac{1}{\\hat{m}_{\\ell}}\\right)^{2 / \\hat{m}_{\\ell-1}} \\\\\n& \\mathbb{E}\\left[\\mathcal{W}_{2}^{2}\\left(\\hat{\\mu}_{\\hat{m}_{\\ell}}^{\\mathcal{T}^{\\ell-1}}, \\mu_{\\ell}^{\\mathcal{T}^{\\ell-1}}\\right)\\right] \\leq \\frac{D_{2}}{\\hat{m}_{\\ell-1}} \\log \\left(\\tilde{m}_{\\ell}\\right)\\left(\\frac{1}{\\hat{m}_{\\ell}}\\right)^{2 / \\hat{m}_{\\ell-1}}\n\\end{aligned}\n$$\n\nIndeed, the first inequality can be obtained by noticing that $\\left(x \\mapsto \\log (x)\\left(\\frac{1}{x}\\right)^{2 / \\hat{m}_{\\ell-1}}\\right)$ is decreasing for $x \\geq \\sqrt{e}^{\\hat{m}_{\\ell-1}}$ and hence one can just increase the constant $E_{3}$ considered: taking $E_{3}=\\max \\left\\{\\sqrt{e}, E_{2}\\right\\}$ and $D_{3}=4 D_{2}$, by triangular inequality,\n\n$$\n\\mathbb{E}\\left[\\mathcal{W}_{2}^{2}\\left(\\hat{\\mu}_{A}^{\\mathcal{T}^{\\ell-1}}, \\hat{\\mu}_{\\hat{m}_{\\ell}}^{\\mathcal{T}^{\\ell-1}}\\right)\\right] \\leq 2\\left(\\mathbb{E}\\left[\\mathcal{W}_{2}^{2}\\left(\\hat{\\mu}_{A}^{\\mathcal{T}^{\\ell-1}}, \\mu_{\\ell}^{\\mathcal{T}^{\\ell-1}}\\right)\\right]+\\mathbb{E}\\left[\\mathcal{W}_{2}^{2}\\left(\\hat{\\mu}_{\\hat{m}_{\\ell}}^{\\mathcal{T}^{\\ell-1}}, \\mu_{\\ell}^{\\mathcal{T}^{\\ell-1}}\\right)\\right]\\right) \\leq \\frac{D_{3}}{\\hat{m}_{\\ell-1}} \\log \\left(\\tilde{m}_{\\ell}\\right)\\left(\\frac{1}{\\hat{m}_{\\ell}}\\right)^{2 / \\hat{m}_{\\ell-1}}\n$$\n\nFor the second part of the lemma, just apply Lemma B. 6 and Jensen inequality.\n\nLemma B. 17 (Version of Assumption 4 for normal variable). $\\forall X \\in \\mathbb{R}^{m_{\\ell-1}}$ we have:\n\n$$\n\\begin{aligned}\n& \\mathbb{E}\\left[\\left\\|W_{A}^{\\ell} X\\right\\|_{2}^{2}\\right] \\leq \\frac{m_{\\ell}}{m_{\\ell-1}}\\|X\\|_{2}^{2} \\\\\n& \\mathbb{E}\\left[\\left\\|W_{\\hat{m}_{\\ell}} X\\right\\|_{2}^{2}\\right] \\leq \\frac{m_{\\ell}}{m_{\\ell-1}}\\|X\\|_{2}^{2}\n\\end{aligned}\n$$\n\nProof. First, given $X \\in \\mathbb{R}^{m_{\\ell-1}}, \\forall i \\in\\left[m_{\\ell}\\right],\\left(W_{A}^{\\ell} X\\right)_{i}=\\sum_{j=1}^{m_{\\ell-1}}\\left(W_{A}^{\\ell}\\right)_{i, j} X_{j}$ where $\\left(W_{A}^{\\ell}\\right)_{i, j}$ are iid following $\\mathcal{N}\\left(0, \\frac{1}{m_{\\ell-1}}\\right)$. Therefore, $\\forall i \\in\\left[m_{\\ell}\\right], \\mathbb{E}\\left[\\left(W_{A}^{\\ell} X\\right)_{i}^{2}\\right]=\\frac{1}{m_{\\ell-1}}\\|X\\|_{2}^{2}$. Finally,\n\n$$\n\\mathbb{E}\\left[\\left\\|W_{A}^{\\ell} X\\right\\|_{2}^{2}\\right]=\\sum_{i=1}^{m_{\\ell}} \\mathbb{E}\\left[\\left(W_{A}^{\\ell} X\\right)_{i}^{2}\\right]=\\frac{m_{\\ell}}{m_{\\ell-1}}\\|X\\|_{2}^{2}\n$$"
    },
    {
      "markdown": "Since the weight matrix associated to $\\tilde{\\mu}_{\\tilde{m}_{\\ell}}$ denoted $W_{\\tilde{m}_{\\ell}}^{\\prime} \\in \\mathcal{M}_{\\tilde{m}_{\\ell}, m_{\\ell-1}}(\\mathbb{R})$ has only $\\tilde{m}_{\\ell}$ raws, we have expanded it to a matrix $W_{\\tilde{m}_{\\ell}} \\in \\mathcal{M}_{m_{\\ell}, m_{\\ell-1}}(\\mathbb{R})$ by cloning raws in the same element of the partition $\\mathcal{I}^{\\ell}$ given by the pairing between $\\tilde{\\mu}_{A}^{\\mathcal{I}^{\\ell}}, \\tilde{\\mu}_{m_{\\ell-1}}^{\\mathcal{I}^{\\ell}}$ that minimizes the Wasserstein distance. Since $W_{\\tilde{m}_{\\ell}}^{\\prime} \\in \\mathcal{M}_{\\tilde{m}_{\\ell}, m_{\\ell-1}}(\\mathbb{R})$ built in the proof of Lemma B. 16 has the same law on raws $\\mu_{\\ell}$ as $W_{A}^{\\ell}, W_{B}^{\\ell}$ but with only $\\tilde{m}_{\\ell}$ raws, we can use what preceeds to get:\n\n$$\n\\forall X \\in \\mathbb{R}^{m_{\\ell-1}}, \\mathbb{E}\\left[\\left\\|W_{\\tilde{m}_{\\ell}}^{\\prime} X\\right\\|_{2}^{2}\\right] \\leq \\frac{\\tilde{m}_{\\ell}}{m_{\\ell-1}}\\|X\\|_{2}^{2}\n$$\n\nNoting that $\\left\\|W_{\\tilde{m}_{\\ell}} X\\right\\|_{2}^{2}=\\frac{m_{\\ell}}{\\tilde{m}_{\\ell}}\\left\\|W_{\\tilde{m}_{\\ell}}^{\\prime} X\\right\\|_{2}^{2}$, we get\n\n$$\n\\forall X \\in \\mathbb{R}^{m_{\\ell-1}}, \\mathbb{E}\\left[\\left\\|W_{\\tilde{m}_{\\ell}} X\\right\\|_{2}^{2}\\right] \\leq \\frac{m_{\\ell}}{m_{\\ell-1}}\\|X\\|_{2}^{2}\n$$\n\nwhich concludes the proof of the lemma.\nHaving the two assumptions we need, we can prove Lemma 5.1.\nWe recall it here:\nLemma 5.1. Under normal initialization of the weights, given $\\varepsilon>0$, if $m_{0} \\geq 5$, there exists minimal widths $\\hat{m}_{1}, \\ldots, \\hat{m}_{L}$ such that if $m_{1} \\geq \\hat{m}_{1}, \\ldots, m_{L} \\geq \\tilde{m}_{L}$, Property 1 is verified at the last hidden layer $L$ for $\\underline{E}_{L}=1, E_{L}=\\varepsilon^{2}$. Moreover, $\\forall \\ell \\in[L], \\exists T_{\\ell}$ which does only depend on $L, \\ell$ such that one can define recursively $\\hat{m}_{\\ell}$ as $\\hat{m}_{0}=m_{0}$ and\n\n$$\n\\tilde{m}_{\\ell}=\\tilde{\\mathcal{O}}\\left(\\frac{T_{\\ell}}{\\varepsilon}\\right)^{\\tilde{m}_{\\ell-1}}\n$$\n\nProof. From $\\mathbb{E}_{x \\sim P}\\left[\\|x\\|_{2}^{2}\\right] \\leq m_{0}$ we get immediately Property 1 at the input layer with $\\underline{E}_{0}=1, E_{0}=0$.\nBy the recursive relation of Lemma 4.1 and using Lemmas B. 16 and B.17, we get Property 1 at each hidden layer $\\ell \\in[L]$ with $\\tilde{m}_{\\ell}$ to be chosen later with $m_{\\ell} \\geq \\tilde{m}_{\\ell} \\geq \\min \\left\\{5, E_{3}^{\\tilde{m}_{\\ell-1}}\\right\\}$ and:\n\n$$\n\\left\\{\\begin{array}{l}\n\\underline{E}_{\\ell}=1 \\\\\nE_{\\ell}=\\sum_{i=1}^{\\ell} 2^{\\ell+1-i} D_{3} \\log \\left(\\tilde{m}_{i}\\right)\\left(\\frac{1}{\\tilde{m}_{i}}\\right)^{3 / \\tilde{m}_{i-1}} \\underline{E}_{i-1}\n\\end{array}\\right.\n$$\n\nTherefore, just take $\\forall i \\in[L], \\log \\left(\\tilde{m}_{i}\\right)\\left(\\frac{1}{\\tilde{m}_{i}}\\right)^{\\frac{2}{\\tilde{m}_{i-1}}} \\leq \\varepsilon^{2} \\frac{1}{2^{L+1-i} L}$, i.e.\n\n$$\n\\tilde{m}_{i}=\\tilde{\\mathcal{O}}\\left(\\frac{T_{i}}{\\varepsilon}\\right)^{\\tilde{m}_{i-1}}\n$$\n\nwhere $T_{i}=\\sqrt{2^{L+1-i} L}$ and the notation $\\tilde{\\mathcal{O}}(\\cdot)$ hides logarithmic terms.\nIn that case,\n\n$$\n\\left\\{\\begin{array}{l}\nE_{L}=1 \\\\\nE_{L}=\\varepsilon^{2}\n\\end{array}\\right.\n$$\n\n# B. 8 Proof of Theorem 5.2 \n\nWe prove here Theorem 5.2 that we recall:"
    },
    {
      "markdown": "Under normal initialization of the weights, for $m_{1} \\geq \\tilde{m}_{1}, \\ldots, m_{L} \\geq \\tilde{m}_{L}$ as defined in Lemma 5.1, $m_{0} \\geq$ 5 , and under Assumption 7 we know that $\\forall t \\in[0,1]$, with $Q$-probability at least $1-\\delta_{Q}$, there exists permutations of hidden layers $1, \\ldots, L$ of network $B$ that are independent of $t$, such that:\n\n$$\n\\mathbb{E}_{P}\\left[\\mathcal{L}\\left(\\hat{f}_{M_{t}}(x), y\\right)\\right] \\leq t \\mathbb{E}_{P}\\left[\\mathcal{L}\\left(\\hat{f}_{A}(x), y\\right)\\right]+(1-t) \\mathbb{E}_{P}\\left[\\mathcal{L}\\left(\\hat{f}_{B}(x), y\\right)\\right]+\\frac{4 \\sqrt{m_{L+1}}}{\\delta_{Q}^{2}} \\varepsilon\n$$\n\nProof. Under assumptions of Lemma 5.1, given $A, B$, we know the existence of (random) permutations of the hidden layers $\\Pi_{1}, \\ldots, \\Pi_{L}$ such that for $1 \\leq \\ell \\leq L$, denoting $M_{t}$ the mean network of weight matrix at layer $\\ell$ : $t W_{A}^{\\ell}+(1-t) \\Pi_{\\ell} W_{B}^{\\ell} \\Pi_{\\ell-1}^{T}$ we know the existence of $\\phi^{L}: \\mathbb{R}^{m_{0}} \\rightarrow \\mathbb{R}^{m_{L}}$ such that:\n\n$$\n\\begin{aligned}\n& \\mathbb{E}_{P, Q}\\left[\\left\\|\\phi_{M_{t}}^{L}(x)-\\phi_{A}^{L}(x)\\right\\|_{2}^{2}\\right] \\leq \\varepsilon^{2} m_{L} \\\\\n& \\mathbb{E}_{P, Q}\\left[\\left\\|\\phi_{M_{t}}^{L}(x)-\\phi_{B}^{L}(x)\\right\\|_{2}^{2}\\right] \\leq \\varepsilon^{2} m_{L}\n\\end{aligned}\n$$\n\nThen, by convexity, we get at the last layer:\n\n$$\n\\begin{aligned}\n\\mathbb{E}_{P, Q}\\left[\\left\\|\\left(t W_{A}^{L+1}+(1-t) W_{B}^{L+1} \\Pi_{L}^{T}\\right)\\right\\| \\phi_{M_{t}}^{L}(x)\\right. & \\left.-t W_{A}^{L+1} \\phi_{A}^{L}(x)-(1-t) W_{B}^{L+1} \\Pi_{L}^{T} \\phi_{B}^{L}(x)\\right\\|_{2}^{2}\\right] \\\\\n& \\leq t \\mathbb{E}\\left[\\left\\|W_{A}^{L+1}\\left(\\phi_{M_{t}}^{L}(x)-\\phi_{A}^{L}(x)\\right)\\right\\|_{2}^{2}\\right] \\\\\n& +(1-t) \\mathbb{E}\\left[\\left\\|W_{B}^{L+1} \\Pi_{L}^{T}\\left(\\phi_{M_{t}}^{L}(x)-\\phi_{B}^{L}(x)\\right)\\right\\|_{2}^{2}\\right] \\\\\n& \\leq \\varepsilon^{2} m_{L+1}\n\\end{aligned}\n$$\n\nFinally, by Jensen inequality,\n\n$$\n\\mathbb{E}\\left[\\left\\|\\left(t W_{A}^{L+1}+(1-t) W_{B}^{L+1} \\Pi_{L}^{T}\\right)\\right\\| \\phi_{M_{\\ell}}^{L}(x)-t W_{A}^{L+1} \\phi_{A}^{L}(x)-(1-t) W_{B}^{L+1} \\Pi_{L}^{T} \\phi_{B}^{L}(x)\\right\\|_{2}\\right] \\leq \\sqrt{m_{L+1}} \\varepsilon\n$$\n\nTherefore, we get by applying two successive Markov lemma, that with probability at least $1-\\delta_{Q}$ over the choice of the networks $A, B$ :\n\n$$\n\\mathbb{E}_{x \\sim P}\\left[\\left\\|\\left(t W_{A}^{L+1}+(1-t) W_{B}^{L+1} \\Pi_{L}^{T}\\right)\\right\\| \\phi_{M_{\\ell}}^{L}(x)-t W_{A}^{L+1} \\phi_{A}^{L}(x)-(1-t) W_{B}^{L+1} \\Pi_{L}^{T} \\phi_{B}^{L}(x)\\right\\|_{2}\\right] \\leq \\frac{\\sqrt{m_{L+1}} \\varepsilon}{\\left(\\frac{\\delta_{Q}}{2}\\right)^{2}}\n$$\n\nIndeed remember that we have introduced an intermediate random measure $\\hat{\\mu}_{\\tilde{m}_{\\ell}}$ which the permutations depend on and that intervenes in the expectation.\nUsing convexity of the loss and 1-Lipschitzness we get for all $t \\in[0,1]$ that with probability at least $1-\\delta_{Q}$ over the choice of the networks $A, B$ :\n\n$$\n\\mathbb{E}_{x \\sim P}\\left[\\mathcal{L}\\left(f_{M_{t}}(x), y\\right)\\right] \\leq t \\mathbb{E}\\left[\\mathcal{L}\\left(f_{A}(x), y\\right)\\right]+(1-t) \\mathbb{E}\\left[\\mathcal{L}\\left(f_{B}(x), y\\right)\\right]+\\frac{4 \\sqrt{m_{L+1}} \\varepsilon}{\\delta_{Q}^{2}}\n$$\n\n# B. 9 Approximately low dimensional underlying weights distribution \n\n## B.9.1 Motivation on the structure of the covariance matrix\n\nRemind that we are given a partition of the input layer of the weights $\\left[m_{\\ell-1}\\right]$ in $\\tilde{m}_{\\ell-1}$ different groups of the same size $\\mathcal{I}^{\\ell-1}=\\left\\{I_{1}^{\\ell-1}, \\ldots, I_{\\tilde{m}_{\\ell-1}}^{\\ell-1}\\right\\}$. Suppose we have already permuted the first layer of network $A$ and $B$ we can suppose that $I_{1}^{\\ell-1}=\\left\\{1, \\ldots, p_{\\ell-1}\\right\\}, \\ldots$ where $p_{\\ell-1}:=\\frac{m_{\\ell-1}}{\\tilde{m}_{\\ell-1}}$. We want a covariance matrix that respects the"
    },
    {
      "markdown": "fact that incoming neurons in a given group behave the same. Therefore the covariance matrix must be invariant under the permutations of indices inside a set of the equi-partition. We will write the Kroenecker product $\\otimes$.\nLemma B.18. To respect symmetries of the incoming layer, the covariance matrix of weights is necessarily of the form\n\n$$\n\\Sigma_{\\ell}=D_{\\ell} \\otimes I_{p_{\\ell-1}}+B_{\\ell} \\otimes \\mathbb{1}_{p_{\\ell-1}}\n$$\n\nwhere $D_{\\ell} \\in \\mathcal{M}_{\\tilde{m}_{\\ell-1}}(\\mathbb{R})$ is diagonal, $B_{\\ell} \\in \\mathcal{S}_{\\tilde{m}_{\\ell-1}}(\\mathbb{R})$ is symmetric.\nProof. Denoting the matrix of covariances by blocks like:\n\n$$\n\\Sigma_{\\ell}=\\left(\\begin{array}{cccc}\nA_{11} & A_{12} & \\ldots & A_{1 \\tilde{m}_{\\ell-1}} \\\\\nA_{21} & A_{22} & \\ldots & A_{2 \\tilde{m}_{\\ell-1}} \\\\\n\\ldots & \\ldots & \\ldots & \\ldots \\\\\nA_{\\tilde{m}_{\\ell-1} 1} & A_{\\tilde{m}_{\\ell-1} 2} & \\ldots & A_{\\tilde{m}_{\\ell-1} \\tilde{m}_{\\ell-1}}\n\\end{array}\\right)\n$$\n\nwe get the relation $\\forall \\Pi_{1}, \\ldots, \\Pi_{\\tilde{m}_{\\ell-1}} \\in \\mathcal{S}_{p_{\\ell-1}}$ permutation matrices, denoting $\\Pi=\\operatorname{Diag}\\left(\\Pi_{1}, \\ldots, \\Pi_{\\tilde{m}_{\\ell-1}}\\right)$\n\n$$\n\\Sigma_{\\ell}=\\mathbb{E}\\left[X X^{T}\\right]=\\mathbb{E}\\left[(\\Pi X)(\\Pi X)^{T}\\right]=\\Pi \\Sigma \\Pi^{T}=\\operatorname{Diag}\\left(\\Pi_{1}, \\ldots, \\Pi_{\\tilde{m}_{\\ell-1}}\\right) \\Sigma \\operatorname{Diag}\\left(\\Pi_{1}^{T}, \\ldots, \\Pi_{\\tilde{m}_{\\ell-1}}^{T}\\right)\n$$\n\nEvaluating this relation for any $\\Pi_{1}, \\Pi_{2}, \\ldots, \\Pi_{\\tilde{m}_{\\ell-1}}$ we get $\\forall \\Pi_{1} \\in \\mathcal{S}_{p_{\\ell-1}}, \\Pi_{1} A_{11} \\Pi_{1}^{T}=A_{11}$ and therefore $A_{11}$ is of the form $d_{11} I_{p_{\\ell-1}}+b_{11} \\mathbb{1}_{p_{\\ell-1}}$.\nWe do the same for $A_{i i}, i \\geq 2$.\nMoreover we get for $A_{12}$ that:\n\n$$\n\\forall \\Pi_{1}, \\Pi_{2} \\in \\mathcal{S}_{p_{\\ell-1}}, \\Pi_{1} A_{12} \\Pi_{2}^{T}=A_{12}\n$$\n\nwhich brings that $A_{12}$ is of the form $b_{12} \\mathbb{1}_{p_{\\ell-1}}$. We do the same for all $A_{i j}$ where $i \\neq j$. This concludes the proof. Finally by summing over columns inside the partition we get:\n\n$$\n\\Sigma_{\\ell}^{\\mathcal{F}^{\\ell-1}}=p_{\\ell-1} D_{\\ell}+p_{\\ell-1}^{2} B_{\\ell}\n$$\n\nThe model that we chose in Section 5.4 is a particular case that corresponds to choosing $D_{\\ell}=\\operatorname{Diag}\\left(\\lambda_{1}^{\\ell}, \\ldots, \\lambda_{\\tilde{m}_{\\ell-1}}^{\\ell}\\right)$ and $B_{\\ell}=0$. This is not the most general since it implies independence between weights coming from different groups but is sufficient to show the influence of low feature dimensionality on LMC efficiency.\n\n# B.9.2 Non diagonal model \n\nA natural direction is to consider the case where the matrix $B_{\\ell}$ is non zero. Since $\\Sigma_{\\ell}^{\\mathcal{F}^{\\ell-1}}$ is symmetric positive, one can orthogonally change the basis where it becomes diagonal. The arguments to prove Assumption 3 remains unchanged since they rely exclusively on the eigenvalues of $\\Sigma_{\\ell}^{\\mathcal{F}^{\\ell-1}}$.\nHowever one important point to check, is about Assumption 4. Indeed having covariance between weights of a given block implies that the errors at all neurons of a given layer may sum up. Take for example $X=$ $(1, \\ldots, 1)^{T}, D_{\\ell}=0, B_{\\ell}=\\mathbb{1}_{\\tilde{m}_{\\ell-1}}$. In the general case, the constant $C_{2}$ in Assumption 4 will be a depending on the matrix $B_{\\ell}$ and therefore potentially on the dimension. We propose a way to address this issue in Appendix B.12.\n\n## B. 10 Proof of Theorem 5.4\n\nLemma B. 19 (Version of Assumption 3 for approximately low dimensional distribution). Denote $\\mu_{\\ell}$ the law of a multivariate normal distribution of covariance matrix $\\operatorname{Diag}\\left(\\lambda_{1}^{\\ell} I_{p_{\\ell-1}}, \\ldots, \\lambda_{\\tilde{m}_{\\ell-1}}^{\\ell} I_{p_{\\ell-1}}\\right)$ where $p_{\\ell-1}=\\frac{m_{\\ell-1}}{\\tilde{m}_{\\ell-1}}$ and $\\frac{1}{k_{\\ell-1}} \\geq \\lambda_{1}^{\\ell} \\ldots \\geq \\lambda_{\\tilde{m}_{\\ell-1}}^{\\ell}$. Let $k_{\\ell-1} \\geq 5$ and denote $\\eta:=\\frac{\\sqrt{\\sum_{i=k_{\\ell-1}+1}^{5} \\lambda_{i}^{\\ell}}}{4 \\sqrt{\\sum_{i=1}^{k_{\\ell-1}+1} \\lambda_{i}^{\\ell}}}$."
    },
    {
      "markdown": "There exists two universal constants $D_{3}^{\\prime}, E_{3}^{\\prime}$ such that $\\forall \\tilde{m}_{\\ell} \\geq 1$ such that $E_{3}^{\\left\\lceil k_{\\ell-1}\\right.} \\leq \\tilde{m}_{\\ell} \\leq \\eta^{-k_{\\ell-1}}$, there exists a random empirical measure $\\hat{\\mu}_{\\hat{m}_{\\ell}}$ with only $\\tilde{m}_{\\ell}$ points such that $\\forall m_{\\ell} \\geq 1$ such that $\\tilde{m}_{\\ell} \\leq m_{\\ell} \\leq \\eta^{-k_{\\ell-1}}$ we have:\n\n$$\n\\mathbb{E}\\left[\\mathcal{W}_{2}^{2}\\left(\\hat{\\mu}_{A}^{2^{\\ell-1}}, \\hat{\\mu}_{\\tilde{m}_{\\ell}}^{2^{\\ell-1}}\\right)\\right] \\leq \\frac{D_{3}^{\\prime}}{k_{\\ell-1}} \\log \\left(\\tilde{m}_{\\ell}\\right)\\left(\\frac{1}{\\tilde{m}_{\\ell}}\\right)^{2 / k_{\\ell-1}}\n$$\n\nProof. We do exactly the same as for the proof of Lemma B.16, i.e. a triangular inequality but now we use rate of convergence of empirical measures in Wasserstein distance with approximately low dimensional support as expressed in Lemma B. 15 .\n\nLemma B. 20 (Version of Assumption 4 for approximately low dimensional distribution). Denote $\\mu_{\\ell}$ the law of a multivariate normal distribution of covariance matrix $\\operatorname{Diag}\\left(\\lambda_{1}^{\\ell} I_{p_{\\ell-1}}, \\ldots, \\lambda_{\\tilde{m}_{\\ell-1}}^{\\ell} I_{p_{\\ell-1}}\\right)$ where $p_{\\ell-1}=\\frac{m_{\\ell-1}}{\\tilde{m}_{\\ell-1}}$ and $\\frac{1}{k_{\\ell-1}} \\geq \\lambda_{1}^{\\ell} \\ldots \\geq \\lambda_{\\tilde{m}_{\\ell-1}}^{\\ell} . \\forall X \\in \\mathbb{R}^{m_{\\ell-1}}$ we have:\n\n$$\n\\begin{aligned}\n& \\mathbb{E}\\left[\\left\\|W_{A}^{\\ell} X\\right\\|_{2}^{2}\\right] \\leq \\frac{\\tilde{m}_{\\ell-1}}{k_{\\ell-1}}\\|X\\|_{2}^{2} \\\\\n& \\mathbb{E}\\left[\\left\\|W_{\\tilde{m}_{\\ell}}^{\\ell} X\\right\\|_{2}^{2}\\right] \\leq \\frac{\\tilde{m}_{\\ell-1}}{k_{\\ell-1}}\\|X\\|_{2}^{2}\n\\end{aligned}\n$$\n\nProof. Just notice that $\\lambda_{1} \\leq \\frac{1}{k_{\\ell-1}}$ and repeat the same steps as in the proof of Lemma B.17.\nWe will now re-state and prove Theorem 5.4:\nTheorem B.21. Under Assumptions 7 and 8, given $\\varepsilon>0$, if $e m_{0} \\geq 5$ there exists minimal widths $\\tilde{m}_{1}, \\ldots, \\tilde{m}_{L}$ such that if $\\eta^{-k_{0}} \\geq m_{1} \\geq \\tilde{m}_{1}, \\ldots, \\eta^{-k_{L-1}} \\geq m_{L} \\geq \\tilde{m}_{L}$, Property 1 is verified at the last hidden layer $L$ for $\\underline{E}_{L}=1, E_{L}=\\varepsilon^{2}$. Moreover, $\\forall \\ell \\in[L], \\exists T_{\\ell}^{\\prime}$ which does only depend on $L, e, \\ell$, such that one can define recursively $\\tilde{m}_{\\ell}$ as\n\n$$\n\\tilde{m}_{\\ell}=\\tilde{\\mathcal{O}}\\left(\\frac{T_{\\ell}^{\\prime}}{\\varepsilon}\\right)^{k_{\\ell-1}}=\\tilde{\\mathcal{O}}\\left(\\frac{T_{\\ell}^{\\prime}}{\\varepsilon}\\right)^{e \\tilde{m}_{\\ell-1}}\n$$\n\nwhere $\\tilde{m}_{0}=m_{0}$. Moreover $\\forall t \\in[0,1]$, with $Q$-probability at least $1-\\delta_{Q}$, there exists permutations of hidden layers $1, \\ldots, L$ of network $B$ s.t.,\n\n$$\n\\mathbb{E}_{P}\\left[\\mathcal{L}\\left(\\hat{f}_{M_{t}}(x), y\\right)\\right] \\leq t \\mathbb{E}_{P}\\left[\\mathcal{L}\\left(\\hat{f}_{A}(x), y\\right)\\right]+(1-t) \\mathbb{E}_{P}\\left[\\mathcal{L}\\left(\\hat{f}_{B}(x), y\\right)\\right]+\\frac{4 \\sqrt{m_{L+1}}}{\\sqrt{e \\delta_{Q}^{2}}} \\varepsilon\n$$\n\nProof. We just need to prove the first part of the theorem as proving the similarity of loss is exactly the same as in the proof of Theorem 5.2 when we have proved that Property 1 holds at layer $L$ with $E_{L}=\\varepsilon^{2}$. The only change comes from the constant $C_{2}$ in Lemma B. 20 which is not 1 anymore but $\\frac{1}{e}$, hence the additional factor $e$.\nTo prove Property 1 at layer $L$ with $E_{L}=\\varepsilon^{2}$ we just combine $L$ different times the two previous lemma Lemmas B. 19 and B. 20 and Lemma 4.1.\nLemma B. 20 brings that at layer $\\ell$ the constant $C_{2}$ is $\\frac{\\tilde{m}_{\\ell-1}}{k_{\\ell-1}} \\leq \\frac{1}{e}$ by Assumption 8\nMoreover, Lemma B. 19 brings that at each layer $\\ell, C_{1}=\\frac{D_{3}^{\\prime}}{k_{\\ell-1}} \\log \\left(\\tilde{m}_{\\ell}\\right)\\left(\\frac{1}{\\tilde{m}_{\\ell}}\\right)^{2 / k_{\\ell-1}}$\nIt brings:\n\n$$\n\\left\\{\\begin{array}{l}\nE_{i+1}=\\frac{1}{e} \\underline{E}_{i} \\\\\nE_{i+1}=\\frac{2}{e} E_{i}+2 \\frac{D_{3}^{\\prime}}{k_{i}} \\log \\left(\\tilde{m}_{i+1}\\right)\\left(\\frac{1}{\\tilde{m}_{i+1}}\\right)^{2 / k_{i}} \\tilde{m}_{i} \\underline{E}_{i}\n\\end{array}\\right.\n$$\n\nFrom there we see that if we have chosen at each layer"
    },
    {
      "markdown": "$$\nD_{3}^{\\prime} \\log \\left(\\tilde{m}_{i+1}\\right)\\left(\\frac{1}{\\tilde{m}_{i+1}}\\right)^{2 / k_{i}}=\\frac{\\sqrt{\\frac{L 2^{L-i+1}}{e^{L}}}}{\\varepsilon^{2}}\n$$\n\nif\n\n$$\n\\tilde{m}_{i}=\\tilde{\\mathcal{O}}\\left(\\frac{T_{i}^{\\prime}}{\\varepsilon}\\right)^{e \\tilde{m}_{i-1}}\n$$\n\nwhere $\\tilde{\\mathcal{O}}(\\cdot)$ hides logarithmic terms and we define $T_{i}^{\\prime}=\\sqrt{\\frac{D_{3}^{\\prime} e^{L}}{L 2^{L-i+1}}}$\n\n# B. 11 Proof of Theorem 5.3 \n\nTheorem 5.3. Let $n \\geq 1, x \\sim P \\in \\mathcal{P}_{1}\\left(\\mathbb{R}^{n}\\right)$ and $\\mu \\in \\mathcal{P}\\left(\\mathbb{R}^{n}\\right)$ such that $\\frac{d \\mu}{d L e b} \\leq F_{1}$. Suppose $\\Sigma=\\mathbb{E}\\left[x x^{T}\\right]$ is full rank $n$. Let $m \\geq 1$ and $W_{A}, W_{B} \\in \\mathcal{M}_{m, n}(\\mathbb{R})$ whose rows are drawn i.i.d. from $\\mu$. Then, there exists $F_{0}$ such that\n\n$$\n\\mathbb{E}_{W_{A}, W_{B}}\\left[\\min _{\\Pi \\in \\mathcal{S}_{m}} \\mathbb{E}_{P}\\left\\|\\left(W_{A}-\\Pi W_{B}\\right) x\\right\\|_{2}^{2}\\right] \\geq F_{0}\\left(\\frac{1}{m}\\right)^{2 / n}\n$$\n\nProof. First since $\\Sigma$ is full rank we see that when writing $\\Sigma=O D O^{T}$ where $O O^{T}=I_{\\tilde{n}}$, the problem is equivalent to consider the matrices $W_{A} O, W_{B} O$ and $\\Sigma=D$. In that case, the raws of $W_{A}, W_{B}$ are still i.i.d. and follow the same law as $\\mu$ modulo a non-degenerated dilatation. We can then still assume $\\frac{d \\mu}{d L e b} \\leq F_{1}$ for a certain constant $F_{1}$.\nLet $\\tau=\\frac{1}{2} . \\forall S \\subset \\mathbb{R}^{n}$ a Borel set such that $\\mu(S) \\geq 1-\\tau$, we know that $\\operatorname{Leb}(S) \\geq \\frac{1-\\tau}{F_{1}}$. In that case, applying Lemma B. 10 we get $\\mathcal{N}_{\\varepsilon}(S) \\geq\\left(\\frac{1}{\\varepsilon}\\right)^{n} \\frac{1-\\tau}{\\operatorname{Leb}\\left(\\mathcal{B}_{2}(0,1)\\right.}$. Denote $F_{2}=\\left(\\frac{1-\\tau}{\\operatorname{Leb}\\left(\\mathcal{B}_{2}(0, \\varepsilon)\\right.}\\right)^{1 / n}$. Using notations of Weed and Bach [2019] we get $\\mathcal{N}_{\\varepsilon}(\\mu, \\tau) \\geq\\left(\\frac{F_{2}}{\\varepsilon}\\right)^{n}$.\nApplying Proposition 6 from Weed and Bach [2019] we get that\n\n$$\n\\mathcal{W}_{2}^{2}\\left(\\hat{\\mu}_{A}, \\mu\\right) \\geq F_{3}\\left(\\frac{1}{m}\\right)^{2 / n}\n$$\n\nFinally noticing that $\\mathbb{E}_{W_{B}}\\left[\\hat{\\mu}_{B}\\right]=\\mu$ and applying Lemma B.2, we get that:\n\n$$\n\\mathbb{E}_{W_{A}, W_{B}}\\left[\\mathcal{W}_{2}^{2}\\left(\\hat{\\mu}_{A}, \\hat{\\mu}_{B}\\right)\\right] \\geq \\mathbb{E}_{W_{A}}\\left[\\mathcal{W}_{2}^{2}\\left(\\hat{\\mu}_{A}, \\mathbb{E}_{W_{B}}\\left[\\hat{\\mu}_{B}\\right]\\right)\\right] \\geq F_{3}\\left(\\frac{1}{m}\\right)^{2 / n}\n$$\n\nFinally, remark that $\\operatorname{sinc} \\Sigma=D=\\operatorname{Diag}\\left(\\lambda_{1}, \\ldots, \\lambda_{\\tilde{n}}\\right)$ and noting $\\lambda(\\Sigma)=\\min \\left\\{d_{i}, 1 \\leq i \\leq n\\right\\}>0$ the smallest eigenvalue of $\\Sigma$,\n\n$$\n\\begin{aligned}\n\\mathbb{E}_{W_{A}, W_{B}}\\left[\\min _{\\Pi \\in \\mathcal{S}_{m}} \\mathbb{E}_{x \\sim P}\\left\\|\\left(W_{A}-\\Pi W_{B}\\right) x\\right\\|_{2}^{2}\\right] & \\leq \\mathbb{E}_{W_{A}, W_{B}}\\left[\\min _{\\Pi \\in \\mathcal{S}_{m}} \\mathbb{E}_{x \\sim P} \\operatorname{tr}\\left(\\left(W_{A}-\\Pi W_{B}\\right)^{T}\\left(W_{A}-\\Pi W_{B}\\right) D\\right)\\right] \\\\\n& \\geq \\lambda(\\Sigma) \\mathbb{E}_{W_{A}, W_{B}}\\left[\\min _{\\Pi \\in \\mathcal{S}_{m}} \\mathbb{E}_{x \\sim P}\\left\\|\\left(W_{A}-\\Pi W_{B}\\right)\\right\\|_{2}^{2}\\right] \\\\\n& \\geq \\lambda(\\Sigma) F_{3}\\left(\\frac{1}{m}\\right)^{2 / n}\n\\end{aligned}\n$$"
    },
    {
      "markdown": "# B. 12 Discussion about a model with no growth in the width needed \n\nFor proving Theorem 5.4, we have used constants $C_{1}, C_{2}$ in Lemma 4.1 given by Lemmas B. 19 and B.20. However we would like to highlight that Lemma B. 20 is very sub-optimal, though it can not really be improved in the general case. Indeed, $\\frac{1}{c^{2}}$ grows to infinity while $\\mathbb{E}\\left[\\left\\|\\phi^{\\ell}(x)\\right\\|_{2}^{2}\\right]$ is supposed to remain bounded for $1 \\leq \\ell$. Therefore from now on suppose that we have the following version of Lemma B.22:\nLemma B. 22 (Extending Assumption 4 for approximately low dimensional distribution). Denote $\\mu_{\\ell}$ the law of a multivariate normal distribution of covariance matrix $\\operatorname{Diag}\\left(\\lambda_{1}^{\\ell} I_{p_{\\ell-1}}, \\ldots, \\lambda_{\\tilde{m}_{\\ell-1}}^{\\ell} I_{p_{\\ell-1}}\\right)$ where $p_{\\ell-1}=\\frac{m_{\\ell-1}}{\\tilde{m}_{\\ell-1}}$ and $\\frac{1}{k_{\\ell-1}} \\geq \\lambda_{1}^{1} \\ldots \\geq \\lambda_{\\tilde{m}_{\\ell-1}}^{\\ell}$. Suppose we have:\n\n$$\n\\mathbb{E}\\left[\\left\\|W_{A}\\left(\\phi_{B}^{\\ell-1}(x)-\\phi_{A}^{\\ell-1}(x)\\right\\|_{2}^{2}\\right] \\leq \\frac{m_{\\ell}}{m_{\\ell-1}} \\mathbb{E}\\left[\\left\\|\\phi_{B}^{\\ell-1}(x)-\\phi_{A}^{\\ell-1}(x)\\right\\|\\right]_{2}^{2}\n$$\n\nNotice that it is equivalent to making an assumption on the distribution of $\\phi_{B}^{\\ell-1}(x)-\\phi_{A}^{\\ell-1}(x)$ which must not put too much mass on the worst case coordinates ( $\\lambda_{i}^{\\ell}$ for $i$ small).\nIn that case, adapting the proof as in Theorem 5.4, we could get an inequality on $\\tilde{m}_{i}$ of the form\n\n$$\n\\tilde{m}_{i}=\\tilde{\\mathcal{O}}\\left(\\frac{T_{i}^{\\prime \\prime}}{\\varepsilon}\\right)^{k_{i-1}}\n$$\n\nwhere $T_{i}^{\\prime \\prime}$ is independent of $e$ which lead to controlled bounds as $L \\rightarrow \\infty$ (without the exponent $e \\tilde{m}_{i}$ as in Theorem 5.4).\n\n## B. 13 Extension of Theorems 5.2 and 5.4 to sub-Gaussian variables\n\nStill under the setting of Assumption 6, suppose now that at a given layer $\\ell$, all the parameters of $W_{A}^{\\ell}$ are still drawn independently but no longer from $\\mathcal{N}\\left(0, \\frac{1}{m_{\\ell-1}}\\right)$ Instead we assume that the underlying distribution $\\mu_{\\ell}$ verifies for each layer $\\ell \\in[L+1]$ : if $X \\sim \\mu_{\\ell}$ then, $\\forall j \\neq k \\in\\left[m_{l-1}\\right], X_{j}$ II $X_{k}$. Moreover $\\forall i \\in\\left[\\tilde{m}_{\\ell-1}\\right], \\forall j, k \\in I_{i}^{\\ell-1}$,\n\n$$\n\\mathbb{E}\\left[X_{j}^{2}\\right]=\\mathbb{E}\\left[X_{k}^{2}\\right]=\\lambda_{i}^{\\ell}\n$$\n\nFinally suppose the variables are sub-Gaussian i.e., $\\exists K>0, \\forall i \\in\\left[\\tilde{m}_{\\ell-1}\\right], \\forall j \\in I_{i}^{\\ell-1}, \\forall c>0$,\n\n$$\n\\mathbb{P}\\left(\\left|X_{j}\\right| \\geq c\\right) \\leq 2 \\exp \\left(-\\frac{c^{2}}{K \\lambda_{i}^{\\ell}}\\right)\n$$\n\nFurther suppose that we are in the setting of Theorem 5.2 (The case of Theorem 5.4 is treated similarly): $\\frac{1}{m_{\\ell-1}} \\geq \\lambda_{1}^{\\ell} \\geq \\ldots \\lambda_{\\tilde{m}_{\\ell-1}^{\\ell}}$.\nIt is clear that Lemma B. 17 is still valid for a constant $C_{2}=1$, the proof being exactly the same.\nWe therefore just need to prove Lemma B. 16 for $C_{1}$ to be determined. To prove Lemma B.16, one just needs an equivalent of Lemma B. 14 for sub-Gaussian variables. To prove Lemma B.14, recall that we have used the fact that a normal distribution doesn't put to much mass outside of a ball of radius $c$ when c grows logarithmically. More precisely we have used the property, that if $X \\sim \\mathcal{N}\\left(0, \\frac{I_{m_{l-1}}}{m_{l-1}}\\right)$, then:\n\n$$\n\\forall c>1, \\mathbb{P}\\left(\\|X\\|_{2}^{2} \\geq c^{2}\\right) \\leq e^{-\\frac{c^{2}}{4}}\n$$\n\nIn our case, for a sub-Gaussian distribution, we know the existence of a constant $K$ such that $\\forall c>0$ :\n\n$$\n\\mathbb{P}\\left(\\left|X_{j}\\right| \\geq c\\right) \\leq 2 \\exp \\left(-\\frac{c^{2}}{K \\lambda_{i}^{\\ell}}\\right)\n$$"
    },
    {
      "markdown": "Therefore, by plugging this into the proof, and scaling parameter $c$ by $\\sqrt{K}$, we get exactly the same version of Lemma B. 16 for sub-Gaussian variable, with different constants scaled by a factor $\\sqrt{K}$.\nPropagating Property 1 with the recurrence formula of Lemma 4.1 we get LMC for networks with sub-Gaussian distributions in the same form as for normal variables.\n\nRemark Finally notice that results for sub-Gaussian variables can be extended in the same way to variables whose tail decreses sufficiently fast (exponentially, polynomially, etc...). The assymptotics of the tail will affect the convergence rate in Wasserstein of the corresponding empirical measure.\n\n# B. 14 Link with dropout stability \n\nWe relate now our previous study to a line of work exploring mode connectivity through dropout stability.\nKuditipudi et al. [2019] define $\\varepsilon$-dropout stable networks, as networks $\\tilde{f}(\\cdot ; \\theta)$ as defined in Equation (1) for which there exists in each layer $\\ell \\in[L]$, a subset of at most $\\frac{m_{\\ell}}{2}$ of neurons (i.e., rows of the weight matrix $W^{\\ell}$ ) such that after renormalizing each layer, the expected loss of the new network increases by no more than $\\varepsilon$ with respect to the original loss. Kuditipudi et al. [2019] shows that two $\\varepsilon$-dropout stable networks are mode connected (with error barrier height $\\varepsilon$ ) and Shevchenko and Mondelli [2020] uses this result to show that two wide enough two-layer neural networks trained with SGD are mode connected (where the continuous path may be non-linear).\nRecall that we have shown in Section 3 the stronger statements that two such networks are in the same local minima modulo permutation symmetries. However, note that Shevchenko and Mondelli [2020] don't allow permutations of neurons). We discuss here how to embrace in the same view our framework with dropout stability results, showing how networks with independent neuron's weights become dropout stable in the same asymptotics of large width than the condition of Lemma 5.1.\nConsider the simplified setting of a 1-hidden layer neural network with 1-Lipschitz activation where the weights of the second layer are fixed to $\\frac{1}{N}: \\tilde{f}(x ; \\theta)=\\frac{1}{N} \\sum_{i=1}^{N} \\sigma\\left(w_{i} x\\right)$ where $w_{i}=W_{c} \\in \\mathbb{R}^{d}$ is the $i-t h$ row of the weight matrix $W$. Suppose that $w_{i}$ are sampled independently from a sub-Gaussian distribution and the data follows a distribution $(x, y) \\sim P$ with $\\operatorname{Supp}(P) \\subset \\mathcal{B}_{2}(0,1)$. Denote $\\mathcal{A}=\\left[\\frac{N}{2}\\right]$. Dropout stability can be quantified by controlling the error between the correctly renormalized network with weights in $\\mathcal{A}$ and the original one,\n\n$$\n\\mathbb{E}\\left[\\left|\\frac{2}{N} \\sum_{i \\in \\mathcal{A}} \\sigma\\left(w_{i} x\\right)-\\frac{1}{N} \\sum_{i=1}^{N} \\sigma\\left(w_{i} x\\right)\\right|\\right] \\leq \\mathcal{W}_{1}\\left(W^{\\mathcal{A}}, W^{\\mathcal{A}^{\\circ}}\\right)\n$$\n\nwhere we have denoted $W^{\\mathcal{A}}$ (respectively $W^{\\mathcal{A}^{\\circ}}$ ) the matrix $W$ where we have kept only the rows in $\\mathcal{A}$ (respectively $\\left.\\left(\\mathcal{A}^{\\circ}\\right)\\right)$. The right hand term can be connected to convergence rates of empirical measure (Lemma B. 16 and the extension to sub-Gaussian distribution discussed in Appendix B.13):\n\n$$\n\\mathcal{W}_{1}\\left(W^{\\mathcal{A}}, W^{\\mathcal{A}^{\\circ}}\\right) \\approx\\left(\\frac{1}{N}\\right)^{1 / d}\n$$\n\nIn a nutshell, showing that previous Equation (8) is tight would provide a formal connection between dropout stability and our results. It is an interesting direction for future work and note that it has strong connections with the dual expression of the Wasserstein 1 distance.\n\nIn that case, the bound on the dropout error evolves as $\\left(\\frac{1}{N}\\right)^{1 / d}$, as for the linear mode connectivity error. Hence networks become dropout stable in the same asymptotics as to exhibit linear mode connectivity.\nThis is consistent with the idea that LMC requires the information to be distributed evenly among neurons without any neuron responsible for the particular behavior of one layer. This is similar to the intuitive requirement for dropout stability."
    },
    {
      "markdown": "# C PROOF OF LMC FOR TWO-LAYER NEURAL NETWORKS IN THE MEAN FIELD REGIME \n\n## C. 1 Description of the mean field regime\n\nWhen training a two-layer neural network with fixed input and output dimensions but with a very wide hidden layer using SGD, the parameters of each neuron can be seen as particles evolving independently one from each other: the dynamic of each neuron's weights depends only on the average distribution of the weights and itself.\nThe main object of study is therefore the empirical distribution of the neurons weights in the intermediate layer after $k$ Stochastic Gradient Descent (SGD) steps. We denote it $\\rho_{k}^{(N)}=\\frac{1}{N} \\sum_{i=1}^{N} \\delta_{\\theta_{i}^{k}}$ where $\\theta_{i}^{k}=\\left(w_{i}^{k}, a_{i}^{k}\\right) \\in \\mathbb{R}^{d+1}$. Multiple works [Chizat and Bach, 2018, Mei et al., 2018, 2019] show that if the hidden layer's width $N$ is big, the learning rate $s_{k}$ is small and setting $T=\\sum_{i=1}^{k} s_{i}$ a time re-normalization after $k$ steps, then $\\rho_{k}^{(N)}$ can be well approximated by $\\rho_{t}$ which follows the following partial differential equation (PDE):\n\n$$\n\\begin{gathered}\n\\partial_{t} \\rho_{t}=2 \\xi(t) \\nabla_{\\theta} \\cdot\\left(\\rho_{t} \\nabla_{\\theta} \\Psi\\left(\\theta ; \\rho_{t}\\right)\\right), \\Psi\\left(\\theta ; \\rho_{t}\\right)=V(\\theta)+\\int U(\\theta, \\tilde{\\theta}) \\rho_{t}(d \\tilde{\\theta}) \\\\\nV(\\theta)=-\\mathbb{E}\\left[y \\sigma_{*}(x ; \\theta)\\right], U\\left(\\theta_{1}, \\theta_{2}\\right)=\\mathbb{E}\\left[\\sigma_{*}\\left(x ; \\theta_{1}\\right) \\sigma_{*}\\left(x ; \\theta_{2}\\right)\\right]\n\\end{gathered}\n$$\n\nHere $\\xi(t)$ represent a scaling of the learning rate where $s_{k}=\\varepsilon \\xi(k \\varepsilon) . U\\left(\\theta_{1}, \\theta_{2}\\right)$ represents a correlation between neurons. $V(\\theta)$ is an energy quantifying the alignment of a neuron function with the data. In the following, as in Mei et al. [2019] we will work with $\\xi=\\frac{1}{2}$ a constant step size function. As in Mei et al. [2019], we highlight that the proof remains valid under Assumption 1.\nWhen considering noisy SGD, the limit PDE becomes:\n\n$$\n\\begin{aligned}\n& \\partial_{t} \\rho_{t}=2 \\xi(t) \\nabla_{\\theta} \\cdot\\left(\\rho_{t}(\\theta) \\nabla_{\\theta} \\Psi_{\\lambda}\\left(\\theta ; \\rho_{t}\\right)\\right)+2 \\xi(t) \\tau d^{-1} \\Delta_{\\theta} \\rho_{t} \\\\\n& \\Psi_{\\lambda}(\\theta ; \\rho)=\\Psi(\\theta ; \\rho)+\\frac{\\lambda}{2}\\|\\theta\\|_{2}^{2}\n\\end{aligned}\n$$\n\nThe crucial point about the mean field view is to show that the empirical distribution of parameters is well enough approximated by $\\rho_{t}$. Then the study of the neural network can be reduced to the study of the partial differential equation. For example global convergence of the test loss results can be deduced as in Chizat and Bach [2018]. This view is also convenient to get insights of typical behaviours of the dynamics while smoothing the effects of local minima [Mei et al., 2019]. In our case, the mean field view allows us to use convergence results in Wasserstein distance of empirical measures towards the underlying distribution. We can then show Linear Mode Connectivity for two-layer neural networks independently trained in the mean field regime.\n\n## C. 2 Proving LMC for noiseless regularization-free SGD\n\nTo prove our results we have to show that the empirical distribution of weights can be well approximated by the solution of the mean field PDE. To achieve this, Mei et al. [2019] introduce four intermediate dynamics that stay close one of each other.\n\nFirst note that our Assumption 1 implies Assumptions 1 to 4 of Mei et al. [2019]. Especially the non-linearity being Lipschitz implies its gradient distribution on the data is bounded and hence sub-Gaussian.\n\n## C.2.1 Intermediate dynamics\n\nMei et al. [2019] introduce 4 different intermediate dynamics between the empirical distribution of weights optimized by SGD and the solution of the PDE that we recall here:\n\n## Nonlinear dynamics\n\nLet consider $\\widetilde{\\theta}_{i}^{t}$ with initialization $\\widetilde{\\theta}_{i}^{0} \\sim \\rho_{0}$ i.i.d. and which follows the dynamics"
    },
    {
      "markdown": "$$\n\\bar{\\theta}_{i}^{t}=\\bar{\\theta}_{i}^{0}+2 \\int_{0}^{t} \\xi(s) G\\left(\\bar{\\theta}_{i}^{s} ; \\rho_{s}\\right) d s\n$$\n\nor equivalently\n\n$$\n\\frac{d}{d t} \\bar{\\theta}_{i}^{t}=-2 \\xi(t)\\left[\\nabla V\\left(\\bar{\\theta}_{i}^{t}\\right)+\\int \\nabla_{1} U\\left(\\bar{\\theta}_{i}^{t}, \\theta\\right) \\rho_{t}(d \\theta)\\right]\n$$\n\nwhere $G(\\theta ; \\rho)=-\\nabla \\Psi(\\theta ; \\rho)$. An important fact is that $\\bar{\\theta}_{i}^{t}$ is random because of the random initialization. Moreover its law at time $t$ is $\\rho_{t}$. It corresponds to the evolution of particles under a velocity field $-2 \\xi(t)\\left[\\nabla V\\left(\\bar{\\theta}_{i}^{t}\\right)+\\int \\nabla_{1} U\\left(\\bar{\\theta}_{i}^{t}, \\theta\\right) \\rho_{t}(d \\theta)\\right]$ which depends only on the position of the optimized particle and the overall distribution of all particles.\n\n# Particle Dynamics \n\nLet $\\theta_{i}^{t}$ have the same initialization as the nonlinear dynamics $\\theta_{i}^{0}=\\bar{\\theta}_{i}^{0}$, and $\\rho_{i}^{(N)}=\\frac{1}{N} \\sum_{i=1}^{N} \\delta_{\\theta_{i}^{t}}$ denote the empirical distribution of $\\theta_{i}^{t}$. Then the particle dynamics is given by:\n\n$$\n\\theta_{i}^{t}=\\theta_{i}^{0}+2 \\int_{0}^{t} \\xi(s) G\\left(\\theta_{i}^{s} ; \\rho_{s}^{(N)}\\right) d s\n$$\n\nor equivalently\n\n$$\n\\frac{d}{d t} \\theta_{i}^{t}=-2 \\xi(t)\\left[\\nabla V\\left(\\theta_{i}^{t}\\right)+\\frac{1}{N} \\sum_{j=1}^{N} \\nabla_{1} U\\left(\\theta_{i}^{t}, \\theta_{j}^{t}\\right)\\right]\n$$\n\n## Gradient descent dynamics\n\nLet $\\bar{\\theta}_{i}^{k}$ with initialization $\\bar{\\theta}_{i}^{0}=\\bar{\\theta}_{i}^{0}$ following the dynamics:\n\n$$\n\\bar{\\theta}_{i}^{k}=\\bar{\\theta}_{i}^{0}+2 \\varepsilon \\sum_{l=0}^{k-1} \\xi(l \\varepsilon) G\\left(\\bar{\\theta}_{i}^{l} ; \\bar{\\rho}_{l}^{(N)}\\right)\n$$\n\nor equivalently:\n\n$$\n\\bar{\\theta}_{i}^{k+1}=\\bar{\\theta}_{i}^{k}-2 s_{k}\\left[\\nabla V\\left(\\bar{\\theta}_{i}^{k}\\right)+\\frac{1}{N} \\sum_{j=1}^{N} \\nabla_{1} U\\left(\\bar{\\theta}_{i}^{k}, \\bar{\\theta}_{j}^{k}\\right)\\right]\n$$\n\n## Stochastic Gradient Descent Dynamics\n\nConsider $\\theta_{i}^{k}$ with initialization $\\theta_{i}^{0}=\\bar{\\theta}_{i}^{0}$ following the dynamics:\n\n$$\n\\theta_{i}^{k}=\\theta_{i}^{0}+2 \\varepsilon \\sum_{l=0}^{k-1} \\xi(l \\varepsilon) F_{i}\\left(\\theta^{l} ; z_{l+1}\\right)\n$$\n\nor equivalently\n\n$$\n\\theta_{i}^{k+1}=\\theta_{i}^{k}-2 s_{k} F_{i}\\left(\\theta^{k} ; z_{k+1}\\right)\n$$\n\nwhere $F_{i}\\left(\\theta^{k} ; z_{k+1}\\right)=\\left(y_{k+1}-\\hat{y}_{k+1}\\right) \\nabla_{\\theta} \\sigma_{*}\\left(x_{k+1} ; \\theta_{i}^{k}\\right), z_{k}=\\left(x_{k}, y_{k}\\right)$ and $\\hat{y}_{k+1}=\\frac{1}{N} \\sum_{j=1}^{N} \\sigma_{*}\\left(x_{k+1} ; \\theta_{j}^{k}\\right)$.\nOne can use Proposition 26,28,29 from Mei et al. [2019] to show the following lemma:"
    },
    {
      "markdown": "Lemma C.1. Consider a two-layer neural network trained with noiseless regularization-free SGD for an underlying time $T$. Then under Assumption 1, there exists constants $K$ and $K_{0}$ such that, if $\\varepsilon \\leq$ $\\min \\left\\{\\frac{1}{K_{0} e^{K_{0}(1+T)^{3}}}, \\frac{1}{K_{0}\\left(d+\\log (N)+z^{2}\\right) e^{K_{0}(1+T)^{3}}}\\right\\}$, then with probability at least $1-3 e^{-z^{2}}$ we have:\n\n$$\n\\begin{aligned}\n\\max _{k \\in[0, T / z] \\bigcap \\mathbb{N}} \\max _{i \\in[N]}\\left\\|\\theta_{i}^{k}-\\hat{\\theta}_{i}^{k \\varepsilon}\\right\\|_{2} & \\leq K e^{K(1+T)^{3}} \\frac{1}{\\sqrt{N}}[\\sqrt{\\log (N T)}+z] \\\\\n& +K e^{K(1+T)^{2} T} \\varepsilon+K e^{K(1+T)^{3} T} \\sqrt{\\varepsilon}[\\sqrt{d+\\log (N)}+z]\n\\end{aligned}\n$$\n\nProof. This is direct application from Proposition 26,28,29 from Mei et al. [2019] by doing two union bounds and two triangular inequalities.\n\nWe moreover recall that as highlighted in Mei et al. [2019], $\\left\\{\\hat{\\theta}_{i}^{t}, 1 \\leq i \\leq N\\right\\}$ are independent from each other and each follows the distribution $\\rho_{t}$ when initialized i.i.d. as $\\rho_{0}$. Therefore, when considering two two-layer neural networks initialized randomly as $\\rho_{0}$ and trained for the same underlying time $T$ with noiseless regularization-free SGD, we know from the previous lemma that the parameters of both networks are close to two samples from $\\rho_{t}$.\n\n# C.2.2 Proof of Theorem 3.1 in the case of noiseless regularization-free SGD \n\nWe now prove Theorem 3.1 in case of noiseless regularization-free SGD.\nTheorem 3.1. Consider two two-layer neural networks as in Equation (3) trained with equation SGD with the same initialization over the weights independently and for the same underlying time $T$. Suppose Assumptions 1 and 2 to hold. Then $\\forall \\delta$, err, $\\exists N_{\\min }$ such that if $N \\geq N_{\\min }, \\exists \\varepsilon_{\\max }(N)$ such that if $\\varepsilon \\leq$ $\\varepsilon_{\\max }(N)$ in Equation (4), then with probability at least $1-\\delta$ over the training process, there exists a permutation of the second network's hidden layer such that for almost every $x \\sim P$ :\n\n$$\n\\begin{aligned}\n& \\left|t \\hat{f}_{N}\\left(x ; \\theta_{A}\\right)+(1-t) \\hat{f}_{N}\\left(x ; \\theta_{B}\\right)\\right. \\\\\n& \\left.-\\hat{f}_{N}\\left(x ; t \\theta_{A}+(1-t) \\hat{\\theta}_{B}\\right)\\right| \\leq \\operatorname{err}, \\quad \\forall t \\in[0,1]\n\\end{aligned}\n$$\n\nProof. We know from Lemma C. 1 that with probability at least $1-3 e^{-z^{2}}, \\quad$ if $\\varepsilon \\leq$ $\\min \\left\\{\\frac{1}{K_{0} e^{K_{0}(1+T)^{3}}}, \\frac{1}{K_{0}\\left(d+\\log (N)+z^{2}\\right) e^{K_{0}(1+T)^{3}}}\\right\\}$,\n\n$$\n\\begin{aligned}\n\\max _{k \\in[0, T / z] \\bigcap \\mathbb{N}} \\max _{i \\in[N]}\\left\\|\\theta_{A, i}^{k}-\\hat{\\theta}_{A, i}^{k \\varepsilon}\\right\\|_{2} & \\leq K e^{K(1+T)^{3}} \\frac{1}{\\sqrt{N}}[\\sqrt{\\log (N T)}+z] \\\\\n& +K e^{K(1+T)^{2} T} \\varepsilon+K e^{K(1+T)^{3} T} \\sqrt{\\varepsilon}[\\sqrt{d+\\log (N)}+z]\n\\end{aligned}\n$$\n\nwhich means that $\\theta_{A, i}$ is close to the non linear dynamics which are samples from $\\rho_{t}$. By a union bound, with probability $1-6 e^{-z^{2}}$ this is true for both networks $A$ and $B$.\nDenoting as before $\\theta_{A, i}=\\left(w_{A, i}, a_{A, i}\\right) \\in \\mathbb{R}^{d+1}$ (respectively $\\theta_{B, i}$ ), $A_{A}=\\left(a_{A, 1}, \\ldots, a_{A, N}\\right)$ (respectively $A_{B}$ ) and $W_{A} \\in \\mathcal{M}_{N, d}(\\mathbb{R})$ the concatenation of vectors $w_{A, i} \\in \\mathbb{R}^{d}$ (respectively $W_{B}$ ). Given $t \\in[0,1]$, we aim at finding a permutation $\\Pi \\in \\mathcal{S}_{N}$ of the second network's hidden layer to get $\\tilde{\\theta}_{B}=\\left(\\tilde{A}_{B}, \\tilde{W}_{B}\\right)=\\left(A_{B} \\Pi^{T}, \\Pi W_{B}\\right)$ bounding\n\n$$\n\\begin{aligned}\n& \\left|t \\hat{f}\\left(x ; \\theta_{A}\\right)+(1-t) \\hat{f}\\left(x ; \\theta_{B}\\right)-\\hat{f}\\left(x ; t \\theta_{A}+(1-t) \\tilde{\\theta}_{B}\\right)\\right| \\\\\n& =\\frac{1}{N}\\left|t A_{A} \\sigma\\left(W_{A} X\\right)+(1-t) \\tilde{A}_{B} \\sigma\\left(\\tilde{W}_{B} X\\right)-\\left(t A_{A}+(1-t) \\tilde{A}_{B}\\right) \\sigma\\left(\\left(t W_{A}+(1-t) \\tilde{W}_{B}\\right) X\\right)\\right| \\\\\n& \\leq t\\left|\\frac{A_{A}}{N}\\left(\\sigma\\left(W_{A} X\\right)-\\sigma\\left(\\left(t W_{A}+(1-t) \\tilde{W}_{B}\\right) X\\right)\\right)\\right|+(1-t)\\left|\\frac{\\tilde{A}_{B}}{N}\\left(\\sigma\\left(\\tilde{W}_{B} X\\right)-\\sigma\\left(\\left(t W_{A}+(1-t) \\tilde{W}_{B}\\right) X\\right)\\right)\\right| \\\\\n& \\leq t\\left\\|A_{A}\\right\\|_{\\infty} \\frac{\\left\\|\\sigma\\left(W_{A} X\\right)-\\sigma\\left(\\left(t W_{A}+(1-t) \\tilde{W}_{B}\\right) X\\right)\\right\\|_{1}}{N}+(1-t)\\left\\|\\tilde{A}_{B}\\right\\|_{\\infty} \\frac{\\left\\|\\sigma\\left(\\tilde{W}_{B} X\\right)-\\sigma\\left(\\left(t W_{A}+(1-t) \\tilde{W}_{B}\\right) X\\right)\\right\\|_{1}}{N}\n\\end{aligned}\n$$"
    },
    {
      "markdown": "Both terms $\\frac{\\left\\|\\sigma\\left(W_{A} X\\right)-\\sigma\\left(\\left(t W_{A}+(1-t) \\tilde{W}_{B}\\right) X\\right)\\right\\|_{1}}{N}$ and $\\frac{\\left\\|\\sigma\\left(\\tilde{W}_{B} X\\right)-\\sigma\\left(\\left(t W_{A}+(1-t) \\tilde{W}_{B}\\right) X\\right)\\right\\|_{1}}{N}$ can be bounded.\nIndeed, first using lemma 22 from Mei et al. [2019] and that from Assumption 1 $\\operatorname{Supp}\\left(\\rho_{0}\\right)$ is bounded, we get that\n\n$$\n\\operatorname{Supp}\\left(\\rho_{t}\\right) \\subset \\mathcal{B}_{2}\\left(0, K\\left(\\left(1+T^{2}\\right) T\\right)+1\\right)\n$$\n\nis bounded with a diameter depending only on the initialization $\\operatorname{Supp}\\left(\\rho_{0}\\right)$ and underlying time $T$.\nTherefore we can apply Theorem 1 from Weed and Bach [2019] and get for $s>d$ the existence of a constant $C$ such that with probability at least $1-\\frac{\\delta}{2}$, there exists a permutation $\\gamma \\in \\mathcal{S}_{N}$ such that by considering $\\|\\cdot\\|_{1}$ as a distance for the Wasserstein:\n\n$$\n\\mathcal{W}_{1}\\left(\\hat{\\mu}_{A}, \\hat{\\mu}_{B}\\right)=\\frac{1}{N} \\sum_{i=1}^{N}\\left\\|\\bar{\\theta}_{A, i}-\\bar{\\theta}_{B, \\gamma_{i}}\\right\\|_{1} \\leq \\frac{C}{\\delta} N^{-1 / s}\n$$\n\nNote that, while $C$ is independent of $N$, it depends on the distribution $\\rho_{t}$ and therefore on $d$, $\\operatorname{diam}\\left(\\operatorname{Supp}\\left(\\rho_{t}\\right)\\right)$ (i.e., $T$ ) and on the constants from Assumption 1.\n\nRecall that we suppose the data distribution P bounded and denote $\\operatorname{Supp}(P) \\subset\\left[-H_{x}, H_{x}\\right]^{d} \\times\\left[-H_{y}, H_{y}\\right]$.\nTherefore, we get that with probability at least $1-\\frac{\\delta}{2}-6 e^{-z^{2}}$ :\n\n$$\n\\begin{aligned}\n& \\forall X \\in\\left[-H_{x}, H_{x}\\right]^{d}, \\frac{\\left\\|\\sigma\\left(W_{A} X\\right)-\\sigma\\left(\\left(t W_{A}+(1-t) \\tilde{W}_{B}\\right) X\\right)\\right\\|_{1}}{N} \\leq g_{2}(T, z, \\delta, N, \\varepsilon) \\\\\n& :=L_{\\sigma}\\left(H_{x} \\frac{C}{\\delta} N^{-\\frac{1}{s}}+2 H_{x} \\sqrt{d}\\left[\\frac{K e^{K(1+T)^{3}}}{\\sqrt{N}}[\\sqrt{\\log (N T)}+z]+K e^{K(1+T)^{2} T} \\varepsilon+K e^{K(1+T)^{2} T} \\sqrt{\\varepsilon}[\\sqrt{d+\\log (N)}+z]\\right]\\right)\n\\end{aligned}\n$$\n\nand same for the other term:\n\n$$\n\\forall X \\in\\left[-H_{x}, H_{x}\\right]^{d}, \\frac{\\left\\|\\sigma\\left(\\tilde{W}_{B} X\\right)-\\sigma\\left(\\left(t W_{A}+(1-t) \\tilde{W}_{B}\\right) X\\right)\\right\\|_{1}}{N} \\leq g_{2}(T, z, \\delta, N, \\varepsilon)\n$$\n\nUsing lemma 20 from Mei et al. [2019] we know that $\\forall i \\in[N], \\bar{n}_{i}^{T} \\leq K(1+T)$ for a certain constant $K$. Therefore we can bound $\\left\\|A_{A}\\right\\|_{\\infty},\\left\\|A_{B}\\right\\|_{\\infty} \\leq g_{1}(T, z, N, \\varepsilon):=K(1+T)+K e^{K(1+T)^{3}} \\frac{1}{\\sqrt{N}}[\\sqrt{\\log (N T)}+z]+K e^{K(1+T)^{2} T} \\varepsilon+$ $K e^{K(1+T)^{2} T} \\sqrt{\\varepsilon}(\\sqrt{d+\\log (N)}+z)$.\nTaking $z=\\sqrt{\\log \\left(\\frac{12}{3}\\right)}$, we have shown the existence of a permutation $\\gamma$ with probability at least $1-\\delta$ such that almost surely on the choice of $x \\sim P$ and $\\forall t \\in[0,1]$, we have:\n\n$$\n\\begin{aligned}\n& \\left|t \\hat{f}\\left(x ; \\theta_{A}\\right)+(1-t) \\hat{f}\\left(x ; \\theta_{B}\\right)-\\hat{f}\\left(x ; t \\theta_{A}+(1-t) \\hat{\\theta}_{B}\\right)\\right| \\leq g_{1}(T, z, N, \\varepsilon) g_{2}(T, z, \\delta, N, \\varepsilon) \\\\\n& \\quad \\leq\\left(K(1+T)+K e^{K(1+T)^{3}} \\frac{1}{\\sqrt{N}}[\\sqrt{\\log (N T)}+z]+K e^{K(1+T)^{2} T} \\varepsilon+K e^{K(1+T)^{2} T} \\sqrt{\\varepsilon}(\\sqrt{d+\\log (N)}+z)\\right) \\\\\n& \\quad\\left(L_{\\sigma}\\left(H_{x} \\frac{C}{\\delta} N^{-\\frac{1}{s}}+2 H_{x} \\sqrt{d}\\left(K e^{K(1+T)^{3}} \\frac{1}{\\sqrt{N}}[\\sqrt{\\log (N T)}+z]\\right.\\right. \\\\\n& \\left.\\left.\\left.\\quad+K e^{K(1+T)^{2} T} \\varepsilon+K e^{K(1+T)^{2} T} \\sqrt{\\varepsilon}(\\sqrt{d+\\log (N)}+z)\\right)\\right)\\right)\n\\end{aligned}\n$$\n\nFor fixed $T, \\delta$ and $z=\\sqrt{\\log \\left(\\frac{12}{\\delta}\\right)}$, denote $\\operatorname{err}(N, \\varepsilon)$ the right hand term.\nIt is clear that:"
    },
    {
      "markdown": "$$\n\\forall \\operatorname{err}>0 \\exists N_{\\min } \\forall N \\geq N_{\\min } \\exists \\varepsilon_{\\max }(N) \\forall \\varepsilon \\leq \\varepsilon_{\\max }(N), \\operatorname{err}(N, \\varepsilon) \\leq \\operatorname{err}\n$$\n\nThis brings the first part of the theorem.\nDiscussion: Let's look more closely at the term $K e^{K(1+T)^{3}} \\frac{1}{\\sqrt{N}}\\left[\\sqrt{\\log (N T)}+z\\right]+K e^{K(1+T)^{2} T} \\varepsilon+$ $K e^{K(1+T)^{2} T} \\sqrt{\\varepsilon}(\\sqrt{d+\\log (N)}+z))$ from Mei et al. [2019] which comes from the mean field approximation. When taken alone, this term yields an error which is independent of the input dimension $d$, since taking $N$ large leads to small error (provided $\\varepsilon$ is small). However, here the growth of the hidden layer $N$ depends on the input dimension $d$ through the exponent $s>d_{1}^{*}(\\mu)$ (with $d_{1}^{*}(\\mu) \\leq d$ ) using notations from Weed and Bach [2019]. This is due to Wasserstein convergence rates of empirical measures in dimension $d$. Without any further assumption on the weight distribution or precise study of the PDE we have to consider that $\\operatorname{Supp}(\\mu)$ ) has dimension $d$. To remove this dependence, one could study a precise model for the data and look more closely at the PDE evolution to better understand the support of the distribution $\\rho_{t}$.\n\nTo prove the second part of the theorem, first notice that as already mentioned, $\\left\\|A_{A}\\right\\|_{\\infty},\\left\\|A_{B}\\right\\|_{\\infty} \\leq g_{1}(T, z, N, \\varepsilon)$. Moreover, the data distribution is bounded and the weights of the first layer of the approximating PDE live in a bounded set thanks to Appendix C.2.2. It brings the existence of a constant $K(T)$ such that:\n\n$$\n\\forall t \\leq T, \\forall w \\in \\operatorname{Supp}(\\rho(t)), \\forall X \\in \\operatorname{Supp}(P),|w . x| \\leq K(T)\n$$\n\nUsing we see that if $\\varepsilon \\leq \\min \\left\\{\\frac{1}{K_{0} e^{K_{0}(1+T)^{3}}}, \\frac{1}{K_{0}\\left(d+\\log (N)+z^{2}\\right) e^{K_{0}(1+T)^{2}}}\\right\\}$, with probability at least $1-\\frac{\\delta}{2}$, setting $z=\\sqrt{\\frac{12}{\\delta}}$\n\n$$\n\\begin{aligned}\n& \\max _{k \\in[0, T / z] \\cap \\mathbb{N}} \\max _{i \\in[N]}\\left|w_{A, i}^{k} x\\right| \\leq H_{x} \\sqrt{d}\\left(K e^{K(1+T)^{3}}\\right. \\\\\n& \\left.\\frac{1}{\\sqrt{N}}\\left[\\sqrt{\\log (N T)}+z\\right]+K e^{K(1+T)^{2} T} \\varepsilon+K e^{K(1+T)^{2} T} \\sqrt{\\varepsilon}(\\sqrt{d+\\log (N)}+z)\\right)+K(T)\n\\end{aligned}\n$$\n\nUp to changing previous $N_{\\min }, \\varepsilon_{\\max }$, we can suppose that $\\forall N \\geq N_{\\min }, \\forall \\varepsilon \\leq \\varepsilon_{\\max }(N), K e^{K(1+T)^{3}} \\frac{1}{\\sqrt{N}}\\left[\\sqrt{\\log (N T)}+\\right.$ $z]+K e^{K(1+T)^{2} T} \\varepsilon+K e^{K(1+T)^{2} T} \\sqrt{\\varepsilon}(\\sqrt{D+\\log (N)}+z) \\leq 1$\nTherefore, since the data distribution is bounded we know that there exists a constant $K(T)$ such that if $N \\geq N_{\\min }, \\varepsilon \\leq \\varepsilon_{\\max }(N), P-$ almost-surely:\n\n$$\n\\left\\{\\begin{array}{l}\n|y| \\leq K(T) \\\\\n\\left|\\bar{f}\\left(x: \\theta_{A}\\right)\\right| \\leq K(T) \\\\\n\\left|\\bar{f}\\left(x: \\theta_{B}\\right)\\right| \\leq K(T)\n\\end{array}\\right.\n$$\n\nWe make a general assumption on the loss of the form: $\\forall y \\in \\mathbb{R}(x \\rightarrow \\mathcal{L}(x, y))$ is convex and $\\forall K>$ $0, \\exists C_{K}, \\forall x_{1}, x_{2}, y \\in[-K, K],\\left|\\mathcal{L}\\left(x_{1}, y\\right)-\\mathcal{L}\\left(x_{2}, y\\right)\\right| \\leq C_{K}\\left|x_{1}-x_{2}\\right|_{2}$. In particular this is true for the square loss.\n\nCombining convexity of the loss, the first part of the theorem already proved and Lipschitzness on a compact domain of the loss, we get the second part of the theorem with a term $C_{K(T)}$ err instead of err. To solve this, just consider $\\min \\left\\{\\frac{\\text { err }}{C_{T}}, \\operatorname{err}\\right\\}$ in the first part of the theorem.\nWe have supposed in the beginning that the non-linearity was bounded. But the previous study shows that with probability at least $1-\\delta,\\left|w_{A, i} x\\right|$ is upper bounded for all $i$ during the training up to time T by some constant depending only on $T, \\delta$ provided $N \\geq N_{\\min }, \\varepsilon \\leq \\varepsilon_{\\max }(N)$ Therefore assuming that the non-linearity is bounded on a big enough compact set is enough to get the the result since it doesn't change the dynamics of the parameters considered. However the size of this set is not made explicit here."
    },
    {
      "markdown": "# C. 3 Proving LMC for general SGD \n\nWe will now study LMC of neural networks trained under general SGD using Theorem 4 part B from Mei et al. [2019]. The study is very similar to the case of noiseless SGD and will yield similar results.\n\nMore precisely in that case the PDE writes as:\n\n$$\n\\begin{aligned}\n& \\partial_{t} \\rho_{t}=2 \\xi(t) \\nabla_{\\theta} \\cdot\\left(\\rho_{t}(\\theta) \\nabla_{\\theta} \\Psi_{\\lambda}\\left(\\theta ; \\rho_{t}\\right)\\right)+2 \\xi(t) \\tau d^{-1} \\Delta_{\\theta} \\rho_{t} \\\\\n& \\Psi_{\\lambda}(\\theta ; \\rho)=\\Psi(\\theta ; \\rho)+\\frac{\\lambda}{2}\\|\\theta\\|_{2}^{2}\n\\end{aligned}\n$$\n\nNotice that our Assumptions 1 and 2 imply assumptions 1 to 6 in Mei et al. [2019].\n\n## C.3.1 Intermediate dynamics for general SGD\n\nMei et al. [2019] define as before intermediate dynamics:\n\n## Non linear dynamics\n\nLet consider $\\bar{\\theta}_{i}^{t}$ with initialization $\\bar{\\theta}_{i}^{0} \\sim \\rho_{0}$ i.i.d. which follows the dynamics\n\n$$\n\\bar{\\theta}_{i}^{t}=\\bar{\\theta}_{i}^{0}+2 \\int_{0}^{t} \\xi(s) G\\left(\\bar{\\theta}_{i}^{s} ; \\rho_{s}\\right) d s+\\int_{0}^{t} \\sqrt{2 \\xi(s) \\tau d^{-1}} d W_{i}(s)\n$$\n\nwhere $G(\\theta ; \\rho)=-\\nabla \\Psi_{\\lambda}(\\theta ; \\rho)$. An important fact is that $\\bar{\\theta}_{i}^{t}$ is random because of the random initialization and its law at time $t$ is $\\rho_{t}$. It corresponds to the evolution of particles under a field which depends only on the position of the optimized particle and the overall distribution of all particles plus and a diffusion term.\n\n## Particle Dynamics\n\nLet $\\theta_{i}^{t}$ with initialization $\\theta_{i}^{0}=\\bar{\\theta}_{i}^{0}$ with the following dynamics where $\\rho_{t}^{(N)}=\\frac{1}{N} \\sum_{i=1}^{N} \\delta_{y_{i}^{t}}$ denote the empirical distribution of $\\theta_{i}^{t}$.\n\n$$\n\\theta_{i}^{t}=\\theta_{i}^{0}+2 \\int_{0}^{t} \\xi(s) G\\left(\\theta_{i}^{s} ; \\rho_{s}^{(N)}\\right) d s+\\int_{0}^{t} \\sqrt{2 \\xi(s) \\tau d^{-1}} d W_{i}(s)\n$$\n\n## Gradient descent dynamics\n\nLet $\\tilde{\\theta}_{i}^{k}$ with initialization $\\tilde{\\theta}_{i}^{0}=\\bar{\\theta}_{i}^{0}$ with the following dynamics:\n\n$$\n\\tilde{\\theta}_{i}^{k}=\\bar{\\theta}_{i}^{0}+2 \\varepsilon \\sum_{l=0}^{k-1} \\xi(l \\varepsilon) G\\left(\\tilde{\\theta}_{i}^{l} ; \\tilde{\\rho}_{l}^{(N)}\\right)+\\int_{0}^{k \\varepsilon} \\sqrt{2 \\xi([s]) \\tau d^{-1}} d W_{i}(s)\n$$\n\n## Stochastic Gradient Descent Dynamics\n\nConsider $\\theta_{i}^{k}$ with initialization $\\theta_{i}^{0}=\\bar{\\theta}_{i}^{0}$ that follows:\n\n$$\n\\theta_{i}^{k}=\\theta_{i}^{0}+2 \\varepsilon \\sum_{l=0}^{k-1} \\xi(l \\varepsilon) F_{i}\\left(\\theta^{l} ; z_{l+1}\\right)+\\int_{0}^{k \\varepsilon} \\sqrt{2 \\xi([s]) \\tau d^{-1}} d W_{i}(s)\n$$\n\nwhere $F_{i}\\left(\\theta^{k} ; z_{k+1}\\right)=-\\lambda \\theta_{i}^{k}+\\left(y_{k+1}-\\hat{y}_{k+1}\\right) \\nabla_{\\theta_{i}} \\sigma_{*}\\left(x_{k+1} ; \\hat{\\theta}_{i}^{k}\\right), z_{k}=\\left(x_{k}, y_{k}\\right)$ and $\\hat{y}_{k+1}=\\frac{1}{N} \\sum_{j=1}^{N} \\sigma_{*}\\left(x_{k+1} ; \\theta_{j}^{k}\\right)$\nAs before we first control the distance between noisy SGD and non linear dynamics with the following lemma:\nLemma C.2. Consider a two-layer neural network with notations as before trained with noisy SGD for an underlying time $T$. Assume $T \\geq 1$. Then under assumptions Assumptions 1 and 2, there exists a constant $K$ such that with probability at least $1-3 e^{-z^{2}}$ we have:"
    },
    {
      "markdown": "$$\n\\begin{aligned}\n\\max _{k \\in[0, T / \\varepsilon] \\cap N} \\max _{i \\in[N]}\\left\\|\\theta_{i}^{k}-\\tilde{\\theta}_{i}^{k \\varepsilon}\\right\\|_{2} & \\leq K e^{e^{K T}\\left[\\sqrt{\\log (N)}+z^{2}\\right]}\\left[\\sqrt{d \\log (N)}+z^{3}+\\log ^{3 / 2}(N T)\\right] / \\sqrt{N} \\\\\n& +K e^{e^{K T}\\left[\\sqrt{\\log (N)}+z^{2}\\right]}\\left[\\log (N(T / \\varepsilon \\vee 1))+z^{4}\\right] \\sqrt{\\varepsilon} \\\\\n& +K e^{e^{K T}\\left[\\sqrt{\\log (N)}+z^{2}\\right]}\\left[\\sqrt{d} \\log (N)+z^{3}+\\log ^{3 / 2}(N)\\right] \\sqrt{\\varepsilon}\n\\end{aligned}\n$$\n\nProof. Just apply proposition 47,49,50 from Mei et al. [2019].\n\nWe moreover recall here Lemma 9 from Mei et al. [2019] which bounds the value of the second layer coefficients $A^{t}=\\left(a_{1}^{t}, \\ldots, a_{N}^{t}\\right)$.\nLemma C. 3 (Lemma 19 in Mei et al. [2019]). There exists a constant $K$ such that with probability at least $1-e^{-z^{2}}$ we have\n\n$$\n\\sup _{t \\in[0, T]}\\left\\|A^{t}\\right\\|_{\\infty} \\leq K e^{K T}[\\sqrt{\\log (N)}+z]\n$$\n\n# C.3.2 Proof of Theorem 3.1 in the case of noisy regularized SGD \n\nWe can now prove LMC for two two-layer networks trained with general SGD:\n\nTheorem 3.1. Consider two two-layer neural networks as in Equation (3) trained with equation SGD with the same initialization over the weights independently and for the same underlying time $T$. Suppose Assumptions 1 and 2 to hold. Then $\\forall \\delta$, err, $\\exists N_{\\min }$ such that if $N \\geq N_{\\min }, \\exists \\varepsilon_{\\max }(N)$ such that if $\\varepsilon \\leq$ $\\varepsilon_{\\max }(N)$ in Equation (4), then with probability at least $1-\\delta$ over the training process, there exists a permutation of the second network's hidden layer such that for almost every $x \\sim P$ :\n\n$$\n\\begin{aligned}\n& \\left|t \\hat{f}_{N}\\left(x ; \\theta_{A}\\right)+(1-t) \\hat{f}_{N}\\left(x ; \\theta_{B}\\right)\\right. \\\\\n& \\left.-\\hat{f}_{N}\\left(x ; t \\theta_{A}+(1-t) \\hat{\\theta}_{B}\\right)\\right| \\leq \\operatorname{err}, \\quad \\forall t \\in[0,1]\n\\end{aligned}\n$$\n\nProof. We follow the same steps as before. Recall that the data distribution is bounded: $\\operatorname{Supp}(P) \\subset\\left[-H_{x}, H_{x}\\right]^{d} \\times$ $\\left[-H_{y}, H_{y}\\right]$.\n\nThe problem is that due to the stochasticity added in the noisy SGD, $\\operatorname{Supp}\\left(\\rho_{t}\\right)$ is not necessarily bounded anymore. However, using step 3 of the proof of lemma 41 in Mei et al. [2019] and the fact that the initial distribution $\\rho_{0}$ has bounded support (sub-Gaussian would be enough), the distribution of weights of the first layer at time $T$ is sub-Gaussian.\nIndeed if $\\tilde{\\theta}_{i}^{t} \\sim \\rho_{t}$ and $\\rho_{0}$ is bounded or sub-Gaussian, we get the existence of $K$ such that:\n\n$$\n\\mathbb{P}\\left(\\left\\|\\tilde{\\theta}_{i}^{T}\\right\\|_{2}^{2} \\geq K e^{K T}(1+z) \\sqrt{T}\\right) \\leq e^{-z^{2}}\n$$\n\nwhich proves that $\\rho_{T}$ is sub-Gaussian.\nWe could adapt the proof done in Lemma B. 14 for Gaussian variable to sub-Gaussian variable to show the existence of constants (Lemma B. 14 dealt with $\\mathcal{W}_{p}^{2}$ but can be extended to $\\mathcal{W}_{1}$ because Proposition 15 of Weed and Bach [2019] is valid for any $\\mathcal{W}_{p}^{p}$ ) $D_{2}^{t}, E_{2}^{t}$ depending only on the constant $K$ of sub-Gaussianity of the previous distribution, and hence independent of $N$ such that by considering the norm $\\|\\cdot\\|_{2}$, we can still bound for $m \\geq E_{2}^{t d}$\n\n$$\n\\mathbb{E}\\left[\\mathcal{W}_{1}\\left(\\hat{\\mu}_{A}, \\hat{\\mu}_{B}\\right)\\right] \\leq \\sqrt{\\frac{D_{2}^{t}}{d} \\log (N)}\\left(\\frac{1}{N}\\right)^{1 / d}\n$$"
    },
    {
      "markdown": "Therefore, with probability at least $1-\\frac{\\delta}{2}-6 e^{-z^{2}}$ :\n\n$$\n\\begin{aligned}\n& \\forall X \\in\\left[-H_{x}, H_{x}\\right]^{D}, \\frac{\\left\\|\\sigma\\left(W_{A} X\\right)-\\sigma\\left(\\left(t W_{A}+(1-t) \\tilde{W}_{B}\\right) X\\right)\\right\\|_{1}}{N} \\leq g_{2}(T, \\varepsilon, N, \\delta) \\\\\n& :=L_{\\sigma} H_{x} \\sqrt{d} \\frac{1}{\\delta} \\sqrt{\\frac{D_{\\sigma}^{\\prime}}{d} \\log (N) N^{-1 / d}} \\\\\n& +2 L_{\\sigma} H_{x} \\sqrt{d}\\left(K e^{e^{K T}\\left[\\sqrt{\\log (N)}+z^{2}\\right]}\\left[\\sqrt{d \\log (N)}+z^{3}+\\log ^{3 / 2}(N T)\\right] / \\sqrt{N}\\right. \\\\\n& +K e^{e^{K T}\\left[\\sqrt{\\log (N)}+z^{2}\\right]}\\left[\\log (N(T / \\varepsilon \\vee 1))+z^{4}\\right] \\sqrt{\\varepsilon} \\\\\n& +K e^{e^{K T}\\left[\\sqrt{\\log (N)}+z^{2}\\right]}\\left[\\sqrt{d} \\log (N)+z^{3}+\\log ^{3 / 2}(N)\\right] \\sqrt{\\varepsilon)}\n\\end{aligned}\n$$\n\nand same for the second term. We moreover have, using Lemmas C. 2 and C. 3 that with probability at least $1-2 e^{-z^{2}}:$\n\n$$\n\\begin{aligned}\n\\max \\left\\{\\left\\|A_{A}\\right\\|_{\\infty},\\left\\|A_{B}\\right\\|_{\\infty}\\right\\} & \\leq g_{1}(T, z, N, \\varepsilon) \\\\\n& :=K e^{K T}\\left[\\sqrt{\\log (N)}+z\\right]+K e^{e^{K T}\\left[\\sqrt{\\log (N)}+z^{2}\\right]}\\left[\\sqrt{d \\log (N)}+z^{3}+\\log ^{3 / 2}(N T)\\right] / \\sqrt{N} \\\\\n& +K e^{e^{K T}\\left[\\sqrt{\\log (N)}+z^{2}\\right]}\\left[\\log (N(T / \\varepsilon \\vee 1))+z^{4}\\right] \\sqrt{\\varepsilon} \\\\\n& +K e^{e^{K T}\\left[\\sqrt{\\log (N)}+z^{2}\\right]}\\left[\\sqrt{d} \\log (N)+z^{3}+\\log ^{3 / 2}(N)\\right] \\sqrt{\\varepsilon}\n\\end{aligned}\n$$\n\nTaking $z=\\sqrt{\\log \\left(\\frac{16}{\\delta}\\right)}$ such that $8 e^{-z^{2}}=\\frac{\\delta}{2}$ we get that with probability at least $1-\\delta$ :\n\n$$\n\\left|t \\hat{f}\\left(x ; \\theta_{A}\\right)+(1-t) \\hat{f}\\left(x ; \\tilde{\\theta}_{B}\\right)-\\hat{f}\\left(x ; t \\theta_{A}+(1-t) \\tilde{\\theta}_{B}\\right)\\right| \\leq g_{1}(T, z, N, \\varepsilon) g_{2}(T, z, N, \\varepsilon)\n$$\n\nAs before, for fixed $T, \\delta$, denote $\\operatorname{err}(N, \\varepsilon)$ the left hand term.\nIt is clear that:\n\n$$\n\\forall \\operatorname{err}>0 \\exists N_{\\min } \\forall N \\geq N_{\\min } \\exists \\varepsilon_{\\min }(N) \\forall \\varepsilon \\leq \\varepsilon_{\\min }(N), \\operatorname{err}(N, \\varepsilon) \\leq \\operatorname{err}\n$$\n\nTherefore, sending $N \\rightarrow \\infty, \\varepsilon \\rightarrow 0$ brings immediately the first part of the theorem.\nTo get the second part of the theorem, we do the same procedure as for noiseless regularization-free SGD.\nNamely, with probability $1-\\delta$ we have both:\n\n$$\n\\left\\{\\begin{array}{l}\nP-\\text { almost surely, }\\left|t \\hat{f}\\left(x ; \\theta_{A}\\right)+(1-t) \\hat{f}\\left(x ; \\theta_{B}\\right)-\\hat{f}\\left(x ; t \\theta_{A}+(1-t) \\tilde{\\theta}_{B}\\right)\\right| \\leq \\operatorname{err} \\\\\n\\max \\left\\{\\left\\|A_{A}\\right\\|_{\\infty},\\left\\|A_{B}\\right\\|_{\\infty}\\right\\} \\leq g_{1}(T, N, z, \\varepsilon)\n\\end{array}\\right.\n$$\n\nUp to changing $N_{\\min }, \\varepsilon_{\\max }(N)$ we can suppose that $\\forall N \\geq N_{\\min }, \\forall \\varepsilon \\leq \\varepsilon_{\\max }(N)$, we have\n\n$$\n\\begin{aligned}\nK e^{e^{K T}\\left[\\sqrt{\\log (N)}+z^{2}\\right]} & {\\left[\\sqrt{d \\log (N)}+z^{3}+\\log ^{3 / 2}(N T)\\right] / \\sqrt{N} } \\\\\n& +K e^{e^{K T}\\left[\\sqrt{\\log (N)}+z^{2}\\right]}\\left[\\log (N(T / \\varepsilon \\vee 1))+z^{4}\\right] \\sqrt{\\varepsilon} \\\\\n& +K e^{e^{K T}\\left[\\sqrt{\\log (N)}+z^{2}\\right]}\\left[\\sqrt{d} \\log (N)+z^{3}+\\log ^{3 / 2}(N)\\right] \\sqrt{\\varepsilon} \\leq 1\n\\end{aligned}\n$$\n\nwhich brings\n\n$$\ng_{1}(T, N, z, \\varepsilon) \\leq K e^{K T}[\\sqrt{\\log (N)}+z]+1\n$$"
    },
    {
      "markdown": "Appendix C.3.2, boundness of the activation, boundness of the input distribution $(x, y) \\sim P$ by assumption imply the existence of $K^{\\prime}$ such that $\\forall t \\in[0,1], P$-almost-surely and for $N$ large enough,\n\n$$\n\\left\\{\\begin{array}{l}\n\\left|\\hat{f}\\left(x ; \\theta_{A}\\right)\\right| \\leq K^{\\prime}\\left(K e^{K T}[\\sqrt{\\log (N)}+z]\\right) \\\\\n\\left|\\hat{f}\\left(x ; \\theta_{B}\\right)\\right| \\leq K^{\\prime}\\left(K e^{K T}[\\sqrt{\\log (N)}+z]\\right) \\\\\n|y| \\leq H_{y} \\leq K^{\\prime}\\left(K e^{K T}[\\sqrt{\\log (N)}+z]\\right)\n\\end{array}\\right.\n$$\n\nUsing convexity and Lipschitzness of the squared loss on compact domains we get the existence (and this is a sufficient condition for the loss function with convexity) of Lip : $\\mathbb{R}_{+} \\rightarrow \\mathbb{R}_{+}$such that: $\\exists L_{1}, L_{2}, \\forall H \\in$ $\\mathbb{R}_{+}, \\operatorname{Lip}(H) \\leq L_{1}+L_{2} \\exp (H), \\forall x_{1}, x_{2}, y \\in\\left[-H, H],\\left|\\mathcal{L}\\left(x_{1}, y\\right)-\\mathcal{L}\\left(x_{2}, y\\right)\\right| \\leq \\operatorname{Lip}(H)\\right| x_{1}-x_{2} \\mid$ and such that with probability at least $1-\\delta$ :\n\n$$\n\\mathbb{E}\\left[\\mathcal{L}\\left(\\hat{f}\\left(x ; t \\theta_{A}+(1-t) \\tilde{\\theta}_{B}\\right), y\\right)\\right] \\leq \\operatorname{Lip}\\left(K^{\\prime}\\left(K e^{K T}[\\sqrt{\\log (N)}+z]+1\\right)\\right) g_{1}(T, z, \\varepsilon, N) g_{2}(T, z, \\varepsilon, N)\n$$\n\nPlugging this back and with the exact same discussion as before we get $\\exists N_{\\min } \\forall N \\geq N_{\\min } \\exists \\varepsilon_{\\max } \\forall \\varepsilon \\leq \\varepsilon_{\\max }$\n\n$$\n\\mathbb{E}\\left[\\mathcal{L}\\left(\\hat{f}\\left(x ; t \\theta_{A}+(1-t) \\tilde{\\theta}_{B}\\right), y\\right)\\right] \\leq \\operatorname{err}+t \\mathbb{E}\\left[\\mathcal{L}\\left(\\hat{f}\\left(x ; \\theta_{A}\\right), y\\right)\\right]+(1-t) \\mathbb{E}\\left[\\mathcal{L}\\left(\\hat{f}\\left(x ; \\theta_{B}\\right), y\\right)\\right]\n$$\n\nTo get both part 1 and 2 of the theorem at the same time we just have to reconsider the max of both $N_{\\min }$ and the min of both $\\varepsilon_{\\max }$.\n\n# C. 4 On the satisfiability of our assumptions \n\nAssumption 1 and 2 are non-trivial but standard in the mean field literature. They are made to ensure that the optimization of the two-layer neural networks happens in the mean-field regime. Indeed, as explained above the weights are then approximately indepedent and we can leverage Wasserstein convergence bounds of empirical measure to prove linear mode connectivity. We used conventioanl assumptions from the mean-field litterature (e.g. see assumptions A1 to A6 in Mei et al. [2019], A1 to A4 in Mei et al. [2018]). $u, v, U, V$ are implicitly defined but once the non-linearity and the data distribution are fixed, $u, v, U, V$ are fully determined as functions of the parameters. Checking their derivability can be done using usual rules for derivation under the integral sign if the non-linearity is smooth. A particular case is to consider a two-layer network, with a sigmoïd activation, a bounded data distribution and a bounded uniform initialization over the parameters. We additionally mention that a lot of works empirically evidence the validity of the mean-field framework, hence we feel validating our approach. In the case of mltilayer networks, the main assumption is the independence of weights inside each layer (Assumption 6). Multiple recent works address this question by using a mean field view for multi-layer networks with bounds on the width needed during optimization with SGD (e.g. Th. 15 in Nguyen and Pham [2023]). Finally, we believe our results could be extended to approximated independence of weights with an additional error term for the error barrier on a linear path corresponding to the approximated independence. Quantifying the impact of correlation between weights constitutes a very interesting avenue for future work.\n\n## D EXPERIMENTS\n\n## D. 1 Experiment on CIFAR10\n\nWe compared activation and weight matching methods on the CIFAR10 dataset for a VGG16 model. Our experiment again shows the correlation between small approximate dimension of the support of the weight distribution and LMC effectiveness hence supporting our main theoretical study. As it is non trivial to compute the covariance of the input with convolutional layers as it is a high dimensional tensor we left the alternative weight matching methods as a future work. Providing a scalable technique to estimate such a covariance for CNNs is a an interesting research direction beyond the scope of this paper."
    },
    {
      "markdown": "![img-3.jpeg](img-3.jpeg)\n(a) Mean test loss of the trained networks $A$ and $B$ and error barrier on the linear path $M_{t}, t \\in[0,1]$ across different learning rate values for each matching problem.\n![img-4.jpeg](img-4.jpeg)\n(b) Approximate dimension $\\operatorname{Dim}(S):=\\operatorname{tr}(S)^{2} / \\operatorname{tr}(S^{2})$ of the matrices considered in the matching problems at each layer.\n\nFigure 3: Statistics of the average network $M$ over the linear path between networks $A$ and $B$ using respectively weight matching (blue) and activation matching (orange)\n\n# D. 2 Details about our new weight matching method \n\nUntil now we have studied the influence of the dimension of the support of the underlying distribution of weights on the convergence rate in Wasserstein distance of the corresponding empirical measure. An interesting question is to look at the influence of the distance used to define the Wasserstein distance.\n\nMore precisely, consider a single layer of two networks $A, B$ with input $X \\in \\mathbb{R}^{n}$ and matrix weights $W_{A, B} \\in$ $\\mathcal{M}_{m, n}(\\mathbb{R})$. Consider that the input data follows a distribution $P$ with $\\mathbb{E}_{P}\\left[X X^{T}\\right]=\\Sigma$\n\nThe underlying method that we use in our proof and which is the one referred to as weight matching method in Ainsworth et al. [2022] consists in minimizing the distances for the euclidean norm between weights matrices, i.e. to find:\n\n$$\n\\underset{\\Pi \\in \\mathcal{S}_{m}}{\\arg \\min }\\left\\|W_{A}-\\Pi W_{B}\\right\\|_{2}\n$$\n\nThis is equivalent to finding:\n\n$$\n\\underset{\\Pi \\in \\mathcal{S}_{m}}{\\arg \\min } \\sqrt{\\frac{1}{m}\\left\\|W_{A}-\\Pi W_{B}\\right\\|_{2}^{2}\\right\\}}=\\underset{\\pi \\in \\mathcal{S}_{m}}{\\arg \\min } \\sqrt{\\frac{1}{m} \\sum_{i=1}^{m}\\left\\|W_{A, i:}-W_{B, \\pi_{i} \\cdot}\\right\\|_{2}^{2}}\n$$\n\nWe get an expected square error between network $A$ and network $B$ permuted at the output layer of:\n\n$$\n\\mathbb{E}\\left[\\left\\|W_{A} X-\\Pi W_{B} X\\right\\|_{2}^{2}\\right]=\\left(W_{A}-\\Pi W_{B}\\right) \\Sigma\\left(W_{A}-\\Pi W_{B}\\right)^{T}=\\left\\|W_{A}-\\Pi W_{B}\\right\\|_{2, \\Sigma}^{2}\n$$\n\nwhere $\\|\\cdot\\|_{2, \\Sigma}$ is the semi-norm coming from $(X, Y) \\rightarrow X^{T} \\Sigma Y$ which is a symmetric positive bilinear product since $\\Sigma$ is symmetric positive (and a norm when $\\Sigma$ is definite positive i.e., when $\\operatorname{Span}(\\operatorname{Supp}(P))=\\mathbb{R}^{n}$ ).\nMinimizing the cost $\\left\\|W_{A}-\\Pi W_{B}\\right\\|_{2}$ contributes to minimizing the expected squared error $\\left\\|W_{A}-\\Pi W_{B}\\right\\|_{2, \\Sigma}^{2}$ but it appears more natural to directly minimize the cost $\\left\\|W_{A}-\\Pi W_{B}\\right\\|_{2, \\Sigma}^{2}$.\nAs explained in Theorem D. 2 below, we can directly link the approximate dimension of the underlying covariance matrix of each method with the decay rate of LMC error barrier. The underlying covariance matrix of each method is $W_{A}^{\\ell}\\left[W_{A}^{\\ell}\\right]^{T}$ for WM (naive), $W_{A}^{\\ell} \\Sigma_{A}^{\\ell-1}\\left[W_{A}^{\\ell}\\right]^{T}$ for WM (ours) and $\\Sigma_{A}^{\\ell}$ for AM.\n\nIndeed,"
    },
    {
      "markdown": "- for naive weight matching, each row of $W_{A}, W_{B}$ follows a distribution with covariance matrix $W_{A}^{\\ell}\\left[W_{A}^{\\ell}\\right]^{T}$,\n- for weight matching (ours), the optimization problem can be seen (Lemma D.1) as for naive weight matching but with covariance matrix $W_{A}^{\\ell} \\Sigma_{A}^{\\ell-1}\\left[W_{A}^{\\ell}\\right]^{T}$,\n- for activation matching, each row of $Z_{A}^{\\ell}$ follows a distribution with covariance matrix $\\Sigma_{A}^{\\ell}$.\n\n\n# D. 3 Gain of our new weight matching method \n\nThis section is motivated by the following question:\nWhat is the gain of optimizing directly the cost $\\left\\|W_{A}-\\Pi W_{B}\\right\\|_{2, \\Sigma}$ when $\\Sigma$ is low dimensional?\nFor example, let's suppose that $\\Sigma=\\operatorname{Diag}(1,1,0, \\ldots, 0)$ and hence the support of $X$ is two dimensional. Suppose moreover that $W_{A}$ and $W_{B}$ are as in Section 5.1 initialized i.i.d. with a distribution $\\mathcal{N}\\left(0, \\frac{l_{n}}{n}\\right)$ on the weights. Hence we have seen before that $\\left\\|W_{A}-\\tilde{W}_{B}\\right\\|_{2} \\sim\\left(\\frac{1}{m}\\right)^{1 / n}$. Since the minimization procedure is unaware of the structure of $\\Sigma$ it is clear by symmetry that $\\sqrt{\\left\\|W_{A, 1:}-\\tilde{W}_{B, 1:}\\right\\|_{2}^{2}+\\left\\|W_{A, 2:}-\\tilde{W}_{B, 2:}\\right\\|_{2}^{2}} \\sim \\sqrt{\\frac{2}{n}}\\left(\\frac{1}{m}\\right)^{1 / n}$. Therefore the convergence is still as $\\left(\\frac{1}{m}\\right)^{1 / n}$. However if we had first aimed at minimizing $\\left\\|W_{A}-\\Pi W_{B}\\right\\|_{2, \\Sigma}$ it is clear that the problem becomes two dimensional and hence $\\arg \\min _{\\Pi \\in \\mathcal{S}_{m}}\\left\\|W_{A}-\\Pi W_{B}\\right\\|_{2, \\Sigma} \\sim\\left(\\frac{1}{m}\\right)^{1 / 2}$ which is extremely faster when $n$ is large.\n\nWe want to apply this idea to our setting where we suspect the distribution of activations at each layer to be low dimensional. We now prove the following lemma:\nLemma D.1. Let $W_{A}, W_{B} \\in \\mathcal{M}_{m, n}(\\mathbb{R})$ satisfy Assumption 6 with underlying distribution $\\mu$ and let $\\Sigma \\in \\mathcal{M}_{n}(\\mathbb{R})$. Write $\\Sigma=O \\sqrt{\\Sigma}^{2} O^{T}$ where $O$ is orthogonal and $\\sqrt{\\Sigma}$ is diagonal. Then we get the equivalence between optimization problems:\n\n$$\n\\mathbb{E}\\left[\\min _{\\Pi \\in \\mathcal{S}_{m}}\\left\\|W_{A}-\\Pi W_{B}\\right\\|_{2, \\Sigma}^{2}\\right]=\\mathbb{E}\\left[\\min _{\\Pi \\in \\mathcal{S}_{m}}\\left\\|\\hat{W}_{A}-\\Pi \\hat{W}_{B}\\right\\|_{2}^{2}\\right]\n$$\n\nwhere $\\hat{W}_{B}, \\hat{W}_{B}$ satisfy Assumption 6 with underlying distribution $f_{*} \\mu$ the image measure of $\\mu$ by $f: X \\mapsto$ $O \\sqrt{\\Sigma} O^{T} X$\n\nProof. Just notice that $\\forall \\Pi \\in \\mathcal{S}_{m}$\n\n$$\n\\left\\|W_{A}-\\Pi W_{B}\\right\\|_{2, \\Sigma}^{2}=\\operatorname{tr}\\left[\\left(W_{A}-\\Pi W_{B}\\right) O \\sqrt{\\Sigma} O^{T}\\left(O \\sqrt{\\Sigma} O^{T}\\right)^{T}\\left(W_{A}-\\Pi W_{B}\\right)^{T}\\right]\n$$\n\nand do the change of variable $\\hat{W}_{A}=W_{A} O \\sqrt{\\Sigma} O^{T}$ (repectively $\\hat{W}_{B}$ )\nTheorem D.2. Consider $X \\in \\mathbb{R}^{n} \\sim P \\in \\mathcal{P}_{1}\\left(\\mathbb{R}^{n}\\right)$ such that $\\mathbb{E}_{P}\\left[X X^{T}\\right]=\\Sigma=\\operatorname{Diag}(1, \\ldots, 1,0, \\ldots, 0)$, $\\operatorname{rank}(\\Sigma)=$ $\\tilde{n} \\leq n$ and $W_{A}, W_{B}$ random weight matrices satisfying Assumption 6 with underlying distribution $\\mathcal{N}\\left(0, \\frac{l_{n}}{n}\\right)$ Denote $\\Pi_{1}, \\Pi_{2} \\in \\mathcal{S}_{m}$ random permutations that minimize the respective costs $\\left\\|W_{A}-\\Pi W_{B}\\right\\|_{2}$ and $\\left\\|W_{A}-\\Pi W_{B}\\right\\|_{2, \\Sigma}$ Then we have:\n\n$$\n\\begin{aligned}\n& \\mathbb{E}\\left[\\left\\|W_{A}-\\Pi_{1} W_{B}\\right\\|_{2, \\Sigma}^{2}\\right]=\\tilde{\\Omega}\\left(\\left(\\frac{1}{m}\\right)^{2 / n}\\right) \\\\\n& \\mathbb{E}\\left[\\left\\|W_{A}-\\Pi_{2} W_{B}\\right\\|_{2, \\Sigma}^{2}\\right]=\\tilde{\\Omega}\\left(\\left(\\frac{1}{m}\\right)^{2 / \\tilde{n}}\\right) \\\\\n& \\mathbb{E}\\left[\\left\\|W_{A}-\\Pi_{1} W_{B}\\right\\|_{2, \\Sigma}^{2}\\right]=\\tilde{\\mathcal{O}}\\left(\\left(\\frac{1}{m}\\right)^{2 / n}\\right) \\\\\n& \\mathbb{E}\\left[\\left\\|W_{A}-\\Pi_{2} W_{B}\\right\\|_{2, \\Sigma}^{2}\\right]=\\tilde{\\mathcal{O}}\\left(\\left(\\frac{1}{m}\\right)^{2 / \\tilde{n}}\\right)\n\\end{aligned}\n$$"
    },
    {
      "markdown": "Proof. Using Lemma D.1, we see that bounds 2 and 4 are just corollaries of Lemma B. 16 and theorem 5.3. To show bounds 1 and 3 just notice that:\n\n$$\n\\Pi_{1}=\\underset{\\Pi \\in \\mathcal{S}_{m}}{\\arg \\min }\\left\\|W_{A}-\\Pi W_{B}\\right\\|_{2}\n$$\n\nis almost surely unique.\nBy symmetry of the problem and Theorem 5.3 we therefore see that $\\forall i \\in[n]$ :\n\n$$\n\\mathbb{E}\\left\\|\\left[W_{A}-\\Pi_{1} W_{B}\\right]_{: i}\\right\\|_{2}^{2}=\\frac{1}{n} \\mathbb{E}\\left\\|W_{A}-\\Pi_{1} W_{B}\\right\\|_{2}^{2}=\\tilde{\\Omega}\\left(\\left(\\frac{1}{m}\\right)^{2 / n}\\right)\n$$\n\nFinally noticing that $\\Sigma=\\operatorname{Diag}(1, \\ldots, 1,0, \\ldots, 0)$ we get by summing:\n\n$$\n\\mathbb{E}\\left[\\left\\|W_{A}-\\Pi_{1} W_{B}\\right\\|_{2, \\Sigma}^{2}\\right]=\\frac{\\tilde{n}}{n} \\tilde{\\Omega}\\left(\\left(\\frac{1}{m}\\right)^{2 / n}\\right)=\\tilde{\\Omega}\\left(\\left(\\frac{1}{m}\\right)^{2 / n}\\right)\n$$\n\nSimilarly, exploiting a.s. uniqueness of $\\Pi_{1}$ and symmetry across dimensions, we get the third inequality."
    }
  ],
  "usage_info": {
    "pages_processed": 40,
    "doc_size_bytes": 1734021
  },
  "_metadata": {
    "model_version": "0.0.0",
    "llm_model": "mistral-ocr-2505",
    "query_data": {
      "pdf": "/Users/satyaortiz-gagne/CODE/paperext/data/cache/fulltext/849b6fb82f7515000e5dc72f5623d2ac/fulltext.pdf"
    },
    "model_id": "parsepdf"
  }
}