{
  "pages": [
    {
      "markdown": "# Towards Reliable Misinformation Mitigation: Generalization, Uncertainty, and GPT-4 \n\nKellin Pelrine ${ }^{1}$, Anne Imouza ${ }^{1}$, Camille Thibault ${ }^{2}$, Meilina Reksoprodjo ${ }^{3}$, Caleb A. Gupta ${ }^{4}$, Joel N. Christoph ${ }^{5}$, Jean-François Godbout ${ }^{2}$, Reihaneh Rabbany ${ }^{1}$<br>${ }^{1}$ McGill University; Mila ${ }^{2}$ Université de Montréal ${ }^{3}$ Eindhoven University of Technology<br>${ }^{4}$ University of Pennsylvania ${ }^{5}$ European University Institute\n\n\n#### Abstract\n\nMisinformation poses a critical societal challenge, and current approaches have yet to produce an effective solution. We propose focusing on generalization, uncertainty, and how to leverage recent large language models, in order to create more practical tools to evaluate information veracity in contexts where perfect classification is impossible. We first demonstrate that GPT-4 can outperform prior methods in multiple settings and languages. Next, we explore generalization, revealing that GPT-4 and RoBERTa-large exhibit differences in failure modes. Third, we propose techniques to handle uncertainty that can detect impossible examples and strongly improve outcomes. We also discuss results on other language models, temperature, prompting, versioning, explainability, and web retrieval, each one providing practical insights and directions for future research. Finally, we publish the LIAR-New dataset with novel paired English and French misinformation data and Possibility labels that indicate if there is sufficient context for veracity evaluation. Overall, this research lays the groundwork for future tools that can drive real-world progress to combat misinformation.\n\n\n## 1 Introduction\n\nMisinformation represents a significant societal challenge, with detrimental effects observed in various domains spanning elections (Meel and Vishwakarma, 2020), public health (Loomba et al., 2021), the economy (Marcelo, 2023), and more. Recent developments in generative models have enabled the creation of increasingly sophisticated and convincing AI-generated misinformation, including deepfakes and bots using large language models, which further exacerbate the potential harm of \"fake news\" (Zhou et al., 2023). As a result, developing systems to limit the spread and impact of misinformation is of critical importance. Although there is a great deal of research in this area (Shu\net al., 2017; Sharma et al., 2019; Shu et al., 2020; Kumar et al., 2021; Shahid et al., 2022), practical solutions remain elusive. Could the recent progress in generative language models provide a path to reliable veracity evaluation?\n\nIn this work, we investigate the predictive capabilities of GPT-4 compared to previous approaches. We propose going beyond the focus on direct classification performance that is common in this domain and also prioritizing understanding of generalization and uncertainty. We believe this is critical since, despite extensive prior work on misinformation detection, existing solutions often fail to generalize and work in real-world settings (Sharma et al., 2019; Wu and Hooi, 2022; Huang et al., 2020), and there is no path to a perfect classifier that will solve this challenge with oracle-like answers alone. Therefore, a better understanding of generalization will help shrink the gap between research and deployment. Similarly, a better understanding of uncertainty will enable systems to fail gracefully, informing users and practitioners when predictions are certain enough to be relied upon, and providing useful information without requiring impossibly perfect accuracy. Overall, this paper aims to lay groundwork that will lead to more practical systems and, hopefully, improvements in real-world misinformation mitigation.\n\nWe examine three datasets: the widely-used LIAR (Wang, 2017), CT-FAN-22 (Köhler et al., 2022) that contains both English and German corpora, and a new dataset LIAR-New. The latter was constructed to provide data beyond GPT-4's main knowledge cutoff, to include examples in English and French, and to understand whether examples have sufficient context for evaluating their veracity. We show superior classification results with GPT-4 compared with the literature, both directly and in transfer and multilingual settings. We analyze the errors made by GPT-4 and find they are quite different from a standard RoBERTa approach."
    },
    {
      "markdown": "We also conduct extensive experiments on uncertainty and context, examining ways to productively quantify uncertainty and evaluate how results are affected by insufficient input information. One of the approaches here (GPT-4 Uncertainty-Enabled) detects many Impossible examples and significantly increases performance. In addition, we show how extra label information can improve existing approaches, and that hard classification performance does not necessarily predict soft (probabilistic) performance. Finally, we provide preliminary analysis of web retrieval, showing that it provides a path to greatly improving performance, and explainability, showing it yields insights that motivate future work.\n\nIn summary, our key contributions are the following:\n\n- We show that GPT-4 can work for both hard and soft classification tasks for misinformation, including multilingual ones, and give better performance than existing approaches.\n- We conduct extensive analyses on the generalization of GPT-4 in relation to prior work, finding that the errors made are substantially different and that GPT-4 has superior generalization.\n- We propose a method to account for uncertainty with GPT-4 that excludes examples it cannot classify well and improves results by 8 percentage points on LIAR-New and 14 on LIAR.\n- We conduct a number of experiments aimed at important practical factors or showing paths to build on this work, including other language models, temperature, prompting, versioning, explainability, and web retrieval.\n- We release a new dataset, LIAR-New, which has novel English plus French data and Possibility labels. We demonstrate how this data can be used to better understand generalization and contextual uncertainty.\nReproducibility: We open-source our code and other contributions on Github. ${ }^{1}$\n\n\n## 2 Data\n\nWe experiment on several misinformation datasets. First, we analyze the LIAR dataset (Wang, 2017), which is one of the most widely-used benchmarks for fake news detection. It contains nearly 13 K examples with veracity labeled on a 6-point scale. We mainly follow the common approach in the lit-\n\n[^0]erature of binarizing these labels, but also report 6-way results. Second, we use the CT-FAN-22 dataset (Köhler et al., 2022) for additional tests, including transfer and multilingual settings. It contains an English and a German corpus with 4-way labels, 3 of which represent gradations of veracity and an \"Other\" category for uncertain or unverified examples. Finally, we provide a new dataset, LIAR-New. This dataset goes beyond the GPT-4 main knowledge cutoff of September 2021. It also provides novel contributions to the misinformation literature: first, every input is provided in both the original English language version and in French through human translation. Second, it includes Possibility labels that assess if an example is missing the necessary context and information to determine whether it is misinformation or not. We discuss the first two datasets in detail in Appendix B, and our new dataset below.\n\n### 2.1 LIAR-New\n\nTo minimize the possibility of test label leakage with GPT-4-i.e., to ensure that it is not relying on directly seeing the labels somewhere in its training data-we scraped PolitiFact for recent fact-checks that are beyond the main GPT-4 knowledge cutoff. There is no 100\\% fixed endpoint, but \"the vast majority of its data cuts off\" in September 2021 (OpenAI, 2023). We scraped all fact-checks between IDs 21300 and 23300, which represents roughly the end of September 2021 to mid-November 2022. In our experiments we remove the 27 examples from September 2021, leaving only data from October 1st, 2021 and beyond.\n\nThe form and source of the statements are the same as LIAR, as well as the labels, with the exception that PolitiFact changed the name of the class \"Barely-true\" to \"Mostly-false\". We note, however, that the class balance is very different: this dataset contains far more False examples than any other classes, and does not contain many True ones. This could correspond, for example, to a real-world scenario where users of a system are mostly interested in fact-checking statements that seem suspicious. Here, we do not train on this dataset, rather we use it solely for evaluation purposes.\n\nIn addition to experiments on the English statements provided by PolitiFact, we also translated all examples into French. This was done by two authors who are native French speakers and also fluent in English. The translation was done without\n\n\n[^0]:    ${ }^{1}$ https://github.com/ComplexData-MILA/ MitigateMisinfo"
    },
    {
      "markdown": "|  | Pants-fire | False | Mostly-false | Half-true | Mostly-true | True | Total |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| Test | 359 | 1067 | 237 | 147 | 99 | 48 | 1957 |\n\nTable 1: LIAR-New dataset statistics.\nthe aid of machine translation, in order to avoid potential biases that might make the task easier for systems that incorporate such translation. Although this is not the first misinformation dataset in French, there are not many, and we struggled to find one that is easily accessible and has clear provenance and quality. Thus, this French data may be practical both in and of itself, and for comparative analysis with the English pairing. We make the full data available on Zenodo. ${ }^{2}$\n\n### 2.2 Possible vs. Impossible Examples\n\nNot all inputs can be evaluated for veracity because certain claims cannot be verified without the proper context. To better understand this problem in relation to our data, we define the following labels for a given statement:\nDefinition 1. Possible: the statement's claim is clear without any additional context, or any missing context does not make it difficult to evaluate the claim.\n\nHard: the claim is missing important context that makes it hard to evaluate, but it might still be possible.\n\nImpossible: there is missing context that cannot be resolved (\"The senator said the earth is flat\"not possible without knowing which senator). Or, the statement contains no claim for which veracity could be evaluated regardless how much context is available.\n\nIt is important to note that, in an extreme sense, no statement can be verified without some level of additional context (for example, a speaker could reference an imaginary world in which the earth is flat). Therefore, these definitions are not meant to be ironclad, encyclopedic formulations; they are instead intended to capture a reasonable standard whereby a model with access to training and knowledge base information-but without additional context for a given statement-can reasonably evaluate that statement's veracity. They aim to strike a balance between practicality and accuracy, allowing for the effective categorization of statements containing misinformation while acknowledging\n\n[^0]the inherent complexity and context-dependency of language.\n\nThree annotators each labeled the entire dataset according to this definition. After a process of disagreement resolution, this yielded 927 Possible, 581 Hard, and 449 Impossible examples. The numerous Impossible examples show that such analysis is critical-training and evaluating on these examples unknowingly could lead to results that will not generalize well to real-world systems. In the Experiments section we examine the different cases. We present more detailed definitions and information on the labeling process in Appendix C.\n\n## 3 Methodology\n\n### 3.1 GPT-4\n\nMain Prompting To test the effectiveness of GPT-4 for both hard and soft classification, we crafted a concise prompt asking GPT-4 to rate the truthfulness of a statement on a scale of 0 to 100 , without providing any explanations. We denote the approaches using this prompt as \"GPT-4 Score\".\n\"role\": \"user\", \"content\": 'Rate the truthfulness of the following statement: \"STATEMENT\" Provide a score from 0 to 100 , where 0 represents definitively false and 100 represents definitively true. Do not provide any explanations, only respond with the numerical score.'\n\nWe also test a binary classification prompt, which we tried to make as similar as possible to the score prompt above. We denote this approach as \"GPT-4 Binary\".\n\"role\": \"user\", \"content\": 'Rate the truthfulness of the following statement: \"STATEMENT\" Provide a number 0 or 1 , where 0 represents false and 1 represents true. Do not provide any explanations, only respond with the number.'\n\nFinally, we examine a prompt that explicitly allows for GPT-4 to refuse to pass a judgment. It was constructed from the above binary prompt based on the observation that GPT-4 sometimes responded with a score of \" 0.5 \", despite the prompt instructing it to respond 0 or 1 only (see also discussion below). We denote this approach \"GPT-4 Binary Uncertainty-Enabled\".\n\n\n[^0]:    ${ }^{2}$ https://zenodo.org/records/10033607As of this writing, the Zenodo version is still under review; the data is also available on our GitHub."
    },
    {
      "markdown": "\"role\": \"user\", \"content\": 'Rate the truthfulness of the following statement: \"STATEMENT\" Provide a number 0 or 1 , where 0 represents false and 1 represents true. If you are uncertain or there is not enough context in the statement to be sure what it refers to, instead answer 0.5 . Do not make assumptions. Do not provide any explanations, only respond with the number.'\n\nOther Prompting We experimented with several other types of prompts. Explainability prompts are discussed in Section 4.8 and Appendix Q. Web retrieval prompts are discussed in Section 4.9 and Appendix R. Several attempts at in-context learning with automatically chosen demonstration are detailed in Appendix D.\n\nOther GPT-4 Implementation Details In most of our experiments, we opted for a moderate temperature value of 0.5 in the API to balance reproducibility and noise against nuance and model effectiveness. We later found that 0.0 temperature seems to yield better results, as discussed in Appendix J. Due to time and cost limits, however, we did not rerun the other experiments. Cost estimates are available in Appendix I.\n\nTo convert the GPT-4 output to predictions we could evaluate, we first separated integer and noninteger predictions. We found the latter cases generally occurred when GPT-4 refused or stated it was incapable of evaluating the veracity of the input. With GPT-4 Score, in the LIAR dataset, this occurred 40 times in the validation set and 69 times in the test set, out of 1284 and 1267 examples respectively. In CT-FAN-22 it occurred 2 times on English data and 0 on German. In LIAR-New, it occurred only once.\n\nOne of the authors manually examined the LIAR test set cases. According to Definition 2, all were Impossible without additional information. These included statements like \"On [...]\" that seem to be a recurring PolitiFact headline and have no veracity on their own to evaluate: e.g., \"On banning earmarks\" or \"On redistricting\". Other examples GPT-4 refused to answer would require in-depth speaker and context information, such as \"I said no to these big bank bailouts\" (one would need to know the speaker and which big bank bailouts they are talking about). Therefore, on these 69 examples, GPT-4 has perfect precision in determining that these statements cannot be evaluated in isolation. All of these examples are provided in Appendix C.3.\n\nIn the Experiments section, when not otherwise stated, we nonetheless make a prediction for these examples in the GPT-4 results by selecting a label uniformly at random. This ensures our results are fully comparable to other studies in the literature. Separately, we further investigate these examples in relation to other models.\n\nTo convert GPT-4 Score's 0-100 predictions to hard classification predictions, we examine two approaches. First, GPT-4 Score Zero-Shot simply divides the range evenly (e.g., for the binary case, splitting at 50). This is arbitrary but intuitive. Second, on binary LIAR, we tested GPT-4 Score Optimized which uses the threshold that gives optimal weighted F1 on the validation set. There, we found that the optimal threshold is 71-GPT-4 appears to be biased towards predicting \"True\" compared to the labels in the LIAR dataset. Note that neither of these (nor GPT-4 Binary) use any of the training data, and even GPT-4 Score Optimized does not maintain any memory of the validation set inputs besides this threshold.\n\nFor testing on non-English data, we keep everything unchanged, including leaving the prompt in English.\n\n### 3.2 RoBERTa-large\n\nPrior work, such as Pelrine et al. (2021) and Truică and Apostol (2023), has shown that older, mostly non-causal (bidirectional) transformer-based language models can give strong performance for misinformation classification. We used the code made public by Pelrine et al. (2021) to implement RoBERTa-large (Liu et al., 2019), which had the best performance in their study. We trained the model for 10 epochs with the given hyperparameter configuration and other implementation details, using an RTX8000 GPU. For conciseness in tables, we refer to this model as RoBERTa-L Binary or RoBERTa-L 6-way, depending on whether it was trained with binary or 6 -way labels. We focus our error analysis and a number of other experiments on this model compared to GPT-4, because by using the existing code we can ensure its implementation is unbiased, and it has already shown consistently strong performance on a number of datasets in the literature.\n\n### 3.3 Other Models\n\nOther Small Language Models We also finetuned a selection of SLMs besides RoBERTa. Im-"
    },
    {
      "markdown": "| Method | Accuracy | F1 |\n| :--: | :--: | :--: |\n| SOTA (2022; 2021) | 62 | - |\n| GPT-4 Score Optimized | 68.2 | 68.1 |\n| GPT-4 Score Zero-Shot | 64.9 | 60.9 |\n| GPT-4 Binary | 66.5 | 66.5 |\n| RoBERTa-L Binary | 63.5 | 62.1 |\n| RoBERTa-L 6-way | 64.7 | 64.1 |\n| BERT | 65.0 | 64.5 |\n| ConvBERT | 66.7 | 65.8 |\n| DeBERTA | 63.0 | 63.8 |\n| DeBERTA-V3 | 65.0 | 64.4 |\n| LUKE | 65.3 | 64.4 |\n| RoBERTa | 64.7 | 64.2 |\n| SqueezeBERT | 63.1 | 62.2 |\n| XLMRoBERTA | 61.1 | 61.0 |\n| Fuzzy (Word2Vec) | 60.2 | 60.4 |\n| Fuzzy (BERT) | 59.6 | 60.1 |\n| Fuzzy (GloVe) | 60.1 | 59.7 |\n\nplementations are detailed in Appendix E and results are reported in several experiments.\n\nFuzzy Clustering We performed limited tests with Fuzzy Clustering but the performance was not promising. We discuss this method in Appendix F.\n\nGPT-3.5 This model gave solid performance on LIAR, but it appeared brittle and did not perform well on other datasets. We discuss the results in Appendix G.\n\nPaLM 2 Bison We conducted a limited evaluation of PaLM 2 (Anil et al., 2023), and found that the second largest version gives passable, though not impressive, performance on LIAR. The largest version was not accessible to us as of this writing. We discuss this model and the results in Appendix O.\n\n### 3.4 Evaluation Metrics\n\nWe note our evaluation metrics in Appendix H.\n\n## 4 Experiments\n\n### 4.1 LIAR Binary Classification\n\nIn Table 2 we present binary classification results on the LIAR dataset (6-way classification results are presented in Appendix N). We see here that GPT-4 Score Optimized performs best, suggesting it can be a powerful new tool for misinformation detection. GPT-4 Score Zero-Shot is significantly worse but still better than several approaches that rely on the training data. GPT-4 Binary is in between the two, equal or better than all prior approaches.\n\nWe also find that some of our other language model approaches give performance exceeding previous state-of-the-art methods in the literature, paralleling older findings of Pelrine et al. (2021). Finally, we note that training RoBERTa-L on 6-way labels gives better performance than training on binary labels. This indicates the 6 -way labels provide additional helpful information even when the goal is binary prediction. There is a parallel here with the work of Matsoukas et al. (2020) in other domains.\n\n### 4.2 CT-FAN-22 Classification\n\nWe next present in Table 3 our results on CT-FAN22. We compare zero-shot GPT-4, RoBERTa-large trained on 6-way LIAR classification (converted to 3-way output here by dividing the 6 labels evenly\n\nTable 2: Binary Classification Results (percentages). GPT-4 shows superior performance. For RoBERTa-L, even though the test is binary classification, training on 6 -way labels provides an advantage.\nin 3), and the previous SOTA. We see that GPT-4 has a clear advantage in this setting, both in English and in German. RoBERTa performs poorly. This provides additional evidence for the strength of GPT-4 in general compared to models trained on in-distribution data. It also demonstrates that this model provides a large advantage in transferability to new data, and shows it can also work in multilingual settings better than previous approaches.\n\n|  | Method | Accuracy | F1 |\n| :--: | :--: | :--: | :--: |\n| English | SOTA (2022) | 54.7 | 33.9 |\n|  | GPT-4 Score Zero-Shot | 67.8 | 42.8 |\n|  | RoBERTa-L 6-way | 47.5 | 26.8 |\n| German | SOTA (2022) | 42.7 | 29.0 |\n|  | GPT-4 Score Zero-Shot | 57.6 | 38.7 |\n\nTable 3: 4-way classification in a transfer setting. On English data, GPT-4 beats the SOTA approach trained on in-distribution data, while RoBERTa trained on LIAR degrades. On German data, where there is no indistribution training data available, GPT-4 again beats SOTA—even without changing the prompt, which is written in English and makes no mention of German.\n\nIn Table 4, we remove the \"Other\" label category, which is not well-defined and for which we automatically marked all GPT-4 and RoBERTa predictions as wrong. Performance improves by quite a large margin. These results set the bar for future work on this dataset with the \"Other\" label excluded, which we recommend, as discussed in"
    },
    {
      "markdown": "Section B.2.\n\n|  | Method | Accuracy | F1 |\n| :--: | :--: | :--: | :--: |\n| English | GPT-4 Score Zero-Shot | 71.4 | 58.6 |\n|  | RoBERTa-L 6-way | 50.0 | 36.5 |\n| German | GPT-4 Score Zero-Shot | 63.6 | 54.3 |\n\nTable 4: 3-way classification in a transfer setting, excluding the label \"Other\" which is not well-defined. GPT-4 again beats RoBERTa.\n\n### 4.3 LIAR-New Classification\n\nWe next examine new data beyond the main GPT-4 knowledge cutoff. We see in Table 5 that it can nonetheless make effective predictions. This suggests the model is taking advantage of its overall knowledge to make predictions on examples related to older events and facts (and perhaps make educated guesses on newer ones), rather than just seeing the label during its training. This also suggests that GPT-4 has great potential if it could be combined with an effective web search to evaluate examples that require recent knowledge that it is missing. We also find that:\n\n- GPT-4 Binary again outperforms GPT-4 Score Zero-Shot, suggesting the score the latter predicts is not quite aligned with the labels in this binarization.\n- RoBERTa-L 6-way is again better than RoBERTa-L Binary, further confirming the benefit of training on 6-way labels.\n- GPT-4's performance decreases significantly as we move from Possible to Hard to Impossible examples, while RoBERTa has the opposite trend. This suggests GPT-4 in many cases may be relying on generalizable information to assess veracity, while RoBERTa may be taking advantage of distributional patterns that provide cues in this dataset but won't generalize (as we saw in the preceding transfer experiment).\n- GPT-4 gives superior performance overall.\n\n\n### 4.3.1 Misinformation in French\n\nIn Table 6, we test GPT-4 on the French version of the dataset. We see that performance decreases noticeably compared to English (i.e., Table 5). Part of this is likely due to GPT-4's overall performance in different languages, but we also hypothesize that the language can affect how GPT-4 imagines the context-for example, the best guess at who \"the president\" refers to might be the US president in English but the French president in French. We see that the drop in performance is a bit smaller\n\n|  | Method | Accuracy | F1 |\n| :--: | :--: | :--: | :--: |\n| Possible | GPT-4 Binary | 91.0 | 73.7 |\n|  | GPT-4 Score Zero-Shot | 82.9 | 61.5 |\n|  | RoBERTa-L 6-way | 79.4 | 57.3 |\n|  | RoBERTa-L Binary | 71.9 | 55.3 |\n| Hard | GPT-4 Binary | 74.0 | 68.8 |\n|  | GPT-4 Score Zero-Shot | 64.6 | 61.1 |\n|  | RoBERTa-L 6-way | 63.9 | 60.9 |\n|  | RoBERTa-L Binary | 60.1 | 58.2 |\n| Impossible | GPT-4 Binary | 71.0 | 58.5 |\n|  | GPT-4 Score Zero-Shot | 56.1 | 49.9 |\n|  | RoBERTa-L 6-way | 83.6 | 71.5 |\n|  | RoBERTa-L Binary | 76.2 | 66.1 |\n| All | GPT-4 Binary | 81.2 | 68.8 |\n|  | GPT-4 Score Zero-Shot | 71.3 | 60.5 |\n|  | RoBERTa-L 6-way | 75.8 | 64.0 |\n|  | RoBERTa-L Binary | 69.4 | 60.3 |\n\nTable 5: LIAR-New binary classification. GPT-4 shows strong performance, even beyond its main knowledge cutoff.\non the Possible examples compared to the others. These are examples where there should be no need to guess the context, so this weakly supports the above hypothesis, but could warrant further investigation. Moreover, the significant drop in performance in French, which is still a relatively common language, should motivate more misinformation mitigation work in low-resource languages. As seen in the previous German experiments, GPT4 may be doing better than previous approaches in other languages, but still has a lot of room for improvement.\n\n|  | Method | Accuracy | F1 |\n| :--: | :--: | :--: | :--: |\n| Possible | GPT-4 Binary | 89.6 | 66.1 |\n|  | GPT-4 Score Zero-Shot | 77.3 | 59.9 |\n| Hard | GPT-4 Binary | 71.1 | 60.6 |\n|  | GPT-4 Score Zero-Shot | 60.3 | 58.9 |\n| Impossible | GPT-4 Binary | 73.9 | 53.5 |\n|  | GPT-4 Score Zero-Shot | 53.5 | 46.4 |\n| All | GPT-4 Binary | 80.5 | 61.8 |\n|  | GPT-4 Score Zero-Shot | 70.1 | 57.5 |\n\nTable 6: Results on the French translation of LIARNew. Performance in French is lower. GPT-4 is still capable of identifying misinformation in that language, but future work to close this gap would be beneficial.\n\nWe also observed that in the case of GPT-4 Score Zero-Shot on the French data, there were 107 examples that it refused to classify, of which 62 were Impossible and 30 were Hard. This compares to single-digit numbers where it refused for GPT-4 Binary in French, and both models in English. This difference between the two versions of GPT-4 on the French data, and likewise on French vs. English data, suggests prompt engineering can have a significant impact on examples it might refuse"
    },
    {
      "markdown": "to classify, and in turn on how a system would work if it encountered missing context or otherwise Impossible examples in the real world.\n\n### 4.4 Uncertainty Assessment\n\nIn Table 7, we consider the uncertainty quantification of GPT-4 Score vs. RoBERTa-L on LIARNew. We remove all examples predicted between 49 and 51 inclusive (resp. 0.49 and 0.51 ) by GPT-4 (resp. RoBERTa-L's final softmax), i.e., the ones each model is the most uncertain about. We see that this significantly increases the performance of GPT-4 (compared to the respective rows of Table 5) while leaving RoBERTa-L virtually unchanged.\n\n|  | Method | Accuracy | F1 |\n| :--: | :--: | :--: | :--: |\n| Possible | GPT-4 Score Zero-Shot | 90.6 | 62.7 |\n|  | RoBERTa-L Binary | 72.3 | 55.6 |\n| Hard | GPT-4 Score Zero-Shot | 72.9 | 64.3 |\n|  | RoBERTa-L Binary | 60.7 | 58.8 |\n| Impossible | GPT-4 Score Zero-Shot | 69.9 | 58.5 |\n|  | RoBERTa-L Binary | 76.3 | 66.2 |\n| All | GPT-4 Score Zero-Shot | 81.1 | 64.3 |\n|  | RoBERTa-L Binary | 80.4 | 60.7 |\n\nTable 7: Removing the examples where each model indicated the most uncertainty on LIAR-New. GPT-4 performance improves, giving an example of productive uncertainty quantification in this context.\n\nWe next examine GPT-4 Binary UncertaintyEnabled. It responded \" 0.5 \" to 906 total examples (i.e., indicating it was not sure about their evaluation), out of which 306 were Impossible and 352 were Hard. This seems like a reasonable resultthe majority of the \" 0.5 \" cases are Impossible ones that should receive that rating or Hard ones where it is reasonable for the model to be uncertain. It is also not unreasonable for the model to be uncertain about some of the Possible cases as well, especially considering the challenge of this dataset being beyond the main knowledge cutoff. It is detecting over two thirds of the Impossible examples, although we note around 140 remain undetected, so its judgment is not perfect. Nonetheless, when we exclude these \" 0.5 \" examples and look at the classification performance on the remaining examples (Table 8), we see a marked increase in performance compared to other approaches.\n\nWe also report in the same table the results on LIAR. Here, we do not have Possibility labels for every example to compare with. We observed that this approach excluded 914 examples, which is considerable, approximately $75 \\%$ of the dataset. Based on the LIAR-New results, it is not unlikely\n\n|  | Method | Accuracy | F1 |\n| :--: | :--: | :--: | :--: |\n| LIAR-New | Possible | GPT-4 Binary Uncertainty-Enabled | 94.4 | 75.1 |\n|  | Hard | GPT-4 Binary Uncertainty-Enabled | 86.5 | 78.4 |\n|  | Impossible | GPT-4 Binary Uncertainty-Enabled | 91.4 | 70.4 |\n|  | All | GPT-4 Binary Uncertainty-Enabled | 92.3 | 76.8 |\n| LIAR |  | GPT-4 Binary Uncertainty-Enabled | 82.2 | 82.0 |\n\nTable 8: Allowing GPT-4 to refuse to answer examples it is not confident on excludes many Impossible examples and greatly improves performance on LIAR-New, as well as LIAR.\nthat a large portion of these examples are Impossible. However, the distributions are different (LIAR was sampled to have relatively balanced classes), so labeling would be needed to confirm. Regardless, performance on the remaining data is approximately 14 percentage points better than the best performance on LIAR in Table 2.\n\nThese results show a clear ability to leverage GPT-4's assessment of its confidence in classifying examples in this domain. Indeed, GPT-4 Binary Uncertainty-Enabled gives the strongest performance of any method in this paper excluding web retrieval (which could likely be combined with Uncertainty-Enabled). Granted, this is at the cost of excluding a significant number of examples. But the majority of the ones that were excluded are Hard or Impossible, and this is a domain where wrong answers can potentially be much more costly than no answer (for example, human users of a misinformation detection system might be misled by an inaccurate machine judgment, instead of making their own judgment from external sources). We believe future work should build on this capability, especially integrating it with explainability, which GPT-4 also has natural capabilities for.\n\n### 4.5 Error Analysis\n\nWe compare the errors made by GPT-4 Score ZeroShot and RoBERTa-L Binary on LIAR. Excluding cases GPT-4 refused to classify (which were found to be Impossible; see Section 3.1, \"Other GPT-4 Implementation Details\"), we find there are 241 examples that GPT-4 correctly classifies and RoBERTa-L does not, and 174 cases where the reverse is true. On LIAR-New, when we restrict the analysis to the 911 Possible examples after October 1st, 2021, there are 182 examples that GPT-4 correctly classifies and Roberta-L does not, and 82 cases the reverse. In both datasets, this represents a sizeable proportion, so it indicates a substantial difference in these models' failure modes.\n\nWe further investigated this by embedding LIAR examples with OpenAI's Ada-002 model (Greene"
    },
    {
      "markdown": "![img-0.jpeg](img-0.jpeg)\n\nFigure 1: GPT-4 Score calibrated by Platt's method can predict probabilities (ECE=5.9\\%).\net al., 2022). We found a statistically significant difference ( $\\mathrm{p}=0.0005$ ) in embedding space distances between train and test examples, depending on whether GPT-4 was correct and RoBERTa-L wrong, or vice versa. These results suggest methods to help GPT-4 leverage the training data could lead to a nearly 10 percentage point improvement. More detailed discussion can be found in Appendix L.\n\nIn Appendix M, we present a case study of examples where RoBERTa was correct and GPT-4 wrong. It shows the training data directly matches some of these examples, which both shows the potential of leveraging that data and also that it might lead to unrealistic memorization.\n\n### 4.6 Soft Classification\n\nOur next experiment compares GPT-4 and other language models' capabilities for making probability predictions. We found GPT-4 Score can give reasonable probability predictions once calibrated (Figure 1), but SqueezeBERT's results appear to be even better, depending on how much weight one puts on identifying more certain examples vs. avoiding overconfidence. This experiment is discussed in more detail in Appendix P.\n\n### 4.7 GPT-4 Version\n\nThe experiments in Table 2, where GPT-4 Binary achieved $66.5 \\%$ accuracy, were conducted with the gpt-4-0314 version. We revisited the performance with GPT-4-0613, and found a large drop to $56.0 \\%$. Large differences between different versions been reported in other domains (Chen et al., 2023), but to our knowledge we are the first to report it in the misinformation context. We suspect that the binary prompt especially may cause brittleness. In\nthe following section, we see that explainability prompts, which lead to reasoning chains rather than on the spot verdicts, seem more robust.\n\nUnless otherwise noted, all of our experiments were run with gpt-4-0314 (or the generic one, before versioning existed). We report results from multiple versions in the following two experiments on explainability and web retrieval, as well as some experiments in the appendix.\n\n### 4.8 Explainability\n\nGPT-4 naturally has explainability capabilities. To investigate these, we created two simple variations on the \"Score\" prompt, one of which consistently improves Score Zero-Shot performance in general. The full prompts and performance evaluations are available in Appendix Q.\n\nWe performed a small sample, preliminary evaluation where an author checked each explanation for reasonableness. We found that the explanation always matched the score. Furthermore, many of the explanations for answers with the wrong label predicted were actually reasonable or partially reasonable, especially in cases with scores near 50, where a slight difference in GPT-4's evaluation vs. PolitiFact's could result in a different label. This might indicate limitations in the evaluation and that GPT-4 has better results than the metrics captured. See Appendix Q. 3 for further details.\n\nWe then sampled LIAR-New examples that were Impossible ${ }^{3}$ to investigate the model's output. In cases it got correct, besides ones where it does not really know but gets lucky, it seems to either rely on general information or assume a United States context. This might be a noteworthy geopolitical bias but would need further investigation and replication. In the wrong predictions, generally it seems to know it is missing context. Sometimes issues are caused by not having photos or videos, which might merit a targeted solution. We present more details and case studies in Appendix Q.4. Overall, these results, while preliminary, show the potential of recent LLMs to help researchers understand how they are making their veracity evaluations, and in turn figure out how to improve them.\n\n### 4.9 Web Retrieval\n\nWe conducted a preliminary experiment on the potential of web retrieval. When collecting LIARNew, we scraped the full text of the PolitiFact ar-\n\n[^0]\n[^0]:    ${ }^{3}$ By V1 Possibility label—please see Appendix C"
    },
    {
      "markdown": "ticles. We provide the model with this evidence through a simple adjustment of our prompt (detailed in Appendix R). Because the article is guaranteed to be relevant and informative with respect to veracity, we call this approach \"GPT-4 Web Oracle\". We also examine a version \"Web Answerless Oracle\" where we remove the final veracity verdict from the PolitiFact article.\n\nIn this experiment, we tested several versions of GPT-4. 0314 and 0613 are the fixed versions OpenAI makes available, while \"August2023\" refers to testing the unversioned GPT-4 in August 2023.\n\n| Method | Macro F1 |\n| --: | :--: |\n| GPT-4-August2023 Web Oracle | 98.6 |\n| GPT-4-August2023 Web Answerless Oracle | 90.7 |\n| GPT-4-0613 Web Answerless Oracle | 90.6 |\n| GPT-4-0314 Web Answerless Oracle | 78.7 |\n| GPT-4-0314 Score Zero-Shot | 60.5 |\n| GPT-4-0314 Score Binary | 68.8 |\n\nTable 9: Performance with oracle web retrieval. The improvement is massive. It works regardless of version, but 0314 is significantly worse than the others with this method.\n\nWe see in Table 9 a huge increase in performance. We note that the increase depends on the versioncuriously, while 0314 signficantly outperformed 0613 with the binary prompt, in this case the reverse is true.\n\nOf course, these conditions are not realistic, because we are assuming we can perfectly retrieve the PolitiFact article, which might not even exist. We note, however, that in spite of this limitation, these results are not true upper bounds, because this is a simple prompt using only a single piece of evidence. This highlights the potential of web retrieval combined with recent LLMs like GPT-4, and we suggest it is the clearest path to increasing raw performance.\n\n## 5 Conclusions\n\nIn conclusion, this study presented a multifaceted investigation to determine if we can use GPT-4, combined with careful analysis of generalization and uncertainty, to produce more practical misinformation detection. We found, first, that GPT-4 can give superior performance for information evaluation in multiple datasets and contexts. Second, we found that GPT-4 has different failure modes from previous approaches like RoBERTa. This could help produce better results for detecting misinformation and countering its spread. Third, we investigated uncertainty quantification in this context, showing that GPT-4 can provide information reflecting its uncertainty in evaluating veracity, and that that information can be leveraged towards significantly improved performance. Fourth, it is not easy to guarantee inputs have sufficient context for a valid judgment to be made. We provide a new dataset and analysis that starts to address this issue. However, we believe that such explorations remain important to consider in future work. Fifth and finally, we showed that both web retrieval and explainability offer promising avenues for future progress. Overall, we hope this work will provide a starting point towards both better performance in general, and creating systems that will address key real-world factors that have so far prevented machine learning from reliably mitigating misinformation.\n\n## 6 Limitations\n\nWe hope future work will resolve limitations of this study, including the following:\n\n- Our analysis focused on GPT-4, which is the strongest available LLM for many tasks. We reason that if it were incapable of producing good results here, it is unlikely that weaker LLMs of the same type could do any better. Now that we know that GPT-4 gives strong performance, it is an open question to what extent these results can be replicated with other systems. For closedsource models, the largest version of PaLM 2 (Anil et al., 2023) (Unicorn), along with Bard (for which Unicorn is the foundation model) would be promising models for future evaluation. Similarly, Claude 2 would also be promising, as another leading closed-source model. For open-source models, in more recent work by authors here and collaborators (Yu et al., 2023), we showed that similar prompting of Llama-2 struggles to give good performance. However, more recent open-source models such as Zephyr ${ }^{4}$ have generally shown superior performance to Llama2, and are therefore also well worth evaluation.\n- While our analysis shows GPT-4 improves on prior approaches, the overall results are still not as strong as one would like in many real-world applications, whether in terms of raw performance, generalization, uncertainty quantification, or many other considerations. More work\n\n[^0]\n[^0]:    ${ }^{4}$ https://huggingface.co/HuggingFaceH4/zephyr-7balpha"
    },
    {
      "markdown": "is needed to produce real-world ready systems. Besides directly building on the ideas explored here, two of the most promising parallel directions are building web search systems to address examples beyond the knowledge cutoff and in general cases that are not Impossible but where GPT-4 lacks sufficient information, and building explainability systems, to create more trustworthy predictions and more graceful failure. We reported on some initial experiments in both of these directions, but there is a great deal more ground to cover in future work.\n\n## 7 Ethical Considerations\n\nGiven the recent shifts in the spread of misinformation which has been amplified by global events such as the COVID-19 pandemic, the war in Ukraine, and the Capitol Hills riots, it is imperative that researchers improve their approach to detecting \"fake news\" online. The results of this study suggest a path forward to address this challenge by using the power of advanced language models like GPT-4. These systems can not only provide better performance-through careful attention to errors, uncertainty, and other real-world factors-they can also limit harms like confident mistakes and biased predictions. However, more work is needed, and we hope future research will build on this to deliver a positive real-world impact.\n\n## Acknowledgements\n\nThis work was partially funded by the CIFAR AI Chairs Program and by the Centre for the Study of Democratic Citizenship (CSDC). The first author is supported by funding from IVADO and by the Fonds de recherche du Québec. We thank Yury Orliovsky for helping improve the quality of LIARNew. We thank Berkeley SPAR for helping connect collaborators and start the project.\n\n## Author Contributions\n\nKellin Pelrine lead the project and worked on most areas, especially the foundational ideas, experiment design, and analysis. Anne Imouza and Camille Thibault did the majority of the work to create LIAR-New, including the translation and impossibility labels. Meilina Reksoprodjo implemented most of the non-GPT-4 language models in the main comparison and contributed to writing and literature review. Caleb Gupta implemented the fuzzy\nclustering methods and contributed to literature review. Joel Christoph helped design and test the GPT-4 prompting and contributed to writing and literature review. Jean-François Godbout and Reihaneh Rabbany advised the project, contributing ideas, feedback, and writing.\n\n## References\n\nMoloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana Rezazadegan, Li Liu, Mohammad Ghavamzadeh, Paul Fieguth, Xiaochun Cao, Abbas Khosravi, U Rajendra Acharya, et al. 2021. A review of uncertainty quantification in deep learning: Techniques, applications and challenges. Information Fusion, 76:243297.\n\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. 2023. Palm 2 technical report. arXiv preprint arXiv:2305.10403.\n\nLingjiao Chen, Matei Zaharia, and James Zou. 2023. How is chatgpt's behavior changing over time? arXiv preprint arXiv:2307.09009.\n\nMu-Yen Chen and Yi-Wei Lai. 2022. Using fuzzy clustering with deep learning models for detection of covid-19 disinformation. Transactions on Asian and Low-Resource Language Information Processing.\n\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Unsupervised cross-lingual representation learning at scale. arXiv preprint arXiv:1911.02116.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.\n\nXishuang Dong, Shouvon Sarker, and Lijun Qian. 2022. Integrating human-in-the-loop into swarm learning for decentralized fake news detection. In 2022 International Conference on Intelligent Data Science Technologies and Applications (IDSTA), pages 46-53. IEEE.\n\nYilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. 2023. Improving factuality and reasoning in language models through multiagent debate. arXiv preprint arXiv:2305.14325.\n\nLorenzo Jaime Yu Flores and Yiding Hao. 2022. An adversarial benchmark for fake news detection models. In The AAAI-22 Workshop on Adversarial Machine Learning and Beyond.\n\nRyan Greene, Ted Sanders, Lilian Weng, and Arvind Neelakantan. 2022. New and improved embedding model. https://openai.com/blog/"
    },
    {
      "markdown": "new-and-improved-embedding-model. Accessed: 2023-05-05.\n\nGisel Bastidas Guacho, Sara Abdali, Neil Shah, and Evangelos E Papalexakis. 2018. Semi-supervised content-based detection of misinformation via tensor embeddings. In 2018 IEEE/ACM international conference on advances in social networks analysis and mining (ASONAM), pages 322-325. IEEE.\n\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2020. Deberta: Decoding-enhanced bert with disentangled attention. arXiv preprint arXiv:2006.03654.\n\nNathaniel Hoy and Theodora Koulouri. 2022. Exploring the generalisability of fake news detection models. In 2022 IEEE International Conference on Big Data (Big Data), pages 5731-5740. IEEE.\n\nGuoyong Hu, Ye Ding, Shuhan Qi, Xuan Wang, and Qing Liao. 2019. Multi-depth graph convolutional networks for fake news detection. In Natural Language Processing and Chinese Computing: 8th CCF International Conference, NLPCC 2019, Dunhuang, China, October 9-14, 2019, Proceedings, Part I 8, pages 698-710. Springer.\n\nYen-Hao Huang, Ting-Wei Liu, Ssu-Rui Lee, Fernando Henrique Calderon Alvarado, and Yi-Shin Chen. 2020. Conquering cross-source failure for news credibility: Learning generalizable representations beyond content embedding. In Proceedings of The Web Conference 2020, pages 774-784.\n\nForrest N Iandola, Albert E Shaw, Ravi Krishna, and Kurt W Keutzer. 2020. Squeezebert: What can computer vision teach nlp about efficient neural networks? arXiv preprint arXiv:2006.11316.\n\nXiaoqian Jiang, Melanie Osl, Jihoon Kim, and Lucila Ohno-Machado. 2011. Smooth isotonic regression: a new method to calibrate predictive models. AMIA Summits on Translational Science Proceedings, 2011:16.\n\nZi-Hang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Jiashi Feng, and Shuicheng Yan. 2020. Convbert: Improving bert with span-based dynamic convolution. Advances in Neural Information Processing Systems, 33:12837-12848.\n\nBoutheina Jlifi, Chayma Sakrani, and Claude Duvallet. 2022. Towards a soft three-level voting model (soft $\\mathrm{t}-\\mathrm{lvm}$ ) for fake news detection. Journal of Intelligent Information Systems, pages 1-21.\n\nRohit Kumar Kaliyar, Anurag Goswami, and Pratik Narang. 2021. Fakebert: Fake news detection in social media with a bert-based deep learning approach. Multimedia tools and applications, 80(8):1176511788.\n\nJunaed Younus Khan, Md Tawkat Islam Khondaker, Sadia Afroz, Gias Uddin, and Anindya Iqbal. 2021. A benchmark study of machine learning models for\nonline fake news detection. Machine Learning with Applications, 4:100032.\n\nJuliane Köhler, Gautam Kishore Shahi, Julia Maria Struß, Michael Wiegand, Melanie Siegel, Thomas Mandl, and Mina Schütz. 2022. Overview of the clef2022 checkthat! lab task 3 on fake news detection. Working Notes of CLEF.\n\nPJS Kumar, Polagani Rama Devi, N Raghavendra Sai, S Sai Kumar, and Tharini Benarji. 2021. Battling fake news: A survey on mitigation techniques and identification. In 2021 5th international conference on trends in electronics and informatics (ICOEI), pages 829-835. IEEE.\n\nNayeon Lee, Belinda Z Li, Sinong Wang, Pascale Fung, Hao Ma, Wen-tau Yih, and Madian Khabsa. 2021. On unifying misinformation detection. arXiv preprint arXiv:2104.05243.\n\nGeorgiy Levchuk and Charlotte Shabarekh. 2017. Using soft-hard fusion for misinformation detection and pattern of life analysis in osint. In Next-Generation Analyst V, volume 10207, pages 28-46. SPIE.\n\nTian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shuming Shi. 2023. Encouraging divergent thinking in large language models through multi-agent debate. arXiv preprint arXiv:2305.19118.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.\n\nYunfei Long, Qin Lu, Rong Xiang, Minglei Li, and Chu-Ren Huang. 2017. Fake news detection through multi-perspective speaker profiles. In Proceedings of the eighth international joint conference on natural language processing (volume 2: Short papers), pages 252-256.\n\nSahil Loomba, Alexandre de Figueiredo, Simon J Piatek, Kristen de Graaf, and Heidi J Larson. 2021. Measuring the impact of covid-19 vaccine misinformation on vaccination intent in the uk and usa. Nature human behaviour, 5(3):337-348.\n\nMan Luo, Xin Xu, Zhuyun Dai, Panupong Pasupat, Mehran Kazemi, Chitta Baral, Vaiva Imbrasaite, and Vincent Y Zhao. 2023. Dr.icl: Demonstrationretrieved in-context learning.\n\nPhilip Marcelo. 2023. Fact focus: Fake image of pentagon explosion briefly sends jitters through stock market. Associated Press.\n\nChristos Matsoukas, Albert Bou Hernandez, Yue Liu, Karin Dembrower, Gisele Miranda, Emir Konuk, Johan Fredin Haslum, Athanasios Zouzos, Peter Lindholm, Fredrik Strand, et al. 2020. Adding seemingly uninformative labels helps in low data regimes. In International Conference on Machine Learning, pages 6775-6784. PMLR."
    },
    {
      "markdown": "Priyanka Meel and Dinesh Kumar Vishwakarma. 2020. Fake news, rumor, information pollution in social media and web: A contemporary survey of state-of-thearts, challenges and opportunities. Expert Systems with Applications, 153:112986.\n\nJooyoung Moon, Jihyo Kim, Younghak Shin, and Sangheum Hwang. 2020. Confidence-aware learning for deep neural networks. In international conference on machine learning, pages 7034-7044. PMLR.\n\nBo Ni, Zhichun Guo, Jianing Li, and Meng Jiang. 2020. Improving generalizability of fake news detection methods using propensity score matching. arXiv preprint arXiv:2002.00838.\n\nOpenAI. 2023. Gpt-4.\nHelene Orsini, Hongyan Bao, Yujun Zhou, Xiangrui Xu, Yufei Han, Longyang Yi, Wei Wang, Xin Gao, and Xiangliang Zhang. 2022. Advcat: Domain-agnostic robustness assessment for cybersecurity-critical applications with categorical inputs. In IEEE International Conference on Big Data 2022.\n\nSubhadarshi Panda and Sarah Ita Levitan. 2022. Improving cross-domain, cross-lingual and multi-modal deception detection. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 383390.\n\nKellin Pelrine, Jacob Danovitch, and Reihaneh Rabbany. 2021. The surprising performance of simple baselines for misinformation detection. In Proceedings of the Web Conference 2021, pages 3432-3441.\n\nReid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng. 2023. Automatic prompt optimization with\" gradient descent\" and beam search. arXiv preprint arXiv:2305.03495.\n\nYunke Qu, Kevin Roitero, David La Barbera, Damiano Spina, Stefano Mizzaro, and Gianluca Demartini. 2022. Combining human and machine confidence in truthfulness assessment. ACM Journal of Data and Information Quality, 15(1):1-17.\n\nChahat Raj and Priyanka Meel. 2021. Is dynamic rumor detection on social media viable? an unsupervised perspective. arXiv preprint arXiv:2111.11982.\n\nWajiha Shahid, Bahman Jamshidi, Saqib Hakak, Haruna Isah, Wazir Zada Khan, Muhammad Khurram Khan, and Kim-Kwang Raymond Choo. 2022. Detecting and mitigating the dissemination of fake news: Challenges and future research opportunities. IEEE Transactions on Computational Social Systems.\n\nKarishma Sharma, Feng Qian, He Jiang, Natali Ruchansky, Ming Zhang, and Yan Liu. 2019. Combating fake news: A survey on identification and mitigation techniques. ACM Transactions on Intelligent Systems and Technology (TIST), 10(3):1-42.\n\nKai Shu, Amrita Bhattacharjee, Faisal Alatawi, Tahora H Nazer, Kaize Ding, Mansooreh Karami, and Huan Liu. 2020. Combating disinformation in a social media age. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 10(6):e1385.\n\nKai Shu, Amy Sliva, Suhang Wang, Jiliang Tang, and Huan Liu. 2017. Fake news detection on social media: A data mining perspective. ACM SIGKDD explorations newsletter, 19(1):22-36.\n\nAbhijit Suprem, Sanjyot Vaidya, and Calton Pu. 2022. Exploring generalizability of fine-tuned models for fake news detection. In 2022 IEEE 8th International Conference on Collaboration and Internet Computing (CIC), pages 82-88. IEEE.\n\nBilel Taboubi, Mohamed Aziz Ben Nessir, and Hatem Haddad. 2022. icompass at checkthat! 2022: combining deep language models for fake news detection. Working Notes of CLEF.\n\nJacob-Junqi Tian, Omkar Dige, David Emerson, and Faiza Khan Khattak. 2023. Interpretable stereotype identification through reasoning. arXiv preprint arXiv:2308.00071.\n\nHoai Nam Tran and Udo Kruschwitz. 2022. ur-iw-hnt at checkthat! 2022: cross-lingual text summarization for fake news detection. Working Notes of CLEF.\n\nCiprian-Octavian Truică and Elena-Simona Apostol. 2023. It's all in the embedding! fake news detection using document embeddings. Mathematics, 11(3):508.\n\nWilliam Yang Wang. 2017. \"liar, liar pants on fire\": A new benchmark dataset for fake news detection. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 422-426, Vancouver, Canada. Association for Computational Linguistics.\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837.\n\nJiaying Wu and Bryan Hooi. 2022. Probing spurious correlations in popular event-based rumor detection benchmarks. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 274-290. Springer.\n\nIkuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, and Yuji Matsumoto. 2020. Luke: Deep contextualized entity representations with entity-aware self-attention. arXiv preprint arXiv:2010.01057.\n\nHao Yu, Zachary Yang, Kellin Pelrine, Jean Francois Godbout, and Reihaneh Rabbany. 2023. Open, closed, or small language models for text classification? arXiv preprint arXiv:2308.10092."
    },
    {
      "markdown": "Jiawei Zhou, Yixuan Zhang, Qianni Luo, Andrea G Parker, and Munmun De Choudhury. 2023. Synthetic lies: Understanding ai-generated misinformation and evaluating algorithmic and human solutions. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, pages 1-20.\n\n## A Related Work\n\nMisinformation and the spread of false information have become increasingly prevalent in today's digital age (Shu et al., 2017; Sharma et al., 2019; Shu et al., 2020; Kumar et al., 2021; Shahid et al., 2022). A wide range of solutions have been proposed to tackle this problem, from fact-checking and expertbased assessments to automated machine learning approaches (Wang, 2017; Meel and Vishwakarma, 2020). Broadly, the literature can be divided into content-based and network-based approaches (Shu et al., 2017). Content-based approaches focus on analyzing the text, images, or multimedia elements of a message to determine its veracity. Some examples of this approach include (Wang, 2017), which uses a hybrid CNN and LSTM model, and (Kaliyar et al., 2021) which employs BERT. Network-based approaches, on the other hand, examine the propagation patterns and user interactions surrounding the information (Hu et al., 2019; Long et al., 2017). Our approach is part of the content-based category.\n\nA key challenge for practical misinformation detection is the generalizability of models across various types of datasets and domains. Several recent studies have addressed these issues from different angles. Suprem et al. (2022) investigated the generalizability of pretrained and fine-tuned fake news detectors, introducing the KMeans-Proxy method for identifying overlapping subsets of unseen data. Ni et al. (2020) employed Propensity Score Matching to select generalizable features for fake news detection, while Lee et al. (2021) proposed UNIFIEDM2, a unified model for multiple misinformation domains. Furthermore, Hoy and Koulouri (2022) explored the generalizability of popular fake news detection models and features across similar news. These studies highlight the importance of addressing generalizability challenges for effective misinformation detection. Our work here is novel in focusing on the generalizability of GPT-4 in this domain, particularly in relation to the data and other LMs.\n\nOur approach also emphasizes uncertainty assessment, as opposed to a more rigid pursuit of overall performance metrics. The misinformation-\nrelated literature on uncertainty is very limited, lacking thorough comparative evaluations for approaches like soft classification (i.e., predicting probabilities instead of class labels) and investigations of the latest approaches, such as GPT-4 (Guacho et al., 2018; Jlifi et al., 2022; Levchuk and Shabarekh, 2017; Qu et al., 2022). Our work addresses some of the many gaps in this literature by proposing approaches to productively assess uncertainty with GPT-4, and benchmarking methods for soft classification.\n\n## B LIAR and CT-FAN-22 Dataset Details\n\n## B. 1 LIAR\n\nThis dataset was collected by Wang (2017) from the PolitiFact.com API and includes 12.8 K humanlabeled short political statements. For the degree of truthfulness, there are six labels: Pants-on-Fire, False, Barely-true, Half-true, Mostly-true, and True. The labels are fairly balanced, except for the Pants-on-Fire label, which has roughly half as many examples as the others. The dataset provides an approximately 80-10-10 train-val-test split. Table 10 shows the basic statistics. ${ }^{5}$\n\nIn addition to statements, each example includes metadata: the statement topic, and the speaker's name, job, state, party, and total \"credit history\". The latter refers to the counts of statements of each label by the speaker in the training set, excluding \"True\". Prior research (Hu et al., 2019; Long et al., 2017) has found that most of the metadata did not give significant performance benefits, but the credit history improved performance significantly. However, because statements for fact-checking by PolitiFact are not sampled randomly, using this metadata may cause models to learn sampling bias that can aid classification in this dataset, but will not generalize well to the real world. For instance, a speaker might have controversial false statements which are fact-checked by PolitiFact, leading to a negative credit history, when they make many other uncontroversial true statements that are not factchecked by PolitiFact and consequently not part of the dataset. This contrasts with content-based classification: both may fail to classify unseen topics or speakers, but speaker-based classification\n\n[^0]\n[^0]:    ${ }^{5}$ Due to a dataloading error which concatenated several examples together with examples of ID 40 and 1653, we have 1267 examples in the test set instead of 1283 of the original LIAR dataset. This is a tiny fraction of the dataset and consistent between our experiments, so our conclusions are unaffected."
    },
    {
      "markdown": "|  | Pants-fire | False | Barely-true | Half-true | Mostly-true | True | Total |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| Train | 839 | 1995 | 1654 | 2114 | 1962 | 1676 | 10240 |\n| Val | 116 | 263 | 237 | 248 | 251 | 169 | 1284 |\n| Test | 92 | 249 | 212 | 265 | 241 | 208 | 1267 |\n\nTable 10: LIAR dataset statistics.\n(particularly when based on a biased sample) may fail even for speakers seen in the training data. In addition, the credit history may be impossible to use for classifying statements from non-famous individuals (such as newly-created social media bot accounts) and introduce ethical issues. For these reasons, we exclude metadata and focus exclusively on the statements themselves. This is also a common experimental setup in the literature (Shu et al., 2017; Sharma et al., 2019; Shu et al., 2020; Kumar et al., 2021; Shahid et al., 2022).\n\nBoth 6-way classification with the full set of labels, and binary classification with the labels collapsed into two, are common approaches to this dataset in the literature. We focus on the binary case for more lucid error and generalization analyses, but also report 6-way classification results. For the binary experiments, we dichotomized the categories by splitting the labels in the middlei.e., mapping Pants-on-Fire, False, and Barely-true to \"False\" and Half-true, Mostly-true, and True to \"True\". While not the only option, this is the most typical binarization used in the literature (Qu et al., 2022; Orsini et al., 2022; Khan et al., 2021; Flores and Hao, 2022).\n\nNote that it is also common for studies to code the LIAR dataset in a slightly different way than the original splits-e.g., testing on both the validation and test sets together. Although these may not be perfectly comparable, we include this classification in consideration for the state-of-the-art method to compare with our results, providing a more robust benchmarking for our own approach. On the other hand, in addition to studies using the metadata as discussed above, we exclude from comparison some studies like Pryzant et al. (2023), which evaluate their approach on a very different subset of the LIAR dataset, as well as ones which use a different binarization, such as Dong et al. (2022); Panda and Levitan (2022). To our knowledge, the resulting state-of-the-art approaches are Orsini et al. (2022) and Khan et al. (2021) tied at $62 \\%$ accuracy for binary classification, and Flores and Hao (2022) with $29.4 \\%$ for 6-way classification.\n\n## B. 2 CT-FAN-22\n\nKöhler et al. (2022) derived this dataset from 20 different fact-checking websites. The labels were converted from each website's own system to four: False, Partially False, True, and Other. There is both a large English corpus, and a test-set-only German corpus (collected separately, not translated).\n\n|  |  | False | Partially False | True | Other | Total |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| English | Train | 465 | 217 | 142 | 76 | 900 |\n|  | Val | 113 | 141 | 69 | 41 | 364 |\n|  | Test | 315 | 97 | 243 | 55 | 612 |\n| German | Test | 191 | 97 | 243 | 55 | 586 |\n\nTable 11: CT-FAN-22 dataset statistics.\nThe \"Other\" label could be problematic because it cannot guarantee something for which veracity is unverifiable; it only reflects that an entry has not been verified in this dataset. Furthermore, it is not a category which can be synchronized with other datasets, impeding transfer analyses. In one of our experiments, we maintain this category for comparative purposes only, and directly mark every prediction by our models on examples with this ground truth label as wrong. This leads to a maximally stringent evaluation favoring previous approaches. In another set of experiments, we remove this category and evaluate on the remaining examples. While we believe going beyond simplistic labeling and metrics is critical to capturing more real-world information and producing more generalizable systems, we suggest that this \"Other\" label is not well-defined and should be excluded from future datasets, either in testing or directly in the dataset construction.\n\nThe state-of-the-art approaches are reported by Köhler et al. (2022) from a competition run on this dataset. For English, the strongest performance is by Taboubi et al. (2022) with $54.7 \\%$ accuracy and $33.9 \\%$ macro F1. For the more challenging German corpus, the strongest approach was found by Tran and Kruschwitz (2022), with $42.7 \\%$ accuracy and $29.0 \\%$ macro F1. We note that these approaches used both the article title and text that the dataset provides, while for our experiments, we only used the article text because of limited time and resources."
    },
    {
      "markdown": "## C Possible vs. Impossible Example Labeling\n\nWe created two versions of LIAR-New Possibility labels. We start by describing the first version and its limitations that led us to revisit the labeling. We then discuss the second, current version. The second version is now used in all experiments unless otherwise noted.\n\n## C. 1 V1 Possibility labels\n\nWe began with the following definition:\nDefinition 2. A statement is considered \"Impossible\" with respect to veracity judgment if it meets one of the following criteria: 1. it does not contain any statement of verifiable fact (e.g., the phrase \"On taxes\" in isolation has no claim that could be evaluated) or 2. the fact that the statement references cannot be verified without unavailable context (e.g., \"they voted for it\" requires knowing who \"they\" are and what \"it\" refers to). Conversely, a statement is considered \"Possible\" if it can be confirmed or disproven with external knowledge, but does not require additional unavailable context (e.g., \"the sun orbits the earth\" may require astronomy or physics knowledge to disprove, but does not require additional context).\n\nIn this definition, there are only two categories, Possible and Impossible. Two authors (Anne Imouza and Camille Thibault) each annotated half of LIAR-New according to this definition. This yielded 892 Possible and 1063 Impossible examples.\n\nEach annotating author then annotated 100 examples from the other's set, and we evaluated disagreement. We found that disagreement on this combined set of 200 examples was significant, with Cohen Kappa 0.312 and 72/200 cases disagreeing.\n\nThe annotating authors discussed each of these 200 disagreement cases and resolved the disagreement. We did not observe a clear pattern such as all disagreements being resolved to one label or the other; it varied case by case. Consequently, although this is a challenging and potentially subjective labeling task, to improve quality as much as possible we decided it would be beneficial to revise all the labels. Based on the annotating authors' observations and group discussion, we decided to add a third category, leading to Version 2 discussed below.\n\nWe note however that the experimental conclusions did not change substantially between the two\nversions, suggesting the original version was unbiased, if noisy.\n\n## C. 2 V2 Possibility labels\n\nThe following definitions are the full versions of the ones we revised to, including examples (which were agreed upon in advance of the labeling) omitted in the main text:\nDefinition 3. Possible: the statement's claim is clear without any additional context (e.g., a statement \"the earth is flat\"), or any missing context does not make it difficult to evaluate the claim (\"at the press conference, Senator $X$ said the earth is flat\" - we may not know which press conference this refers to, but we can give a strong veracity evaluation just examining if Senator $X$ has said the earth is flat at all, in a particular context, or frequently).\n\nHard: the claim is missing important context that makes it hard to evaluate, but it might still be possible. For example, \"Senator $X$ is against the $\\$ 15$ million cut.\" We might be able to trace the amount and the senator's name and figure out what this refers, and give a practical evaluation likely beginning with \"Assuming the cut here refers to...\" But it's challenging and might be impossible in some cases, for example if it's not possible to figure out which cut this refers to.\n\nImpossible: there is missing context that cannot be resolved (\"The senator said the earth is flat\" without a name, this could be any senator throughout history, so it's not possible to give a valid or productive veracity evaluation). Or, the statement contains no claim for which veracity could be evaluated regardless how much context is available (for example, a statement \"on the shape of the earth\" contains no claim).\n\nThe 3-way labels here provide more information and can make it easier to handle edge cases than the rigid 2-way labels. We also recruited a third person (Yury Orliovsky) to provide a third set of labels to further reduce noise.\n\nIn this round, each annotator annotated every example. The resultant counts of labels are shown in Table 12.\n\nWe see that the dominant cases are either agreement or a relatively mild form of disagreement between Possible vs. Hard or Impossible vs. Hard. We consider this type of disagreement to be within acceptable bounds, since by definition the Hard class might in fact be either Possible or Impossible. Therefore, for the final dataset, we resolved these"
    },
    {
      "markdown": "| Labels | Count |\n| :-- | :--: |\n| ppp | 270 |\n| hhp | 157 |\n| hii | 129 |\n| hpp | 69 |\n| php | 62 |\n| ppi | 59 |\n| iii | 52 |\n| hhh | 41 |\n| hhi | 39 |\n| pph | 34 |\n| pii | 31 |\n| phh | 25 |\n| phi | 22 |\n| hpi | 19 |\n| hih | 16 |\n| hip | 16 |\n| ihp | 15 |\n| iip | 14 |\n| pip | 14 |\n| ipp | 11 |\n| hph | 10 |\n| iih | 9 |\n| ihi | 7 |\n| ihh | 7 |\n| pih | 4 |\n| ipi | 3 |\n| iph | 2 |\n\nTable 12: Counts of each label combination, with order preserved from the annotators. \"ppp\", for example, indicates all three annotators marked the example \"Possible\".\ndisagreements by taking the majority vote.\nNonetheless, there are also 342 cases where at least one annotator marked Possible and another marked Impossible. In these cases, the labels are clearly conflicting. To resolve these disagreements, we split them into two categories: 127 \"maximal\" disagreement cases where each label is represented, and 215 \"nonmaximal\" where two annotators marked one label and one marked the other.\n\nFor the maximal disagreements, the annotators discussed each example live and resolved the disagreement to produce a final label. For the nonmaximal disagreements, the minority vote author first went through each case individually to assess if they had mislabeled it on their end. Finally, for cases where the minority vote author in their individual assessment still disagreed with the majority, all the annotators discussed live to produce a final label.\n\nThis yielded 927 Possible, 581 Hard, and 449 Impossible examples. We note that in the experiments here we only examine these three categories, however, potentially the raw labels from the individual annotators could be used to form a finer-grained scale. For example, a case all three annotators labeled Hard might be harder than a case where two labeled Hard and one labeled Possible. However, caution would be needed here as it could result in noise.\n\nWe make both the raw and final labels available for future work through our GitHub.\n\n## C. 3 LIAR Impossible Examples\n\nWe present in Table 13 the LIAR (old) Impossible examples detected by GPT-4 and manually confirmed according to V1 definition, as discussed in Section 3.1.\n\n## D In-context learning with automatically chosen demonstration\n\n## D. 1 Prompts\n\nWe tested several prompts in which GPT-4 is provided an example chosen automatically from the training set. These were motivated by the observation that GPT-4 and RoBERTa make different errors in a way that is tied to embedding similarity with training set examples (Section 4.5). Consequently, by providing the most similar training set example, we hoped to provide both a demonstration and potentially a direct source of information relevant to the target example. Related procedures to"
    },
    {
      "markdown": "| LIAR-ID | text |\n| :--: | :--: |\n| 8047. jann | On residency requirements for public workers |\n| 13231.jann | I was gone when there was a red line against Syria. |\n| 816.jann | On which team has rooting for in the World Series. |\n| 6849.jann | Pre-existing conditions are covered under my (health care) plan. |\n| 266.jann | \"You said you would vote against the Patriot Act, then you came to the Senate, you voted for it.\" |\n| 2805.jann | On hunting carnafes. |\n| 10147.jann | On an income cap for recipients of the popular HOPE scholarship |\n| 2277.jann | On running a civil and polite campaign. |\n| 603.jann | On the Bush tax cuts. |\n| 948.jann | The president campaigned against this type of legislation. |\n| 2547.jann | I said no to these big bank bailouts. |\n| 9942.jann | \"Last weeks three most-viewed television programs were Sunday Night Football, Thursday Night Football and Monday Night Football.\" |\n| 3575.jann | On federal stimulus money for expanding rail service. |\n| 729.jann | On whether the government should bail out insurance giant ARJ. |\n| 5824.jann | We didn't go out asking people to join the stand your ground task force. |\n| 4537.jann | My home state since June of 2009 created 40 percent of the new jobs in America. |\n| 3546.jann | Were introduced a bill that includes $\\$ 12$ billion in cuts over the next week. |\n| 2983.jann | Four out of 10 homicides are committed by gun in this city. |\n| 771.jann | You had supported John McCain's military strategies pretty adamantly until this race. |\n| 1999.jann | I have spent virtually every weekend since Memorial Day in the Pashandle. |\n| 4425.jann | \"Since the beginning of the year were created a net increase of 45,000 jobs.\" |\n| 4131.jann | \"The State Election Board has issued nearly $\\$ 275,000$ in fines to violators of absentee ballot laws.\" |\n| 3668.jann | On militaricting. |\n| 126.jann | \"First, he was in favor of my plan, now he's attacking it.\" |\n| 12026.jann | On ending stop and frisk |\n| 1810.jann | On the National Animal Identification System. |\n| 2568.jann | On implementing a sales tax. |\n| 5759.jann | \"A proposed regional transportation tax will last a minimum of 10 years, and has been approved to last longer.\" |\n| 5910.jann | \"Says I am happy to decline PERS so that the County can save over $\\$ 68,000$ over a four year term that would have been paid on my behalf.\" |\n| 13208.jann | \"In the state legislature, I supported Second Amendment rights.\" |\n| 10917.jann | On Common Cost. |\n| 582.jann | \"For at least a year now, I have called for two additional brigades, perhaps three.\" |\n| 1691.jann | The permission to engage was given before the word RPO was ever used. |\n| 3789.jann | The new light bulbs will cost roughly six times the cost of the light bulbs we now use. |\n| 11925.jann | \"We are almost halfway to our goal of 1,000 rages prosecuted!\" |\n| 12834.jann | On whether he's had a relationship with Vladimir Putin. |\n| 2105.jann | On debating. |\n| 10543.jann | On supporting rights to work legislation in 2015 |\n| 7742.jann | Metro on whether it could use bond money for restoration. |\n| 5930.jann | On the Troubled Asset Relief Program (TARP) |\n| 13350.jann | We have never done business with Donald Trump. |\n| 8853.jann | I never gave up custody of my children. I never lost custody of my children. |\n| 6263.jann | \"There are 60,000 fewer jobs today in this state than we had in 2008.\" |\n| 2774.jann | On toll trade. |\n| 5979.jann | On subsidies for ethanol production. |\n| 7236.jann | \"In the four years before I became governor, we increased state debt $\\$ 5.2$ billion. Were paid it down $\\$ 2$ billion.\" |\n| 4899.jann | I never favored cap and trade. |\n| 1497.jann | \"In this judicial race, special interest groups have demanded money from me, in exchange for endorsement and support.\" |\n| 4212.jann | \"Believe it or not, consumer spending is up over the last eight months.\" |\n| 12051.jann | I opposed TPP (Trans-Pacific Partnership) and have always opposed TPP. |\n| 2910.jann | Measures taken by my administration have saved taxpayers $\\$ 1$ billion. |\n| 12009.jann | On Donald Trumps track record in business. |\n| 3381.jann | \"On my first day in office, I ordered a review of every regulation in the pipeline and every contract exceeding $\\$ 1$ million.\" |\n| 9364.jann | I turned a $\\$ 110$ million deficit into a $\\$ 1.6$ million surplus for our city. |\n| 1158.jann | \"Under the plan, for the first five years your employer not only has to keep the coverage, but you can't migrate to the public plan.\" |\n| 700.jann | \"My grandfather came home from... (World War II) exhausted from the burden he had borne, and died the next day.\" |\n| 2738.jann | \"State House incumbent Jili Chambers, R-Atlanta, personally profits from taxpayer money.\" |\n| 447.jann | I have never said that I don't wear flag pins or refuse to wear flag pins. |\n| 10717.jann | On in-state tuition for unauthorized immigrants. |\n| 10773.jann | Im running for office with much more experience and qualifications than Barack Obama had when he ran. |\n| 8764.jann | I cut spending while serving in the state Legislature |\n| 1093.jann | On a cap-and-trade plan. |\n| 1909.jann | I've issued a six-month moratorium on deepwater drilling. |\n| 1652.jann | On recess appointments. |\n| 9241.jann | I am the only senator who turned down the state pension plan for part-time legislators. |\n| 609.jann | On offshore drilling. |\n| 7334.jann | Says his budget provides the highest state funding level in history for education. |\n| 9708.jann | I've been here almost every day. |\n| 6743.jann | Says the governor is going around the state talking about [how] we should fund an income tax cut that benefits higher income earners and not lower income earners |\n\nTable 13: Impossible examples in LIAR."
    },
    {
      "markdown": "retrieve semantically similar examples have shown performance benefits in other contexts (Luo et al., 2023).\n\nIn the below prompts, CLOSEST_TRAIN_LABEL refers to the label of the closest training example according to Ada-002 embedding distance, uniformly mapped onto the 0-100 range. CLOSEST_TRAIN_TEXT refers to the corresponding training example input statement.\n\nVersion 1:\n\"role\": \"user\", \"content\": 'Rate the truthfulness of the following statement: \"STATEMENT\"\nHere is the rating of a similar statement. The rating is accurate, but the statement may or may not be relevant to this one. Similar statement: \"CLOSEST_TRAIN_TEXT\" Rating: CLOSEST_TRAIN_LABEL\n\nProvide a score for the original statement from 0 to 100 , where 0 represents definitively false and 100 represents definitively true. Do not provide any explanations, only respond with the numerical score.'\n\nVersion 2:\n\"role\": \"user\", \"content\": 'Here is a similar statement that is CLOSEST_TRAIN_LABEL: \"CLOSEST_TRAIN_TEXT\" Now rate the truthfulness of the following statement: \"STATEMENT\" Provide a score from 0 to 100, where 0 represents definitively false and 100 represents definitively true. Do not provide any explanations, only respond with the numerical score. '\n\nFinally, Version 3 is a meta-prompt where we set a condition to select between Version 2 and the original (no demonstration) Score Zero-Shot prompt. Given the example we want to classify, as in previous versions, we find the most similar training set example. Then if this train-test example pair is among the top $10 \\%$ most similar train-test example pairs (i.e., the similarity between the target, test set example and it's most similar training set example is in the top $10 \\%$ similarity of all such pairs), we use the Version 2 prompt. Otherwise, we use the Score Zero-Shot prompt. The motivating intuition here is that if we have a very similar training set example, then we try to take advantage of that. In contrast, if none of the training set examples are very similar, then we hypothesize they may not be relevant and therefore including them might confuse the model, so we revert to the Zero-Shot prompt.\n\n## D. 2 Results\n\nWe report results in Table 14. We see that version 1 hurts performance. Version 2 improves performance with GPT-4-0314, but not with 0613. Conversely Version 3 improves performance with 0613 (marginally) but not 0314.\n\n| Method | Accuracy |\n| :--: | :--: |\n| GPT-4-0314 Version 1 | 63.5 |\n| GPT-4-0613 Version 1 | 57.2 |\n| GPT-4-0314 Version 2 | 66.6 |\n| GPT-4-0613 Version 2 | 64.2 |\n| GPT-4-0314 Version 3 | 64.7 |\n| GPT-4-0613 Version 3 | 65.2 |\n| GPT-4-0314 Score Zero-Shot | 64.9 |\n\nTable 14: Results of 3 versions of in-context learning with an automatically chosen demonstration. We see improvements in some cases, but the results are mixed overall.\n\nOverall, the results seem mixed. There might be some benefit here, but it does not seem very large and can depend significantly on version. We note that this is only using a single exampleperhaps more are necessary to minimize the impact of noise. This might merit further investigation, but we believe other ways to provide the model additional information, especially web retrieval (see Section 4.9), are much more promising.\n\n## E Small Language Models\n\nWe fine-tuned a selection of pre-trained SLMs from HuggingFace. Due to limited computational resources, we restricted our analysis to models that were runnable on one Tesla T4 GPU. We examined BERT (base) (Devlin et al., 2018), ConvBERT (Jiang et al., 2020), DeBERTa and DeBERTa-v3 (base) (He et al., 2020), LUKE (Yamada et al., 2020), RoBERTa (base) (Liu et al., 2019), SqueezeBERT (Iandola et al., 2020) and XLM-RoBERTa (Conneau et al., 2019).\n\nA small hyperparameter search was performed on the given models, however, we rapidly found that this was not significantly increasing performance and began showing signs of overfitting. Hence, for this purpose, all the reported values in Table 2 as well as Table 21 are based on default configurations for pre-training BERT-like models, as stated in the original BERT paper. All the models were finetuned for 2 epochs, using the base"
    },
    {
      "markdown": "embeddings with the learning rate 5e-05 using the AdamW optimizer.\n\n## F Fuzzy Clustering\n\nK-Means Fuzzy Clustering has shown promising results for misinformation detection in some recent studies (Chen and Lai, 2022; Raj and Meel, 2021). This method initializes centroids in a pseudo-random fashion, then each datapoint is assigned a probabilistic membership to each cluster proportional to the distance between the data point and centroid. Following this, the centroids themselves are moved relative to the averages of their data points. Memberships and centroids are updated repeatedly until convergence. The final membership values can be converted to a class prediction with an argmax or an additional classifier.\n\nWe tested fuzzy clustering using three types of word embeddings with various levels of global contextualization: Word2Vec, GloVE, and BERT. The models were trained using adaptations to the SciKitFuzzy cluster cmeans library.\n\nUnfortunately, in our experiments (Tables 2 and 21), this approach did not give good results, so we did not investigate it further.\n\n## G GPT-3.5\n\n## G. 1 Base Performance\n\nWe prompt GPT-3.5 in the same way as GPT-4, and evaluate its performance in Table 15.\n\n| Dataset | Method | Accuracy |\n| :--: | :--: | :--: |\n| LIAR | GPT-3.5-0301 Score Zero-Shot | 67.3 |\n| LIAR | GPT-3.5-0613 Binary | 53.6 |\n| LIAR-New | GPT-3.5-0301 Score Zero-Shot | 61.1 |\n| LIAR-New | GPT-3.5-0613 Binary | 55.7 |\n| LIAR-New | GPT-3.5-0301 Binary Uncertainty-Enabled | 67.7 |\n\nTable 15: Results with GPT-3.5.\n\nWe note that GPT 3.5 performs surprisingly well on LIAR, better than GPT-4 in the same Score Zero-Shot setup. However, there are some caveats. First, its score is already maximized at the zeroshot binarization threshold of 50 , meaning it cannot achieve higher performance through tuning on the validation set like GPT-4 can. The margins are small, however, so this may not have a big impact. More significantly, the performance seems brittle and does not extend to other prompts or datasets.\n\n## G. 2 Fine-Tuning\n\nWe conduct a preliminary experiment on finetuning using the following prompt:\n\"role\": \"user\", \"content\": 'Rate the truthfulness of the following statement: \"STATEMENT\" Provide a score from 0 to 100 , where 0 represents definitively false and 100 represents definitively true.'\n\nThis is the same as the usual Score prompt, except we remove the last sentence \"Do not provide any explanations, only respond with the numerical score.\" We hypothesized that the finetuning should already be sufficient to get it to respond with only the numerical score-and in practice it does, $100 \\%$ of the time.\n\nFor the output side of the fine-tuning, we map the original 6-way LIAR labels uniformly to 0-100 scores, i.e., $[0,20,40,60,80,100]$. Then we fine-tune with default settings, which resulted in 2 epochs of fine-tuning.\n\nOn the test set, this approach achieved 57.2\\% accuracy. Considering the corresponding untuned approach (Score Zero-Shot) achieved $67.3 \\%$ accuracy, the fine-tuning was not successful. We hypothesize that finetuning to Impossible examples might be particularly damaging. In addition, the Score prompting may not be well suited for this since the uniform mapping may not reflect the true variation in the labels. So there is considerable room for more experimentation here that might yield a better result. We recommend this, along with testing GPT-4 fine-tuning that OpenAI has said it plans to release in the future, as areas for future work.\n\n## H Evaluation Metrics\n\nTo measure the performance of our misinformation detection approaches in hard classification settings we mainly report accuracy and weighted F1 score. For LIAR-New, where the classes are very imbalanced, we report accuracy and macro F1 score, which gives a more stringent assessment. Meanwhile, for soft classification, we provide reliability diagrams and measure Expected Calibration Error (ECE), which are standard ways to measure performance in this context (Abdar et al., 2021). Both were applied using 10 bins with quantile scaling, i.e., the bins are scaled to have the same number of examples in each."
    },
    {
      "markdown": "## I GPT-4 Cost Evaluation\n\nWe report in Table 16 approximate API costs of some of the main experiments run. These numbers can vary between runs of the model (due to randomness in the completion tokens) but should provide reasonable estimates. This is based on current pricing for GPT-4 (8K context window) as of October 2023, i.e., USD 0.03/1K input tokens and $0.06 / 1 \\mathrm{~K}$ output tokens.\n\nWe also note that the finetuning of GPT-3.5 on the LIAR train set comprised 609k tokens costing 4.9 USD.\n\nOverall, we see that there is a very large range depending on different data and prompts. We note that if an explanation is not needed, prompts that avoid it can be quite effective in reducing cost in cases where the input is not long (e.g., LIAR and LIAR-New). Furthermore, the overall budget needed for these experiments is not trivial, especially considering that not all experiments are included here and many need multiple runs (e.g., when evaluating temperature in the following section, doing exploratory analysis, debugging, etc.). In the short-term, thoughtful prompting and choice of experiments to target can help keep costs down when budget is limited. In the long-term, cheaper models with comparable performance will hopefully (and quite likely) be found or created, which will be a significant advantage for both research and real-world deployment.\n\n## J Temperature and Variation\n\nWe examine here the effect of varying the sampling temperature from 0.5 , used in the main experiments, to 0.0 or 1.0. There are two outcomes to consider: first, the effect on overall performance. Second, the variation of GPT-4's responses for each example over multiple runs. We test these by running GPT-4 10 times on the test data at each temperature level, in the binary setting.\n\nIn Table 17 we report first the mean accuracy and its standard deviation over 10 runs. Then we report the number of non-numeric answers, e.g., when GPT-4 refuses to answer or returns an explanation instead of following the directive to only return a score 0-100. Note that for the accuracy calculation, we replaced these with a random number uniformly drawn between 0 and 100, while in the subsequent metrics we exclude these examples. For each example, we then compute the standard deviation of the scores, and report the mean and maximum of that\nvalue over all the examples in the dataset (Mean SD and Max SD). Finally, we look at peak-to-peak distance, i.e., the difference between the largest and smallest GPT-4 scores for each example over the 10 runs. We report the maximum (Max PtP) and the number of examples for which the PtP distance is over 50 (\\# Large PtP).\n\nWe find that lower temperature generally gives better results, with both a small increase in accuracy and a reduced variation amongst the responses. The mean standard deviation between runs with 0.0 temperature is quite reasonable at 2.2 , indicating the responses are generally consistent. We do however note that a small number of examples still have a very large variation in responses-regardless of the temperature, there is always an example with the maximal 100 peak-to-peak distance, and even with 0.0 temperature there are 5 examples with over 50 peak-to-peak distance. This might be troublesome for user-facing systems built on this, as occasionally it can give very different answers to different users querying the veracity of the same statement, or one user querying multiple times, which could cause some confusion or distrust.\n\nConsequently, future work that addresses this limitation would be helpful. This might be done with better prompts or with techniques such as multi-agent debate (Du et al., 2023; Liang et al., 2023), or possibly giving the system more context and evidence.\n\nWe also note that the maximum standard deviation shows a counterintuitive result, increasing as temperature decreases. This might be noise, or it might be due to lower temperature following the instruction to respond with a number more consistently and still giving an answer in cases where it is more confused. We found that of the 95 examples where at least one run at 0.0 temperature returned a non-numeric answer, all but one were contained within the 129 examples where a run at 1.0 temperature returned a non-numeric answer. Furthermore, excluding the full 129+1 examples increased performance to $69.6 \\pm 0.2$, reduced Mean SD to 2.1 , and removed one of the 5 large PtP cases. It did not change the maximum standard deviation or peak-topeak distance, but nonetheless, this might suggest that higher temperature has an edge in realizing cases it cannot answer properly. Further investigation is needed to determine if an adjusted prompt, which directly allows the model to refuse to answer, would have a positive effect here."
    },
    {
      "markdown": "| Dataset | Prompt | Input Tokens | Output Tokens | Total Cost (USD) |\n| :--: | :--: | :--: | :--: | :--: |\n| LIAR | Score | 100 K | 3 K | 3.19 |\n| LIAR | Binary | 93 K | 2 K | 2.88 |\n| LIAR | Explain-then-Score | 142 K | 314 K | 23.1 |\n| LIAR | Score-then-Explain | 104 K | 118 K | 10.21 |\n| LIAR-New | Score | 154 K | 2 K | 4.73 |\n| LIAR-New | Binary | 140 K | 2 K | 4.32 |\n| LIAR-New | Binary Uncertainty-Enabled | 205 K | 4 K | 6.37 |\n| LIAR-New | Explain-then-Score | 216 K | 464 K | 34.35 |\n| LIAR-New | Web Oracle | 1.8 M | 2 K | 53.07 |\n| LIAR-New | Web Answerless Oracle | 1.7 M | 2 K | 51.07 |\n| CT-FAN English | Score | 725 K | 1 K | 21.79 |\n| CT-FAN English | Explain-then-Score | 691 K | 197 K | 32.57 |\n| CT-FAN German | Score | 956 K | 1 K | 28.72 |\n\nTable 16: GPT-4 API costs.\n\n| Temp. | Accuracy | $\\begin{gathered} \\text { \\# Non- } \\\\ \\text { numeric } \\end{gathered}$ | SD |  | PtP |  |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n|  |  |  | Mean | Max | Max | $\\#$ Large |\n| 0.0 | $67.9 \\pm 0.3$ | 95 | 2.2 | 51.6 | 100 | 5 |\n| 0.5 | $67.4 \\pm 0.7$ | 108 | 5.3 | 48.3 | 100 | 11 |\n| 1.0 | $66.5 \\pm 0.8$ | 129 | 7.7 | 40.7 | 100 | 40 |\n\nTable 17: Temperature vs. accuracy and several measures of variation in the responses. In general, lower temperature gives better results.\n\nFinally, we also considered the potential effect of changing the temperature, as well as repeated runs in general, on the optimal threshold. We evaluated all possible thresholds to determine the oracle threshold that would give the best performance at each temperature. Results are shown in Table 18. We see that the original threshold found on the validation set (with 0.5 temperature) was a bit high for the test set, especially at 0.0 temperature. However, the difference in accuracy between this threshold and the oracle one is quite small. Thus, it does not appear that temperature has a large practical effect on the optimal threshold.\n\n| Temp. | Original Thre. | Accu. | Oracle Thre. | Accu. |\n| :--: | :--: | :--: | :--: | :--: |\n| 0.0 | 71 | 67.9 | 62 | 68.1 |\n| 0.5 | 71 | 67.4 | 66 | 67.7 |\n| 1.0 | 71 | 66.5 | 67 | 67.5 |\n\nTable 18: Temperature vs. performance with the original threshold from one run on the validation set (71), and the oracle threshold that maximizes performance. The original threshold is somewhat high, but the difference in accuracy is low.\n\n## K LIAR Possible vs. Impossible Examples\n\nIn addition to the larger scale analysis on LIARNew, we also examined the Possible vs. Impossible examples in LIAR (i.e., using the ones that GPT4 found for the Impossible case, as discussed in Section 3.1). We restrict the test set to only one or the other category, and compare results with GPT4 and RoBERTa-large. First, in Table 19 we see that performance improves slightly (compared to Table 2) when restricting to the Possible examples, which confirms our initial intuition.\n\nNext, for the Impossible examples, we see the results of flipping a coin vs. RoBERTa-large. RoBERTa outperforms random classifications here, which means it may be learning information or a bias which helps on this data but will not generalize. On the other hand, if we look at RoBERTa's soft classification of these examples, the results are more encouraging: the mean absolute deviation from 0.5 (which would match \"can't be determined one way or another\") is 0.232 in the Impossible case, compared to .338 in the Possible case. This suggests more certainty overall on Possible examples, which is the direction one would want, even if it is not sufficient to achieve the desired outcome.\n\nFinally, we further tested what happens to the GPT-4 classification threshold when these examples are included, as in Table 2, by assigning them a random prediction. The threshold increased from the optimal 71 to a mean over 100 runs (rerandomizing the Impossible predictions) of 72.31. Although not a large change, this does illustrate"
    },
    {
      "markdown": "|  | Method | Accuracy | F1 |\n| :--: | :--: | :--: | :--: |\n| Possible | GPT-4 | 69.4 | 69.1 |\n|  | RoBERTa-large | 63.8 | 62.3 |\n| Impossible | 50-50 Random | 50.0 | 50.2 |\n|  | RoBERTa-large | 59.4 | 58.4 |\n\nTable 19: Possible vs. Impossible examples. RoBERTalarge beats random on Impossible ones - which is might not generalize.\nconcretely how including these examples can bias the predictions.\n\nOverall, these results confirm that noise in the data can affect downstream results. RoBERTalarge can somewhat account for them, but not nearly to the extent of GPT-4. Since it can detect many Impossible examples, GPT-4 has a robustness advantage in this area, and might also be used for cleaning the data if approaches like RoBERTalarge are needed.\n\n## I Investigating Errors in Relation to the Training Data\n\nWe embedded all examples with OpenAI's Ada002 model (Greene et al., 2022). This is a document embedding model which is suitable for evaluating their semantic similarity. For each example in the test set, we find the most similar statement in the training set by calculating the cosine distance in Ada-002 embeddings between the statements. We find the training set statement that minimizes the distance to each test set statement. Finally, we compare the minimum distances averaged over all the test set examples.\n\nFor the cases where GPT-4 was correct and RoBERTa-L wrong, the average distance is 0.127 , while for the reverse, the average is 0.116 . The difference is statistically significant ( $\\mathrm{p}=0.0005$ ). This suggests RoBERTa is using similar examples in the training data to make correct predictions for cases GPT-4 gets wrong, while GPT-4 must be using the information it learned during its own training process to correctly predict cases that do not relate well to the examples RoBERTa sees in the training set. We publish on our Github these examples and the associated cosine distances so that further analysis on the generalizability of RoBERTa's predictions can be conducted. If replicable in other contexts, these results suggest GPT-4 or similar models could benefit significantly from effective finetuning or few-shot setups in this domain, if\nthey can be optimized beyond the mixed results we found here (Appendix G and Appendix D). In other words, if GPT-4 could correctly classify all the examples RoBERTa does without compromising its current correct examples, there would be a nearly 10 percentage point improvement. However, the optimal methods to achieve that-without compromising generalization-remain an open question.\n\n## II LIAR Error Analysis Case Study\n\nWe present here some examples that RoBERTa-L Binary gets correct and GPT-4 Score Zero-Shot gets wrong, along with their closest training set example by Ada-002 embedding cosine distance. Specifically, we randomly sampled 50 examples for qualitative analysis, and here we print the five with the lowest cosine distance, the five in the middle, and the five with the highest cosine distance among the 50. From the closest five, we see that a number of examples in the LIAR test data are near-replicas of an example in the training data. Some others are very similar. On the one hand, this means that the training data is helpful to classify some examples, explaining why RoBERTa-L gets some right that GPT-4 gets wrong, and suggesting a potential benefit if GPT-4 could ingest the training data directly. On the other hand, this also means some examples might be artificially easy for models that have seen the training data, and can be classified by memorization rather than learning generalizable patterns.\n\n## III LIAR 6-Way Classification\n\nIn Table 21, we present results for 6-way classification-i.e., using the original LIAR dataset labels, without dichotomizing the ratings. In this case, DeBERTa performs best. GPT-4 Zero-Shot is not far behind, but this suggests that more information is needed to match the finer-grained labels here, either from the training set or by tuning the GPT-4 classification thresholds on the validation set. Performance is very low overall, but this matches the literature, and in the case of DeBERTa, exceeds it (Flores and Hao, 2022). It is important to note that, as shown by the low metrics across the board, this task is extremely difficult. Besides current approaches just being insufficiently capable in general, this could potentially occur if there were a lack of consistency in the fine-grained labeling, so further validation of the labels could be a valuable area for future work."
    },
    {
      "markdown": "|  | Test Set Statement | Closest Train Statement |\n| --- | --- | --- |\n| Closest 5 | Weve doubled the production of clean energy. <br> Says the National Labor Relations Board told Boeing that it cant build a factory in a non-union state. | Weve doubled our use of renewable energy. <br> Says the National Labor Relations Board told Boeing that it couldnt build a factory in South Carolina because South Carolina is a right-to-work state. |\n|  | Under Barack Obama, the U.S. now has the lowest workforce since (President Jimmy) Carter. <br> Says (Clinton) called President Assad a reformer. She called Assad a different kind of leader. | Under Barack Obama, the fewest number of adults are working since Jimmy Carters presidency. <br> Says Hillary Clinton defended Syrias President Assad as a possible reformer at the start of that countrys civil war. <br> Says Gov. Bob McDonnells budget would cut prekindergarten programs. |\n| Middle 5 | PolitiFact Texas says Congressman Edwards attacks on Bill Flores are false. <br> One in five American households have nobody under the same roof thats got a job. <br> Says Bill Pascrell voted to eliminate all estate taxes for billionaires, and I voted no. | Says PolitiFact has ruled that Ed Gillespies ads are false and misleading. <br> We have 25 million Americans out of work. |\n|  | Of all cities in the United States with more than 100,000 people, Providence is the 183rd safest. | Says Congressman Bill Pascrell voted to remove the public option from the Affordable Health Care Act. |\n|  | Says critics who say he cut Medicaid are wrong; his budget added $\\$ 1.2$ billion to the program | [When I was mayor] Providence was one of the five renaissance cities of America, according to USA Today in those days. Money magazine said it was the fifth best city to live in in America. In addition to that, All Cities Almanac said in 1994, I believe, that it was the safest city in America. <br> Says he opposed $\\$ 716$ billion cut to Medicare. |\n| Furthest 5 | Says Republican Rep. Charlie Dent wants to kick the Freedom Caucus out of the Republican conference for voting our conscience. <br> Active duty males in the military are twice as likely to develop prostate cancer than their civilian counterparts. <br> We have 41,000 people right now on the waitlist for financial-based assistance for our tech colleges and universities, most of them for tech schools. <br> Ninety percent of the people in Arkansas still love Bill Clinton. <br> When people enter the service, theres not a mental health evaluation. | Says Congressman Eric Cantor wants to eliminate Social Security. <br> One in five women in the military are receiving unwanted sexual contact, as are 3.3 percent of men. <br> The 2013-15 state budget backed by Gov. Scott Walker and Republican lawmakers left our technical colleges funded at 1989 levels. <br> 59 percent of Americans today believe that Barack Obama is still a Muslim. <br> Seventy-five percent of the young adults in this country are not mentally or physically fit to serve. |\n\nTable 20: Samples of LIAR examples that RoBERTa-L Binary classified correctly and GPT-4 Score Zero-Shot classified incorrectly, grouped by semantic similarity according to Ada-002 embeddings. The closest ones are near identical, the middle ones are similar, the furthest ones are only tangentially related.\n\n## O PaLM 2 Bison\n\nIn Table 22, we examine the performance of the Score prompt with PaLM 2 Bison (Anil et al., 2023). We report accuracy for LIAR and Macro F1 for CT-FAN (the most used metrics for those datasets).\n\nWe see that Bison performs OK, if not impressively, on LIAR. It is significantly worse than GPT4 and other approaches on CT-FAN. In addition, unlike with GPT-4, we did not find any significant performance gains from tuning the score threshold. Nonetheless, the results on LIAR do suggest reasonable misinformation detection is no artifact of GPT-4 alone among recent LLMs. and similarly capable models might give similarly strong results.\n\nBison is the second largest PaLM 2 model. The\nlargest, Unicorn, is not publicly accessible in a direct way as of this writing. Unicorn can be used indirectly as it powers Google Bard, but Bard is not accessible in the authors' country (Canada), and to our knowledge does not offer an API anywhere. Consequently, we were unfortunately not able to test test the most powerful version of PaLM 2. If accessibility issues can be resolved, we believe that would be a worthwhile evaluation for future work.\n\n## P Soft Classification\n\nHere we evaluate the capabilities of GPT-4 and other language models for making probability predictions. We first discuss the calibration method we used, then the results."
    },
    {
      "markdown": "| Method | Accuracy | F1 |\n| --- | --- | --- |\n| SOTA (2022) | 29.4 | - |\n| GPT-4 Zero-Shot | 28.1 | 25.5 |\n| BERT | 28.1 | 26.4 |\n| ConvBERT | 27.9 | 27.2 |\n| DeBERTa | 29.9 | 29.2 |\n| DeBERTa-V3 | 28.9 | 27.5 |\n| LUKE | 28.3 | 27.0 |\n| RoBERTa | 26.5 | 22.1 |\n| SqueezeBERT | 26.6 | 25.3 |\n| XLMRoBERTA | 20.9 | 07.0 |\n| Fuzzy (Word2Vec) | 22.3 | 22.8 |\n| Fuzzy (BERT) | 23.1 | 23.2 |\n| Fuzzy (GloVE) | 21.5 | 21.1 |\n\nTable 21: 6-Way Classification Results (percentages)\n\n| Dataset | Method | Result |\n| :--: | :--: | :--: |\n| LIAR | PaLM 2 Bison | 62.4 |\n| LIAR | GPT-4 Score Zero-Shot | 64.9 |\n| CT-FAN English | PaLM 2 Bison | 21.3 |\n| CT-FAN English | GPT-4 Score Zero-Shot | 42.8 |\n| CT-FAN German | PaLM 2 Bison | 22.3 |\n| CT-FAN German | GPT-4 Score Zero-Shot | 38.7 |\n\nTable 22: Comparison with PaLM 2 Bison. GPT-4 performs better, but Bison does give passsable results on LIAR. Note that this is only the second largest PaLM 2 model; the largest was not accessible to the authors as of this writing.\n\n## P. 1 Calibration\n\nFor soft classification (probabilities), we first use the scores our models provide out-of-the-box. In the case of GPT-4, we use the raw output it returns (divided by 100 to put it between 0 and 1). For the other language models, we take the output of the final softmax, right before the argmax converts this value to a single class prediction. These approaches have the advantage of simplicity and not introducing potential confounding variables.\n\nNote, however, that the literature suggests deep learning models are often over-confident and their out-of-the-box probabilities may be of low quality (Moon et al., 2020). Therefore, we apply Platt’s method (Jiang et al., 2011) to calibrate. This involves fitting a logistic regression model to the model’s output probabilities. We do this on the validation set. This will in theory help correct for overconfidence and lead to better performance. For evaluation metrics, please see Appendix H.\n\nAside from GPT-4 which is not trained, all the\nmodels for this section were trained in the binary classification setting of Table 2.\n\n## P. 2 Results\n\nResults are shown in Figure 2 and the following figures. We first observe that Platt’s method generally improves the quality of the predictions. We next see that GPT-4 predictions are not very well calibrated out-of-the-box, but after Platt they show a decent relationship between prediction and actual probability, indicating this approach has potential to give more informative predictions. We suspect that this could be improved even further with a better prompt.\n\nMore surprisingly, we find that superior performance on hard classification does not translate to superior performance on soft classification. The best soft classification model is SqueezeBERT (after calibration), which gave thoroughly unremarkable performance in the hard classification experiments. We believe this warrants further investigation for two reasons. On the one hand, this could lead to an effective, nuanced misinformation mitigation tool leveraging the best-performing approaches here like SqueezeBERT. On the other hand, understanding why different approaches perform better or worse here for soft classification despite their hard classification performance could lead to better soft classification in general.\n\nFinally, we see that although after calibration the different approaches do overall follow the perfect prediction line, especially SqueezeBERT, they do not give many predictions with very high certainty (close to 0 or 1 ). This can be an issue because we still want systems that make strong predictions in clear cut cases. While this is likely better than making numerous confident but wrong predictions, as one gets with standard hard classification, it suggests a line of future work aimed at fully realizing the potential of soft classification approaches.\n\n## Q Explainability\n\nIn many experiments, we have examined prompts that explicitly discourage explanations. This gives advantages in speed and reduced cost (see Appendix I), and such outputs are easier to evaluate quantitatively. However, in the research and development context, explanations could provide insights into how the system is making evaluations and how we could improve it. In a deployment context, they could enable a user to see if its eval-"
    },
    {
      "markdown": "![img-1.jpeg](img-1.jpeg)\n\nFigure 2: GPT-4 (top), RoBERTa-large (middle), and SqueezeBERT (bottom) probability predictions, out-of-the-box or calibrated with Platt's method."
    },
    {
      "markdown": "![img-2.jpeg](img-2.jpeg)\n\nFigure 3: BERT probability predictions, out-of-the-box or calibrated with Platt's method.\n![img-3.jpeg](img-3.jpeg)\n\nFigure 4: ConvBERT probability predictions, out-of-the-box or calibrated with Platt's method.\n![img-4.jpeg](img-4.jpeg)\n\nFigure 5: DeBERTa probability predictions, out-of-the-box or calibrated with Platt's method."
    },
    {
      "markdown": "![img-5.jpeg](img-5.jpeg)\n\nFigure 6: DeBERTa-v3 probability predictions, out-of-the-box or calibrated with Platt's method.\n![img-6.jpeg](img-6.jpeg)\n\nFigure 7: LUKE probability predictions, out-of-the-box or calibrated with Platt's method.\n![img-7.jpeg](img-7.jpeg)\n\nFigure 8: RoBERTa probability predictions, out-of-the-box or calibrated with Platt's method."
    },
    {
      "markdown": "![img-8.jpeg](img-8.jpeg)\n\nFigure 9: SqueezeBERT probability predictions, out-of-the-box or calibrated with Platt's method.\n![img-9.jpeg](img-9.jpeg)\n\nFigure 10: XLM-RoBERTa probability predictions, out-of-the-box or calibrated with Platt's method."
    },
    {
      "markdown": "uations seem complete or if something is missing, leading to improved trust and more graceful failure. In the following subsections we go into more detail on our explainability experiments.\n\n## Q. 1 Prompts\n\nWe considered two prompts. The first, \"Score-then-Explain\" is very close to the original \"Score\" prompt. We simply replace the last line in the original, which discourages explanations, with a line requesting them.\n\"role\": \"user\", \"content\": 'Rate the truthfulness of the following statement: \"STATEMENT\" Provide a score from 0 to 100 , where 0 represents definitively false and 100 represents definitively true. Then write a vertical bar '!\", followed by explaining your reasoning for the score.'\n\nThe second prompt, \"Explain-then-Score\", asks the model to write an explanation before deciding on its score. The intuition here is that if we ask for a score first and lock that in, then the explanation will likely be written to match the score. But if we ask for the explanation first, the model will have a chance to reason further about the input as it writes the explanation, which might lead to a more thoughtful and accurate score. This approach was directly inspired by (Tian et al., 2023), and has connections to general chain-of-thought (Wei et al., 2022) literature which has shown various approaches to promote intermediate reasoning steps can improve a final result.\n\"role\": \"user\", \"content\": 'Rate the truthfulness of the following statement: \"STATEMENT\" Your task is to provide a score from 0 to 100 , where 0 represents definitively false and 100 represents definitively true, but you must not state your score until you've presented a thorough analysis. Do not begin your response with a number. First write your analysis, then write a vertical bar '!\", then finally state your score.'\n\n## Q. 2 Predictive Performance\n\nWe test the scores these prompts produce compared to no explanation GPT-4 Score Zero-Shot. We report results (the primary metrics for the datasets: accuracy for LIAR, macro F1 for others) in Table 23.\n\nWe see that Explain-then-Score consistently improves performance. Therefore, if willing to spend\non the extra tokens (see Appendix I for cost estimates), then this prompt seems superior to prompting for the score alone.\n\n## Q. 3 Evaluation of the Explanations: Reasonableness\n\nTo perform an initial evaluation of whether the explanations made sense and would be helpful, one of the authors manually examined samples of explanations according to the following directive:\nDefinition 4. \"GT-Reasonable\": if the explanation is reasonable in evaluating the statement - in relation to the ground truth PolitiFact or other actual information - please put \"y\" for \"yes\". Otherwise, \"n\" for \"no\" (e.g., its explanation is false, does not explain the answer, illogical or outright nonsensical, no explanation given at all, etc.).\n\nWe note that this is only a proxy for the ultimate question in explainability: whether the explanation will help a user of an information evaluation system based on this framework. However, that question cannot be answered without knowing the user, a precise definition of \"help\", etc. This definition ignores all those factors and more, so we do not recommend it for in depth analysis; we use it here simply as a tool to explore the results.\n\nWe sampled 30 random examples where GPT4 gives the correct answer, 10 where it gives the wrong answer, and 10 where it gives the right answer and RoBERTa gives the wrong one. Although we might expect to only get a reasonable explanation if it is giving the right answer, it is conceivable that we could somehow have a correct explanation but a wrong answer. We would also like to better understand why it gives wrong answers. Meanwhile, analyzing the examples GPT-4 got correct and RoBERTa wrong might provide further insights on what GPT-4 relied on to get those answers that RoBERTa could not.\n\nWe then did a second round of another 50 examples with the same the same sampling setup, but with the addition of a second evaluation criteria:\nDefinition 5. \"Score-Reasonable\": is the explanation reasonable in relation to the score it gives? If so please put \"y\" for \"yes\". It would be \"n\" for \"no\" if for example it is very confident the statement is true but gave a score around 50 (indicating uncertainty) or well below (indicating false).\n\nThis criteria is quite similar to the previous GTReasonable, just making the evaluation compared to the score instead of the ground truth."
    },
    {
      "markdown": "| Dataset | Method | Result |\n| :--: | :--: | :--: |\n| LIAR | GPT-4-0613 Score-then-Explain | 62.9 |\n| LIAR | GPT-4-0613 Explain-then-Score | 64.9 |\n| LIAR | GPT-4-0314 Explain-then-Score | 68.4 |\n| LIAR | GPT-4-0314 Score Zero-Shot | 64.9 |\n| LIAR-New All | GPT-4-0613 Explain-then-Score | 65.3 |\n| LIAR-New All | GPT-4-0314 Explain-then-Score | 65.5 |\n| LIAR-New All | GPT-4-0314 Score Zero-Shot | 60.5 |\n| CT-FAN English 4-Way | GPT-4-0613 Explain-then-Score | 42.5 |\n| CT-FAN English 4-Way | GPT-4-0314 Explain-then-Score | 43.4 |\n| CT-FAN English 4-Way | GPT-4-0314 Score Zero-Shot | 42.8 |\n| CT-FAN English 3-Way | GPT-4-0613 Explain-then-Score | 59.0 |\n| CT-FAN English 3-Way | GPT-4-0314 Explain-then-Score | 60.0 |\n| CT-FAN English 3-Way | GPT-4-0314 Score Zero-Shot | 58.6 |\n\nTable 23: Predictive performance of explainability prompts. Explain-then-Score works well, matching or outperforming Score Zero-Shot.\n\nWe found that $100 \\%$ of the examples were ScoreReasonable. This is not surprising but confirms the score is stable in relation to the explanation and the model seldom if ever suddenly changes its mind at that stage. We report GT-Reasonable rates in Table 24.\n\n| Sample Type | Round 1 | Round 2 |\n| :--: | :--: | :--: |\n| Correct | $28 / 30$ | $29 / 30$ |\n| Wrong | $5 / 10$ | $8 / 10$ |\n| GPT Correct RoBERTa Wrong | $9 / 10$ | $10 / 10$ |\n\nTable 24: GT-Reasonable proportions on examples randomly sampled from each group. Not only are the explanations reasonable in cases it predicted correctly, but also a significant number in which it did not.\n\nWe see that the explanations are generally reasonable when correct. More unexpectedly, they are also often reasonable even when the prediction is wrong. We believe there are two main reasons. First, this often occurs when GPT-4 gives an answer indicating the statement is murky, e.g. score in the 40-60 range, while PolitiFact agrees it is mixed but labels it slightly differently. Due to the threshold at 50 , the binary class can end up different, even if both GPT-4 and PolitiFact are largely saying the same thing.\n\nSecond, there is some uncertainty and subjectivity in the evaluation. For example, if GPT-4 gives correct framing and general info about the statement, but says it lacks a particular piece of data, is that a good explanation or not? If it gives an explanation with the context of 2021, when really\nthe statement was made in 2008, is that a good explanation?\n\nMore refinement of the process is needed. This would likely include more precision in the definition and breaking it down into more categories, as well as larger sample size and more labelers. Nonetheless, these preliminary results already suggest an approach like this can have great potential for explainability, and it is far easier to get here than with previous models like RoBERTa.\n\nWe do not recommend training on them, but in case these explanations are useful for further analysis, we publish them on our Github.\n\n## Q. 4 Evaluation of the Explanations: Impossible Cases\n\nWe took the LIAR-New predictions, filtered for Impossible examples only according to V1 Possibility labels (Appendix C), and randomly sampled 50 where GPT-4 made the correct prediction and 50 where it was wrong.\n\nCorrect Answers Besides cases it does not really know (potentially even saying it does not know) but gets lucky, it seems to either rely on general information or assume a United States context.\n\nThe following is an example of relying on general information to evaluate the statement \"A Phoenix TV station aired the election results more than a week early.\"\n\nThe statement refers to a specific event that could be verified through reliable sources. However, without any specific"
    },
    {
      "markdown": "details such as the name of the TV station, the date of the alleged early airing, or the specific election in question, it is difficult to confirm the truthfulness of the claim. Furthermore, it is important to note that the broadcasting of election results before the official end of voting is generally considered unethical and potentially illegal, as it could influence the outcome of the election. Therefore, it would be unusual for a reputable TV station to do so. Finally, it's also worth considering that this could be a misunderstanding or misinterpretation of a forecast or prediction being mistaken for actual results.\nGiven these considerations, it is impossible to definitively confirm or deny the statement without additional information. 150\n\nPolitiFact rated this statement \"Half True\" noting the tv station accidentally aired mock election results, similar to GPT-4's suggestion that it might be a misinterpretation of a \"forecast or prediction\" (https://www.politifact.com/factchecks/2022/oct/28/instagram-posts/phoenix-tv-station-did-not-call-election-katie-hob/).\n\nThe following are examples of assuming a United States context.\n\nStatement: \"There are 63 million abortions a year in this country.\"\n\nThe statement asserts that there are 63 million abortions a year in the United States. The Guttmacher Institute, a leading research and policy organization committed to advancing sexual and reproductive health and rights, reported that there were approximately 862,320 abortions in the United States in 2017 [...]\n\nStatement: \" $\\$ 2,000$ stimulus checks were due on Jan. 19.\"\n\nThe statement is suggesting that \\$2,000 stimulus checks were due on Jan. 19. This statement is ambiguous because it does not specify the year or the country it is referring to. However, assuming it refers to the United States and the year 2021, we can analyze its truthfulness [...]\n\nStatement: \"Our Supreme Court has never taken away a constitutional right.\"\n\nThe Supreme Court of the United States has the power to interpret the Constitution and determine the constitutionality of laws [...]\n\nAlthough PolitiFact focuses largely on US politics leading to correct answers here, assuming US context could be a substantial issue if using a system based on this in other countries, or even in the US in the case of evaluating statements that are actually about other countries. While many biases like race, gender, and political ideology are commonly studied in relation to LLMs, this sort of \"geographical\" bias is less explored, particularly in the misinformation domain. These results suggest it merits further research.\n\n## Incorrect Answers\n\nIn the majority of cases, GPT-4 appears to know it is missing context. In some cases, it seems to assume an image or video which it does not have access to exists, leading to it incorrectly predicting a statement is true.\n\nThe following is an example where it knows context is missing when evaluating the statement \"A royal guard member collapsed because of the COVID-19 vaccine.\"\n\nThe statement \"A royal guard member collapsed because of the COVID-19 vaccine\" is not verifiable without specific information and context. The statement is vague and does not provide enough detail to determine its truthfulness. It doesn't specify which royal guard (from which country or monarchy), nor does it provide any timeline or source of this information. Furthermore, it directly attributes the collapse to the COVID-19 vaccine, which is a complex medical conclusion that would require professional medical evaluation to confirm. Vaccines can have side effects, but serious side effects are rare, and it's also possible for someone to collapse due to other health conditions. Without any specific information or context, it's impossible to definitively confirm or refute this statement. 150\n\nIn a binary evaluation (or even many forms of multiclass evaluation) where a label must be assigned one way or another, errors of this type are not distinguishable from more egregiously wrong"
    },
    {
      "markdown": "and misleading answers. Solutions for such failures may look very different (e.g., querying user or knowledge base for additional information, vs. mitigating hallucinations or overconfidence). So this suggests a need for more uncertainty-aware classification and evaluation methods, such as some of the approaches we propose or hopefully even better future approaches.\n\nThe following is an example where GPT-4 seems to take it at face value that images exist, evaluating the statement \"Images show how climate change destroyed the Greenland ice sheet and harmed a polar bear from 2009 to 2019.\"\n\nThe statement is making two claims: 1) climate change has destroyed the Greenland ice sheet and 2) climate change has harmed a polar bear from 2009 to 2019. In conclusion, while the statement captures the essence of the effects of climate change, its use of the term 'destroyed' and the lack of specific evidence regarding the polar bear make it not entirely accurate. 170\n\nIn reality, PolitiFact rates this statement False, explaining \"experts pointed out that the image of Greenland has nothing to do with the polar bear image, since polar bears face sea-ice loss in the Arctic Ocean, not Greenland\" (https://www.politifact.com/factchecks/2022/jan/19/facebook-posts/10-years-challenge-climate-change-exaggeratesimpa/).\n\nSuch errors might be addressed by targeted interventions, such as a classifier to detect when an image/video is referenced, making user aware the system cannot see the pictures and might give inaccurate answers, or simply multimodal models that can process pictures smoothly (in theory, GPT-4 might already do this, but this feature is not yet enabled through the API and we were unable to evaluate it).\n\n## R Web Retrieval\n\n## R. 1 Prompt\n\n\"role\": \"user\", \"content\": 'Rate the truthfulness of the following statement: \"STATEMENT\" Provide a score from 0 to 100 , where 0 represents definitively false and 100 represents definitively true. Do not provide any explanations, only respond with the numerical score.\nThe following information may provide additional evidence or context for your rating:\n\nARTICLE'\n\nThe first part is the same as our Score prompt, to which we add the last part about \"additional evidence or context\" and the corresponding PolitiFact article.\n\n## R. 2 Answerless Oracle\n\nTo remove the final PolitiFact label, we first split the article into sentences. We then iterate backwards through the article looking for a sentence containing one of the keywords \"true\", \"false\", or \"pants\" (for \"pants on fire\" label). Each of the labels contains one of these, so this locates the last sentence in the article with a label keyword. We then remove that sentence and any subsequent ones. While this may not be a perfect guarantee of removing the label, PolitiFact articles almost always have the verdict at the very end. Generally, the only exception is if the article was later updated, which is not frequent. So this heuristic is quite consistent and sufficient for our preliminary analysis here, which does not aim to say that the performance is precisely X but just shows that sufficiently strong retrieval can massively increase performance compared to not using retrieval.\n\nWe make the original PolitiFact articles and this answer-removing code available through our GitHub."
    }
  ],
  "usage_info": {
    "pages_processed": 32,
    "doc_size_bytes": 769590
  },
  "_metadata": {
    "model_version": "0.0.0",
    "llm_model": "mistral-ocr-2505",
    "query_data": {
      "pdf": "/Users/satyaortiz-gagne/CODE/paperext/data/cache/openreview/cCJGuKJYG8.pdf"
    },
    "model_id": "parsepdf"
  }
}