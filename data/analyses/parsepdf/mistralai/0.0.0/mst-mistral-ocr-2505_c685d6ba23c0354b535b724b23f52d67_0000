{
  "pages": [
    {
      "markdown": "# Scalable regret for learning to control network-coupled subsystems with unknown dynamics \n\nSagar Sudhakara, Aditya Mahajan, Ashutosh Nayyar, and Yi Ouyang\n\n\n#### Abstract\n\nWe consider the problem of controlling an unknown linear quadratic Gaussian (LQG) system consisting of multiple subsystems connected over a network. Our goal is to minimize and quantify the regret (i.e. loss in performance) of our strategy with respect to an oracle who knows the system model. Viewing the interconnected subsystems globally and directly using existing LQG learning algorithms for the global system results in a regret that increases super-linearly with the number of subsystems. Instead, we propose a new Thompson sampling based learning algorithm which exploits the structure of the underlying network. We show that the expected regret of the proposed algorithm is bounded by $\\mathcal{O}(n \\sqrt{T})$ where $n$ is the number of subsystems, $T$ is the time horizon and the $\\mathcal{O}(\\cdot)$ notation hides logarithmic terms in $n$ and $T$. Thus, the regret scales linearly with the number of subsystems. We present numerical experiments to illustrate the salient features of the proposed algorithm.\n\n\nIndex Terms-Linear quadratic systems, networked control systems, reinforcement learning, Thompson sampling.\n\n## I. INTRODUCTION\n\nLarge-scale systems comprising of multiple subsystems connected over a network arise in a number of applications including power systems, traffic networks, communication networks and some economic systems [1]. A common feature of such systems is the coupling in their subsystems' dynamics and costs, i.e., the state evolution and local costs of one subsystem depend not only on its own state and control action but also on the states and control actions of other subsystems in the network. Analyzing various aspects of the behavior of such systems and designing control strategies for them under a variety of settings have been long-standing problems of interest in the systems and control literature [2]-[7]. However, there are still many unsolved challenges, especially on the interface between learning and control in the context of these large-scale systems.\n\nIn this paper, we investigate the problem of designing control strategies for large-scale network-coupled subsystems when some parameters of the system model are not known. Due to\n\n[^0]the unknown parameters, the control problem is also a learning problem. We adopt a reinforcement learning framework for this problem with the goal of minimizing and quantifying the regret (i.e. loss in performance) of our learning-and-control strategy with respect to the optimal control strategy based on the complete knowledge of the system model.\n\nThe networked system we consider follows linear dynamics with quadratic costs and Gaussian noise. Such linear-quadraticGaussian (LQG) systems are one of the most commonly used modeling framework in numerous control applications. Part of the appeal of LQG models is the simple structure of the optimal control strategy when the system model is completely known-the optimal control action in this case is a linear or affine function of the state-which makes the optimal strategy easy to identify and easy to implement. If some parameters of the model are not fully known during the design phase or may change during operation, then it is better to design a strategy that learns and adapts online. Historically, both adaptive control [8] and reinforcement learning [9], [10] have been used to design asymptotically optimal learning algorithms for such LQG systems. In recent years, there has been considerable interest in analyzing the transient behavior of such algorithms which can be quantified in terms of the regret of the algorithm as a function of time. This allows one to assess, as a function of time, the performance of a learning algorithm compared to an oracle who knows the system parameters upfront.\n\nSeveral learning algorithms have been proposed for LQG systems [11]-[23], and in most cases the regret is shown to be bounded by $\\mathcal{O}\\left(d_{x}^{0.5}\\left(d_{x}+d_{u}\\right) \\sqrt{T}\\right)$, where $d_{x}$ is the dimension of the state, $d_{u}$ is the dimension of the controls, $T$ is the time horizon, and the $\\mathcal{O}(\\cdot)$ notation hides logarithmic terms in $T$. Given the lower bound of $\\hat{\\Omega}\\left(d_{x}^{0.5} d_{u} \\sqrt{T}\\right)$ (where $\\hat{\\Omega}(\\cdot)$ notation hides logarithmic terms in $T$ ) for regret in LQG systems identified in a recent work [19], the regrets of the existing algorithms have near optimal scaling in terms of time and dimension. However, when directly applied to a networked system with $n$ subsystems, these algorithms would incur $\\mathcal{O}\\left(n^{1.5} d_{x}^{0.5}\\left(d_{x}+d_{u}\\right) \\sqrt{T}\\right)$ regret because the effective dimension of the state and the controls is $n d_{x}$ and $n d_{u}$, where $d_{x}$ and $d_{u}$ are the dimensions of each subsystem. This superlinear dependence on $n$ is prohibitive in large-scale networked systems because the regret per subsystem (which is $\\mathcal{O}(\\sqrt{n})$ ) grows with the number of subsystems.\n\nThe learning algorithms mentioned above are for a general LQG system and do not take into account any knowledge of the underlying network structure. Our main contribution is to\n\n\n[^0]:    Sagar Sudhakara and Ashutosh Nayyar are with the Department of Electrical and Computer Engineering, University of Southern California, Los Angeles, CA, USA. (email: sagarsud@usc.edu, ashutoso@usc.edu)\n\n    Aditya Mahajan is with the department of Electrical and Computer Engineering, McGill University, Montreal, QC, Canada. (email: aditya.mahajan@mcgill.ca)\n\n    Yi Ouyang is with Preferred Networks America, Burlingame, CA, USA (email: ouyangyi@preferred-america.com)\n\n    The work of Aditya Mahajan was supported in part by the Innovation for Defence Excellence and Security (IDEaS) Program of the Canadian Department of National Defence through grant CFPMN2-30."
    },
    {
      "markdown": "show that by exploiting the structure of the network model, it is possible to design learning algorithms for large-scale networkcoupled subsystems where the regret does not grow superlinearly in the number of subsystems. In particular, we utilize a spectral decomposition technique, recently proposed in [24], to decompose the large-scale system into $L$ decoupled systems, where $L$ is the rank of the coupling matrix corresponding to the underlying network. Using the decoupled systems, we propose a Thompson sampling based algorithm with $\\tilde{\\mathcal{O}}\\left(n\\right.$ $\\left.d_{x}^{0.5}\\left(d_{x}+d_{n}\\right) \\sqrt{T}\\right)$ regret bound.\na) Related work: Broadly speaking, three classes of lowregret learning algorithms have been proposed for LQG systems: certainty equivalence (CE) based algorithms, optimism in the face of uncertainty (OFU) based algorithms, and Thompson sampling (TS) based algorithms. CE is a classical adaptive control algorithm [8]. Recent papers [16]-[20] have established near optimal high probability bounds on regret for CE-based algorithms. OFU-based algorithms are inspired by the OFU principle for multi-armed bandits [25]. Starting with the work of [11], [12], most of the papers following the OFU approach [13]-[15] also provide similar high probability regret bounds. TS-based algorithms are inspired by TS algorithm for multi-armed bandits [26]. Most papers following this approach [20]-[23] establish bounds on expected Bayesian regret of similar near-optimal orders. As argued earlier, most of these papers show that the regret scales super-linearly with the number of subsystems and are, therefore, of limited value for large-scale systems.\n\nThere is an emerging literature on learning algorithms for networked systems both for LQG models [27]-[29] and MDP models [30]-[33]. The papers on LQG models propose distributed value- or policy-based learning algorithms and analyze their convergence properties, but they do not characterize their regret. Some of the papers on MDP models [32], [33] do characterize regret bounds for OFU and TS-based learning algorithms but these bounds are not directly applicable to the LQG model considered in this paper.\n\nAn important special class of network-coupled systems is mean-field coupled subsystems [34], [35]. There has been considerable interest in reinforcement learning for mean-field models [36]-[38], but most of the literature does not consider regret. The basic mean-field coupled model can be viewed as a special case of the network-coupled subsystems considered in this paper (see Sec. VI-A). In a preliminary version of this paper [39], we proposed a TS-based algorithm for mean-field coupled subsystems which has a $\\tilde{\\mathcal{O}}((1+1 / n) \\sqrt{T})$ regret per subsystem. The current paper extends the TS-based algorithm to general network-coupled subsystems and establishes scalable regret bounds for arbitrarily coupled networks.\nb) Organization: The rest of the paper is organized as follows. In Section II, we introduce the model of networkcoupled subsystems. In Section III, we summarize the spectral decomposition idea and the resulting scalable method for synthesizing optimal control strategy when the model parameters are known. Then, in Section IV, we consider the learning problem for unknown network-coupled subsystems and present a TS-based learning algorithm with scalable regret bound. We subsequently provide regret analysis in Section V and numerical\nexperiments in Section VI. We conclude in Section VII.\nc) Notation: The notation $A=\\left[a^{i j}\\right]$ means that $a^{i j}$ is the $(i, j)$ th element of the matrix $A$. For a matrix $A, A^{\\boldsymbol{\\top}}$ denotes its transpose. Given matrices (or vectors) $A_{1}, \\ldots, A_{n}$ with the same number of rows, $\\left[A_{1}, \\ldots, A_{n}\\right]$ denotes the matrix formed by horizontal concatenation. For a random vector $v, \\operatorname{var}(v)$ denotes its covariance matrix. The notation $\\mathcal{N}(\\mu, \\Sigma)$ denotes the multivariate Gaussian distribution with mean vector $\\mu$ and covariance matrix $\\Sigma$.\n\nFor stabilizable $(A, B)$ and positive definite matrices $Q, R$, $\\operatorname{DARE}(A, B, Q, R)$ denotes the unique positive semidefinite solution of the discrete time algebraic Riccati equation (DARE), which is given as\n\n$$\nS=A^{\\top} S A-\\left(A^{\\top} S B\\right)\\left(R+B^{\\top} S B\\right)^{-1}\\left(B^{\\top} S A\\right)+Q\n$$\n\n## II. MODEL OF NETWORK-COUPlEd SUBSYSTEMS\n\nWe start by describing a minor variation of a model of network-coupled subsystems proposed in [24]. The model in [24] was described in continuous time. We translate the model and the results to discrete time.\n\n## A. System model\n\n1) Graph stucture: Consider a network consisting of $n$ subsystems/agents connected over an undirected weighted graph denoted by $\\mathcal{G}\\left(N, E, W_{\\mathcal{G}}\\right)$, where $N=\\{1, \\ldots, n\\}$ is the set of nodes, $E \\subseteq N \\times N$ is the set of edges, and $W_{\\mathcal{G}}=\\left[w^{i j}\\right] \\in \\mathbb{R}^{n \\times n}$ is the weighted adjacency matrix. Let $M=\\left[m^{i j}\\right] \\in \\mathbb{R}^{n \\times n}$ be a symmetric coupling matrix corresponding to the underlying graph $\\mathcal{G}$. For instance, $M$ may represent the underlying adjacency matrix (i.e., $M=W_{\\mathcal{G}}$ ) or the underlying Laplacian matrix (i.e., $M=\\operatorname{diag}\\left(W_{\\mathcal{G}} \\mathbb{1}_{n}\\right)-$ $W_{\\mathcal{G}}$ ).\n2) State and dynamics: The states and control actions of agents take values in $\\mathbb{R}^{d_{x}}$ and $\\mathbb{R}^{d_{n}}$, respectively. For agent $i \\in N$, we use $x_{t}^{i} \\in \\mathbb{R}^{d_{x}}$ and $u_{t}^{i} \\in \\mathbb{R}^{d_{u}}$ to denote its state and control action at time $t$.\n\nThe system starts at a random initial state $x_{1}=\\left(x_{1}^{i}\\right)_{i \\in N}$, whose components are independent across agents. For agent $i$, the initial state $x_{1}^{i} \\sim \\mathcal{N}\\left(0, \\Xi_{1}^{i}\\right)$, and at any time $t \\geq 1$, the state evolves according to\n\n$$\nx_{t+1}^{i}=A x_{t}^{i}+B u_{t}^{i}+D x_{t}^{\\mathcal{G}, i}+E u_{t}^{\\mathcal{G}, i}+w_{t}^{i}\n$$\n\nwhere $x_{t}^{\\mathcal{G}, i}$ and $u_{t}^{\\mathcal{G}, i}$ are the locally perceived influence of the network on the state of agent $i$ and are given by\n\n$$\nx_{t}^{\\mathcal{G}, i}=\\sum_{j \\in N} m^{i j} x_{t}^{j} \\quad \\text { and } \\quad u_{t}^{\\mathcal{G}, i}=\\sum_{j \\in N} m^{i j} u_{t}^{j}\n$$\n\n$A, B, D, E$ are matrices of appropriate dimensions, and $\\left\\{w_{t}^{i}\\right\\}_{t \\geq 1}, i \\in N$, are i.i.d. zero-mean Gaussian processes which are independent of each other and the initial state. In particular, $w_{t}^{i} \\in \\mathbb{R}^{d_{x}}$ and $w_{t}^{i} \\sim \\mathcal{N}(0, W)$. We call $x_{t}^{\\mathcal{G}, i}$ and $u_{t}^{\\mathcal{G}, i}$ the network-field of the states and control actions at node $i$ at time $t$.\n\nThus, the next state of agent $i$ depends on its current local state and control action, the current network-field of the states and control actions of the system, and the current local noise."
    },
    {
      "markdown": "We follow the same atypical representation of the \"vectorized\" dynamics as used in [24]. Define $x_{t}$ and $u_{t}$ as the global state and control actions of the system:\n\n$$\nx_{t}=\\left[x_{t}^{1}, \\ldots, x_{t}^{n}\\right] \\quad \\text { and } \\quad u_{t}=\\left[u_{t}^{1}, \\ldots, u_{t}^{n}\\right]\n$$\n\nWe also define $w_{t}=\\left[w_{t}^{1}, \\ldots, w_{t}^{n}\\right]$. Similarly, define $x_{t}^{\\mathcal{G}}$ and $u_{t}^{\\mathcal{G}}$ as the global network field of states and actions:\n\n$$\nx_{t}^{\\mathcal{G}}=\\left[x_{t}^{\\mathcal{G}, 1}, \\ldots, x_{t}^{\\mathcal{G}, n}\\right] \\quad \\text { and } \\quad u_{t}^{\\mathcal{G}}=\\left[u_{t}^{\\mathcal{G}, 1}, \\ldots, u_{t}^{\\mathcal{G}, n}\\right]\n$$\n\nNote that $x_{t}, x_{t}^{\\mathcal{G}}, w_{t} \\in \\mathbb{R}^{d_{e} \\times n}$ and $u_{t}, u_{t}^{\\mathcal{G}} \\in \\mathbb{R}^{d_{u} \\times n}$ are matrices and not vectors. The global system dynamics may be written as:\n\n$$\nx_{t+1}=A x_{t}+B u_{t}+D x_{t}^{\\mathcal{G}}+E u_{t}^{\\mathcal{G}}+w_{t}\n$$\n\nFurthermore, we may write\n\n$$\nx_{t}^{\\mathcal{G}}=x_{t} M^{\\boldsymbol{\\top}}=x_{t} M \\quad \\text { and } \\quad u_{t}^{\\mathcal{G}}=u_{t} M^{\\boldsymbol{\\top}}=u_{t} M\n$$\n\n3) Per-step cost: At any time $t$ the system incurs a per-step cost given by\n\n$$\nc\\left(x_{t}, u_{t}\\right)=\\sum_{i \\in N} \\sum_{j \\in N}\\left[h_{x}^{i j}\\left(x_{t}^{i}\\right)^{\\top} Q\\left(x_{t}^{j}\\right)+h_{u}^{i j}\\left(u_{t}^{i}\\right)^{\\top} R\\left(u_{t}^{j}\\right)\\right]\n$$\n\nwhere $Q$ and $R$ are matrices of appropriate dimensions and $h_{x}^{i j}$ and $h_{u}^{i j}$ are real valued weights. Let $H_{x}=\\left[h_{x}^{i j}\\right]$ and $H_{u}=\\left[h_{u}^{i j}\\right]$. It is assumed that the weight matrices $H_{x}$ and $H_{u}$ are polynomials of $M$, i.e.,\n\n$$\nH_{x}=\\sum_{k=0}^{K_{x}} q_{k} M^{k} \\quad \\text { and } \\quad H_{u}=\\sum_{k=0}^{K_{u}} r_{k} M^{k}\n$$\n\nwhere $K_{x}$ and $K_{u}$ denote the degrees of the polynomials and $\\left\\{q_{k}\\right\\}_{k=0}^{K_{x}}$ and $\\left\\{r_{k}\\right\\}_{k=0}^{K_{u}}$ are real-valued coefficients.\n\nThe assumption that $H_{x}$ and $H_{u}$ are polynomials of $M$ captures the intuition that the per-step cost respects the graph structure. In the special case when $H_{x}=H_{u}=I$, the per-step cost is decoupled across agents. When $H_{x}=H_{u}=I+M$, the per-step cost captures a cross-coupling between one-hop neighbors. Similarly, when $H_{u}=I+M+M^{2}$, the per-step cost captures a cross-coupling between one- and two-hop neighbors. See [24] for more examples of special cases of the per-step cost defined above.\n\n## B. Assumptions on the model\n\nSince $M$ is real and symmetric, it has real eigenvalues. Let $L$ denote the rank of $M$ and $\\lambda^{1}, \\ldots, \\lambda^{L}$ denote the non-zero eigenvalues. For ease of notation, for $\\ell \\in 1, \\ldots, L$, define\n\n$$\nq^{\\ell}=\\sum_{k=0}^{K_{x}} q_{k}\\left(\\lambda^{\\ell}\\right)^{k} \\quad \\text { and } \\quad r^{\\ell}=\\sum_{k=0}^{K_{u}} r_{k}\\left(\\lambda^{\\ell}\\right)^{k}\n$$\n\nwhere $\\left\\{q_{k}\\right\\}_{k=0}^{K_{x}}$ and $\\left\\{r_{k}\\right\\}_{k=0}^{K_{u}}$ are the coefficients in (5). Furthermore, for $\\ell \\in\\{1, \\ldots, L\\}$, define:\n\n$$\nA^{\\ell}=A+\\lambda^{\\ell} D \\quad \\text { and } \\quad B^{\\ell}=B+\\lambda^{\\ell} E\n$$\n\nWe impose the following assumptions:\n(A1) The systems $(A, B)$ and $\\left\\{\\left(A^{\\ell}, B^{\\ell}\\right)\\right\\}_{\\ell=1}^{L}$ are stabilizable.\n(A2) The matrices $Q$ and $R$ are symmetric and positive definite.\n(A3) The parameters $q_{0}, r_{0},\\left\\{q^{\\ell}\\right\\}_{\\ell=1}^{L}$, and $\\left\\{r^{\\ell}\\right\\}_{\\ell=1}^{L}$ are strictly positive.\nAssumption (A1) is a standard assumption. Assumptions (A2) and (A3) ensure that the per-step cost is strictly positive.\n\n## C. Admissible policies and performance criterion\n\nThere is a system operator who has access to the state and action histories of all agents and who selects the agents' control actions according to a deterministic or randomized (and potentially history-dependent) policy\n\n$$\nu_{t}=\\pi_{t}\\left(x_{1: t}, u_{1: t-1}\\right)\n$$\n\nLet $\\theta^{\\top}=[A, B, D, E]$ denote the parameters of the system dynamics. The performance of any policy $\\pi=\\left(\\pi_{1}, \\pi_{2}, \\ldots\\right)$ is measured by the long-term average cost given by\n\n$$\nJ(\\pi ; \\theta)=\\limsup _{T \\rightarrow \\infty} \\frac{1}{T} \\mathbb{E}^{\\pi}\\left[\\sum_{t=1}^{T} c\\left(x_{t}, u_{t}\\right)\\right]\n$$\n\nLet $J(\\theta)$ denote the minimum of $J(\\pi ; \\theta)$ over all policies.\nWe are interested in the setup where the graph coupling matix $M$, the cost coupling matrices $H_{x}$ and $H_{u}$, and the cost matrices $Q$ and $R$ are known but the system dynamics $\\theta$ are unknown and there is a prior distribution on $\\theta$. The Bayesian regret of a policy $\\pi$ operating for a horizon $T$ is defined as\n\n$$\nR(T ; \\pi):=\\mathbb{E}^{\\pi}\\left[\\sum_{t=1}^{T} c\\left(x_{t}, u_{t}\\right)-T J(\\theta)\\right]\n$$\n\nwhere the expectation is with respect to the prior on $\\theta$, the noise processes, the initial conditions, and the potential randomizations done by the policy $\\pi$.\n\n## III. BACKGROUND ON SPECTRAL DECOMPOSITION OF THE SYSTEM\n\nIn this section, we summarize the main results of [24], translated to the discrete-time model used in this paper.\n\nThe spectral decomposition described in [24] relies on the spectral factorization of the graph coupling matrix $M$. Since $M$ is a real and symmetric matrix with rank $L$, we can write it as\n\n$$\nM=\\sum_{\\ell=1}^{L} \\lambda^{\\ell} v^{\\ell}\\left(v^{\\ell}\\right)^{\\top}\n$$\n\nwhere $\\left(\\lambda^{1}, \\ldots, \\lambda^{L}\\right)$ are the non-zero eigenvalues of $M$ and $\\left(v^{1}, \\ldots, v^{L}\\right)$ are the corresponding orthonormal eigenvectors.\n\nWe now present the decomposition of the dynamics and the cost based on (9) as described in [24].\n\n## A. Spectral decomposition of the dynamics and per-step cost\n\nFor $\\ell \\in\\{1,2, \\ldots, L\\}$, define eigenstates and eigencontrols as\n\n$$\nx_{t}^{\\ell}=x_{t} v^{\\ell}\\left(v^{\\ell}\\right)^{\\top} \\quad \\text { and } \\quad u_{t}^{\\ell}=u_{t} v^{\\ell}\\left(v^{\\ell}\\right)^{\\top}\n$$"
    },
    {
      "markdown": "respectively. Furthermore, define auxiliary state and auxiliary control as\n\n$$\n\\bar{x}_{t}=x_{t}-\\sum_{\\ell=1}^{L} x_{t}^{\\ell} \\quad \\text { and } \\quad \\bar{u}_{t}=u_{t}-\\sum_{\\ell=1}^{L} u_{t}^{\\ell}\n$$\n\nrespectively. Similarly, define $w_{t}^{\\ell}=w_{t} v^{\\ell}\\left(v^{\\ell}\\right)^{\\top}$ and $\\bar{w}_{t}=w_{t}-$ $\\sum_{\\ell=1}^{L} w_{t}^{\\ell}$. Let $x_{t}^{\\ell, i}$ and $u_{t}^{\\ell, i}$ denote the $i$-th column of $x_{t}^{\\ell}$ and $u_{t}^{\\ell}$ respectively; thus we can write\n\n$$\nx_{t}^{\\ell}=\\left[x_{t}^{\\ell, 1}, \\ldots, x_{t}^{\\ell, n}\\right] \\quad \\text { and } \\quad u_{t}^{\\ell}=\\left[u_{t}^{\\ell, 1}, \\ldots, u_{t}^{\\ell, n}\\right]\n$$\n\nSimilar interpretations hold for $w_{t}^{\\ell, i}$ and $\\bar{w}_{t}^{i}$. Following [24, Lemma 2], we can show that for any $i \\in N$,\n\n$$\n\\operatorname{var}\\left(w_{t}^{\\ell, i}\\right)=\\left(v^{\\ell, i}\\right)^{2} W \\quad \\text { and } \\quad \\operatorname{var}\\left(\\bar{w}_{t}^{i}\\right)=\\left(\\bar{v}^{i}\\right)^{2} W\n$$\n\nwhere $\\left(\\bar{v}^{i}\\right)^{2}=1-\\sum_{\\ell=1}^{L}\\left(v^{\\ell, i}\\right)^{2}$. These covariances do not depend on time because the noise processes are i.i.d.\n\nUsing the same argument as in [24, Propositions 1 and 2], we can show the following.\nProposition 1 For each node $i \\in N$, the state and control action may be decomposed as\n\n$$\nx_{t}^{i}=\\bar{x}_{t}^{i}+\\sum_{\\ell=1}^{L} x_{t}^{\\ell, i} \\quad \\text { and } \\quad u_{t}^{i}=\\bar{u}_{t}^{i}+\\sum_{\\ell=1}^{L} u_{t}^{\\ell, i}\n$$\n\nwhere the dynamics of eigenstate $x_{t}^{\\ell, i}$ depend only on $u_{t}^{\\ell, i}$ and $w_{t}^{\\ell, i}$, and are given by\n\n$$\nx_{t+1}^{\\ell, i}=\\left(A+\\lambda^{\\ell} D\\right) x_{t}^{\\ell, i}+\\left(B+\\lambda^{\\ell} E\\right) u_{t}^{\\ell, i}+w_{t}^{\\ell, i}\n$$\n\nand the dynamics of the auxiliary state $\\bar{x}_{t}^{i}$ depend only on $\\bar{u}_{t}^{i}$ and $\\bar{w}_{t}^{i}$ and are given by\n\n$$\n\\bar{x}_{t+1}^{i}=A \\bar{x}_{t}^{i}+B \\bar{u}_{t}^{i}+\\bar{w}_{t}^{i}\n$$\n\nFurthermore, the per-step cost decomposes as follows:\n\n$$\nc\\left(x_{t}, u_{t}\\right)=\\sum_{i \\in N}\\left[q_{0} \\bar{c}\\left(\\bar{x}_{t}^{i}, \\bar{u}_{t}^{i}\\right)+\\sum_{\\ell=1}^{L} q^{\\ell} c^{\\ell}\\left(x_{t}^{\\ell, i}, u_{t}^{\\ell, i}\\right)\\right]\n$$\n\nwhere ${ }^{1}$\n\n$$\n\\begin{aligned}\n\\bar{c}\\left(\\bar{x}_{t}^{i}, \\bar{u}_{t}^{i}\\right) & =\\left(\\bar{x}_{t}^{i}\\right)^{\\top} Q \\bar{x}_{t}^{i}+\\frac{r_{0}}{q_{0}}\\left(\\bar{u}_{t}^{i}\\right)^{\\top} R \\bar{u}_{t}^{i} \\\\\nc^{\\ell}\\left(x_{t}^{\\ell, i}, u_{t}^{\\ell, i}\\right) & =\\left(x_{t}^{\\ell, i}\\right)^{\\top} Q x_{t}^{\\ell, i}+\\frac{c^{\\ell}}{q^{\\ell}}\\left(u_{t}^{\\ell, i}\\right)^{\\top} R u_{t}^{\\ell, i}\n\\end{aligned}\n$$\n\n## B. Planning solution for network-coupled subsystems\n\nWe now present the main result of [24], which provides a scalable method to synthesize the optimal control policy when the system dynamics are known.\n\nBased on Proposition 1, we can view the overall system as the collection of the following subsystems:\n\n- Eigen-system $(\\ell, i), \\ell \\in\\{1, \\ldots, L\\}$ and $i \\in N$ with state $x_{t}^{\\ell, i}$, controls $u_{t}^{\\ell, i}$, dynamics (14), and per-step cost $q^{\\ell} c^{\\ell}\\left(x^{\\ell, i}, u^{\\ell, i}\\right)$.\n- Auxiliary system $i, i \\in N$, with state $\\bar{x}_{t}^{i}$, controls $\\bar{u}_{t}^{i}$, dynamics (15), and per-step cost $\\bar{q}_{0} \\bar{c}\\left(\\bar{x}_{t}^{i}, \\bar{u}_{t}^{i}\\right)$.\n${ }^{1}$ Recall that (A3) ensures that $q_{0}$ and $\\left\\{q^{\\ell}\\right\\}_{\\ell=1}^{L}$ are strictly positive.\n\nLet $\\left(\\theta^{\\ell}\\right)^{\\top}=\\left[A^{\\ell}, B^{\\ell}\\right]:=\\left[\\left(A+\\lambda^{\\ell} D\\right),\\left(B+\\lambda^{\\ell} E\\right)\\right], \\ell \\in$ $\\{1, \\ldots, L\\}$, and $\\bar{\\theta}^{\\top}=[A, B]$ denote the parameters of the dynamics of the eigen and auxiliary systems, respectively. Then, for any policy $\\pi=\\left(\\pi_{1}, \\pi_{2}, \\ldots\\right)$, the performance of the eigensystem $(\\ell, i), \\ell \\in\\{1, \\ldots, L\\}$ and $i \\in N$, is given by $q^{\\ell} J^{\\ell, i}\\left(\\pi ; \\theta^{\\ell}\\right)$, where\n\n$$\nJ^{\\ell, i}\\left(\\pi ; \\theta^{\\ell}\\right)=\\limsup _{T \\rightarrow \\infty} \\frac{1}{T} \\mathbb{E}^{\\pi}\\left[\\sum_{t=1}^{T} c\\left(x_{t}^{\\ell, i}, u_{t}^{\\ell, i}\\right)\\right]\n$$\n\nSimilarly, the performance of the auxiliary system $i, i \\in N$, is given by $\\bar{q}_{0} \\bar{J}^{i}(\\pi ; \\bar{\\theta})$, where\n\n$$\n\\bar{J}^{i}(\\pi ; \\bar{\\theta})=\\limsup _{T \\rightarrow \\infty} \\frac{1}{T} \\mathbb{E}^{\\pi}\\left[\\sum_{t=1}^{T} c\\left(\\bar{x}_{t}^{i}, \\bar{u}_{t}^{i}\\right)\\right]\n$$\n\nProposition 1 implies that the overall performance of policy $\\pi$ can be decomposed as\n\n$$\nJ(\\pi ; \\theta)=\\sum_{i \\in N} q_{0} \\bar{J}^{i}(\\pi ; \\bar{\\theta})+\\sum_{i \\in N} \\sum_{\\ell=1}^{L} q^{\\ell} J^{\\ell, i}\\left(\\pi ; \\theta^{\\ell}\\right)\n$$\n\nThe key intuition behind the result of [24] is as follows. By the certainty equivalence principle for LQ systems, we know that (when the system dynamics are known) the optimal control policy of a stochastic LQ system is the same as the optimal control policy of the corresponding deterministic LQ system where the noises $\\left\\{w_{t}^{i}\\right\\}_{t \\geq 1}$ are assumed to be zero. Note that when noises $\\left\\{w_{t}^{i}\\right\\}_{t \\geq 1}$ are zero, then the noises $\\left\\{w_{t}^{\\ell, i}\\right\\}_{t \\geq 1}$ and $\\left\\{\\bar{w}_{t}^{i}\\right\\}_{t \\geq 1}$ of the eigen- and auxiliary-systems are also zero. This, in turn, implies that the dynamics of all the eigen- and auxiliary systems are decoupled. These decoupled dynamics along with the cost decoupling in (17) imply that we can choose the controls $\\left\\{u_{t}^{\\ell, i}\\right\\}_{t \\geq 1}$ for the eigensystem $(\\ell, i), \\ell \\in$ $\\{1, \\ldots, L\\}$ and $i \\in N$, to minimize ${ }^{2} J^{\\ell, i}\\left(\\pi ; \\theta^{\\ell}\\right)$ and choose the controls $\\left\\{\\bar{u}_{t}^{i}\\right\\}_{t \\geq 1}$ for the auxiliary system $i, i \\in N$, to $\\operatorname{minimize}^{3} \\bar{J}^{i}(\\pi ; \\bar{\\theta})$. These optimization problems are standard optimal control problems. Therefore, similar to [24, Thoerem 3], we obtain the following result.\nTheorem 1 Let $\\bar{S}$ and $\\left\\{S^{\\ell}\\right\\}_{\\ell=1}^{L}$ be the solution of the following discrete time algebraic Riccati equations (DARE):\n\n$$\n\\bar{S}(\\bar{\\theta})=\\operatorname{DARE}\\left(A, B, Q, \\frac{r_{0}}{q_{0}} R\\right)\n$$\n\nand for $\\ell \\in\\{1, \\ldots, L\\}$,\n\n$$\nS^{\\ell}\\left(\\theta^{\\ell}\\right)=\\operatorname{DARE}\\left(A^{\\ell}, B^{\\ell}, Q, \\frac{r_{0}^{\\ell}}{q^{\\ell}} R\\right)\n$$\n\nDefine the gains:\n\n$$\n\\bar{G}(\\bar{\\theta})=-\\left((B)^{\\top} \\bar{S}(\\bar{\\theta}) B+\\frac{r_{0}}{q_{0}} R\\right)^{-1}(B)^{\\top} \\bar{S}(\\bar{\\theta}) A\n$$\n\nand for $\\ell \\in\\{1, \\ldots, L\\}$,\n\n$$\nG^{\\ell}\\left(\\theta^{\\ell}\\right)=-\\left(\\left(B^{\\ell}\\right)^{\\top} S^{\\ell}\\left(\\theta^{\\ell}\\right) B^{\\ell}+\\frac{r^{\\ell}}{q^{\\ell}} R\\right)^{-1}\\left(B^{\\ell}\\right)^{\\top} S^{\\ell}\\left(\\theta^{\\ell}\\right) A^{\\ell}\n$$\n\n[^0]\n[^0]:    ${ }^{2}$ The cost of the eigensystem $(\\ell, i)$ is $q^{\\ell} J^{\\ell, i}\\left(\\pi ; \\theta^{\\ell}\\right)$. From (A3), we know that $q^{\\ell}$ is positive. Therefore, minimizing $q^{\\ell} J^{\\ell, i}\\left(\\pi ; \\theta^{\\ell}\\right)$ is the same as minimizing $J^{\\ell, i}\\left(\\pi ; \\theta^{\\ell}\\right)$.\n    ${ }^{3}$ The same remark as footnote 2 applies here."
    },
    {
      "markdown": "Then, under assumptions (A1)-(A3), the policy\n\n$$\nu_{t}^{i}=\\bar{G}(\\bar{\\theta}) \\bar{x}_{t}^{i}+\\sum_{\\ell=1}^{L} G^{\\ell}\\left(\\theta^{\\ell}\\right) x_{t}^{\\ell, i}\n$$\n\nminimizes the long-term average cost in (7) over all admissible policies. Furthermore, the optimal performance is given by\n\n$$\nJ(\\theta)=\\sum_{i \\in N} q_{0} \\bar{J}^{i}(\\bar{\\theta})+\\sum_{i \\in N} \\sum_{\\ell=1}^{L} q^{\\ell} J^{\\ell, i}\\left(\\theta^{\\ell}\\right)\n$$\n\nwhere\n\n$$\n\\bar{J}^{i}(\\bar{\\theta})=\\left(\\bar{v}^{i}\\right)^{2} \\operatorname{Tr}(W \\bar{S})\n$$\n\nand for $\\ell \\in\\{1, \\ldots, L\\}$,\n\n$$\nJ^{\\ell, i}\\left(\\theta^{\\ell}\\right)=\\left(v^{\\ell, i}\\right)^{2} \\operatorname{Tr}\\left(W S^{\\ell}\\right)\n$$\n\n## IV. LEARNING FOR NETWORK-COUPLED SUBSYSTEMS\n\nFor the ease of notation, we define $z_{t}^{\\ell, i}=\\operatorname{vec}\\left(x_{t}^{\\ell, i}, u_{t}^{\\ell, i}\\right)$ and $\\bar{z}_{t}^{i}=\\operatorname{vec}\\left(\\bar{x}_{t}^{i}, \\bar{u}_{t}^{i}\\right)$. Then, we can write the dynamics (14), (15) of the eigen and the auxiliary systems as\n\n$$\n\\begin{array}{ll}\nx_{t+1}^{\\ell, i}=\\left(\\theta^{\\ell}\\right)^{\\top} z_{t}^{\\ell, i}+w_{t}^{\\ell, i}, & \\forall i \\in N, \\forall \\ell \\in\\{1, \\ldots, L\\} \\\\\n\\bar{x}_{t+1}^{i}=(\\bar{\\theta})^{\\top} \\bar{z}_{t}^{i}+\\bar{w}_{t}^{i}, & \\forall i \\in N\n\\end{array}\n$$\n\n## A. Simplifying assumptions\n\nWe impose the following assumptions to simplify the description of the algorithm and the regret analysis.\n(A4) The noise covariance $W$ is a scaled identity matrix given by $\\sigma_{w}^{2} I$.\n(A5) For each $i \\in N, \\bar{v}^{i} \\neq 0$.\nAssumption (A4) is commonly made in most of the literature on regret analysis of LQG systems. An implication of (A4) is that $\\operatorname{var}\\left(\\bar{w}_{t}^{i}\\right)=\\left(\\bar{\\sigma}^{i}\\right)^{2} I$ and $\\operatorname{var}\\left(w_{t}^{\\ell, i}\\right)=\\left(\\sigma^{\\ell, i}\\right)^{2} I$, where\n\n$$\n\\left(\\bar{\\sigma}^{i}\\right)^{2}=\\left(\\bar{v}^{i}\\right)^{2} \\sigma_{w}^{2} \\quad \\text { and } \\quad\\left(\\sigma^{\\ell, i}\\right)^{2}=\\left(v^{\\ell, i}\\right)^{2} \\sigma_{w}^{2}\n$$\n\nAssumption (A5) is made to rule out the case where the dynamics of some of the auxiliary systems are deterministic.\n\n## B. Prior and posterior beliefs:\n\nWe assume that the unknown parameters $\\tilde{\\theta}$ and $\\left\\{\\theta^{\\ell}\\right\\}_{\\ell=1}^{L}$ lie in compact subsets $\\bar{\\Theta}$ and $\\left\\{\\Theta^{\\ell}\\right\\}_{\\ell=1}^{L}$ of $\\mathbb{R}^{\\left(d_{e}+d_{u}\\right) \\times d_{e}}$. Let $\\tilde{\\theta}^{k}$ denote the $k$-th column of $\\tilde{\\theta}$. Thus $\\tilde{\\theta}=\\left[\\tilde{\\theta}^{1}, \\ldots, \\tilde{\\theta}^{d_{e}}\\right]$. Similarly, let $\\theta^{\\ell, k}$ denote the $k$-th column of $\\theta^{\\ell}$. Thus, $\\theta^{\\ell}=\\left[\\theta^{\\ell, 1}, \\ldots, \\theta^{\\ell, d_{e}}\\right]$. We use $p\\left.\\right|_{\\Theta}$ to denote the restriction of probability distribution $p$ on the set $\\Theta$.\n\nWe assume that $\\tilde{\\theta}$ and $\\left\\{\\theta^{\\ell}\\right\\}_{\\ell=1}^{L}$ are random variables that are independent of the initial states and the noise processes. Furthermore, we assume that the priors $\\bar{p}_{1}$ and $\\left\\{p_{1}^{\\ell}\\right\\}_{\\ell=1}^{L}$ on $\\tilde{\\theta}$ and $\\left\\{\\theta^{\\ell}\\right\\}_{\\ell=1}^{L}$, respectively, satisfy the following:\n(A6) $\\bar{p}_{1}$ is given as:\n\n$$\n\\bar{p}_{1}(\\tilde{\\theta})=\\left.\\left[\\prod_{k=1}^{d_{e}} \\bar{\\xi}_{1}^{k}\\left(\\tilde{\\theta}^{k}\\right)\\right]\\right|_{\\tilde{\\Theta}}\n$$\n\nwhere for $k \\in\\left\\{1, \\ldots, d_{x}\\right\\}, \\bar{\\xi}_{1}^{k}=\\mathcal{N}\\left(\\bar{\\mu}_{1}^{k}, \\bar{\\Sigma}_{1}\\right)$ with mean $\\bar{\\mu}_{1}^{k} \\in \\mathbb{R}^{d_{e}+d_{u}}$ and positive-definite covariance $\\bar{\\Sigma}_{1} \\in \\mathbb{R}^{\\left(d_{e}+d_{u}\\right) \\times\\left(d_{e}+d_{u}\\right)}$.\n(A7) For each $\\ell \\in\\{1, \\ldots, L\\}$, $p_{1}^{\\ell}$ is given as:\n\n$$\np_{1}^{\\ell}\\left(\\theta^{\\ell}\\right)=\\left.\\left[\\prod_{k=1}^{d_{e}} \\xi_{1}^{\\ell, k}\\left(\\theta^{\\ell, k}\\right)\\right]\\right|_{\\Theta^{\\ell}}\n$$\n\nwhere for $k \\in\\left\\{1, \\ldots, d_{x}\\right\\}, \\xi_{1}^{\\ell, k}=\\mathcal{N}\\left(\\mu_{1}^{\\ell, k}, \\Sigma_{1}^{\\ell}\\right)$ with mean $\\mu_{1}^{\\ell, k} \\in \\mathbb{R}^{\\left(d_{e}+d_{u}\\right)}$ and positive-definite covariance $\\Sigma_{1}^{l} \\in \\mathbb{R}^{\\left(d_{e}+d_{u}\\right) \\times\\left(d_{e}+d_{u}\\right)}$.\nThese assumptions are similar to the assumptions on the prior in the recent literature on Thompson sampling for LQ systems [21], [22].\nOur learning algorithm (and TS-based algorithms in general) keeps track of a posterior distribution on the unknown parameters based on observed data. Motivated by the nature of the planning solution (see Theorem 1), we maintain separate posterior distributions on $\\tilde{\\theta}$ and $\\left\\{\\theta^{\\ell}\\right\\}_{\\ell=1}^{L}$. For each $\\ell$, we select an subsystem $i_{s}^{\\ell}$ such that the $i_{s}^{\\ell}$-th component of the eigenvector $v^{\\ell}$ is non-zero (i.e. $v^{\\ell, i_{s}^{\\ell}} \\neq 0$ ). At time $t$, we maintain a posterior distribution $p_{t}^{\\ell}$ on $\\theta^{\\ell}$ based on the corresponding eigen state and action history of the $i_{s}^{\\ell}$-th subsystem. In other words, for any Borel subset $B$ of $\\mathbb{R}^{\\left(d_{e}+d_{u}\\right) \\times d_{e}}, p_{t}^{\\ell}(B)$ gives the following conditional probability\n\n$$\np_{t}^{\\ell}(B)=\\mathbb{P}\\left(\\theta^{\\ell} \\in B \\mid x_{1: t}^{\\ell, i_{s}^{\\ell}}, u_{1: t-1}^{\\ell, i_{s}^{\\ell}}\\right)\n$$\n\nWe maintain a separate posterior distribution $\\bar{p}_{t}$ on $\\tilde{\\theta}$ as follows. At each time $t>1$, we select an subsystem $j_{t-1}=$ $\\arg \\max _{i \\in N} \\bar{z}_{t-1}^{\\top} \\bar{\\Sigma}_{t-1} \\bar{z}_{t-1}^{i} /\\left(\\bar{\\sigma}_{t}^{i}\\right)^{2}$, where $\\bar{\\Sigma}_{t-1}$ is a covariance matrix defined recursively in Lemma 1 below. Then, for any Borel subset $B$ of $\\mathbb{R}^{\\left(d_{e}+d_{u}\\right) \\times d_{e}}$,\n\n$$\n\\bar{p}_{t}(B)=\\mathbb{P}\\left(\\bar{\\theta} \\in B \\mid\\left\\{\\bar{x}_{s}^{j_{s}}, \\bar{u}_{s}^{j_{s}}, \\bar{x}_{s+1}^{j_{s}}\\right\\}_{1 \\leq s<t}\\right\\})\n$$\n\nSee [39] for a discussion on the rule to select $j_{t-1}$.\nLemma 1 The posterior distributions $p_{t}^{\\ell}, \\ell \\in\\{1,2, \\ldots, L\\}$, and $\\bar{p}_{t}$ are given as follows:\n\n1) $p_{1}^{\\ell}$ is given by Assumption (A7) and for any $t \\geq 1$,\n\n$$\np_{t+1}^{\\ell}\\left(\\theta^{\\ell}\\right)=\\left.\\left[\\prod_{k=1}^{d_{e}} \\xi_{t+1}^{\\ell, k}\\left(\\theta^{\\ell, k}\\right)\\right]\\right|_{\\Theta^{\\ell}}\n$$\n\nwhere for $k \\in\\left\\{1, \\ldots, d_{x}\\right\\}, \\xi_{t+1}^{\\ell, k}=\\mathcal{N}\\left(\\mu_{t+1}^{\\ell, k}, \\Sigma_{t+1}^{\\ell}\\right)$, and\n\n$$\n\\begin{aligned}\n\\mu_{t+1}^{\\ell} & =\\mu_{t}^{\\ell}+\\frac{\\sum_{t=1}^{\\ell, i_{t}^{\\ell}}\\left(x_{t+1}^{\\ell, i_{t}^{\\ell}}-\\left(\\mu_{t}^{\\ell}\\right)^{\\top} z_{t}^{\\ell, i_{t}^{\\ell}}\\right)^{\\top}}{\\left(\\sigma^{\\ell, i_{t}^{\\ell}}\\right)^{2}+\\left(z_{t}^{\\ell, i_{t}^{\\ell}}\\right)^{\\top} \\Sigma_{t}^{\\ell} z_{t}^{\\ell, i_{t}^{\\ell}}}, \\\\\n\\left(\\Sigma_{t+1}^{\\ell}\\right)^{-1} & =\\left(\\Sigma_{t}^{\\ell}\\right)^{-1}+\\frac{1}{\\left(\\sigma^{\\ell, i_{t}^{\\ell}}\\right)^{2}} z_{t}^{\\ell, i_{t}^{\\ell}}\\left(z_{t}^{\\ell, i_{t}^{\\ell}}\\right)^{\\top}\n\\end{aligned}\n$$\n\nwhere, for each $t, \\mu_{t}^{\\ell}$ denotes the matrix $\\left[\\mu_{t}^{\\ell, 1}, \\ldots, \\mu_{t}^{\\ell, d_{e}}\\right]$.\n2) $\\bar{p}_{1}$ is given by Assumption (A6) and for any $t \\geq 1$,\n\n$$\n\\bar{p}_{t+1}(\\bar{\\theta})=\\left.\\left[\\prod_{k=1}^{d_{e}} \\bar{\\xi}_{t+1}^{k}\\left(\\bar{\\theta}^{k}\\right)\\right]\\right|_{\\tilde{\\Theta}}\n$$\n\nwhere for $k \\in\\left\\{1, \\ldots, d_{x}\\right\\}, \\bar{\\xi}_{t+1}^{k}=\\mathcal{N}\\left(\\bar{\\mu}_{t+1}^{k}, \\bar{\\Sigma}_{t+1}\\right)$, and\n\n$$\n\\begin{aligned}\n\\bar{\\mu}_{t+1} & =\\bar{\\mu}_{t}+\\frac{\\bar{\\Sigma}_{t} \\bar{z}_{t}^{j_{t}}\\left(\\bar{x}_{t+1}^{j_{t}}-\\left(\\bar{\\mu}_{t}\\right)^{\\top} \\bar{z}_{t}^{j_{t}}\\right)^{\\top}}{\\left(\\bar{\\sigma}^{j_{t}}\\right)^{2}+\\left(\\bar{z}_{t}^{j_{t}}\\right)^{\\top} \\bar{\\Sigma}_{t} \\bar{z}_{t}^{j_{t}}} \\\\\n\\left(\\bar{\\Sigma}_{t+1}\\right)^{-1} & =\\left(\\bar{\\Sigma}_{t}\\right)^{-1}+\\frac{1}{\\left(\\bar{\\sigma}^{j_{t}}\\right)^{2}} \\bar{z}_{t}^{j_{t}}\\left(\\bar{z}_{t}^{j_{t}}\\right)^{\\top}\n\\end{aligned}\n$$"
    },
    {
      "markdown": "where, for each $t, \\bar{\\mu}_{t}$ denotes the matrix $\\left[\\bar{\\mu}_{t}^{1}, \\ldots, \\bar{\\mu}_{t}^{d_{x}}\\right]$.\nProof Note that the dynamics of $x_{t}^{\\ell, i_{x}^{t}}$ and $\\bar{x}_{t}^{i}$ in (24) are linear and the noises $w_{t}^{\\ell, i_{x}^{t}}$ and $\\bar{w}_{t}^{i}$ are Gaussian. Therefore, the result follows from standard results in Gaussian linear regression [40].\n\n## C. The Thompson sampling algorithm:\n\nWe propose a Thompson sampling based algorithm called Net-TSDE which is inspired by the TSDE (Thompson sampling with dynamic episodes) algorithm proposed in [21], [22] and the structure of the optimal planning solution described in Sec. III-B. The Thompson sampling part of our algorithm is modeled after the modification of TSDE presented in [41].\nThe Net-TSDE algorithm consists of a coordinator $\\mathcal{C}$ and $|L|+1$ actors: an auxiliary actor $\\overline{\\mathcal{A}}$ and an eigen actor $\\mathcal{A}^{\\ell}$ for each $\\ell \\in\\{1,2, \\ldots, L\\}$. These actors are described below and the whole algorithm is presented in Algorithm 1.\n\n- At each time, the coordinator $\\mathcal{C}$ observes the current global state $x_{t}$, computes the eigenstates $\\left\\{x_{t}^{\\ell}\\right\\}_{t=1}^{L}$ and the auxiliary states $\\bar{x}_{t}$, and sends the eigenstate $x_{t}^{\\ell}$ to the eigen actor $\\mathcal{A}^{\\ell}, \\ell \\in\\{1, \\ldots, L\\}$, and sends the auxiliary state $\\bar{x}_{t}$ to the auxiliary actor $\\overline{\\mathcal{A}}$. The eigen actor $\\mathcal{A}^{\\ell}, \\ell \\in\\{1, \\ldots, L\\}$, computes the eigencontrol $u_{t}^{\\ell}$ and the auxiliary actor $\\overline{\\mathcal{A}}$ computes the auxiliary control $\\bar{u}_{t}$ (as per the details presented below) and both send their computed controls back to the coordinator $\\mathcal{C}$. The coordinator then computes and executes the control action $u_{t}^{i}=\\sum_{\\ell=1}^{L} u_{t}^{\\ell, i}+\\bar{u}_{t}^{i}$ for each subsystem $i \\in N$.\n- The eigen actor $\\mathcal{A}^{\\ell}, \\ell \\in\\{1, \\ldots, L\\}$, maintains the posterior $p_{t}^{\\ell}$ on $\\theta^{\\ell}$ according to (28). The actor works in episodes of dynamic length. Let $t_{k}^{\\ell}$ and $T_{k}^{\\ell}$ denote the starting time and the length of episode $k$, respectively. Each episode is of a minimum length $T_{\\min }^{\\ell}+1$, where $T_{\\min }^{\\ell}$ is chosen as described in [41]. Episode $k$ ends if the determinant of covariance $\\Sigma_{t}^{\\ell}$ falls below half of its value at time $t_{k}^{\\ell}$ (i.e., $\\operatorname{det}\\left(\\Sigma_{t}^{\\ell}\\right)<\\frac{1}{2} \\operatorname{det} \\Sigma_{t_{k}^{\\ell}}$ ) or if the length of the episode is one more than the length of the previous episode (i.e., $t-t_{k}^{\\ell}>T_{k-1}^{\\ell}$ ). Thus,\n\n$$\nt_{k+1}^{\\ell}=\\min \\left\\{t>t_{k}^{\\ell}+T_{\\min }^{\\ell} \\left\\lvert\\, \\begin{array}{l}\nt-t_{k}^{\\ell}>T_{k-1}^{\\ell} \\text { or } \\\\\n\\operatorname{det} \\Sigma_{t}^{\\ell}<\\frac{1}{2} \\operatorname{det} \\Sigma_{t_{k}^{\\ell}}\n\\end{array}\\right.\\right\\}\n$$\n\nAt the beginning of episode $k$, the eigen actor $\\mathcal{A}^{\\ell}$ samples a parameter $\\theta_{k}^{\\ell}$ according to the posterior distribution $p_{t_{k}^{\\ell}}^{\\ell}$. During episode $k$, the eigen actor $\\mathcal{A}^{\\ell}$ generates the eigen controls using the sampled parameter $\\theta_{k}^{\\ell}$, i.e., $u_{t}^{\\ell}=$ $G^{\\ell}\\left(\\theta_{k}^{\\ell}\\right) x_{t}^{\\ell}$.\n\n- The auxiliary actor $\\overline{\\mathcal{A}}$ is similar to the eigen actor. Actor $\\overline{\\mathcal{A}}$ maintains the posterior $\\bar{p}_{t}$ on $\\bar{\\theta}$ according to (29). The actor works in episodes of dynamic length. The episodes of the auxiliary actor $\\overline{\\mathcal{A}}$ and the eigen actors $\\mathcal{A}^{\\ell}, \\ell \\in\\{1,2, \\ldots, L\\}$, are separate from each other. ${ }^{4}$ Let $\\bar{t}_{k}$ and $\\bar{T}_{k}$ denote the starting time and the length of episode $k$, respectively. Each episode is of a minimum length $\\bar{T}_{\\min }+1$, where $\\bar{T}_{\\min }$ is chosen as described in [41].\n${ }^{4}$ The episode count $k$ is used as a local variable for each actor.\n\n\n## Algorithm 1 Net-TSDE\n\n1: initialize eigen actor: $\\Theta^{\\ell},\\left(\\mu_{1}^{\\ell}, \\Sigma_{1}^{\\ell}\\right), t_{0}^{\\ell}=-T_{\\min }, T_{-1}^{\\ell}=$ $T_{\\min }, k=0, \\theta_{k}^{\\ell}=0$\n2: initialize auxiliary actor: $\\bar{\\Theta},\\left(\\bar{\\mu}_{1}, \\bar{\\Sigma}_{1}\\right), \\bar{t}_{0}=-T_{\\min }$, $\\bar{T}_{-1}=T_{\\min }, k=0, \\bar{\\theta}_{k}=0$.\n3: for $t=1,2, \\ldots$ do\n4: observe $x_{t}$\n5: compute $\\left\\{x_{t}^{\\ell}\\right\\}_{\\ell=1}^{L}$ and $\\bar{x}_{t}$ using (10) and (11).\n6: for $\\ell=1,2, \\ldots, L$ do\n$u_{t}^{\\ell} \\leftarrow \\operatorname{EIGEN}-\\operatorname{ACTOR}\\left(x_{t}^{\\ell}\\right)$\n8: $\\bar{u}_{t} \\leftarrow$ AUXILIARY-ACTOR $\\left(\\bar{x}_{t}\\right)$\n9: for $i \\in N$ do\n10: Subsystem $i$ applies control $u_{t}^{i}=u_{t}^{\\ell, i}+\\bar{u}_{t}^{i}$\n11: function $\\operatorname{EIGEN}-\\operatorname{ACTOR}\\left(x_{t}^{\\ell}\\right)$\nglobal var $t$\nUpdate $p_{t}^{\\ell}$ according (28)\nif $\\left(t-t_{k}^{\\ell}>T_{\\min }\\right)$ and\n$\\left(\\left(t-t_{k}^{\\ell}>T_{k-1}^{\\ell}\\right)\\right.$ or $\\left(\\operatorname{det} \\Sigma_{t}^{\\ell}<\\frac{1}{2} \\operatorname{det} \\Sigma_{t_{k}^{\\ell}}\\right)$ )\nthen\n$T_{k}^{\\ell} \\leftarrow t-t_{k}^{\\ell}, k \\leftarrow k+1, t_{k}^{\\ell} \\leftarrow t$\nsample $\\theta_{k}^{\\ell} \\sim p_{t}^{\\ell}$\nreturn $G^{\\ell}\\left(\\theta_{k}^{\\ell}\\right) x_{t}^{\\ell}$\n\n1: function AUXILIARY-ACTOR $\\left(\\bar{x}_{t}\\right)$\nglobal var $t$\nUpdate $\\bar{p}_{t}$ according (29)\nif $\\left(t-\\bar{t}_{k}>T_{\\min }\\right)$ and\n$\\left(\\left(t-\\bar{t}_{k}>\\bar{T}_{k-1}\\right)\\right.$ or $\\left(\\operatorname{det} \\bar{\\Sigma}_{t}<\\frac{1}{2} \\operatorname{det} \\bar{\\Sigma}_{t_{k}^{\\ell}}\\right)$ )\nthen\n$\\bar{T}_{k} \\leftarrow t-\\bar{t}_{k}, k \\leftarrow k+1, \\bar{t}_{k} \\leftarrow t$\nsample $\\bar{\\theta}_{k} \\sim \\bar{p}_{t}$\nreturn $\\bar{G}\\left(\\bar{\\theta}_{k}\\right) \\bar{x}_{t}$\n\nThe termination condition for each episode is similar to that of the eigen actor $\\mathcal{A}^{\\ell}$. In particular,\n\n$$\n\\bar{t}_{k+1}=\\min \\left\\{t>\\bar{t}_{k}+\\bar{T}_{\\min } \\left\\lvert\\, \\begin{array}{l}\nt-\\bar{t}_{k}>\\bar{T}_{k-1} \\text { or } \\\\\n\\operatorname{det} \\bar{\\Sigma}_{t}<\\frac{1}{2} \\operatorname{det} \\bar{\\Sigma}_{\\bar{t}_{k}}\n\\end{array}\\right.\\right\\}\n$$\n\nAt the beginning of episode $k$, the auxillary actor $\\overline{\\mathcal{A}}$ samples a parameter $\\bar{\\theta}_{k}$ from the posterior distribution $\\bar{p}_{\\bar{t}_{k}}$. During episode $k$, the auxiliary actor $\\overline{\\mathcal{A}}$ generates the auxiliary controls using the the sampled parameter $\\bar{\\theta}_{k}$, i.e., $\\bar{u}_{t}=\\bar{G}\\left(\\bar{\\theta}_{k}\\right) \\bar{x}_{t}$.\n\nNote that the algorithm does not depend on the horizon $T$.\n\n## D. Regret bounds:\n\nWe impose the following assumption to ensure that the closed loop dynamics of the eigenstates and the auxiliary states of each subsystem are stable.\n(A8) There exists a positive number $\\delta \\in(0,1)$ such that\n\n- for any $\\ell \\in\\{1,2, \\ldots, L\\}$ and $\\theta^{\\ell}, \\phi^{\\ell} \\in \\Theta^{\\ell}$ where $\\left(\\theta^{\\ell}\\right)^{\\top}=\\left[A_{\\theta^{\\ell}}^{\\ell}, B_{\\theta^{\\ell}}^{\\ell}\\right]$, we have\n\n$$\n\\rho\\left(A_{\\theta^{\\ell}}^{\\ell}+B_{\\theta^{\\ell}}^{\\ell} G^{\\ell}(\\phi^{\\ell})\\right) \\leq \\delta\n$$"
    },
    {
      "markdown": "- for any $\\bar{\\theta}, \\bar{\\phi} \\in \\bar{\\Theta}$, where $(\\bar{\\theta})^{\\top}=\\left[A_{\\bar{\\theta}}, B_{\\bar{\\theta}}\\right]$, we have\n\n$$\n\\rho\\left(A_{\\bar{\\theta}}+B_{\\bar{\\theta}} \\bar{G}(\\bar{\\phi})\\right) \\leq \\delta\n$$\n\nThis assumption is similar to an assumption made in [41] for TS for LQG systems. According to [42, Lemma 1] (also see [19, Theorem 11]), (A8) is satisfied if\n\n$$\n\\begin{aligned}\n\\Theta^{\\ell} & =\\left\\{\\theta^{\\ell} \\in \\mathbb{R}^{\\left(d_{x}+d_{u}\\right) \\times d_{x}}:\\left\\|\\theta^{\\ell}-\\theta_{o}^{\\ell}\\right\\| \\leq \\varepsilon^{\\ell}\\right\\} \\\\\n\\bar{\\Theta} & =\\left\\{\\bar{\\theta} \\in \\mathbb{R}^{\\left(d_{x}+d_{u}\\right) \\times d_{x}}:\\left\\|\\bar{\\theta}-\\bar{\\theta}_{o}\\right\\| \\leq \\bar{\\varepsilon}\\right\\}\n\\end{aligned}\n$$\n\nwhere $\\theta^{\\ell}$ and $\\bar{\\theta}$ are stabilizable and $\\varepsilon^{\\ell}$ and $\\bar{\\varepsilon}$ are sufficiently small. In other words, the assumption holds when the true system is in a small neighborhood of a known nominal system. Such a the small neighborhood can be learned with high probability by running appropriate stabilizing procedures for finite time [19], [42].\nThe following result provides an upper bound on the regret of the proposed algorithm.\nTheorem 2 Under (A1)-(A8), the regret of Net-TSDE is upper bounded as follows:\n\n$$\nR(T ; \\text { Net-TSDE }) \\leq \\tilde{\\mathcal{O}}\\left(\\alpha^{\\mathcal{G}} \\sigma_{w}^{2} d_{x}^{0.5}\\left(d_{x}+d_{u}\\right) \\sqrt{T}\\right)\n$$\n\nwhere $\\alpha^{\\mathcal{G}}=\\sum_{\\ell=1}^{L} q^{\\ell}+q_{0}(n-L)$.\nSee Section V for proof.\nRemark 1 The term $\\alpha^{\\mathcal{G}}$ in the regret bound partially captures the impact of the network on the regret. The coefficients $r_{0}$ and $\\left\\{r^{\\ell}\\right\\}_{\\ell=1}^{L}$ depend on the network and also affect the regret but their dependence is hidden inside the $\\tilde{\\mathcal{O}}(\\cdot)$ notation. It is possible to explicitly characterize this dependence but doing so does not provide any additional insights. We discuss the impact of the network coupling on the regret in Section VI via some examples.\nRemark 2 The regret per subsystem is given by $R(T ;$ Net-TSDE $) / n$, which is proportional to\n\n$$\n\\alpha^{\\mathcal{G}} / n=\\mathcal{O}\\left(\\frac{L}{n}\\right)+\\mathcal{O}\\left(\\frac{n-1}{n}\\right)=\\mathcal{O}\\left(1+\\frac{L}{n}\\right)\n$$\n\nThus, the regret per-subsystem scales as $\\mathcal{O}(1+L / n)$. In contrast, for the standard TSDE algorithm [21], [22], [41], the regret per subsystem is proportional to $\\alpha^{\\mathcal{G}}(\\mathrm{TSDE}) / n=$ $\\mathcal{O}\\left(n^{0.5}\\right)$. This clearly illustrates the benefit of the proposed learning algorithm.\n\n## V. REGRET ANALYSIS\n\nFor the ease of notation, we simply use $R(T)$ instead of $R(T ;$ Net-TSDE $)$ in this section. Based on Proposition 1 and Theorem 1, the regret may be decomposed as\n\n$$\nR(T)=\\sum_{i \\in N} q_{0} \\bar{R}^{i}(T)+\\sum_{i \\in N} \\sum_{\\ell=1}^{L} q^{\\ell} R^{\\ell, i}(T)\n$$\n\nwhere\n\n$$\n\\bar{R}^{i}(T):=\\mathbb{E}\\left[\\sum_{t=1}^{T} \\bar{\\varepsilon}\\left(\\bar{x}_{t}^{i}, \\bar{u}_{t}^{i}\\right)-T \\bar{J}^{i}(\\bar{\\theta})\\right]\n$$\n\nand, for $\\ell \\in\\{1, \\ldots, L\\}$,\n\n$$\nR^{\\ell, i}(T):=\\mathbb{E}\\left[\\sum_{t=1}^{T} c^{\\ell}\\left(x_{t}^{\\ell, i}, u_{t}^{\\ell, i}\\right)-T J^{\\ell, i}\\left(\\theta^{\\ell}\\right)\\right]\n$$\n\nBased on the discussion at the beginning of Sec. III-B, $\\bar{q}_{0} \\bar{R}^{i}(T), i \\in N$, is the regret associated with auxiliary system $i$ and $q^{\\ell} R^{\\ell, i}(T), \\ell \\in\\{1, \\ldots, L\\}$ and $i \\in N$, is the regret associated with eigensystem $(\\ell, i)$. We now bound $\\bar{R}^{i}(T)$ and $R^{\\ell, i}(T)$ separately.\n\n## A. Bound on $R^{\\ell, i}(T)$\n\nFix $\\ell \\in\\{1, \\ldots, L\\}$. For the component $i_{*}^{\\ell}$, the Net-TSDE algorithm is exactly same as the variation of the TSDE algorithm of [22] presented in [41]. Therefore, from [41, Theorem 1], it follows that\n\n$$\nR^{\\ell, i_{*}^{\\ell}}(T) \\leq \\tilde{\\mathcal{O}}\\left(\\left(\\sigma^{\\ell, i_{*}^{\\ell}}\\right)^{2} d_{x}^{0.5}\\left(d_{x}+d_{u}\\right) \\sqrt{T}\\right)\\right)\n$$\n\nWe now show that the regret of other eigensystems $(\\ell, i)$ with $i \\neq i_{*}^{\\ell}$ also satisfies a similar bound.\nLemma 2 The regret of eigensystem $(\\ell, i), \\ell \\in\\{1, \\ldots, L\\}$ and $i \\in N$, is bounded as follows:\n\n$$\nR^{\\ell, i}(T) \\leq \\tilde{\\mathcal{O}}\\left(\\left(\\sigma^{\\ell, i}\\right)^{2} d_{x}^{0.5}\\left(d_{x}+d_{u}\\right) \\sqrt{T}\\right)\n$$\n\nProof Fix $\\ell \\in\\{1, \\ldots, L\\}$. Recall from (10) that $x_{t}^{\\ell}=$ $x_{t} v^{\\ell}\\left(v^{\\ell}\\right)^{\\top}$. Therefore, for any $i \\in N$,\n\n$$\nx_{t}^{\\ell, i}=x_{t} v^{\\ell} v^{\\ell, i}=v^{\\ell, i} x_{t} v^{\\ell}\n$$\n\nwhere the last equality follows because $v^{\\ell, i}$ is a scalar. Since we are using the same gain $G^{\\ell}\\left(\\theta_{k}^{\\ell}\\right)$ for all agents $i \\in N$, we have\n\n$$\nu_{t}^{\\ell, i}=G^{\\ell}\\left(\\theta_{k}^{\\ell}\\right) x_{t}^{\\ell, i}=v^{\\ell, i} G^{\\ell}\\left(\\theta_{k}^{\\ell}\\right) x_{t} v^{\\ell}\n$$\n\nThus, we can write (recall that $i_{*}^{\\ell}$ is chosen such that $v^{\\ell, i_{*}^{\\ell}} \\neq 0$ ),\n\n$$\nx_{t}^{\\ell, i}=\\left(\\frac{v^{\\ell, i}}{v^{\\ell, i_{*}^{\\ell}}}\\right) x_{t}^{\\ell, i_{*}^{\\ell}} \\text { and } u_{t}^{\\ell, i}=\\left(\\frac{v^{\\ell, i}}{v^{\\ell, i_{*}^{\\ell}}}\\right) u_{t}^{\\ell, i_{*}^{\\ell}}, \\quad \\forall i \\in N\n$$\n\nThus, for any $i \\in N$,\n\n$$\nc^{\\ell}\\left(x_{t}^{\\ell, i}, u_{t}^{\\ell, i}\\right)=\\left(\\frac{v^{\\ell, i}}{v^{\\ell, i_{*}^{\\ell}}}\\right)^{2} c^{\\ell}\\left(x_{t}^{\\ell, i_{*}^{\\ell}}, u_{t}^{\\ell, i_{*}^{\\ell}}\\right)\n$$\n\nMoreover, from (23), we have\n\n$$\nJ^{\\ell, i}\\left(\\theta^{\\ell}\\right)=\\left(\\frac{v^{\\ell, i}}{v^{\\ell, i_{*}^{\\ell}}}\\right)^{2} J^{\\ell, i_{*}^{\\ell}}\\left(\\theta^{\\ell}\\right)\n$$\n\nBy combining (33) and (34), we get\n\n$$\nR^{\\ell, i}(T)=\\left(\\frac{v^{\\ell, i}}{v^{\\ell, i_{*}^{\\ell}}}\\right)^{2} R^{\\ell, i_{*}^{\\ell}}(T)\n$$\n\nSubstituting the bound for $R^{\\ell, i_{*}^{\\ell}}(T)$ from (31) and observing that $\\left(v^{\\ell, i} / v^{\\ell, i_{*}^{\\ell}}\\right)^{2}=\\left(\\sigma^{\\ell, i} / \\sigma^{\\ell, i_{*}^{\\ell}}\\right)^{2}$ gives the result."
    },
    {
      "markdown": "## B. Bound on $\\check{R}^{i}(T)$\n\nThe update of the posterior $\\check{p}_{t}$ on $\\check{\\theta}$ does not depend on the history of states and actions of any fixed agent $i$. Therefore, we cannot directly use the argument presented in [41] to bound the regret $\\check{R}^{i}(T)$. We present a bound from first principles below.\n\nFor the ease of notation, for any episode $k$, we use $\\breve{G}_{k}$ and $\\breve{S}_{k}$ to denote $\\breve{G}\\left(\\breve{\\theta}_{k}\\right)$ and $\\breve{S}\\left(\\breve{\\theta}_{k}\\right)$ respectively. From LQ optimal control theory [43], we know that the average cost $\\breve{J}^{i}\\left(\\breve{\\theta}_{k}\\right)$ and the optimal policy $\\breve{u}_{t}^{i}=\\breve{G}_{k} \\breve{x}_{t}^{i}$ for the model parameter $\\breve{\\theta}_{k}$ satisfy the following Bellman equation:\n\n$$\n\\begin{aligned}\n\\breve{J}^{i}\\left(\\breve{\\theta}_{k}\\right)+\\left(\\breve{x}_{t}^{i}\\right)^{\\top} \\breve{S}_{k} \\breve{x}_{t}^{i} & =\\breve{c}\\left(\\breve{x}_{t}^{i}, \\breve{u}_{t}^{i}\\right) \\\\\n& +\\mathbb{E}\\left[\\left(\\breve{\\theta}_{k}^{\\top} \\breve{z}_{t}^{i}+\\breve{w}_{t}^{i}\\right)^{\\top} \\breve{S}_{k}\\left(\\breve{\\theta}_{k}^{\\top} \\breve{z}_{t}^{i}+\\breve{w}_{t}^{i}\\right)\\right]\n\\end{aligned}\n$$\n\nAdding and subtracting $\\mathbb{E}\\left[\\left(\\breve{x}_{t+1}^{i}\\right)^{\\top} \\breve{S}_{k} \\breve{x}_{t+1}^{i} \\mid \\breve{z}_{t}^{i}\\right]$ and noting that $\\breve{x}_{t+1}^{i}=\\breve{\\theta}^{\\top} \\breve{z}_{t}^{i}+\\breve{w}_{t}^{i}$, we get that\n\n$$\n\\begin{aligned}\n& \\breve{c}\\left(\\breve{x}_{t}^{i}, \\breve{u}_{t}^{i}\\right)=\\breve{J}^{i}\\left(\\breve{\\theta}_{k}\\right)+\\left(\\breve{x}_{t}^{i}\\right)^{\\top} \\breve{S}_{k} \\breve{x}_{t}^{i}-\\mathbb{E}\\left[\\left(\\breve{x}_{t+1}^{i}\\right)^{\\top} \\breve{S}_{k} \\breve{x}_{t+1}^{i} \\mid \\breve{z}_{t}^{i}\\right] \\\\\n& \\quad+\\left(\\breve{\\theta}^{\\top} \\breve{z}_{t}^{i}\\right)^{\\top} \\breve{S}_{k}\\left((\\breve{\\theta})^{\\top} \\breve{z}_{t}^{i}\\right)-\\left(\\breve{\\theta}_{k}^{\\top} \\breve{z}_{t}^{i}\\right)^{\\top} \\breve{S}_{k}\\left(\\left(\\breve{\\theta}_{k}\\right)^{\\top} \\breve{z}_{t}^{i}\\right)\n\\end{aligned}\n$$\n\nLet $\\breve{K}_{T}$ denote the number of episodes of the auxiliary actor until horizon $T$. For each $k>\\breve{K}_{T}$, we define $\\breve{t}_{k}$ to be $T+1$. Then, using (35), we have that for any agent $i$,\n\n$$\n\\begin{aligned}\n& \\breve{R}^{i}(T)=\\underbrace{\\mathbb{E}\\left[\\sum_{k=1}^{\\breve{K}_{T}} \\breve{T}_{k} \\breve{J}^{i}\\left(\\breve{\\theta}_{k}\\right)-T \\breve{J}^{i}(\\breve{\\theta})\\right]}_{\\text {regret due to sampling error }=\\breve{R}_{0}^{i}(T)} \\\\\n& +\\underbrace{\\mathbb{E}\\left[\\sum_{k=1}^{\\breve{K}_{T}} \\sum_{t=\\breve{t}_{k}}^{t_{k+1}-1}\\left[\\left(\\breve{x}_{t}^{i}\\right)^{\\top} \\breve{S}_{k} \\breve{x}_{t}^{i}-\\left(\\breve{x}_{t+1}^{i}\\right)^{\\top} \\breve{S}_{k} \\breve{x}_{t+1}^{i}\\right]\\right]}_{\\text {regret due to time-varying controller }} \\\\\n& +\\underbrace{\\mathbb{E}\\left[\\sum_{k=1}^{\\breve{K}_{T}} \\sum_{t=\\breve{t}_{k}}^{t_{k+1}-1}\\left[\\left(\\breve{\\theta}^{\\top} \\breve{z}_{t}^{i}\\right)^{\\top} \\breve{S}_{k}\\left((\\breve{\\theta})^{\\top} \\breve{z}_{t}^{i}\\right)\\right.\\right.}_{\\text {regret due to model mismatch }=\\breve{R}_{2}^{i}(T)} \\\\\n& \\text { Regret due to model mismatch }=\\breve{R}_{2}^{i}(T)\n\\end{aligned}\n$$\n\nLemma 3 The terms in (36) are bounded as follows:\n\n1) $\\breve{R}_{0}^{i}(T) \\leq \\breve{\\mathcal{O}}\\left(\\left(\\breve{\\sigma}^{i}\\right)^{2}\\left(d_{x}+d_{u}\\right)^{0.5} \\sqrt{T}\\right)$.\n2) $\\breve{R}_{1}^{i}(T) \\leq \\breve{\\mathcal{O}}\\left(\\left(\\breve{\\sigma}^{i}\\right)^{2}\\left(d_{x}+d_{u}\\right)^{0.5} \\sqrt{T}\\right)$.\n3) $\\breve{R}_{2}^{i}(T) \\leq \\breve{\\mathcal{O}}\\left(\\left(\\breve{\\sigma}^{i}\\right)^{2} d_{x}^{0.5}\\left(d_{x}+d_{u}\\right) \\sqrt{T}\\right)$.\n\nCombining these three, we get that\n\n$$\n\\breve{R}^{i}(T) \\leq \\breve{\\mathcal{O}}\\left(\\left(\\breve{\\sigma}^{i}\\right)^{2} d_{x}^{0.5}\\left(d_{x}+d_{u}\\right) \\sqrt{T}\\right)\n$$\n\nSee Appendix for the proof.\n\n## C. Proof of Theorem 2\n\nFor ease of notation, let $R^{*}=\\breve{\\mathcal{O}}\\left(d_{x}^{0.5}\\left(d_{x}+d_{u}\\right) \\sqrt{T}\\right)$. Then, by subsituting the result of Lemmas 2 and 3 in (30), we get\nthat\n\n$$\n\\begin{aligned}\nR(T) & \\leq \\sum_{i \\in N} q_{0}\\left(\\breve{\\sigma}^{i}\\right)^{2} R^{*}+\\sum_{i \\in N} \\sum_{\\ell=1}^{L} q^{\\ell}\\left(\\sigma^{\\ell, i}\\right)^{2} R^{*} \\\\\n& \\stackrel{(a)}{=} \\sum_{i \\in N} q_{0}\\left(\\breve{v}^{i}\\right)^{2} \\sigma_{w}^{2} R^{*}+\\sum_{i \\in N} \\sum_{\\ell=1}^{L} q^{\\ell}\\left(v^{\\ell, i}\\right)^{2} \\sigma_{w}^{2} R^{*} \\\\\n& \\stackrel{(b)}{=}\\left(q_{0}(n-L)+\\sum_{\\ell=1}^{L} q^{\\ell}\\right) \\sigma_{w}^{2} R^{*}\n\\end{aligned}\n$$\n\nwhere $(a)$ follows from (25) and $(b)$ follows from observing that $\\sum_{i \\in N}\\left(v^{\\ell, i}\\right)^{2}=1$ and therefore $\\sum_{i \\in N}\\left(\\breve{v}^{i}\\right)^{2}=n-L$. Eq. (38) establishes the result of Theorem 2.\n\n## VI. Some examples\n\n## A. Mean-field system\n\nConsider a complete graph $\\mathcal{G}$ where the edge weights are equal to $1 / n$. Let $M$ be equal to the adjacency matrix of the graph, i.e., $M=\\frac{1}{n} \\mathbb{1}_{n \\times n}$. Thus, the system dynamics are given by\n\n$$\nx_{t+1}^{i}=A x_{t}^{i}+B u_{t}^{i}+D \\breve{x}_{t}+E \\breve{u}_{t}+w_{t}^{i}\n$$\n\nwhere $\\breve{x}_{t}=\\frac{1}{n} \\sum_{i \\in N} x_{t}^{i}$ and $\\breve{u}_{t}=\\frac{1}{n} \\sum_{i \\in N} u_{t}^{i}$. Suppose $K_{x}=$ $K_{u}=1$ and $q_{0}=r_{0}=1 / n$ and $q_{1}=r_{1}=\\kappa / n$, where $\\kappa$ is a positive constant.\n\nIn this case, $M$ has rank $L=1$, the non-zero eigenvalue of $M$ is $\\lambda^{1}=1$, the corresponding normalized eigenvector is $\\frac{1}{\\sqrt{n}} \\mathbb{1}_{n \\times 1}$ and $q^{1}=r^{1}=q_{0}+q_{1}=(1+\\kappa) / n$. The eigenstate is given by $x_{t}^{i}=\\left[\\breve{x}_{t}, \\ldots, \\breve{x}_{t}\\right]$ and a similar structure holds for the eigencontrol $u_{t}^{i}$. The per-step cost can be written as (see Proposition 1)\n\n$$\n\\begin{aligned}\n& c\\left(x_{t}, u_{t}\\right)=(1+\\kappa)\\left[\\breve{x}_{t}^{\\top} Q \\breve{x}_{t}+\\breve{u}_{t}^{\\top} R \\breve{u}_{t}\\right] \\\\\n& \\quad+\\frac{1}{n} \\sum_{i \\in N}\\left[\\left(x_{t}^{i}-\\breve{x}_{t}\\right)^{\\top} Q\\left(x_{t}^{i}-\\breve{x}_{t}\\right)+\\left(u_{t}^{i}-\\breve{u}_{t}\\right)^{\\top} R\\left(u_{t}^{i}-\\breve{u}_{t}\\right)\\right]\n\\end{aligned}\n$$\n\nThus, the system is similar to the mean-field team system investigated in [6], [7].\nFor this model, the network dependent constant $\\alpha^{\\mathcal{G}}$ in the regret bound of Theorem 2 is given by $\\alpha^{\\mathcal{G}}=\\left(1+\\frac{\\kappa}{n}\\right)=$ $\\mathcal{O}\\left(1+\\frac{1}{n}\\right)$. Thus, for the mean-field system, the regret of Net-TSDE scales as $\\mathcal{O}\\left(1+\\frac{1}{n}\\right)$ with the number of agents. This is consistent with the discussion following Theorem 2.\n\nWe test these conclusions via numerical simulations of a scalar mean-field model with $d_{x}=d_{u}=1, \\sigma_{w}^{2}=1, A=1$, $B=0.3, D=0.5, E=0.2, Q=1, R=1$, and $\\kappa=0.5$. The uncertain sets are chosen as $\\Theta^{1}=\\left\\{\\theta^{1} \\in \\mathbb{R}^{2}: A+D+\\right.$ $\\left.(B+E) G^{1}\\left(\\theta^{1}\\right)<\\delta\\right\\}$ and $\\breve{\\Theta}=\\left\\{\\breve{\\theta} \\in \\mathbb{R}^{2}: A+B \\breve{G}(\\breve{\\theta})<\\right.$ $\\delta\\}$ where $\\delta=0.99$. The prior over these uncertain sets is chosen according to (A6)-(A7) where $\\breve{\\mu}_{1}=\\mu_{1}^{1}=[1,1]^{\\top}$ and $\\breve{\\Sigma}_{1}=\\Sigma_{1}^{1}=I$. We set $T_{\\min }=0$ in Net-TSDE. The system is simulated for a horizon of $T=5000$ and the expected regret $R(T)$ averaged over 500 sample trajectories is shown in Fig. 1. As expected, the regret scales as $\\breve{\\mathcal{O}}(\\sqrt{T})$ with time and $\\mathcal{O}\\left(1+\\frac{1}{n}\\right)$ with the number of agents."
    },
    {
      "markdown": "![img-0.jpeg](img-0.jpeg)\n\nFigure 1: Regret for mean-field system.\n![img-1.jpeg](img-1.jpeg)\n\nFigure 2: Graph $\\mathcal{G}^{\\circ}$ with $n=4$ nodes and its adjacency matrix\n![img-2.jpeg](img-2.jpeg)\n(a) $R(T) / \\sqrt{T}$ vs $T$\n![img-3.jpeg](img-3.jpeg)\n(b) $R(T) / \\sqrt{T}$ vs number of agents.\n\nFigure 3: Regret for general low-rank network.\n\n## B. A general low-rank network\n\nWe consider a network with $4 n$ nodes given by the graph $\\mathcal{G}=\\mathcal{G}^{\\circ} \\otimes \\mathcal{C}_{n}$, where $\\mathcal{G}^{\\circ}$ is a 4 -node graph shown in Fig. 2 and $\\mathcal{C}_{n}$ is the complete graph with $n$ nodes and each edge weight equal to $\\frac{1}{n}$. Let $M$ be the adjacency matrix of $\\mathcal{G}$ which is given as $M=M^{\\circ} \\otimes \\frac{1}{n} \\mathbb{1}_{n \\times n}$, where $M^{\\circ}$ is the adjacency matrix of $\\mathcal{G}^{\\circ}$ shown in Fig. 2. Moreover, suppose $K_{x}=2$ with $q_{0}=1, q_{1}=-2$, and $q_{2}=1$ and $K_{u}=0$ with $r_{0}=1$. Note that the cost is not normalized per-agent.\n\nIn this case, the rank of $M^{\\circ}$ is 2 with eigenvalues $\\pm \\rho$, where $\\rho=\\sqrt{2\\left(a^{2}+b^{2}\\right)}$ and the rank of $\\frac{1}{n} \\mathbb{1}_{n \\times n}$ is 1 with eigenvalue 1. Thus, $M=M^{\\circ} \\otimes \\frac{1}{n} \\mathbb{1}_{n \\times n}$ has the same non-zero eigenvalues as $M^{\\circ}$ given by $\\lambda^{1}=\\rho$ and $\\lambda^{2}=-\\rho$. Further, $q^{\\ell}=\\left(1-\\lambda^{\\ell}\\right)^{2}$ and $r^{\\ell}=1$, for $\\ell \\in\\{1,2\\}$. We assume that $a^{2}+b^{2} \\neq 0.5$, so that the model satisfies (A3).\n\nFor this model, the scaling parameter $\\alpha^{\\mathcal{G}}$ in the regret bound in Theorem 2 is given by\n\n$$\n\\alpha^{\\mathcal{G}}=(1-\\rho)^{2}+(1+\\rho)^{2}+(4 n-2)=4 n+2 \\rho^{2}\n$$\n\nRecall that $\\rho^{2}=\\left(\\lambda^{1}\\right)^{2}=\\left(\\lambda^{2}\\right)^{2}$. Thus, $\\alpha^{\\mathcal{G}}$ has an explicit dependence on the square of the eigenvalues and the number of nodes.\n\nWe verify this relationship via numerical simulations. We consider the graph above with two choices of parameters $(a, b)$ : (i) $a=b=0.05$ and (ii) $a=b=5$. For both cases, we consider a scalar system with parameters same as the mean-field system\nconsidered in Sec. VI-A. The regret for both cases with different choices of number of agents $4 n \\in\\{4,40,80,100\\}$ is shown in Fig. 3. As expected, the regret scales as $\\mathcal{O}(\\sqrt{T})$ with time and $\\mathcal{O}\\left(4 n+2 \\rho^{2}\\right)$ with the number of agents.\n\n## VII. CONCLUSION\n\nWe consider the problem of controlling an unknown LQG system consisting of multiple subsystems connected over a network. By utilizing a spectral decomposition technique, we decompose the coupled subsystems into eigen and auxiliary systems. We propose a TS-based learning algorithm Net-TSDE which maintains separate posterior distributions on the unknown parameters $\\theta^{\\ell}, \\ell \\in\\{1, \\ldots, L\\}$, and $\\hat{\\theta}$ associated with the eigen and auxiliary systems respectively. For each eigen-system, Net-TSDE learns the unknown parameter $\\theta^{\\ell}$ and controls the system in a manner similar to the TSDE algorithm for single agent LQG systems proposed in [21], [22], [41]. Consequently, the regret for each eigen system can be bounded using the results of [21], [22], [41]. However, the part of the Net-TSDE algorithm that performs learning and control for the auxiliary system has an agent selection step and thus requires additional analysis to bound its regret. Combining the regret bounds for the eigen and auxiliary systems shows that the total expected regret of Net-TSDE is upper bounded by $\\mathcal{O}\\left(n d_{x}^{0.5}\\left(d_{x}+d_{u}\\right) \\sqrt{T}\\right)$. The empirically observed scaling of regret with respect to the time horizon $T$ and the number of subsystems $n$ in our numerical experiments agrees with the theoretical upper bound.\n\nThe results presented in this paper rely on the spectral decomposition developed in [24]. A limitation of this decomposition is that the local dynamics (i.e., the $(A, B)$ matrices) are assumed to be identical for all subsystems. Interesting generalizations overcoming this limitation include settings where (i) there are multiple types of subsystems and the $(A, B)$ matrices are the same for subsystems of the same type but different across types; and (ii) the subsystems are not identical but approximately identical, i.e., there are nominal dynamics $\\left(A^{\\circ}, B^{\\circ}\\right)$ and the local dynamics $\\left(A^{i}, B^{i}\\right)$ of subsystem $i$ are in a small neighborhood of $\\left(A^{\\circ}, B^{\\circ}\\right)$.\n\nThe decomposition in [24] exploits the fact that the dynamics and the cost couplings have the same spectrum (i.e., the same orthonormal eigenvectors). It is also possible to consider learning algorithms which exploit other features of the network such as sparsity in the case of networked MDPs [32], [33].\n\n## REFERENCES\n\n[1] N. Sandell, P. Varaiya, M. Athans, and M. Safonov, \"Survey of decentralized control methods for large scale systems,\" IEEE Trans. Autom. Control, vol. 23, no. 2, pp. 108-128, 1978.\n[2] J. Lanze, \"Dynamics of strongly coupled symmetric composite systems,\" International Journal of Control, vol. 44, no. 6, pp. 1617-1640, 1986.\n[3] M. K. Sundarsshan and R. M. Elbanna, \"Qualitative analysis and decentralized controller synthesis for a class of large-scale systems with symmetrically interconnected subsystems,\" Automatica, vol. 27, no. 2, pp. 383-388, 1991.\n[4] G.-H. Yang and S.-Y. Zhang, \"Structural properties of large-scale systems possessing similar structures,\" Automatica, vol. 31, no. 7, pp. 1011-1017, 1995.\n[5] S. C. Hamilton and M. E. Broucke, \"Patterned linear systems,\" Automatica, vol. 48, no. 2, pp. 263-272, 2012.\n[6] J. Arabneydi and A. Mahajan, \"Team-optimal solution of finite number of mean-field coupled LQG subsystems,\" in Conf. Decision and Control, (Kyoto, Japan), Dec. 2015."
    },
    {
      "markdown": "[7] J. Arabneydi and A. Mahajan, \"Linear Quadratic Mean Field Teams: Optimal and Approximately Optimal Decentralized Solutions,\" 2016. arXiv:1609.00056.\n[8] K. J. Astrom and B. Wittenmark, Adaptive Control. Addison-Wesley Longman Publishing Co., Inc., 1994.\n[9] S. J. Bradtke, \"Reinforcement learning applied to linear quadratic regulation,\" in Neural Information Processing Systems, pp. 295-302, 1993.\n[10] S. J. Bradtke, B. E. Ydstie, and A. G. Barto, \"Adaptive linear quadratic control using policy iteration,\" in Proceedings of American Control Conference, vol. 3, pp. 3475-3479, 1994.\n[11] M. C. Campi and P. Kumar, \"Adaptive linear quadratic Gaussian control: the cost-biased approach revisited,\" SIAM Journal on Control and Optimization, vol. 36, no. 6, pp. 1890-1907, 1998.\n[12] Y. Abbasi-Yadkori and C. Szepesvri, \"Regret bounds for the adaptive control of linear quadratic systems,\" in Annual Conference on Learning Theory, pp. 1-26, 2011.\n[13] M. K. S. Faradonbeh, A. Tewari, and G. Michailidis, \"Optimism-based adaptive regulation of linear-quadratic systems,\" IEEE Trans. Autom. Control, vol. 66, no. 4, pp. 1802-1808, 2021.\n[14] A. Cohen, T. Koren, and Y. Mansour, \"Learning linear-quadratic regulators efficiently with only $\\sqrt{T}$ regret,\" in International Conference on Machine Learning, pp. 1300-1309, 2019.\n[15] M. Abeille and A. Lazaric, \"Efficient optimistic exploration in linearquadratic regulators via Lagrangian relaxation,\" in International Conference on Machine Learning, pp. 23-31, 2020.\n[16] S. Dean, H. Mania, N. Matni, B. Recht, and S. Tu, \"Regret bounds for robust adaptive control of the linear quadratic regulator,\" in Neural Information Processing Systems, pp. 4192-4201, 2018.\n[17] H. Mania, S. Tu, and B. Recht, \"Certainty equivalent control of LQR is efficient.\" arXiv:1902.07826, 2019.\n[18] M. K. S. Faradonbeh, A. Tewari, and G. Michailidis, \"Input perturbations for adaptive control and learning,\" Automatica, vol. 117, p. 108950, 2020.\n[19] M. Simchowitz and D. Foster, \"Naive exploration is optimal for online LQR,\" in International Conference on Machine Learning, pp. 8937-8948, 2020.\n[20] M. K. S. Faradonbeh, A. Tewari, and G. Michailidis, \"On adaptive Linear-Quadratic regulators,\" Automatica, vol. 117, p. 108982, July 2020.\n[21] Y. Ouyang, M. Gagrani, and R. Jain, \"Control of unknown linear systems with Thompson sampling,\" in Allerton Conference on Communication, Control, and Computing, pp. 1198-1205, 2017.\n[22] Y. Ouyang, M. Gagrani, and R. Jain, \"Posterior sampling-based reinforcement learning for control of unknown linear systems,\" IEEE Trans. Autom. Control, vol. 65, no. 8, pp. 3600-3607, 2020.\n[23] M. Abeille and A. Lazaric, \"Improved regret bounds for Thompson sampling in linear quadratic control problems,\" in International Conference on Machine Learning, pp. 1-9, 2018.\n[24] S. Gao and A. Mahajan, \"Optimal control of network-coupled subsystems: Spectral decomposition and low-dimensional solutions,\" submitted to IEEE Trans. Control of Networked Sys., 2020.\n[25] P. Auer, N. Cesa-Bianchi, and P. Fischer, \"Finite-time analysis of the multiarmed bandit problem,\" Machine learning, vol. 47, no. 2-3, pp. 235256, 2002.\n[26] S. Agrawal and N. Goyal, \"Analysis of Thompson sampling for the multi-armed bandit problem,\" in Conference on Learning Theory, 2012.\n[27] H. Wang, S. Lin, H. Jafarkhani, and J. Zhang, \"Distributed Q-learning with state tracking for multi-agent networked control,\" in AAMAS, pp. $1692-1694,2021$.\n[28] G. Jing, H. Bai, J. George, A. Chakrabortty, and P. K. Sharma, \"Learning distributed stabilizing controllers for multi-agent systems,\" IEEE Control Systems Letters, 2021.\n[29] Y. Li, Y. Tang, R. Zhang, and N. Li, \"Distributed reinforcement learning for decentralized linear quadratic control: A derivative-free policy optimization approach,\" in Proc. Conf. Learning for Dynamics and Control, pp. 814-814, June 2020.\n[30] K. Zhang, Z. Yang, H. Liu, T. Zhang, and T. Basar, \"Fully decentralized multi-agent reinforcement learning with networked agents,\" in International Conference on Machine Learning, pp. 5872-5881, 2018.\n[31] K. Zhang, Z. Yang, and T. Baar, \"Decentralized multi-agent reinforcement learning with networked agents: Recent advances,\" arXiv preprint arXiv:1912.03821, 2019.\n[32] I. Osband and B. Van Roy, \"Near-optimal reinforcement learning in factored MDPs,\" in Advances in Neural Information Processing Systems, vol. 27, Curran Associates, Inc., 2014.\n[33] X. Chen, J. Hu, L. Li, and L. Wang, \"Efficient reinforcement learning in factored MDPs with application to constrained RL,\" arXiv preprint arXiv:2008.13319, 2021.\n[34] M. Huang, P. E. Caines, and R. P. Malham, \"Large-population cost-coupled LQG problems with nonuniform agents: individual-mass behavior and decentralized epsilon-Nash equilibria,\" IEEE Transactions on Automatic Control, vol. 52, no. 9, pp. 1560-1571, 2007.\n[35] J.-M. Lasry and P.-L. Lions, \"Mean field games,\" Japanese Journal of Mathematics, vol. 2, no. 1, pp. 229-260, 2007.\n[36] J. Subramanian and A. Mahajan, \"Reinforcement learning in stationary mean-field games,\" in International Conference on Autonomous Agents and Multi-Agent Systems, pp. 251-259, 2019.\n[37] S. G. Subramanian, P. Poupart, M. E. Taylor, and N. Hegde, \"Multi type mean field reinforcement learning,\" in International Conference on Autonomous Agents and Multiagent Systems, pp. 411-419, 2020.\n[38] M. A. uz Zaman, K. Zhang, E. Miehling, and T. Basar, \"Reinforcement learning in non-stationary discrete-time linear-quadratic mean-field games,\" in Conference on Decision and Control, pp. 2278-2284, 2020.\n[39] M. Gagrani, S. Sudhakara, A. Mahajan, A. Nayyar, and Y. Ouyang, \"Thompson sampling for linear quadratic mean-field teams.\" arXiv preprint arXiv:2011.04686, 2020.\n[40] J. Sternby, \"On consistency for the method of least squares using martingale theory,\" IEEE Trans. Autom. Control, vol. 22, no. 3, pp. 346352, 1977.\n[41] M. Gagrani, S. Sudhakara, A. Mahajan, A. Nayyar, and Y. Ouyang, \"A relaxed technical assumption for posterior sampling-based reinforcement learning for control of unknown linear systems.\" http://cim.mcgill.ca/ \"adityam/projects/reinforcement-learning/preprint/tsde.pdf, 2021.\n[42] M. K. S. Faradonbeh, A. Tewari, and G. Michailidis, \"Finite-time adaptive stabilization of linear systems,\" IEEE Trans. Autom. Control, vol. 64, pp. 3498-3505, Aug. 2019.\n[43] P. R. Kumar and P. Varaiya, Stochastic systems: Estimation, identification, and adaptive control. SIAM Classics, 2015.\n[44] Y. Abbasi-Yadkori and C. Szepesvari, \"Bayesian optimal control of smoothly parameterized systems: The lazy posterior sampling algorithm.\" arXiv preprint arXiv:1406.3926, 2014.\n\n## APPENDIX\n\n## A. Preliminary Results\n\nSince $\\hat{S}(\\cdot)$ and $\\hat{G}(\\cdot)$ are continuous functions on a compact set $\\hat{\\Theta}$, there exist finite constants $\\hat{M}_{J}, \\hat{M}_{\\hat{\\theta}}, \\hat{M}_{S}, \\hat{M}_{G}$ such that $\\operatorname{Tr}(\\hat{S}(\\hat{\\theta})) \\leq \\hat{M}_{J},\\|\\hat{\\theta}\\| \\leq \\hat{M}_{\\hat{\\theta}},\\|\\hat{S}(\\hat{\\theta})\\| \\leq \\hat{M}_{S}$ and $\\left\\|[I, \\hat{G}(\\hat{\\theta})^{T}]^{\\top}\\right\\| \\leq \\hat{M}_{G}$ for all $\\hat{\\theta} \\in \\hat{\\Theta}$ where $\\|\\cdot\\|$ is the induced matrix norm.\n\nLet $\\hat{X}_{T}^{i}=\\max _{1 \\leq t \\leq T}\\left\\|\\hat{x}_{t}^{i}\\right\\|$ be the maximum norm of the auxiliary state along the entire trajectory. The next bound follows from [41, Lemma 4].\nLemma 4 For each node $i \\in N$, any $q \\geq 1$ and any $T>1$,\n\n$$\n\\mathbb{E}\\left[\\frac{\\left(\\hat{X}_{T}^{i}\\right)^{q}}{\\left(\\hat{\\sigma}^{i}\\right)^{q}}\\right] \\leq \\mathcal{O}(\\log T)\n$$\n\nThe following lemma gives an upper bound on the number of episodes $\\hat{K}_{T}$.\nLemma 5 For any $q \\geq 1$, we have\n\n$$\n\\begin{aligned}\n\\mathbb{E}\\left[\\frac{\\left(\\hat{X}_{T}^{i}\\right)^{q}}{\\left(\\hat{\\sigma}^{i}\\right)^{q}} \\log \\left(\\sum_{t=1}^{T}\\left(\\hat{X}_{T}^{j_{t}}\\right)^{2}\\right)\\right] & \\leq \\mathbb{E}\\left[\\frac{\\left(\\hat{X}_{T}^{i}\\right)^{q}}{\\left(\\hat{\\sigma}^{i}\\right)^{q}} \\log \\left(\\sum_{t=1}^{T} \\sum_{i \\in N}\\left(\\hat{X}_{T}^{i}\\right)^{2}\\right)\\right] \\\\\n& \\leq \\hat{\\mathcal{O}}(1)\n\\end{aligned}\n$$\n\nThis can be proved along the same lines as [41, Lemma 5].\nLemma 6 The number of episodes $\\hat{K}_{T}$ is bounded as follows:\n\n$$\n\\hat{K}_{T} \\leq \\mathcal{O}\\left(\\sqrt{\\left(d_{x}+d_{u}\\right) T \\log \\left(\\sum_{t=1}^{T-1} \\frac{\\left(\\hat{X}_{T}^{j_{t}}\\right)^{2}}{\\left(\\hat{\\sigma}^{j_{t}}\\right)^{2}}\\right)}\\right)\n$$"
    },
    {
      "markdown": "Proof We can follow the same argument as in the proof of Lemma 5 in [41]. Let $\\bar{\\eta}-1$ be the number of times the second stopping criterion is triggered for $\\bar{p}_{t}$. Using the analysis in the proof of Lemma 5 in [41], we can get the following inequalities:\n\n$$\n\\begin{gathered}\n\\bar{K}_{T} \\leq \\sqrt{2 \\bar{\\eta} T} \\\\\n\\operatorname{det}\\left(\\bar{\\Sigma}_{T}^{-1}\\right) \\geq 2^{\\bar{\\eta}-1} \\operatorname{det}\\left(\\bar{\\Sigma}_{1}^{-1}\\right) \\geq 2^{\\bar{\\eta}-1} \\bar{\\lambda}_{\\min }^{d}\n\\end{gathered}\n$$\n\nwhere $d=d_{x}+d_{u}$ and $\\bar{\\lambda}_{\\min }$ is the minimum eigenvalue of $\\bar{\\Sigma}_{1}^{-1}$.\n\nCombining (41) with $\\operatorname{Tr}\\left(\\bar{\\Sigma}_{T}^{-1}\\right) / d \\geq \\operatorname{det}\\left(\\bar{\\Sigma}_{T}^{-1}\\right)^{1 / d}$, we get $\\operatorname{Tr}\\left(\\bar{\\Sigma}_{T}^{-1}\\right) \\geq d \\bar{\\lambda}_{\\min } 2^{(\\bar{\\eta}-1) / d}$. Thus,\n\n$$\n\\eta \\leq 1+\\frac{d}{\\log 2} \\log \\left(\\frac{\\operatorname{Tr}\\left(\\bar{\\Sigma}_{T}^{-1}\\right)}{d \\bar{\\lambda}_{\\min }}\\right)\n$$\n\nNow, we bound $\\operatorname{Tr}\\left(\\bar{\\Sigma}_{T}^{-1}\\right)$. From (29b), we have\n\n$$\n\\operatorname{Tr}\\left(\\bar{\\Sigma}_{T}^{-1}\\right)=\\operatorname{Tr}\\left(\\bar{\\Sigma}_{1}^{-1}\\right)+\\sum_{t=1}^{T-1} \\frac{1}{\\left(\\bar{\\sigma}^{j_{t}}\\right)^{2}} \\underbrace{\\operatorname{Tr}\\left(\\bar{z}_{t}^{j_{t}}\\left(\\bar{z}_{t}^{j_{t}}\\right)^{\\top}\\right)}_{=\\left\\|\\bar{z}_{t}^{j_{t}}\\right\\|^{2}}\n$$\n\nNote that $\\left\\|\\bar{z}_{t}^{j_{t}}\\right\\|=\\left\\|\\left[I, \\bar{G}(\\bar{\\theta})^{\\top}\\right]^{\\top} \\bar{x}_{t}^{j_{t}}\\right\\| \\leq \\bar{M}_{G}\\left\\|\\bar{x}_{t}^{j_{t}}\\right\\| \\leq \\bar{M}_{G} \\bar{X}_{T}^{j_{t}}$. Using $\\left\\|\\bar{z}_{t}^{j_{t}}\\right\\|^{2} \\leq \\bar{M}_{G}^{2}\\left(\\bar{X}_{T}^{j_{t}}\\right)^{2}$ in (43) and substituting the resulting bound on $\\operatorname{Tr}\\left(\\bar{\\Sigma}_{T}^{-1}\\right)$ in (42) and then combining it with the bound on $\\eta$ in (40), gives the result of the lemma.\n\nLemma 7 The expected value of $\\bar{K}_{T}$ is bounded as follows:\n\n$$\n\\mathbb{E}\\left[\\bar{K}_{T}\\right] \\leq \\mathcal{O}\\left(\\sqrt{\\left(d_{x}+d_{u}\\right) T}\\right)\n$$\n\nProof From Lemma 6, we get\n\n$$\n\\begin{aligned}\n\\mathbb{E}\\left[\\bar{K}_{T}\\right] & \\leq \\mathcal{O}\\left(\\mathbb{E}\\left[\\sqrt{\\left(d_{x}+d_{u}\\right) T \\log \\left(\\sum_{t=1}^{T-1} \\frac{\\left(\\bar{X}_{T}^{j_{t}}\\right)^{2}}{\\left(\\bar{\\sigma}^{j_{t}}\\right)^{2}}\\right)}\\right]\\right) \\\\\n& \\stackrel{(a)}{\\leq} \\mathcal{O}\\left(\\sqrt{\\left(d_{x}+d_{u}\\right) T \\log \\left(\\mathbb{E}\\left[\\sum_{t=1}^{T-1} \\frac{\\left(\\bar{X}_{T}^{j_{t}}\\right)^{2}}{\\left(\\bar{\\sigma}^{j_{t}}\\right)^{2}}\\right]\\right)}\\right) \\\\\n& \\leq \\mathcal{O}\\left(\\sqrt{\\left(d_{x}+d_{u}\\right) T \\log \\left(\\mathbb{E}\\left[\\sum_{t=1}^{T-1} \\sum_{i \\in N} \\frac{\\left(\\bar{X}_{T}^{j_{t}}\\right)^{2}}{\\left(\\bar{\\sigma}^{j}\\right)^{2}}\\right]\\right)}\\right) \\\\\n& \\leq \\mathcal{O}\\left(\\sqrt{\\left(d_{x}+d_{u}\\right) T}\\right)\n\\end{aligned}\n$$\n\nwhere $(a)$ follows from Jensen's inequality and $(b)$ follows from Lemma 4.\n\n## B. Proof of Lemma 3\n\nProof We will prove each part separately.\n\n1) Bounding $\\bar{R}_{0}^{i}(T)$ : From an argument similar to the proof of Lemma 5 of [22], we get that $\\bar{R}_{0}^{i}(T) \\leq\\left(\\bar{\\sigma}^{i}\\right)^{2} \\bar{M}_{J} \\mathbb{E}\\left[\\bar{K}_{T}\\right]$. The result then follows from substituting the bound on $\\mathbb{E}\\left[\\bar{K}_{T}\\right]$ from Lemma 7.\n2) Bounding $\\bar{R}_{1}^{i}(T)$ :\n\n$$\n\\begin{aligned}\n\\bar{R}_{1}^{i}(T) & =\\mathbb{E}\\left[\\sum_{k=1}^{\\bar{K}_{T}} \\sum_{t=\\bar{t}_{k}}^{\\bar{t}_{k+1}-1}\\left[\\left(\\bar{x}_{t}^{i}\\right)^{\\top} \\bar{S}_{k} \\bar{x}_{t}^{i}-\\left(\\bar{x}_{t+1}^{i}\\right)^{\\top} \\bar{S}_{k} \\bar{x}_{t+1}^{i}\\right]\\right] \\\\\n& =\\mathbb{E}\\left[\\sum_{k=1}^{\\bar{K}_{T}}\\left[\\left(\\bar{x}_{t_{k}}^{i}\\right)^{\\top} \\bar{S}_{k} \\bar{x}_{t_{k}}^{i}-\\left(\\bar{x}_{t_{k+1}}^{i}\\right)^{\\top} \\bar{S}_{k} \\bar{x}_{t_{k+1}}^{i}\\right]\\right] \\\\\n& \\leq \\mathbb{E}\\left[\\sum_{k=1}^{\\bar{K}_{T}}\\left(\\bar{x}_{t_{k}}^{i}\\right)^{\\top} \\bar{S}_{k} \\bar{x}_{t_{k}}^{i}\\right] \\leq \\mathbb{E}\\left[\\sum_{k=1}^{\\bar{K}_{T}}\\left\\|\\bar{S}_{k}\\right\\|\\left\\|\\bar{x}_{t_{k}}^{i}\\right\\|^{2}\\right] \\\\\n& \\leq \\bar{M}_{S} \\mathbb{E}\\left[\\bar{K}_{T}\\left(\\bar{X}_{T}^{i}\\right)^{2}\\right]\n\\end{aligned}\n$$\n\nwhere the last inequality follows from $\\left\\|\\bar{S}_{k}\\right\\| \\leq \\bar{M}_{S}$. Using the bound for $\\bar{K}_{T}$ in Lemma 6, we get\n$\\bar{R}_{1}^{i}(T) \\leq \\mathcal{O}\\left(\\sqrt{\\left(d_{x}+d_{u}\\right) T} \\mathbb{E}\\left[\\left(\\bar{X}_{T}^{i}\\right)^{2} \\sqrt{\\log \\left(\\sum_{t=1}^{T-1} \\frac{\\left(\\bar{X}_{T}^{j_{t}}\\right)^{2}}{\\left(\\bar{\\sigma}^{j_{t}}\\right)^{2}}\\right)}\\right]\\right)$.\nNow, consider the term\n\n$$\n\\begin{aligned}\n& \\mathbb{E}\\left[\\left(\\bar{X}_{T}^{i}\\right)^{2} \\sqrt{\\log \\left(\\sum_{t=1}^{T-1} \\frac{\\left(\\bar{X}_{T}^{j_{t}}\\right)^{2}}{\\left(\\bar{\\sigma}^{j_{t}}\\right)^{2}}\\right)}\\right]\\right) \\\\\n& \\stackrel{(a)}{\\leq} \\sqrt{\\mathbb{E}\\left[\\left(\\bar{X}_{T}^{i}\\right)^{4}\\right] \\mathbb{E}\\left[\\log \\left(\\sum_{t=1}^{T-1} \\frac{\\left(\\bar{X}_{T}^{j_{t}}\\right)^{2}}{\\left(\\bar{\\sigma}^{j_{t}}\\right)^{2}}\\right)\\right]} \\\\\n& \\stackrel{(b)}{\\leq} \\sqrt{\\mathbb{E}\\left[\\left(\\bar{X}_{T}^{i}\\right)^{4}\\right] \\log \\left(\\mathbb{E}\\left[\\sum_{t=1}^{T-1} \\sum_{i \\in N} \\frac{\\left(\\bar{X}_{T}^{i}\\right)^{2}}{\\left(\\bar{\\sigma}^{j_{t}}\\right)^{2}}\\right]\\right)} \\\\\n& \\stackrel{(c)}{\\leq} \\mathcal{O}\\left(\\left(\\bar{\\sigma}^{i}\\right)^{2}\\right)\n\\end{aligned}\n$$\n\nwhere $(a)$ follows from Cauchy-Schwarz, $(b)$ follows from Jensen's inequality and $(c)$ follows from Lemma 4. The result then follows from substituting (46) in (44).\n3) Bounding $\\bar{R}_{2}^{i}(T)$ : As in [22], we can bound the inner summand in $\\bar{R}_{2}^{i}(T)$ as\n\n$$\n\\begin{aligned}\n& {\\left[\\left(\\bar{\\theta}^{\\top} \\bar{z}_{t}^{i}\\right)^{\\top} \\bar{S}_{k}\\left(\\bar{\\theta}^{\\top} \\bar{z}_{t}^{i}\\right)-\\left(\\bar{\\theta}_{k}^{\\top} \\bar{z}_{t}^{i}\\right)^{\\top} \\bar{S}_{k}\\left(\\left(\\bar{\\theta}_{k}\\right)^{\\top} \\bar{z}_{t}^{i}\\right)\\right]} \\\\\n& \\leq \\mathcal{O}\\left(\\bar{X}_{T}^{i}\\left\\|\\left(\\bar{\\theta}-\\bar{\\theta}_{k}\\right)^{\\top} \\bar{z}_{t}^{i}\\right\\|\\right)\n\\end{aligned}\n$$\n\nTherefore,\n\n$$\n\\bar{R}_{2}^{i}(T) \\leq \\mathcal{O}\\left(\\mathbb{E}\\left[\\bar{X}_{T}^{i} \\sum_{k=1}^{\\bar{K}_{T}} \\sum_{t=\\bar{t}_{k}}^{\\bar{t}_{k+1}-1}\\left\\|\\left(\\bar{\\theta}-\\bar{\\theta}_{k}\\right)^{\\top} \\bar{z}_{t}^{i}\\right\\|\\right]\\right)\n$$\n\nThe term inside $\\mathcal{O}(\\cdot)$ can be written as\n\n$$\n\\begin{aligned}\n& \\mathbb{E}\\left[\\bar{X}_{T}^{i} \\sum_{k=1}^{\\bar{K}_{T}} \\sum_{t=\\bar{t}_{k}}^{\\bar{t}_{k+1}-1}\\left\\|\\left(\\bar{\\theta}-\\bar{\\theta}_{k}\\right)^{\\top} \\bar{z}_{t}^{i}\\right\\|\\right] \\\\\n& \\quad=\\mathbb{E}\\left[\\bar{X}_{T}^{i} \\sum_{k=1}^{\\bar{K}_{T}} \\sum_{t=\\bar{t}_{k}}^{\\bar{t}_{k+1}-1}\\left\\|\\left(\\bar{\\Sigma}_{t_{k}}^{-0.5}\\left(\\bar{\\theta}-\\bar{\\theta}_{k}\\right)\\right)^{\\top} \\bar{\\Sigma}_{t_{k}}^{0.5} \\bar{z}_{t}^{i}\\right\\|\\right] \\\\\n& \\quad \\leq \\mathbb{E}\\left[\\sum_{k=1}^{\\bar{K}_{T}} \\sum_{t=\\bar{t}_{k}}^{\\bar{t}_{k+1}-1}\\left\\|\\bar{\\Sigma}_{t_{k}}^{-0.5}\\left(\\bar{\\theta}-\\bar{\\theta}_{k}\\right)\\right\\| \\times \\bar{X}_{T}^{i}\\left\\|\\bar{\\Sigma}_{t_{k}}^{0.5} \\bar{z}_{t}^{i}\\right\\|\\right]\n\\end{aligned}\n$$"
    },
    {
      "markdown": "$$\n\\begin{aligned}\n\\leq & \\sqrt{\\mathbb{E}\\left[\\sum_{k=1}^{\\bar{K}_{T}} \\sum_{t=f_{k}}^{t_{k+1}-1}\\left\\|\\bar{\\Sigma}_{t_{k}}^{-0.5}\\left(\\bar{\\theta}-\\bar{\\theta}_{k}\\right)\\right\\|^{2}\\right]} \\\\\n& \\times \\sqrt{\\mathbb{E}\\left[\\sum_{k=1}^{\\bar{K}_{T}} \\sum_{t=f_{k}}^{t_{k+1}-1}\\left(\\bar{X}_{T}^{i}\\right)^{2}\\left\\|\\bar{\\Sigma}_{t_{k}}^{0.5} \\bar{z}_{t}^{i}\\right\\|^{2}\\right]}\n\\end{aligned}\n$$\n\nwhere the last inequality follows from Cauchy-Schwarz inequality.\n\nFollowing the same argument as [41, Lemma 7], the first part of (48) is bounded by\n\n$$\n\\mathbb{E}\\left[\\sum_{k=1}^{\\bar{K}_{T}} \\sum_{t=f_{k}}^{t_{k+1}-1}\\left\\|\\bar{\\Sigma}_{t_{k}}^{-0.5}\\left(\\bar{\\theta}-\\bar{\\theta}_{k}\\right)\\right\\|^{2}\\right] \\leq \\mathcal{O}\\left(d_{x}\\left(d_{x}+d_{u}\\right) T\\right)\n$$\n\nFor the second part of the bound in (48), we follow the same argument as [41, Lemma 8]. Recall that $\\bar{\\lambda}_{\\min }$ is the smallest eigenvalue of $\\bar{\\Sigma}_{1}^{-1}$. Therefore, by (29b), all eigenvalues of $\\bar{\\Sigma}_{t}^{-1}$ are no smaller than $\\bar{\\lambda}_{\\min }$. Or, equivalently, all eigenvalues of $\\bar{\\Sigma}_{t}$ are no larger than $1 / \\bar{\\lambda}_{\\min }$.\n\nUsing [12, Lemma 11], we can show that for any $t \\in$ $\\left\\{t_{k}, \\ldots, t_{k+1}-1\\right\\}$,\n\n$$\n\\begin{aligned}\n\\left\\|\\bar{\\Sigma}_{t_{k}}^{0.5} \\bar{z}_{t}^{i}\\right\\|^{2} & =\\left(\\bar{z}_{t}^{i}\\right)^{\\top} \\bar{\\Sigma}_{t_{k}} \\bar{z}_{t}^{i} \\leq \\frac{\\operatorname{det} \\bar{\\Sigma}_{t}^{-1}}{\\operatorname{det} \\bar{\\Sigma}_{t_{k}}^{-1}}\\left(\\bar{z}_{t}^{i}\\right)^{\\top} \\bar{\\Sigma}_{t} \\bar{z}_{t}^{i} \\\\\n& \\leq F_{1}\\left(\\bar{X}_{T}^{i}\\right)\\left(\\bar{z}_{t}^{i}\\right)^{\\top} \\bar{\\Sigma}_{t} \\bar{z}_{t}^{i}\n\\end{aligned}\n$$\n\nwhere $F_{1}\\left(\\bar{X}_{T}^{i}\\right)=\\left(1+\\left(\\bar{M}_{G}^{2}\\left(\\bar{X}_{T}^{i}\\right)^{2} / \\bar{\\lambda}_{\\min } \\bar{\\sigma}_{w}^{2}\\right)\\right)^{\\bar{T}_{\\min } \\vee 1}$ and the last inequality follows from [41, Lemma 10].\n\nMoreover, since all eigenvalues of $\\bar{\\Sigma}_{t}$ are no larger than $1 / \\bar{\\lambda}_{\\min }$, we have $\\left(\\bar{z}_{t}^{i}\\right)^{\\top} \\bar{\\Sigma}_{t} \\bar{z}_{t}^{i} \\leq\\left\\|\\bar{z}_{t}^{i}\\right\\|^{2} / \\bar{\\lambda}_{\\min } \\leq \\bar{M}_{G}^{2}\\left(\\bar{X}_{T}^{i}\\right)^{2} / \\bar{\\lambda}_{\\min }$. Therefore,\n\n$$\n\\begin{aligned}\n\\left(\\bar{z}_{t}^{i}\\right)^{\\top} \\bar{\\Sigma}_{t} \\bar{z}_{t}^{i} & \\leq\\left(\\left(\\bar{\\sigma}^{i}\\right)^{2} \\vee \\frac{\\bar{M}_{G}^{2}\\left(\\bar{X}_{T}^{i}\\right)^{2}}{\\bar{\\lambda}_{\\min }}\\right)\\left(1 \\wedge \\frac{\\left(\\bar{z}_{t}^{i}\\right)^{\\top} \\bar{\\Sigma}_{t} \\bar{z}_{t}^{i}}{\\left(\\bar{\\sigma}^{i}\\right)^{2}}\\right) \\\\\n& \\leq\\left(\\left(\\bar{\\sigma}^{i}\\right)^{2}+\\frac{\\bar{M}_{G}^{2}\\left(\\bar{X}_{T}^{i}\\right)^{2}}{\\bar{\\lambda}_{\\min }}\\right)\\left(1 \\wedge \\frac{\\left(\\bar{z}_{t}^{j_{t}}\\right)^{\\top} \\bar{\\Sigma}_{t} \\bar{z}_{t}^{j_{t}}}{\\left(\\bar{\\sigma}^{j_{t}}\\right)^{2}}\\right)\n\\end{aligned}\n$$\n\nwhere the last inequality follows from the definition of $j_{t}$. Let $F_{2}\\left(\\bar{X}_{T}^{i}\\right)=\\left(\\left(\\bar{\\sigma}^{i}\\right)^{2}+\\left(\\bar{\\lambda}_{\\min } / \\bar{M}_{G}^{2}\\left(\\bar{X}_{T}^{i}\\right)^{2}\\right)\\right)$. Then,\n\n$$\n\\begin{aligned}\n& \\sum_{t=1}^{T}\\left(\\bar{z}_{t}^{i}\\right)^{\\top} \\bar{\\Sigma}_{t} \\bar{z}_{t}^{i} \\leq F_{2}\\left(\\bar{X}_{T}^{i}\\right) \\sum_{t=1}^{T}\\left(1 \\wedge \\frac{\\left(\\bar{z}_{t}^{j_{t}}\\right)^{\\top} \\bar{\\Sigma}_{t} \\bar{z}_{t}^{j_{t}}}{\\left(\\bar{\\sigma}^{j_{t}}\\right)^{2}}\\right) \\\\\n& =F_{2}\\left(\\bar{X}_{T}^{i}\\right) \\sum_{t=1}^{T}\\left(1 \\wedge\\left\\|\\frac{\\sum_{t}^{0.5} \\bar{z}_{t}^{j_{t}}\\left(\\bar{z}_{t}^{j_{t}}\\right)^{\\top} \\Sigma_{t}^{0.5}}{\\left(\\bar{\\sigma}^{j_{t}}\\right)^{2}}\\right\\|\\right) \\\\\n& \\stackrel{(a)}{\\leq} F_{2}\\left(\\bar{X}_{T}^{i}\\right)\\left[2 d \\log \\left(\\frac{\\operatorname{Tr}\\left(\\bar{\\Sigma}_{T+1}^{-1}\\right)}{d}\\right)-\\log \\operatorname{det} \\Sigma_{1}^{-1}\\right] \\\\\n& \\stackrel{(b)}{\\leq} F_{2}\\left(\\bar{X}_{T}^{i}\\right)\\left[2 d \\log \\left(\\frac{1}{d}\\left(\\operatorname{Tr}\\left(\\bar{\\Sigma}_{1}^{-1}\\right)+\\bar{M}_{G} \\sum_{t=1}^{T} \\frac{\\left(\\bar{X}_{T}^{j_{t}}\\right)^{2}}{\\left(\\bar{\\sigma}^{j_{t}}\\right)^{2}}\\right)\\right) \\\\\n& \\quad-\\log \\operatorname{det} \\Sigma_{1}^{-1}\n\\end{aligned}\n$$\n\nwhere $d=d_{x}+d_{u}$ and $(a)$ follows from (29b) and the intermediate step in the proof of [44, Lemma 6]. and (b) follows from (43) and the subsequent discussion.\n\nUsing (50) and (52), we can bound the second term of (48) as follows\n\n$$\n\\begin{aligned}\n\\mathbb{E}\\left[\\sum_{t=1}^{T}\\left(\\bar{X}_{T}^{i}\\right)^{2}\\left\\|\\bar{\\Sigma}_{t_{k}}^{0.5} \\bar{z}_{t}^{i}\\right\\|^{2}\\right] \\leq & \\mathcal{O}\\left(d \\mathbb{E}\\left[F_{1}\\left(\\bar{X}_{t}^{i}\\right) F_{2}\\left(\\bar{X}_{T}^{i}\\right)\\left(\\bar{X}_{T}^{i}\\right)^{2}\\right.\\right. \\\\\n& \\left.\\left.\\times \\log \\left(\\sum_{t=1}^{T}\\left(\\bar{X}_{T}^{j_{t}}\\right)^{2}\\right)\\right]\\right) \\\\\n\\leq & \\mathcal{O}\\left(d\\left(\\bar{\\sigma}^{i}\\right)^{4} \\mathbb{E}\\left[F_{1}\\left(\\bar{X}_{T}^{i}\\right) \\frac{F_{2}\\left(\\bar{X}_{T}^{i}\\right)}{\\left(\\bar{\\sigma}^{i}\\right)^{2}} \\frac{\\left(\\bar{X}_{T}^{i}\\right)^{2}}{\\left(\\bar{\\sigma}^{i}\\right)^{2}} \\log \\left(\\sum_{t=1}^{T}\\left(\\bar{X}_{T}^{j_{t}}\\right)^{2}\\right)\\right]\\right) \\\\\n\\leq & \\mathcal{O}\\left(d\\left(\\bar{\\sigma}^{i}\\right)^{4}\\right)\n\\end{aligned}\n$$\n\nwhere the last inequality follows by observing that $F_{1}\\left(\\bar{X}_{T}^{i}\\right) \\frac{F_{2}\\left(\\bar{X}_{T}^{i}\\right)}{\\left(\\bar{\\sigma}^{i}\\right)^{2}} \\frac{\\left(\\bar{X}_{T}^{i}\\right)^{2}}{\\left(\\bar{\\sigma}^{i}\\right)^{2}} \\log \\left(\\sum_{t=1}^{T}\\left(\\bar{X}_{T}^{j_{t}}\\right)^{2}\\right)$ is a polynomial in $\\bar{X}_{T}^{i} / \\bar{\\sigma}^{i}$ multiplied by $\\log \\left(\\sum_{t=1}^{T}\\left(X_{T}^{j_{t}}\\right)^{2}\\right)$ and, using Lemma 5.\n\nThe result then follows by substituting (49) and (53) in (48).\n\n## PLACE\n\nPHOTO\nHERE\n\nSagar Sudhakara received M.Tech Degree in the area of Communication and Signal Processing from Indian Institute of Technology Bombay, Mumbai, India, in 2016 and is currently pursuing PhD in Electrical and Computer Engineering at University of Southern California. He is a recipient of USC Annenberg fellowship and his research interests include reinforcement learning and decentralized stochastic control.\n\n## PLACE\n\nPHOTO\nHERE\n\nAditya Mahajan (S'06-M'09-SM'14) is Associate Professor in the the department of Electrical and Computer Engineering, McGill University, Montreal, Canada. He currently serves as Associate Editor of Mathematics of Control, Signal, and Systems. He is the recipient of the 2015 George Axelby Outstanding Paper Award, 2014 CDC Best Student Paper Award (as supervisor), and the 2016 NecSys Best Student Paper Award (as supervisor). His principal research interests include learning and control of centralized and decentralized stochastic systems.\n\n## PLACE\n\nPHOTO\nHERE\n\nAshutosh Nayyar (S'09-M'11-SM'18) received the B.Tech. degree in electrical engineering from the Indian Institute of Technology, Delhi, India, in 2006. He received the M.S. degree in electrical engineering and computer science in 2008, the MS degree in applied mathematics in 2011, and the Ph.D. degree in electrical engineering and computer science in 2011, all from the University of Michigan, Ann Arbor. He was a Post-Doctoral Researcher at the University of Illinois at Urbana-Champaign and at the University of California, Berkeley before joining the University of Southern California in 2014. His research interests are in decentralized stochastic control, decentralized decision-making in sensing and communication systems, reinforcement learning, game theory, mechanism design and electric energy systems."
    },
    {
      "markdown": "|  | Yi Ouyang received the B.S. degree in Electrical |\n| :--: | :--: |\n| PLACE | Engineering from the National Taiwan University, |\n| PHOTO | Taipei, Taiwan in 2009, and the M.Sc and Ph.D. |\n| HERE | in Electrical Engineering and Computer Science at the University of Michigan, in 2012 and 2015, respectively. He is currently a researcher at Preferred |\n|  | Networks, Burlingame, CA. His research interests include reinforcement learning, stochastic control, and stochastic dynamic games. |"
    }
  ],
  "usage_info": {
    "pages_processed": 13,
    "doc_size_bytes": 570340
  },
  "_metadata": {
    "model_version": "0.0.0",
    "llm_model": "mistral-ocr-2505",
    "query_data": {
      "pdf": "/Users/satyaortiz-gagne/CODE/paperext/data/cache/fulltext/c685d6ba23c0354b535b724b23f52d67/fulltext.pdf"
    },
    "model_id": "parsepdf"
  }
}