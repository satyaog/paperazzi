{
  "pages": [
    {
      "markdown": "# Tensorized Random Projections \n\nBeheshteh T. Rakhshan<br>Department of Mathematics, Purdue University\n\n\n#### Abstract\n\nWe introduce a novel random projection technique for efficiently reducing the dimension of very high-dimensional tensors. Building upon classical results on Gaussian random projections and Johnson-Lindenstrauss transforms (JLT), we propose two tensorized random projection maps relying on the tensor train (TT) and CP decomposition format, respectively. The two maps offer very low memory requirements and can be applied efficiently when the inputs are low rank tensors given in the CP or TT format. Our theoretical analysis shows that the dense Gaussian matrix in JLT can be replaced by a low-rank tensor implicitly represented in compressed form with random factors, while still approximately preserving the Euclidean distance of the projected inputs. In addition, our results reveal that the TT format is substantially superior to CP in terms of the size of the random projection needed to achieve the same distortion ratio. Experiments on synthetic data validate our theoretical analysis and demonstrate the superiority of the TT decomposition.\n\n\n## 1 Introduction\n\nRandom projections (RP) are commonly used in data science and machine learning to project down highdimensional data into a lower dimensional space while preserving most of the relevant information in the data [38, 8]. These methods have been successfully used to trade accuracy in order to reduce time and storage complexity of classical learning algorithms such as $k$-nearest neighbors [3, 4, 17, 22], $k$-means [9], support vector\n\n[^0]Poceedings of the $23^{\\text {rd }}$ International Conference on Artificial Intelligence and Statistics (AISTATS) 2020, Palermo, Italy. PMLR: Volume 108. Copyright 2020 by the author(s).\n\n## Guillaume Rabusseau*\n\nDIRO and Mila, Université de Montréal\nmachines [31] and learning high-dimensional Gaussian mixtures $[12,13]$ to name a few. Most modern RP techniques build upon the celebrated Johnson-Lindenstrauss lemma [20] which shows that an arbitrary number of high-dimensional points can be linearly projected into an exponentially lower dimensional subspace while preserving distances between points. One of the simplest Johnson-Lindenstrauss transforms (JLT) is constructed from a random matrix $\\mathbf{A}$ whose entries are independently and identically drawn from a normal distribution. Fast variants of JLT have been proposed by introducing sparsity in $\\mathbf{A}[1,25]$ and by leveraging fast matrix multiplication algorithms $[3,4,5]$.\n\nAt the same time, tensor decomposition techniques have also recently emerged as a powerful tool for dealing with high-dimensional data. Tensor methods are particularly suited to handle high-dimensional multi-modal data and have been successfully applied in neuroimaging [39], signal processing [11, 35], spatio-temporal analysis [7] and computer vision [26]. But even when the data is not inherently multi-modal in nature, tensor decomposition techniques can be used to speed-up and scale classical learning algorithms to very high-dimensional spaces [28, 27]. Such algorithms exploit the ability of tensor decomposition techniques to implicitly represent very highdimensional data in compressed form, by first tensorizing the data before applying tensor decomposition techniques. In particular, the CANDECOMP/PARAFAC (CP) [16] and tensor-train (TT) [30] decomposition can represent $N$ th order $d$-dimensional tensors (or equivalently $d^{N}$ dimensional vectors) using only $\\mathcal{O}(N d R)$ and $\\mathcal{O}\\left(N d R^{2}\\right)$ parameters respectively, where the rank parameter $R$ controls the coarseness of the decomposition. Crucially, the number of parameters of these decomposition only grows linearly with the order of the tensor $N$, which is not the case for other popular decomposition models such as the Tucker decomposition [37].\n\nWhile efficient random projection techniques have been proposed to deal with high-dimensional data, RP still suffer from the curse of dimensionality when the input\n\n[^1]\n[^0]:    * Proceedings of the $23^{\\text {rd }}$ International Conference on Artificial Intelligence and Statistics (AISTATS) 2020, Palermo, Italy. PMLR: Volume 108. Copyright 2020 by the author(s).\n\n[^1]:    * CIFAR Al Chair"
    },
    {
      "markdown": "dimension is very large, which is the case for high-order tensor inputs. In this work, we propose to leverage tensor decomposition techniques to tensorize Gaussian random projections. In doing so, we design efficient random projections that can be applied to any high-order tensor inputs with arbitrary rank and structure. In particular, projecting an input tensor given in the CP or TT format can be done very efficiently. More precisely, we propose two tensorized random projection maps, $f_{\\mathrm{TT}(R)}$ and $f_{\\mathrm{CP}(R)}$, relying on the TT and CP formats respectively.\nIntuitively, the random projection maps $f_{\\mathrm{TT}(R)}$ and $f_{\\mathrm{CP}(R)}$ are constructed by enforcing a low rank tensor structure (CP or TT) on the rows of the random projection matrix $\\mathbf{A} \\in \\mathbb{R}^{k \\times d^{N}}$ where $k \\ll d^{N}$ is the size of the random projection and the inputs are $N$ th-order $d$ dimensional tensors. The parameter $R$ corresponds to the rank of the CP/TT decomposition used to represent the rows of $\\mathbf{A}$ and controls the tradeoff between the quality of the embedding and the computational and memory cost of projecting input points. More precisely, if the input $\\mathcal{X}$ is given as a rank $\\tilde{R} \\mathrm{CP}$ or TT tensor, computing $f_{\\mathrm{TT}(R)}(\\mathcal{X})$ and $f_{\\mathrm{CP}(R)}(\\mathcal{X})$ can be done in time $\\mathcal{O}\\left(k N d \\max (R, \\tilde{R})^{3}\\right)$. In terms of memory requirements, $f_{\\mathrm{TT}(R)}$ and $f_{\\mathrm{CP}(R)}$ have $\\mathcal{O}\\left(k N d R^{2}\\right)$ and $\\mathcal{O}(k N d R)$ parameters respectively. In comparison the cost of transformation for a Gaussian JLT is in $\\mathcal{O}\\left(k d^{N}\\right)$ which can be improved to $\\mathcal{O}\\left(k+N d^{N} \\log d\\right)$ using fast JLT.\n\nOur theoretical analysis shows that the key properties of Gaussian random projections are preserved after tensorization: for any $\\varepsilon>0$, with high probability, our tensorized RP embed any set of $m$ points up to multiplicative distortion $(1 \\pm \\varepsilon)$ as soon as $k \\gtrsim \\varepsilon^{-2}(1+2 / R)^{N} \\log ^{2 N} m$ for $f_{\\mathrm{TT}(R)}$ and $k \\gtrsim \\varepsilon^{-2} 3^{N-1}(1+2 / R) \\log ^{2 N} m$ for $f_{\\mathrm{CP}(R)}$. Besides showing that both tensorizations lead to efficient random projections (in terms of time and memory complexity), our analysis further reveals that $f_{\\mathrm{TT}(R)}$ is substantially superior to $f_{\\mathrm{CP}(R)}$ in terms of the size of the random projection needed to achieve the same multiplicative distortion. This can be seen by comparing the exponential dependency on the order $N$ of input tensors in the lower bounds on $k$ given above (and how increasing the rank $R$ of the tensorized map can mitigate this dependency). In particular, our analysis shows that the CP format is not a reasonable decomposition format for tensorizing random projections in the case of high order input tensors.\n\nSummary of contributions. We present two tensorized random projection maps, $f_{\\mathrm{TT}(R)}$ and $f_{\\mathrm{CP}(R)}$, using the TT and CP decomposition models respectively. We show that both maps are Johnson-Lindenstrauss Transforms offering appealing computational and memory requirements. In particular, our work is the first to design efficient RP\nfor input tensors given in the CP or TT format. Our theoretical analysis for $f_{\\mathrm{CP}(R)}$ extends the one first initiated in [36] (which was limited to matrix inputs) to high-order input tensors. To the best of our knowledge, this is the first time that the TT decomposition model is leveraged to design RP that can scale to very high-dimensional inputs. Our theoretical analysis further shows that the TT format is a better decomposition model than CP for tensorizing random projection maps. Our numerical simulations substantially validate this conclusion. It is worth mentioning that our analysis is not focused on rank-one tensors and holds for arbitrary input tensors with low CP rank or TT rank structure.\n\nRelated work. Tensor Sketch [32] is an extension of the Count Sketch algorithm [10] using fast FFT which can efficiently approximate polynomial kernels. More recently, [34] extended Tensor Sketch to exploit the multimodal structure of tensor inputs, but their approach relies on the Tucker decomposition format and cannot scale to very high-order tensors. Kapralov et al. [2] also consider sketching tensor products of data points without explicitly forming the resulting tensor, and propose an algorithm to compute a linear sketch for degree- $N$ polynomial kernels.\n\nMore closely related to our work, Sun et al. [36] introduce a Tensor Random Projection map (TRP) using a row-wise Kronecker product of random matrices. We show that their method is equivalent to the CP tensorized random projection map studied in this paper. Their theoretical analysis is limited to order 2 tensors (i.e. matrices) and rank one projection maps: they show that TRP satisfies the JL property when $k \\gtrsim \\varepsilon^{-2} \\log ^{8} m$ for $N=2$ and $R=1$. Our results for $f_{\\mathrm{CP}(R)}$ extend theirs to arbitrary values of $N$ and $R$ and provide tighter bounds even for the case of $N=2$ and $R=1$.\n\nLastly, Jin et al. [19] extend the fast JLT for embedding vectors with a Kronecker product structure. They show that the map they propose satisfy the JL property when $k \\gtrsim \\varepsilon^{-2} \\log ^{2 N-1} m \\log \\left(d^{N}\\right)$ (up to polylog factors) and that projecting a rank one tensor can be done in $\\mathcal{O}(N d \\log d+k)$. While the upper bound we derive for $f_{\\mathrm{TT}(R)}$ is comparable, the choice of the rank parameter gives more flexibility to control the trade-off between accuracy and computational efficiency. In particular, computing $f_{\\mathrm{TT}(R)}(\\mathcal{X})$ can be considerably faster than the method proposed in [19] when $\\mathcal{X}$ is a low rank tensor given in the TT format (see Section 4.1).\n\n## 2 Preliminaries\n\nIn this section, we introduce our notations and present the necessary background on tensor algebra, tensor decomposition and random projections. More details can be found"
    },
    {
      "markdown": "in $[23,38,14]$.\n\n### 2.1 Notations\n\nWe use lower case bold letters for vectors (e.g. $\\mathbf{a}, \\mathbf{b}$, ...), upper case bold letters for matrices (e.g. $\\mathbf{A}, \\mathbf{B}, \\ldots$ ), and bold calligraphic letters for higher order tensors (e.g. $\\boldsymbol{\\mathcal { A }}, \\boldsymbol{\\mathcal { B }}, \\ldots$ ). If $\\mathbf{v} \\in \\mathbb{R}^{d_{1}}$ and $\\mathbf{u} \\in \\mathbb{R}^{d_{2}}$, we use $\\mathbf{v} \\otimes \\mathbf{u} \\in \\mathbb{R}^{d_{1} d_{2}}$ to denote the Kronecker product between vectors. The 2norm of a vector $\\mathbf{u}$ is denoted by $\\|\\mathbf{u}\\|_{2}$ or simply $\\|\\mathbf{u}\\|$. The Khatri-Rao product is defined as the \"matching columnwise\" Kronecker product: if $\\mathbf{A} \\in \\mathbb{R}^{m \\times R}$ and $\\mathbf{B} \\in \\mathbb{R}^{n \\times R}$, it is denoted by $\\mathbf{A} \\odot \\mathbf{B}$ and given by $\\left[\\mathbf{a}_{1} \\otimes \\mathbf{b}_{1} \\cdots \\mathbf{a}_{R} \\otimes\\right.$ $\\left.\\mathbf{b}_{R}\\right] \\in \\mathbb{R}^{m n \\times R}$. We use the symbol \" $\\circ$ \" to denote the outer product (or tensor product) between vectors. Given a matrix $\\mathbf{S} \\in \\mathbb{R}^{d_{1} \\times d_{2}}$, we use $\\operatorname{vec}(\\mathbf{S}) \\in \\mathbb{R}^{d_{1}, d_{2}}$ to denote the column vector obtained by concatenating the columns of $\\mathbf{S}$. The $d \\times d$ identity matrix will be written as $\\mathbf{I}_{d}$ and the transpose of a matrix $\\mathbf{A}$ is denoted by $\\mathbf{A}^{\\mathrm{T}}$. For any integer $k$ we use $[k]$ to denote the set of integers from 1 to $k$. For scalars $x, y \\in \\mathbb{R}$, we use $x \\gtrsim y$ to denote that $x \\geq c y$ for some constant $c$.\n\n### 2.2 Tensors\n\nA $N$-th order tensor $\\mathcal{S} \\in \\mathbb{R}^{d_{1} \\times \\cdots \\times d_{N}}$ can simply be seen as a multidimensional array $\\left(\\mathcal{S}_{i_{1}, \\cdots, i_{N}}: i_{n} \\in\\right.$ $\\left.\\left[d_{n}\\right], n \\in[N]\\right)$. The inner product between tensors is defined by $\\langle\\mathcal{S}, \\mathcal{T}\\rangle=\\sum_{i_{1}, \\cdots, i_{N}} \\mathcal{S}_{i_{1}, \\cdots, i_{N}} \\mathcal{T}_{i_{1}, \\cdots, i_{N}}$ for $\\mathcal{T} \\in \\mathbb{R}^{d_{1} \\times \\cdots \\times d_{N}}$ and the Frobenius norm is defined by $\\|\\mathcal{S}\\|_{F}^{2}=\\langle\\mathcal{S}, \\mathcal{S}\\rangle$. If $\\mathcal{A} \\in \\mathbb{R}^{I_{1} \\times \\cdots \\times I_{N}}$ and $\\boldsymbol{B} \\in \\mathbb{R}^{J_{1} \\times \\cdots \\times J_{N}}$, we use $\\mathcal{A} \\otimes \\boldsymbol{B} \\in \\mathbb{R}^{I_{1} J_{1} \\times \\cdots \\times I_{N} J_{N}}$ to denote the Kronecker product of tensors. The mode- $n$ fibers of $\\mathcal{S}$ are the vectors obtained by fixing all indices except the $n$th one. The $n$-th mode matricization of $\\mathcal{S}$ is the matrix having the mode- $n$ fibers of $\\mathcal{S}$ for columns* and is denoted by $\\mathcal{S}_{(n)} \\in \\mathbb{R}^{d_{n} \\times d_{1} \\cdots d_{n-1} d_{n+1} \\cdots d_{N}}$. The vectorization of a tensor is the vector obtained by concatenating its mode-1 fibers, i.e., $\\operatorname{vec}(\\mathcal{S})=\\operatorname{vec}\\left(\\mathcal{S}_{(1)}\\right)$. The notion of matricization can be extended to any subset $I \\subset[N]$ of the modes of $\\mathcal{S}$, resulting in a matrix $\\mathcal{S}_{(I)}$ of size $\\prod_{i \\in I} d_{i} \\times \\prod_{j \\in[N]\\backslash I} d_{j}$.\nA rank $R C P$ decomposition of a tensor $\\mathcal{S} \\in \\mathbb{R}^{d_{1} \\times \\cdots \\times d_{N}}$ consists in factorizing $\\mathcal{S}$ into a sum of $R$ rank one tensors: $\\mathcal{S}=\\sum_{r=1}^{R} \\mathbf{a}_{r}^{r} \\circ \\mathbf{a}_{r}^{2} \\circ \\cdots \\mathbf{a}_{r}^{N}$ where each $\\mathbf{a}_{r}^{n} \\in \\mathbb{R}^{d_{n}}$. Stacking the vectors $\\mathbf{a}_{1}^{n}, \\ldots, \\mathbf{a}_{R}^{n}$ into a factor matrix $\\mathbf{A}^{n} \\in \\mathbb{R}^{d_{n} \\times R}$ for each $n \\in[N]$, we will concisely denote the CP decomposition by $\\mathcal{S}=\\llbracket \\mathbf{A}^{1}, \\cdots, \\mathbf{A}^{N} \\rrbracket$.\nA rank $R$ tensor train decomposition of a tensor $\\mathcal{S} \\in$ $\\mathbb{R}^{d_{1} \\times \\cdots \\times d_{N}}$ consists in factorizing $\\mathcal{S}$ into the the product\n\n[^0]of $N$ 3rd-order core tensors $\\boldsymbol{G}^{1} \\in \\mathbb{R}^{1 \\times d_{1} \\times R}, \\boldsymbol{G}^{2} \\in$ $\\mathbb{R}^{R \\times d_{2} \\times R}, \\cdots, \\boldsymbol{G}^{N-1} \\in \\mathbb{R}^{R \\times d_{N-1} \\times R}, \\boldsymbol{G}^{N} \\in$ $\\mathbb{R}^{R \\times d_{N} \\times 1}$, and is defined ${ }^{\\dagger}$ by $\\mathcal{S}_{i_{1}, \\cdots, i_{N}}=$ $\\left(\\boldsymbol{G}^{1}\\right)_{i_{1},},\\left(\\boldsymbol{G}^{2}\\right)_{:, i_{2},}, \\cdots\\left(\\boldsymbol{G}^{N-1}\\right)_{:, i_{N-1},},\\left(\\boldsymbol{G}^{N}\\right)_{:, i_{N}}$, for all indices $i_{1} \\in\\left[d_{1}\\right], \\cdots, i_{N} \\in\\left[d_{N}\\right]$; we will use the notation $\\mathcal{S}=\\left\\langle\\left\\langle\\mathcal{G}^{1}, \\mathcal{G}^{2}, \\cdots, \\mathcal{G}^{N-1}, \\mathcal{G}^{N}\\right\\rangle\\right\\rangle$ to denote the TT decomposition.\n\n### 2.3 Johnson-Lindenstrauss Transform\n\nA classical result of Johnson-Lindenstrauss (JL) [20] states that any $m$-point set $P$ in $d$ dimension can be linearly projected to $k=\\Omega\\left(\\varepsilon^{-2} \\log (m)\\right)$ dimensions while approximately preserving the pairwise distances between the points. More precisely, there exists a map $f: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}^{k}(d \\gg k)$ such that for all $\\mathbf{u}, \\mathbf{v} \\in P$,\n$(1-\\varepsilon)\\|\\mathbf{u}-\\mathbf{v}\\|^{2} \\leq\\|f(\\mathbf{u})-f(\\mathbf{v})\\|^{2} \\leq(1+\\varepsilon)\\|\\mathbf{u}-\\mathbf{v}\\|^{2}$.\nWe will call a map satisfying this property a JohnsonLindenstrauss transform (JLT). One of the simplest examples of a JL transform is the so-called Gaussian random projection map $f: \\mathbf{x} \\mapsto \\frac{1}{\\sqrt{k}} \\mathbf{A} \\mathbf{x}$ where $\\mathbf{A} \\in \\mathbb{R}^{k \\times d}$ is a random matrix whose entries are independently drawn from a normal distribution. For a fixed set of input points in $\\mathbb{R}^{d}, f$ will satisfy the JL property with high probability. To cope with the computational cost and storage requirements of Gaussian random projections, sparse and very-sparse random projections were proposed in [1] and [25] respectively. These maps leverage the fact that the JL property is preserved even if only a small subset of the entries of $\\mathbf{A}$ are normal variables while the other ones are set to 0 .\n\nIt is easy to see that in order to be a JL transform, a map $f$ must satisfy two fundamental properties: (i) it has to be an expected isometry, i.e. $\\mathbb{E}\\left[\\|f(\\mathbf{x})\\|^{2}\\right]=\\|\\mathbf{x}\\|^{2}$, and (ii) the variance of $\\|f(\\mathbf{x})\\|^{2}$ should quickly decrease to 0 as the size of the random projection $k$ increases.\n\n## 3 Tensorized Random Projections\n\nAs mentioned in the previous section, sparse and verysparse Gaussian RP reduce time and memory complexity by enforcing the rows of the matrix $\\mathbf{A}$ in the Gaussian RP $f: \\mathbf{x} \\rightarrow \\frac{1}{\\sqrt{k}} \\mathbf{A} \\mathbf{x}$ to be sparse. In this work, we propose to enforce a low rank tensor structure on the rows of $\\mathbf{A}$ instead to obtain better scalability w.r.t. the input dimension, which is crucial when dealing with high-order tensor inputs.\n\n[^1]\n[^0]:    *The specific ordering of the fibers does not matter as long as it is consistent across all reshaping operations.\n\n[^1]:    ${ }^{\\dagger}$ The general definition of the TT-decomposition allows the rank $R$ to be different for each mode, but this definition is sufficient for the purpose of this paper."
    },
    {
      "markdown": "We present two tensorized random projection maps, $f_{\\mathrm{TT}(R)}$ and $f_{\\mathrm{CP}(R)}$, relying on the TT and CP decomposition respectively. These maps embed any tensor $\\boldsymbol{\\mathcal { X }} \\in \\mathbb{R}^{d_{1} \\times \\cdots \\times d_{N}}$ into $\\mathbb{R}^{k}$, where $k \\ll d_{1} d_{2} \\cdots d_{N}$. Considering the case $d_{1}=\\cdots=d_{N}=d$ for simplicity, classical random projection maps would require $\\mathcal{O}\\left(k d^{N}\\right)$ parameters (or $\\mathcal{O}\\left(k \\sqrt{d^{N}}\\right)$ with very sparse random projections) which is costly when $N$ is large. In contrast, $f_{\\mathrm{TT}(R)}$ and $f_{\\mathrm{CP}(R)}$ only require $\\mathcal{O}\\left(k N d R^{2}\\right)$ and $\\mathcal{O}(k N d R)$ parameters respectively. The two maps are constructed similarly: each component of the projection is given by the inner product between the input and a random tensor with a low rank structure (w.r.t. either the TT or CP decomposition format). Formally, we have the following two definitions:\nDefinition 1. $A$ TT random projection of rank $R$ is a linear map $f_{\\mathrm{TT}(R)}: \\mathbb{R}^{d_{1} \\times \\cdots \\times d_{N}} \\rightarrow \\mathbb{R}^{k}$ defined componentwise by\n$\\left(f_{\\mathrm{TT}(R)}(\\boldsymbol{\\mathcal { X }})\\right)_{i}:=\\frac{1}{\\sqrt{k}}\\left\\langle\\left\\langle\\left\\langle\\mathcal{G}_{i}^{1}, \\mathcal{G}_{i}^{2}, \\cdots, \\mathcal{G}_{i}^{N}\\right\\rangle\\right\\rangle, \\boldsymbol{\\mathcal { X }}\\right\\rangle, i \\in[k]$\nwhere $\\boldsymbol{\\mathcal { G }}_{i}^{1} \\in \\mathbb{R}^{1 \\times d_{1} \\times R}, \\boldsymbol{\\mathcal { G }}_{i}^{2} \\in \\mathbb{R}^{R \\times d_{2} \\times R}, \\cdots, \\boldsymbol{\\mathcal { G }}_{i}^{N-1} \\in$ $\\mathbb{R}^{R \\times d_{N-1} \\times R}, \\boldsymbol{\\mathcal { G }}_{i}^{N} \\in \\mathbb{R}^{R \\times d_{N} \\times 1}$ for $i \\in[k]$, and the entries of each $\\boldsymbol{\\mathcal { G }}_{i}^{n}$ for $i \\in[k], n \\in[N]$ are drawn independently from a Gaussian distribution with mean 0 and variance $\\frac{1}{\\sqrt{R}}$ if $n \\in\\{1, N\\}$ and variance $\\frac{1}{R}$ if $1<n<N$.\nDefinition 2. $A$ CP random projection of rank $R$ is a linear map $f_{\\mathrm{CP}(R)}: \\mathbb{R}^{d_{1} \\times \\cdots \\times d_{N}} \\rightarrow \\mathbb{R}^{k}$ defined componentwise by\n$\\left(f_{\\mathrm{CP}(R)}(\\boldsymbol{\\mathcal { X }})\\right)_{i}:=\\frac{1}{\\sqrt{k}}\\left\\langle\\left\\langle\\left\\langle\\mathbf{A}_{i}^{1}, \\mathbf{A}_{i}^{2}, \\cdots, \\mathbf{A}_{i}^{N}\\right\\rangle\\right\\rangle, \\boldsymbol{\\mathcal { X }}\\right\\rangle, i \\in[k]$ where each $\\mathbf{A}_{i}^{n} \\in \\mathbb{R}^{d_{n} \\times R}$ for $i \\in[k], n \\in[N]$ and the entries of each $\\mathbf{A}_{i}^{n}$ are drawn independently from a Gaussian distribution with mean 0 and variance $\\left(\\frac{1}{R}\\right)^{\\frac{1}{R}}$.\nOne can check that applying these projection maps on an input tensor given in the CP or the TT format can be done efficiently: the complexity of computing $f_{\\mathrm{TT}(R)}(\\boldsymbol{\\mathcal { X }})$ is in $\\mathcal{O}\\left(k N d \\max (R, \\tilde{R})^{3}\\right)$ if $\\boldsymbol{\\mathcal { X }}$ is given as a rank $\\tilde{R} \\mathrm{CP}$ or TT tensor, and the complexity for $f_{\\mathrm{CP}(R)}(\\boldsymbol{\\mathcal { X }})$ is in $\\mathcal{O}\\left(k N d \\max (R, \\tilde{R})^{2}\\right)$ if $\\boldsymbol{\\mathcal { X }}$ is in the CP format and in $\\mathcal{O}\\left(k N d \\max (R, \\tilde{R})^{3}\\right)$ if $\\boldsymbol{\\mathcal { X }}$ is in the TT format (where we assumed $d_{1}=\\cdots=d_{N}=d$ for simplicity).\nBefore studying the properties of these tensorized random projections in the next section, we show how $f_{\\mathrm{CP}(\\cdot)}$ is equivalent to the tensor random projection map proposed in [36]. In this work, the authors introduce the map\n$f_{\\mathrm{TRP}}(\\boldsymbol{\\mathcal { X }}):=\\frac{1}{\\sqrt{k}}\\left(\\mathbf{A}^{1} \\odot \\mathbf{A}^{2} \\odot \\cdots \\odot \\mathbf{A}^{N}\\right)^{\\top} \\operatorname{vec}(\\boldsymbol{\\mathcal { X }}) \\in \\mathbb{R}^{k}$,\nwhere each $\\mathbf{A}^{n} \\in \\mathbb{R}^{d_{n} \\times k}$ for $n \\in[N]$ is a random matrix whose entries are i.i.d random variables with\nmean zero and variance one. One can check that $f_{\\text {TRP }}$ is strictly equivalent to $f_{\\mathrm{CP}(1)}$ using basic properties of the CP decomposition. Furthermore, the authors introduce a variance reduction technique with the map $f_{\\text {TRP }(T)}$, a scaled average of $T$ independent TRPs, defined by $f_{\\text {TRP }(T)}(\\boldsymbol{\\mathcal { X }}):=\\frac{1}{\\sqrt{T}} \\sum_{t=1}^{T} f_{\\text {TRP }}^{(t)}(\\boldsymbol{\\mathcal { X }})$. Again, one can easily check the strict equivalence between $f_{\\mathrm{CP}(R)}$ and $f_{\\text {TRP }(T)}$ when $R=T$.\n\n## 4 Main Results\n\nIn this section, we present our main results showing that the tensorized projection $f_{\\mathrm{TT}(R)}$ and $f_{\\mathrm{CP}(R)}$ still benefits from the fundamental properties of Gaussian random projections: they are expected isometry and the variance of the norm of the projections decreases to 0 as the embedding dimension $k$ grows. These results imply that, in addition to be particularly efficient in terms of storage requirement and computational cost, these maps are JL transforms: they approximately preserve Euclidean distances between projected points. Moreover, our analysis will show that there is a crucial difference between the two tensorized random projections: as the order of the input tensor $\\boldsymbol{\\mathcal { X }}$ grows, the embedding dimension of $f_{\\mathrm{CP}(R)}$ needs to grow exponentially in comparison to the one of $f_{\\mathrm{TT}(R)}$ in order to achieve the same distortion ratio $\\varepsilon$. Our results rely on the following theorem which shows that both maps are expected isometries and gives bounds on the variance of the two projections.\nTheorem 1. Let $\\boldsymbol{\\mathcal { X }} \\in \\mathbb{R}^{d_{1} \\times d_{2} \\times \\cdots \\times d_{N}}$. The random projection maps $f_{\\mathrm{TT}(R)}$ and $f_{\\mathrm{CP}(R)}$ (see Definitions 1 and 2) satisfy the following properties:\n\n- $\\mathbb{E}\\left[\\left\\|f_{\\mathrm{CP}(R)}(\\boldsymbol{\\mathcal { X }})\\right\\|_{2}^{2}\\right]=\\mathbb{E}\\left[\\left\\|f_{\\mathrm{TT}(R)}(\\boldsymbol{\\mathcal { X }})\\right\\|_{2}^{2}\\right]=\\|\\boldsymbol{\\mathcal { X }}\\|_{F}^{2}$\n- $\\operatorname{Var}\\left(\\left\\|f_{\\mathrm{TT}(R)}(\\boldsymbol{\\mathcal { X }})\\right\\|_{2}^{2}\\right) \\leq \\frac{1}{k}\\left(3\\left(1+\\frac{2}{R}\\right)^{N-1}-1\\right)\\|\\boldsymbol{\\mathcal { X }}\\|_{F}^{4}$\n- $\\operatorname{Var}\\left(\\left\\|f_{\\mathrm{CP}(R)}(\\boldsymbol{\\mathcal { X }})\\right\\|_{2}^{2}\\right) \\leq \\frac{1}{k}\\left(3^{N-1}\\left(1+\\frac{2}{R}\\right)-1\\right)\\|\\boldsymbol{\\mathcal { X }}\\|_{F}^{4}$\n\nThe proof of this theorem for the TT random projection map is given in the next section and the proof for $f_{\\mathrm{CP}(R)}$ can be found in the Appendix.\nIn the case of vector inputs, i.e. $N=1$, we recover the classical expression for the variance of Gaussian random projections given by $\\operatorname{Var}\\left(\\|f(\\mathbf{x})\\|\\right)^{2}=\\frac{2}{k}\\|\\mathbf{x}\\|^{4}$ (note that in this setting $R$ is necessarily equal to 1 since $N=1$ ).\nIt is worth mentioning that the only inequality used to derive the bounds comes from the sub-multiplicativity of the Frobenius norm applied to matricizations of the input tensor $\\boldsymbol{\\mathcal { X }}$. For example, for the case of order 2 input tensors, i.e. matrices, the variance of $f_{\\mathrm{TT}(R)}$ is given by $\\operatorname{Var}\\left(\\left\\|f_{\\mathrm{TT}(R)}(\\mathbf{X})\\right\\|^{2}\\right)=\\frac{1}{k}\\left(2\\|\\mathbf{X}\\|_{F}^{4}+\\frac{6}{R} \\operatorname{Tr}\\left[\\left(\\mathbf{X}^{\\top} \\mathbf{X}\\right)^{2}\\right]\\right)$."
    },
    {
      "markdown": "Comparing now the bounds on the variance of $f_{\\mathrm{TT}(R)}$ and $f_{\\mathrm{CP}(R)}$, we observe that while both bounds have an exponential dependency on the order $N$ of the input tensors, slightly increasing the rank $R$ of the TT random projection mitigates this dependency while it has no effect for the CP random projection. This shows that $f_{\\mathrm{CP}(R)}$ is not a suitable RP since $k$ has to grow exponentially in $N$ in order to approach the variance of classical Gaussian random projections. Using the bounds on the variance of the projections, we can now derive lower bounds on the size $k$ of the random projections $f_{\\mathrm{CP}(R)}$ and $f_{\\mathrm{TT}(R)}$ needed to satisfy the JL property with high probability.\nTheorem 2. Let $P \\subset \\mathbb{R}^{d_{1} \\times d_{2} \\times \\cdots \\times d_{N}}$ be a set of $m$ order $N$ tensors. Then, for any $\\varepsilon>0$ and any $\\delta>0$, the following hold simultaneously for all $\\boldsymbol{\\mathcal { X } \\in P}$ :\n\n- if $k \\gtrsim \\varepsilon^{-2}(1+2 / R)^{N} \\log ^{2 N}\\left(\\frac{m}{2}\\right)$ then $\\mathbb{P}\\left(\\left\\|f_{\\mathrm{TT}(R)}(\\boldsymbol{\\mathcal { X }})\\right\\|_{2}^{2}=(1 \\pm \\varepsilon)\\|\\boldsymbol{\\mathcal { X }}\\|_{F}^{2}\\right) \\geq 1-\\delta$,\n- if $k \\gtrsim \\varepsilon^{-2} 3^{N-1}(1+2 / R) \\log ^{2 N}\\left(\\frac{m}{2}\\right)$ then $\\mathbb{P}\\left(\\left\\|f_{\\mathrm{CP}(R)}(\\boldsymbol{\\mathcal { X }})\\right\\|_{2}^{2}=(1 \\pm \\varepsilon)\\|\\boldsymbol{\\mathcal { X }}\\|_{F}^{2}\\right) \\geq 1-\\delta$.\n\n\n### 4.1 Comparison to related work\n\nWe conclude this section by comparing the previous theorem with the closest related work. Jin et al. [19] proposed a Kronecker structured JL transform satisfying the JL property for $m$ points with probability $1-\\delta$ as soon as $k \\gtrsim \\varepsilon^{-2} \\log ^{2 N-1}\\left(\\frac{m}{2}\\right) \\log \\left(d^{N}\\right)$, up to polylog factors. Our results are similar to theirs but differ in one key aspect. In their work, projecting a rank one tensor can be done in $\\mathcal{O}(N d \\log d+k)$. Hence, by linearity, projecting a tensor of rank $R$ given in the CP format can be done in $\\mathcal{O}(\\bar{R}(N d \\log d+k))$. However, low rank tensors given in the TT format cannot be efficiently projected using their method ${ }^{\\ddagger}$. In contrast, $f_{\\mathrm{TT}(R)}(\\boldsymbol{\\mathcal { X }})$ and $f_{\\mathrm{CP}(R)}(\\boldsymbol{\\mathcal { X }})$ can both be computed in $\\mathcal{O}\\left(k N d \\max (R, \\bar{R})^{3}\\right)$ when $\\boldsymbol{\\mathcal { X }}$ is given as a rank $\\bar{R} \\mathrm{CP}$ or TT tensor; our approach is thus better suited for inputs given in the TT format. In [36], they proposed a tensor random projection map for subGaussian random variables. They give a lower bound of $k \\gtrsim \\varepsilon^{-2} \\log ^{8}\\left(\\frac{m}{2}\\right)$ only for the case of order 2 input tensors, treating the rank parameter $R$ as a constant. Moreover, even in the case of order 2 input tensors our lower bound of $\\varepsilon^{-2}(1+2 / R) \\log ^{4}\\left(\\frac{m}{2}\\right)$ is tighter than the one they provide.\n\n## 5 Proofs\n\nIn this section, we present the proofs of our results for the random projection map $f_{\\mathrm{TT}(R)}$. The techniques used for\n\n[^0]the map $f_{\\mathrm{CP}(R)}$ are of a similar flavor and can be found in the Appendix.\n\n### 5.1 Proof of Theorem 1: TT case\n\nExpected isometry. We start by showing that $f_{\\mathrm{TT}(R)}$ is an expected isometry, i.e. that $\\mathbb{E}\\left\\|f_{\\mathrm{TT}(R)}(\\boldsymbol{\\mathcal { X }})\\right\\|_{2}^{2}=$ $\\|\\boldsymbol{\\mathcal { X }}\\|_{F}^{2}$. Let $y_{i}=\\left\\langle\\left\\langle\\left\\langle\\boldsymbol{\\mathcal { G }}\\right|, \\boldsymbol{\\mathcal { G }}\\right\\rangle, \\cdots, \\boldsymbol{\\mathcal { G }}\\left\\langle\\right.\\right\\rangle_{i} \\boldsymbol{\\mathcal { X }}\\right\\rangle$ and $\\mathbf{y}=\\left[y_{1}, y_{2}, \\cdots, y_{k}\\right]$. With these definitions we have $f_{\\mathrm{TT}(R)}(\\boldsymbol{\\mathcal { X }})=\\frac{1}{\\sqrt{k}} \\mathbf{y}$ and it is thus sufficient to find $\\mathbb{E}\\left[y_{1}^{2}\\right]$. To lighten the notation, let $\\boldsymbol{\\mathcal { G }}^{n}=\\boldsymbol{\\mathcal { G }}_{1}^{n}$ for each $n \\in[N]$ and let $\\boldsymbol{\\mathcal { S }}=\\left\\langle\\left\\langle\\boldsymbol{\\mathcal { G }}^{1}, \\boldsymbol{\\mathcal { G }}^{2}, \\cdots, \\boldsymbol{\\mathcal { G }}^{N}\\right\\rangle\\right\\rangle$. We have\n\n$$\n\\begin{aligned}\n\\mathbb{E}\\left[y_{1}^{2}\\right] & =\\mathbb{E}\\left[\\left\\langle\\boldsymbol{\\mathcal { S }}, \\boldsymbol{\\mathcal { X }}\\right\\rangle^{2}\\right]=\\mathbb{E}\\left[\\left\\langle\\boldsymbol{\\mathcal { S }} \\otimes \\boldsymbol{\\mathcal { X }} \\otimes \\boldsymbol{\\mathcal { X }}\\right\\rangle\\right] \\\\\n& =\\left\\langle\\mathbb{E}\\left[\\boldsymbol{\\mathcal { S }} \\otimes \\boldsymbol{\\mathcal { S }}\\right], \\boldsymbol{\\mathcal { X }} \\otimes \\boldsymbol{\\mathcal { X }}\\right\\rangle\n\\end{aligned}\n$$\n\nUsing the fact that the core tensors $\\boldsymbol{\\mathcal { G }}^{n}$ are independent, we have\n\n$$\n\\begin{aligned}\n\\mathbb{E}[\\boldsymbol{\\mathcal { S }} \\otimes \\boldsymbol{\\mathcal { S }}]=\\mathbb{E}\\left[\\left\\langle\\left\\langle\\boldsymbol{\\mathcal { G }}^{1} \\otimes \\boldsymbol{\\mathcal { G }}^{1}, \\cdots, \\boldsymbol{\\mathcal { G }}^{N} \\otimes \\boldsymbol{\\mathcal { G }}^{N}\\right\\rangle\\right\\rangle\\right] \\\\\n& =\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left\\langle\\left"
    },
    {
      "markdown": "Proof. Setting $\\mathbf{a}=\\operatorname{vec}(\\mathbf{A}) \\in \\mathbb{R}^{m n}$ and $\\mathbf{b}=\\operatorname{vec}(\\mathbf{B}) \\in$ $\\mathbb{R}^{m n}$, we have\n\n$$\n\\begin{aligned}\n\\mathbb{E}\\langle\\mathbf{A}, \\mathbf{B}\\rangle^{4} & =\\mathbb{E}\\langle\\mathbf{a}, \\mathbf{b}\\rangle^{4} \\\\\n& =\\mathbb{E}\\left\\langle\\mathbf{a}^{\\otimes 4}, \\mathbf{b}^{\\otimes 4}\\right\\rangle=\\left\\langle\\mathbb{E}\\left[\\mathbf{a}^{\\otimes 4}\\right], \\mathbb{E}\\left[\\mathbf{b}^{\\otimes 4}\\right]\\right\\rangle\n\\end{aligned}\n$$\n\nwhere the last equality is obtained by using the independence between $\\mathbf{a}$ and $\\mathbf{b}$. Element-wise, by using Isselis' theorem [18] and using the fact that $\\mathbf{a} \\sim \\mathcal{N}\\left(\\mathbf{0}, \\sigma^{2} \\mathbf{I}\\right)$ we have,\n\n$$\n\\begin{aligned}\n& \\left(\\mathbb{E}\\left[\\mathbf{a}^{\\otimes 4}\\right]\\right)\\rangle_{i_{1}, i_{2}, i_{3}, i_{4}}=\\mathbb{E}\\left[\\mathbf{a}_{i_{1}} \\mathbf{a}_{i_{2}} \\mathbf{a}_{i_{3}} \\mathbf{a}_{i_{4}}\\right] \\\\\n& \\quad=\\mathbb{E}\\left[\\mathbf{a}_{i_{1}} \\mathbf{a}_{i_{2}}\\right] \\mathbb{E}\\left[\\mathbf{a}_{i_{3}} \\mathbf{a}_{i_{4}}\\right]+\\mathbb{E}\\left[\\mathbf{a}_{i_{1}} \\mathbf{a}_{i_{3}}\\right] \\mathbb{E}\\left[\\mathbf{a}_{i_{2}} \\mathbf{a}_{i_{4}}\\right] \\\\\n& \\quad+\\mathbb{E}\\left[\\mathbf{a}_{i_{1}} \\mathbf{a}_{i_{4}}\\right] \\mathbb{E}\\left[\\mathbf{a}_{i_{2}} \\mathbf{a}_{i_{3}}\\right] \\\\\n& \\quad=\\left(\\delta_{i_{1} i_{2}} \\delta_{i_{3} i_{4}}+\\delta_{i_{1} i_{3}} \\delta_{i_{2} i_{4}}+\\delta_{i_{1} i_{4}} \\delta_{i_{2} i_{3}}\\right) \\sigma^{4}\n\\end{aligned}\n$$\n\nwhere $\\delta$ is the Kronecker symbol. Therefore, letting $\\Delta_{i_{1} i_{2} i_{3} i_{4}}=\\delta_{i_{1} i_{2}} \\delta_{i_{3} i_{4}}+\\delta_{i_{1} i_{3}} \\delta_{i_{2} i_{4}}+\\delta_{i_{1} i_{4}} \\delta_{i_{2} i_{3}}$, we obtain\n\n$$\n\\begin{aligned}\n\\mathbb{E}\\langle\\mathbf{A}, \\mathbf{B}\\rangle^{4} & =\\sum_{i_{1}, i_{2}, i_{3}, i_{4}} \\mathbb{E}\\left[\\mathbf{a}_{i_{1}} \\mathbf{a}_{i_{2}} \\mathbf{a}_{i_{3}} \\mathbf{a}_{i_{4}}\\right] \\mathbb{E}\\left[\\mathbf{b}_{i_{1}} \\mathbf{b}_{i_{2}} \\mathbf{b}_{i_{3}} \\mathbf{b}_{i_{4}}\\right] \\\\\n& =\\sigma^{4} \\sum_{i_{1}, i_{2}, i_{3}, i_{4}} \\Delta_{i_{1} i_{2} i_{3} i_{4}} \\mathbb{E}\\left[\\mathbf{b}_{i_{1}} \\mathbf{b}_{i_{2}} \\mathbf{b}_{i_{3}} \\mathbf{b}_{i_{4}}\\right] \\\\\n& =\\sigma^{4} \\mathbb{E}\\left[\\sum_{i_{1}, i_{3}} \\mathbf{b}_{i_{1}}^{2} \\mathbf{b}_{i_{3}}^{2}+\\sum_{i_{1}, i_{4}} \\mathbf{b}_{i_{1}}^{2} \\mathbf{b}_{i_{4}}^{2}+\\sum_{i_{1}, i_{2}} \\mathbf{b}_{i_{1}}^{2} \\mathbf{b}_{i_{2}}^{2}\\right] \\\\\n& =3 \\sigma^{4} \\mathbb{E}\\|\\mathbf{B}\\|_{F}^{4}\n\\end{aligned}\n$$\n\nLemma 4. Let $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ be a random matrix whose entries are i.i.d Gaussian random variables with mean zero and variance $\\sigma^{2}$, and let $\\mathbf{B} \\in \\mathbb{R}^{p \\times m}$ be a (random) matrix independent of $\\mathbf{A}$. Then,\n\n$$\n\\begin{aligned}\n\\mathbb{E}\\|\\mathbf{B A}\\|_{F}^{4} & =n \\sigma^{4}\\left(n \\mathbb{E}\\|\\mathbf{B}\\|_{F}^{4}+2 \\mathbb{E} \\operatorname{tr}\\left(\\left(\\mathbf{B}^{\\top} \\mathbf{B}\\right)^{2}\\right)\\right) \\\\\n& \\leq \\sigma^{4} n(n+2) \\mathbb{E}\\|\\mathbf{B}\\|_{F}^{4}\n\\end{aligned}\n$$\n\nProof. By definition of the Frobenius norm we have\n\n$$\n\\mathbb{E}\\|\\mathbf{B A}\\|_{F}^{4}=\\mathbb{E}\\left[\\operatorname{tr}\\left(\\mathbf{B}^{\\top} \\mathbf{B A A}^{\\top}\\right) \\operatorname{tr}\\left(\\mathbf{B}^{\\top} \\mathbf{B A A}^{\\top}\\right)\\right]\n$$\n\nSince $\\mathbf{A}_{i j} \\sim \\mathcal{N}\\left(0, \\sigma^{2}\\right)$ for any $i \\in[m], j \\in[n], \\mathbf{A A}^{\\top} \\in$ $\\mathbb{R}^{m \\times m}$ is a random symmetric positive definite matrix following a Wishart distribution with parameters $m, n$ and $\\sigma^{2} \\mathbf{I}_{m} \\in \\mathbb{R}^{m \\times m}$. Therefore,\n\n$$\n\\begin{aligned}\n\\mathbb{E}\\left[\\operatorname{tr}\\left(\\mathbf{B}^{\\top} \\mathbf{B A A}^{\\top}\\right) \\operatorname{tr}\\left(\\mathbf{B}^{\\top} \\mathbf{B A A}^{\\top}\\right)\\right] \\\\\n\\quad= & n \\sigma^{4}\\left(n \\mathbb{E}\\|\\mathbf{B}\\|_{F}^{4}+2 \\mathbb{E} \\operatorname{tr}\\left(\\left(\\mathbf{B}^{\\top} \\mathbf{B}\\right)^{2}\\right)\\right) \\\\\n& \\leq \\sigma^{4} n(n+2) \\mathbb{E}\\|\\mathbf{B}\\|_{F}^{4}\n\\end{aligned}\n$$\n\nwhere the equality follows from standard properties of the Wishart distribution (see e.g., Section 3.3.6 of [15]), and the inequality follows from the sub-multiplicativity of the Frobenius norm.\n\nLet us now start by defining the tensor $\\mathcal{M}^{n} \\in$ $\\mathbb{R}^{R \\times d_{1} \\times d_{2} \\times \\cdots \\times d_{n-1}}$ for each $2 \\leq n \\leq N$ componentwise by\n\n$$\n\\begin{aligned}\n& \\mathcal{M}_{r, i_{1}, \\ldots, i_{n-1}}^{n}=\\sum_{\\substack{r_{n} \\ldots, i_{N} \\\\\nr_{n} \\ldots, r_{N-1}}}(\\boldsymbol{G}^{n})_{r, i_{n}, r_{n}}\\left(\\boldsymbol{G}^{n+1}\\right)_{r_{n}, i_{n+1}, r_{n+1}} \\\\\n& \\left.\\ldots\\left(\\boldsymbol{G}^{N-1}\\right)_{r_{N-2}, i_{N-1}, r_{N-1}}\\left(\\boldsymbol{G}^{N}\\right)_{r_{N-1}, i_{N}} \\boldsymbol{X}_{i_{1}, \\ldots, i_{N}}\\right)\n\\end{aligned}\n$$\n\nfor each $r \\in[R], i_{1} \\in\\left[d_{1}\\right], \\ldots, i_{n-1} \\in\\left[d_{n-1}\\right]$. In some sense, $\\mathcal{M}^{n}$ is the tensor obtained by removing the first $n-1$ cores from the computation of $y_{1}=\\left\\langle\\left\\langle\\left\\langle\\mathcal{G}^{1}, \\mathcal{G}^{2}, \\cdots, \\mathcal{G}^{N}\\right\\rangle\\right\\rangle, \\boldsymbol{X}\\right\\rangle$. With this definition, one can check that $\\bullet\\left\\langle\\left\\langle\\left\\langle\\mathcal{G}^{1}, \\mathcal{G}^{2}, \\cdots, \\mathcal{G}^{N}\\right\\rangle\\right\\rangle \\boldsymbol{X}\\right\\rangle=$ $\\left\\langle\\mathcal{G}^{1}, \\mathcal{M}^{2}\\right\\rangle, \\bullet \\mathcal{M}_{(1)}^{25}=\\left(\\mathcal{G}^{N}\\right)_{(1)} \\boldsymbol{X}_{(N)}$ and $\\bullet \\mathcal{M}_{(1)}^{n}=$ $\\left(\\mathcal{G}^{n}\\right)_{(1)}\\left(\\mathcal{M}^{n+1}\\right)_{(1, n+1)}$ for each $n \\in[N]$, where $\\left(\\mathcal{M}^{n+1}\\right)_{(1, n+1)} \\in \\mathbb{R}^{R d_{n} \\times d_{1} \\ldots d_{n-1}}$ denotes the matricization of $\\mathcal{M}^{n+1}$ obtained by mapping its first and last modes to rows and the other ones to columns. Let $\\sigma_{n}^{2}$ denote the variance used to draw the entries of each core $\\mathcal{G}^{n}$. Using Lemma 3 we obtain\n\n$$\n\\begin{aligned}\n\\mathbb{E} y_{1}^{4} & =\\mathbb{E}\\left\\langle\\left\\langle\\left\\langle\\mathcal{G}^{1}, \\mathcal{G}^{2}, \\cdots, \\mathcal{G}^{N}\\right\\rangle\\right\\rangle, \\boldsymbol{X}\\right\\rangle^{4}=\\mathbb{E}\\left\\langle\\mathcal{G}^{1}, \\mathcal{M}_{(1)}^{2}\\right\\rangle^{4} \\\\\n& =3 \\sigma_{1}^{4} \\mathbb{E}\\left\\|\\mathcal{M}_{(1)}^{2}\\right\\|_{F}^{4}=3 \\sigma_{1}^{4} \\mathbb{E}\\left\\|\\left(\\mathcal{G}^{2}\\right)_{(1)} \\mathcal{M}_{(1,3)}^{3}\\right\\|_{F}^{4}\n\\end{aligned}\n$$\n\nUsing the fact that the Frobenius norm of a tensor is constant across all matricizations and by Lemma 4 we get\n\n$$\n\\begin{aligned}\n\\mathbb{E}\\left[y_{1}^{4}\\right] & =3 \\sigma_{1}^{4} \\mathbb{E}\\left\\|\\left(\\mathcal{G}^{2}\\right)_{(1)} \\mathcal{M}_{(1,3)}^{3}\\right\\|_{F}^{4} \\\\\n& \\leq 3 \\sigma_{1}^{4} \\sigma_{2}^{4} R(R+2) \\mathbb{E}\\left\\|\\mathcal{M}_{(1,3)}^{3}\\right\\|_{F}^{4} \\\\\n& =3 \\sigma_{1}^{4} \\sigma_{2}^{4} R(R+2) \\mathbb{E}\\left\\|\\mathcal{M}_{(1)}^{3}\\right\\|_{F}^{4} \\\\\n& =3 \\sigma_{1}^{4} \\sigma_{2}^{4} R(R+2) \\mathbb{E}\\left\\|\\left(\\mathcal{G}^{3}\\right)_{(1)} \\mathcal{M}_{(1,4)}^{4}\\right\\|_{F}^{4} \\\\\n& \\leq 3 \\sigma_{1}^{4} \\sigma_{2}^{4} \\sigma_{3}^{4} R^{2}(R+2)^{2} \\mathbb{E}\\left\\|\\mathcal{M}_{(1,4)}^{4}\\right\\|_{F}^{4}\n\\end{aligned}\n$$\n\nSimilarly, using successive applications of Lemma 4 it then follows that\n\n$$\n\\begin{aligned}\n& \\mathbb{E}\\left[y_{1}^{4}\\right] \\leq 3 \\sigma_{1}^{4} \\cdots \\sigma_{N-1}^{4} R^{N-2}(R+2)^{N-2} \\mathbb{E}\\left\\|\\mathcal{M}_{(1)}^{N}\\right\\|_{F}^{4} \\\\\n& =3 \\sigma_{1}^{4} \\cdots \\sigma_{N-1}^{4} R^{N-2}(R+2)^{N-2} \\mathbb{E}\\left\\|\\left(\\mathcal{G}^{N}\\right)_{(1)} \\boldsymbol{X}_{(N)}\\right\\|_{F}^{4} \\\\\n& \\leq 3 \\sigma_{1}^{4} \\cdots \\sigma_{N}^{4} R^{N-1}(R+2)^{N-1}\\left\\|\\boldsymbol{X}_{(N)}\\right\\|_{F}^{4} \\\\\n& =3 \\frac{1}{R}\\left(\\frac{1}{R^{2}}\\right)^{N-2} \\frac{1}{R} R^{N-1}(R+2)^{N-1}\\|\\boldsymbol{X}\\|_{F}^{4} \\\\\n& =3\\left(1+\\frac{2}{R}\\right)^{N-1}\\|\\boldsymbol{X}\\|_{F}^{4}\n\\end{aligned}\n$$"
    },
    {
      "markdown": "Therefore we obtain $\\mathbb{E}\\|\\mathbf{y}\\|_{2}^{4} \\leq 3 k\\left(1+\\frac{2}{R}\\right)^{N-1}\\|\\boldsymbol{\\mathcal { X }}\\|_{F}^{4}+$ $k(k-1)\\|\\boldsymbol{\\mathcal { X }}\\|_{F}^{4}$. Finally,\n\n$$\n\\begin{aligned}\n& \\operatorname{Var}\\left(\\left\\|f_{\\mathrm{TT}(R)}(\\boldsymbol{\\mathcal { X }})\\right\\|_{2}^{2}\\right)=\\mathbb{E}\\left[\\left\\|k^{-\\frac{1}{2}} \\mathbf{y}\\right\\|_{2}^{4}\\right]-\\mathbb{E}\\left[\\left\\|k^{-\\frac{1}{2}} \\mathbf{y}\\right\\|_{2}^{2}\\right]^{2} \\\\\n& \\quad=\\frac{1}{k^{2}} \\mathbb{E}\\|\\mathbf{y}\\|_{2}^{4}-\\|\\boldsymbol{\\mathcal { X }}\\|_{F}^{4} \\\\\n& \\quad \\leq \\frac{1}{k}\\left[3\\left(1+\\frac{2}{R}\\right)^{N-1}-1\\right]\\|\\boldsymbol{\\mathcal { X }}\\|_{F}^{4}\n\\end{aligned}\n$$\n\n### 5.2 Proof of Theorem 2: TT case\n\nTheorem 2 for the map $f_{\\mathrm{TT}(R)}$ directly follows from the following concentration bound.\nTheorem 5. Let $\\boldsymbol{\\mathcal { X }} \\in \\mathbb{R}^{d_{1} \\times d_{2} \\times \\cdots \\times d_{N}}$. There exist absolute constants $C$ and $K>0$ such that the random projection map $f_{\\mathrm{TT}(R)}$ (see Definition 1) satisfies\n\n$$\n\\begin{gathered}\n\\mathbb{P}\\left(\\left\\|\\int_{\\mathrm{TT}(R)}(\\boldsymbol{\\mathcal { X }})\\right\\|_{2}^{2}-\\|\\boldsymbol{\\mathcal { X }}\\|_{F}^{2}\\right| \\geq \\varepsilon\\|\\boldsymbol{\\mathcal { X }}\\|_{F}^{2}\\right) \\leq \\\\\nC \\exp \\left[-\\frac{(\\sqrt{k} \\varepsilon)^{\\frac{1}{N}}}{(3 K)^{\\frac{1}{2 N}} \\sqrt{1+2 / R}}\\right]\n\\end{gathered}\n$$\n\nTo show this concentration bound, we will use the following extension of the Hanson-Wright inequality whose proof can be found in [33].\nTheorem 6. (Hypercontractivity Concentration Inequality) Consider a degree $q$ polynomial $f(Y)=$ $f\\left(Y_{1}, \\ldots, Y_{n}\\right)$ of independent centered Gaussian or Rademacher random variables $Y_{1}, \\ldots, Y_{n}$. Then for any $\\lambda>0$\n\n$$\n\\mathbb{P}[|f(Y)-\\mathbb{E}[f(Y)]| \\geq \\lambda] \\leq e^{2} \\cdot e^{-\\left(\\frac{\\lambda^{2}}{K \\operatorname{Var}[f(Y)]\\right)} \\frac{1}{\\lambda}}\n$$\n\nwhere $\\operatorname{Var}([f(Y)])$ is the variance of the random variable $f(Y)$ and $K>0$ is an absolute constant.\n\nUsing the bound on the variance of $\\left\\|f_{\\mathrm{TT}(R)}(\\boldsymbol{\\mathcal { X }})\\right\\|_{2}^{2}$ and the fact that $\\left\\|f_{\\mathrm{TT}(R)}(\\boldsymbol{\\mathcal { X }})\\right\\|_{2}^{2}$ is a polynomial of degree $2 N$ of independent Gaussian random variables (the entries of the core tensors $\\boldsymbol{G}_{i}^{1}, \\boldsymbol{G}_{i}^{2}, \\cdots, \\boldsymbol{G}_{i}^{N}$ ), we can use Theorem 6 to obtain\n\n$$\n\\begin{aligned}\n& \\mathbb{P}\\left[\\left\\|\\int_{\\mathrm{TT}(R)}(\\boldsymbol{\\mathcal { X }})\\right\\|_{2}^{2}-\\|\\boldsymbol{\\mathcal { X }}\\|_{F}^{2}\\right| \\geq \\lambda\\right] \\\\\n& \\quad \\leq e^{2} \\exp \\left[-\\left(\\frac{\\lambda^{2}}{K \\operatorname{Var}\\left(\\left\\|f_{\\mathrm{TT}(R)}(\\boldsymbol{\\mathcal { X }})\\right\\|_{2}^{2}\\right)}\\right)^{\\frac{1}{2 N}}\\right]\n\\end{aligned}\n$$\n\nLet $C=e^{2}$ and let $\\lambda=\\varepsilon\\|\\boldsymbol{\\mathcal { X }}\\|_{F}^{2}$, we finally get\n\n$$\n\\begin{aligned}\n& \\mathbb{P}\\left[\\left\\|\\int_{\\mathrm{TT}(R)}(\\boldsymbol{\\mathcal { X }})\\right\\|_{2}^{2}-\\|\\boldsymbol{\\mathcal { X }}\\|_{F}^{2}\\right| \\geq \\varepsilon\\|\\boldsymbol{\\mathcal { X }}\\|_{F}^{4}] \\\\\n& \\leq C \\exp \\left[-\\left(\\frac{k \\varepsilon^{2}\\|\\boldsymbol{\\mathcal { X }}\\|_{F}^{4}}{3 K(1+2 / R)^{N-1}\\|\\boldsymbol{\\mathcal { X }}\\|_{F}^{4}}\\right)^{\\frac{1}{2 N}}\\right] \\\\\n& \\leq C \\exp \\left[-\\frac{(\\sqrt{k} \\varepsilon)^{\\frac{1}{N}}}{(3 K)^{\\frac{1}{2 N}} \\sqrt{1+2 / R}}\\right]\n\\end{aligned}\n$$\n\nwhere the last inequality follows from the fact that\n\n$$\n(1+2 / R)^{\\frac{N-1}{2 N}} \\leq \\sqrt{1+2 / R}\n$$\n\n## 6 Experiments\n\nIn this section we compare the embedding quality of the tensorized projection maps $f_{\\mathrm{TT}(R)}, f_{\\mathrm{CP}(R)}$ and Gaussian RP in a simulation study ${ }^{\\S}$. In particular, we investigate the effect of the rank parameter $R$ for different sizes and orders of input tensors. We first randomly generate an $N$-th order $d$-dimensional tensor $\\boldsymbol{\\mathcal { X }}$ (i.e. vector of size $d^{N}$ ) with unit norm in the TT format with rank $\\tilde{R}=10$. To assess how well the tensorized maps scale to very high order tensors, we consider three cases: - small-order: $(d=15, N=3)$, - medium-order: $(d=3, N=12)$ and - high-order $(d=3, N=25)$.\nWe compare several values of the rank parameter for the two tensorized map: $R=4,25,100$ for $f_{\\mathrm{CP}(R)}$ and $R=$ $2,5,10$ for $f_{\\mathrm{TT}(R)}$. Note that these values correspond to roughly the same number of parameters for the two maps since $f_{\\mathrm{TT}(R)}$ requires the storage of $(N-2) d R^{2}+2 d R$ parameters while $f_{\\mathrm{CP}(R)}$ only needs $N d R$. Additional experiment on image data from the CIFAR-10 dataset [24] are presented in Appendix B.1.\nThe quality of embedding is evaluated using the distortion ratio metric defined by $D(f, \\boldsymbol{\\mathcal { X }})=\\left|\\frac{\\|f(\\boldsymbol{\\mathcal { X }})\\|^{2}}{\\|\\boldsymbol{\\mathcal { X }}\\|^{2}}-1\\right|$. Due to memory limitation, we compare tensorized RP with Gaussian RP for the small-order case tensors and with very sparse RP [25] for medium-order tensors (the highorder case cannot be handled with Gaussian or very sparse RP).\nThe average distortion ratios over 100 trials are reported as a function of the embedding dimension $k$ in Figure 1. In the small-order case, we see that $f_{\\mathrm{TT}(R)}$ and $f_{\\mathrm{CP}(R)}$ perform similarly to Gaussian RP for all values of the rank parameter. In the medium-order case, we see that\n\n[^0]\n[^0]:    ${ }^{4}$ For these experiments we use Tensor Toolbox v3.1 [6] and TT-Toolbox v2.2 [29]."
    },
    {
      "markdown": "![img-0.jpeg](img-0.jpeg)\n\nFigure 1: Comparison of the distortion ratio of $f_{\\mathrm{TT}(R)}, f_{\\mathrm{CP}(R)}$, and Gaussian RP for different value of the rank parameter $R$ for small-order (left), medium-order (center) and high-order (right) input tensors.\n![img-1.jpeg](img-1.jpeg)\n\nFigure 2: Comparison of embedding time between tensorized and very sparse RP for the medium-order case $(d=3, N=12)$ when the input is given in the TT format (top) or CP format (bottom).\nthe rank of the tensorized RP significantly affects the quality of the embedding. Moreover, $f_{\\mathrm{CP}(R)}$ struggles to achieve a good distortion ratio even when $R=100$ while $f_{\\mathrm{TT}(R)}$ almost reaches the performance of very sparse RP. This behavior is accentuated in the high-order case where $f_{\\mathrm{CP}(R)}$ obtains poor performances even for high values of $R$ and $k$ while $f_{\\mathrm{TT}(R)}$ provides good embeddings for $R=5,10$. Note that this behavior is expected from our theoretical analysis.\n\nTo illustrate the time complexity of the algorithms, we\nreport the average running time needed to project the input tensor for the medium-order case in Figure 2, when $\\mathcal{X}$ is either given as a TT or a CP tensor of rank 10. We see that $f_{\\mathrm{TT}(R)}$ (resp. $f_{\\mathrm{CP}(R)}$ ) is more efficient when the input tensor is given in the TT format (resp. CP format), which is somehow expected. We also report the average running time needed to project the different input tensor in medium-order case $(d=3, N=8,11,12,13)$ with respect to the dimension $d^{N}$ (Appendix B.2). It is also worth observing that $f_{\\mathrm{TT}(R)}$ is always faster than very sparse RP while it is not the case for $f_{\\mathrm{CP}(R)}$.\n\n## 7 Conclusion\n\nWe propose a novel efficient RP technique for high-order tensor data: tensorized random projections maps. We theoretically and empirically studied two tensorized maps relying on the CP and TT deocmposition format, respectively. Our theoretical analysis and simulation study show that the TT format is better suited than the CP format for tensorizing random projections.\n\nFuture work include leveraging and extending our theoretical results to design efficient sketching algorithms for high-order tensor data. In particular, we plan to develop fast low rank approximation algorithms for matrices given in the TT format, which could prove particularly useful for designing efficient PCA and CCA algorithms for high-dimensional tensor data.\n\n## Acknowledgment\n\nThis research is supported by the Canadian Institute for Advanced Research (CIFAR AI chair program). This work was completed while Beheshteh T. Rakhshan interned at Montreal Institute for Learning Algorithms (Mila), Montreal, QC."
    },
    {
      "markdown": "## References\n\n[1] Dimitris Achlioptas. Database-friendly random projections: Johnson-Lindenstrauss with binary coins. Journal of Computer and System Sciences, 66(4):671-687, 2003.\n[2] Thomas D Ahle, Michael Kapralov, Jakob BT Knudsen, Rasmus Pagh, Ameya Velingker, David P Woodruff, and Amir Zandieh. Oblivious sketching of high-degree polynomial kernels. Proceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 141-160, 2020.\n[3] Nir Ailon and Bernard Chazelle. Approximate nearest neighbors and the fast Johnson-Lindenstrauss transform. In Proceedings of the thirty-eighth annual ACM symposium on Theory of computing, pages 557-563. ACM, 2006.\n[4] Nir Ailon and Bernard Chazelle. The fast JohnsonLindenstrauss transform and approximate nearest neighbors. SIAM Journal on computing, 39(1):302322, 2009.\n[5] Nir Ailon and Edo Liberty. An almost optimal unrestricted fast Johnson-Lindenstrauss transform. ACM Transactions on Algorithms (TALG), 9(3):21, 2013.\n[6] Brett W. Bader, Tamara G. Kolda, et al. Matlab tensor toolbox version 3.1. Available online, June 2019.\n[7] Mohammad Taha Bahadori, Qi Rose Yu, and Yan Liu. Fast multivariate spatio-temporal analysis via low rank tensor learning. In Advances in Neural Information Processing Systems, pages 3491-3499, 2014.\n[8] Ella Bingham and Heikki Mannila. Random projection in dimensionality reduction: applications to image and text data. In Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining, pages 245-250. ACM, 2001.\n[9] Christos Boutsidis, Anastasios Zouzias, and Petros Drineas. Random projections for $k$-means clustering. In Advances in Neural Information Processing Systems, pages 298-306, 2010.\n[10] Moses Charikar, Kevin Chen, and Martin FarachColton. Finding frequent items in data streams. In International Colloquium on Automata, Languages, and Programming, pages 693-703. Springer, 2002.\n[11] A. Cichocki, R. Zdunek, A.H. Phan, and S.I. Amari. Nonnegative Matrix and Tensor Factorizations. Applications to Exploratory Multi-way Data Analysis and Blind Source Separation. Wiley, 2009.\n[12] Sanjoy Dasgupta. Learning mixtures of Gaussians. In 40th Annual Symposium on Foundations of Computer Science (Cat. No. 99CB37039), pages 634644. IEEE, 1999.\n[13] Sanjoy Dasgupta. Experiments with random projection. In Proceedings of the Sixteenth conference on Uncertainty in artificial intelligence, pages 143-151, 2000.\n[14] Sanjoy Dasgupta and Anupam Gupta. An elementary proof of a theorem of Johnson and Lindenstrauss. Random Structures \\& Algorithms, 22(1):6065, 2003.\n[15] Arjun K Gupta and Daya K Nagar. Matrix variate distributions. Chapman and Hall/CRC, 2018.\n[16] Frank L Hitchcock. The expression of a tensor or a polyadic as a sum of products. Journal of Mathematics and Physics, 6(1-4):164-189, 1927.\n[17] Piotr Indyk and Rajeev Motwani. Approximate nearest neighbors: towards removing the curse of dimensionality. In Proceedings of the thirtieth annual ACM symposium on Theory of computing, pages 604-613. ACM, 1998.\n[18] Leon Isserlis. On a formula for the product-moment coefficient of any order of a normal frequency distribution in any number of variables. Biometrika, 12(1/2):134-139, 1918.\n[19] Ruhui Jin, Tamara G Kolda, and Rachel Ward. Faster Johnson-Lindenstrauss transforms via Kronecker products. arXiv preprint arXiv:1909.04801, 2019.\n[20] William B Johnson and Joram Lindenstrauss. Extensions of Lipschitz mappings into a Hilbert space. Contemporary mathematics, 26(189-206):1, 1984.\n[21] Valentin Khrulkov, Alexander Novikov, and Ivan Oseledets. Expressive power of recurrent neural networks. In International Conference on Learning Representations, 2018.\n[22] Jon M Kleinberg. Two algorithms for nearestneighbor search in high dimensions. In Proceedings of the thirtieth annual ACM symposium on Theory of computing, volume 97, pages 599-608, 1997.\n[23] Tamara G Kolda and Brett W Bader. Tensor decompositions and applications. SIAM review, 51(3):455500, 2009.\n[24] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Master's thesis, Department of Computer Science, University of Toronto, 2009.\n[25] Ping Li, Trevor J Hastie, and Kenneth W Church. Very sparse random projections. In Proceedings of"
    },
    {
      "markdown": "the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 287-296. ACM, 2006.\n[26] H. Lu, K.N. Plataniotis, and A. Venetsanopoulos. Multilinear Subspace Learning: Dimensionality Reduction of Multidimensional Data. CRC Press, 2013.\n[27] Alexander Novikov, Dmitrii Podoprikhin, Anton Osokin, and Dmitry P Vetrov. Tensorizing neural networks. In Advances in neural information processing systems, pages 442-450, 2015.\n[28] Alexander Novikov, Anton Rodomanov, Anton Osokin, and Dmitry Vetrov. Putting MRFs on a tensor train. In International Conference on Machine Learning, pages 811-819, 2014.\n[29] Ivan Oseledets, Vladimir Kazeev, et al. Matlab tttoolbox 2.2: Fast multidimensional array operations in tt-format. Available online, June 2014.\n[30] Ivan V Oseledets. Tensor-train decomposition. SIAM Journal on Scientific Computing, 33(5):22952317, 2011.\n[31] Saurabh Paul, Christos Boutsidis, Malik MagdonIsmail, and Petros Drineas. Random projections for support vector machines. In In Proceeding of the Artificial Intelligence and Statistics, pages 498-506, 2013.\n[32] Ninh Pham and Rasmus Pagh. Fast and scalable polynomial kernels via explicit feature maps. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 239-247. ACM, 2013.\n[33] Warren Schudy and Maxim Sviridenko. Concentration and moment inequalities for polynomials of independent random variables. In Proceedings of the twenty-third annual ACM-SIAM symposium on Discrete Algorithms, pages 437-446. Society for Industrial and Applied Mathematics, 2012.\n[34] Yang Shi and Animashree Anandkumar. Multidimensional tensor sketch. arXiv preprint arXiv:1901.11261, 2019.\n[35] Nicholas D Sidiropoulos, Lieven De Lathauwer, Xiao Fu, Kejun Huang, Evangelos E Papalexakis, and Christos Faloutsos. Tensor decomposition for signal processing and machine learning. IEEE Transactions on Signal Processing, 65(13):3551-3582, 2017.\n[36] Yiming Sun, Yang Guo, Joel A Tropp, and Madeleine Udell. Tensor random projection for low memory dimension reduction. In NeurIPS Workshop on Relational Representation Learning, 2018.\n[37] Ledyard R Tucker. Some mathematical notes on three-mode factor analysis. Psychometrika, 31(3):279-311, 1966.\n[38] Santosh S Vempala. The random projection method, volume 65. American Mathematical Soc., 2005.\n[39] H. Zhou, L. Li, and H. Zhu. Tensor regression with applications in neuroimaging data analysis. Journal of the American Statistical Association, 108(502):540-552, 2013."
    },
    {
      "markdown": "# Tensorized Random Projections (Supplementary Material) \n\n## A Proof of the Theorems for the CP case\n\n## A. 1 Proof of Theorem 1: CP case\n\nTheorem. Let $\\mathcal{X} \\in \\mathbb{R}^{d_{1} \\times d_{2} \\times \\cdots \\times d_{\\mathrm{N}}}$. The random projection maps $f_{\\mathrm{TT}(R)}$ and $f_{\\mathrm{CP}(R)}$ (see Definitions 1 and 2) satisfy the following properties:\n\n- $\\mathbb{E}\\left[\\left\\|f_{\\mathrm{CP}(R)}(\\mathcal{X})\\right\\|_{2}^{2}\\right]=\\mathbb{E}\\left[\\left\\|f_{\\mathrm{TT}(R)}(\\mathcal{X})\\right\\|_{2}^{2}\\right]=\\|\\mathcal{X}\\|_{F}^{2}$,\n- $\\operatorname{Var}\\left(\\left\\|f_{\\mathrm{TT}(R)}(\\mathcal{X})\\right\\|_{2}^{2}\\right) \\leq \\frac{1}{k}\\left(3\\left(1+\\frac{2}{R}\\right)^{N-1}-1\\right)\\|\\mathcal{X}\\|_{F}^{4}$,\n- $\\operatorname{Var}\\left(\\left\\|f_{\\mathrm{CP}(R)}(\\mathcal{X})\\right\\|_{2}^{2}\\right) \\leq \\frac{1}{k}\\left(3^{N-1}\\left(1+\\frac{2}{R}\\right)-1\\right)\\|\\mathcal{X}\\|_{F}^{4}$.\n\nProof. Expected isometry. We start by showing that $f_{\\mathrm{CP}(R)}$ is an expected isometry, i.e. that $\\mathbb{E}\\left\\|f_{\\mathrm{CP}(R)}(\\mathcal{X})\\right\\|_{2}^{2}=$ $\\|\\mathcal{X}\\|_{F}^{2}$. Let $y_{i}=\\left\\langle\\left[\\mathbf{A}_{i}^{1}, \\mathbf{A}_{i}^{2}, \\cdots, \\mathbf{A}_{i}^{N}\\right], \\mathcal{X}\\right\\rangle$ and $\\mathbf{y}=\\left[y_{1}, y_{2}, \\cdots, y_{k}\\right]$. With these definitions we have $f_{\\mathrm{CP}(R)}(\\mathcal{X})=$ $\\frac{1}{\\sqrt{k}} \\mathbf{y}$ and it is thus sufficient to find $\\mathbb{E}\\left[y_{1}^{2}\\right]$. To lighten the notation, let $\\mathbf{A}^{n}=\\mathbf{A}_{1}^{n}$ for each $n \\in[N]$ and let $\\mathcal{T}=$ $\\left\\|\\mathbf{A}^{1}, \\mathbf{A}^{2}, \\cdots, \\mathbf{A}^{N}\\right\\|$. We have\n\n$$\n\\begin{aligned}\n\\mathbb{E}\\left[y_{1}^{2}\\right] & =\\mathbb{E}\\left[\\left\\langle\\mathcal{T}, \\mathcal{X}\\right\\rangle^{2}\\right]=\\mathbb{E}\\left[\\left\\langle\\mathcal{T} \\otimes \\mathcal{T}, \\mathcal{X} \\otimes \\mathcal{X}\\right\\rangle\\right] \\\\\n& =\\langle\\mathbb{E}[\\mathcal{T} \\otimes \\mathcal{T}], \\mathcal{X} \\otimes \\mathcal{X}\\rangle\n\\end{aligned}\n$$\n\nUsing the fact that the factor matrices $\\mathbf{A}^{n}$ are independent, we have\n\n$$\n\\begin{aligned}\n\\mathbb{E}[\\mathcal{T} \\otimes \\mathcal{T}] & =\\mathbb{E}\\left[\\left\\|\\mathbf{A}^{1} \\otimes \\mathbf{A}^{1}, \\cdots, \\mathbf{A}^{N} \\otimes \\mathbf{A}^{N}\\right\\|\\right] \\\\\n& =\\left\\|\\mathbb{E}\\left[\\mathbf{A}^{1} \\otimes \\mathbf{A}^{1}\\right], \\cdots, \\mathbb{E}\\left[\\mathbf{A}^{N} \\otimes \\mathbf{A}^{N}\\right]\\right\\|\n\\end{aligned}\n$$\n\nNow, for $n \\in[N]$, since the entries of each factor matrix $\\mathbf{A}^{n}$ are i.i.d. Gaussian random variables with mean 0 and variance $\\left(\\frac{1}{R}\\right)^{\\frac{1}{N}}$, we have\n\n$$\n\\mathbb{E}\\left[\\mathbf{A}^{n} \\otimes \\mathbf{A}^{n}\\right]=\\left(\\frac{1}{R}\\right)^{\\frac{1}{N}} \\operatorname{vec}\\left(\\mathbf{I}_{d_{n}}\\right) \\circ \\operatorname{vec}\\left(\\mathbf{I}_{R}\\right)\n$$\n\nOne can then show that\n\n$$\n\\mathbb{E}[\\mathcal{T} \\otimes \\mathcal{T}]=\\operatorname{vec}\\left(\\mathbf{I}_{d_{1}}\\right) \\circ \\cdots \\circ \\operatorname{vec}\\left(\\mathbf{I}_{d_{N}}\\right)\n$$\n\nwhich implies that\n\n$$\n\\mathbb{E}\\left[y_{1}^{2}\\right]=\\langle\\mathbb{E}[\\mathcal{T} \\otimes \\mathcal{T}], \\mathcal{X} \\otimes \\mathcal{X}\\rangle=\\|\\mathcal{X}\\|_{F}^{2}\n$$\n\nfrom which $\\mathbb{E}\\left\\|f_{\\mathrm{CP}(R)}(\\mathcal{X})\\right\\|_{2}^{2}=\\|\\mathcal{X}\\|_{F}^{2}$ directly follows.\nBound on the variance of $f_{\\mathrm{CP}(R)}$. Similar to TT case, in order to bound the variance of $\\|\\mathbf{y}\\|_{2}^{4}$ we need to bound $\\mathbb{E}\\left[\\|\\mathbf{y}\\|_{2}^{4}\\right]$. We have\n\n$$\n\\mathbb{E}\\left[\\|\\mathbf{y}\\|_{2}^{4}\\right]=\\sum_{i=1}^{k} \\mathbb{E}\\left[y_{i}^{4}\\right]+\\sum_{i \\neq j} \\mathbb{E}\\left[y_{i}^{2} y_{j}^{2}\\right]\n$$\n\nSince $y_{i}$ and $y_{j}$ are independent whenever $i \\neq j$ and $\\mathbb{E}\\left[y_{i}^{2}\\right]=\\|\\mathcal{X}\\|_{F}^{4}$ for all $i$, the second summand is equal to $k(k-1)\\|\\mathcal{X}\\|_{F}^{4}$. We now derive a bound on $\\mathbb{E}\\left[y_{i}^{4}\\right]$. First define the tensor $\\mathcal{S}^{n}$ of order $2(n-1)$ and shape $\\underbrace{R \\times R \\cdots \\times R}_{n-1} \\times d_{1} \\times d_{2} \\cdots \\times d_{n-1}$ for any $2 \\leq n<N$ by"
    },
    {
      "markdown": "$$\n\\mathcal{S}_{r_{1}, r_{2}, \\cdots, r_{n-1}, i_{1}, i_{2}, \\cdots, i_{n-1}}^{n}=\\sum_{r_{n}, \\ldots, r_{N}} \\sum_{i_{n}, \\cdots, i_{N}}\\left(\\mathbf{A}^{n}\\right)_{i_{n} r_{n}}\\left(\\mathbf{A}^{n+1}\\right)_{i_{n+1} r_{n+1}} \\cdots\\left(\\mathbf{A}^{N}\\right)_{i_{N} r_{N}} \\boldsymbol{\\mathcal { I }} r_{1}, \\ldots, r_{N} \\boldsymbol{\\mathcal { X }}_{i_{1}, \\ldots, i_{N}}\n$$\n\nwhere $\\mathcal{I} \\in\\left(\\mathbb{R}^{R}\\right)^{\\otimes N}$ is the $N$ th order identity tensor, i.e., $\\mathcal{I}_{r_{1}, \\ldots, r_{n}}=1$ if $r_{1}=\\cdots=r_{n}$ and 0 otherwise. In some sense, $\\mathcal{S}^{n}$ is the tensor obtained by removing the first $n-1$ factor matrices from the computation of $y_{1}=$ $\\left\\langle\\left\\|\\mathbf{A}^{1}, \\mathbf{A}^{2}, \\cdots, \\mathbf{A}^{N}\\right\\|, \\boldsymbol{\\mathcal { X }}\\right\\rangle$. With this definition one can check that\n\n- $\\left\\langle\\left\\|\\mathbf{A}^{1}, \\mathbf{A}^{2}, \\cdots, \\mathbf{A}^{N}\\right\\|, \\boldsymbol{\\mathcal { X }}\\right\\rangle=\\left\\langle\\left(\\mathbf{A}^{1}\\right)^{\\top}, \\mathbf{S}^{2}\\right\\rangle$,\n- $\\left(\\mathcal{S}_{\\left(1, \\ldots, N-1\\right)}^{N}\\right)^{\\top}=\\left(\\boldsymbol{\\mathcal { X }_{( } N}\\right)^{\\top} \\mathbf{A}^{N} \\boldsymbol{\\mathcal { I }}_{(1)}$ (recall that $\\left.\\left(\\mathcal{S}^{N}\\right)_{(1, \\ldots, N-1)} \\in \\mathbb{R}^{R^{N-1} \\times d_{1} \\ldots d_{N-1}}\\right.$ denotes the matricization of $\\mathcal{S}^{N}$ obtained by mapping its first $N-1$ modes to rows and the other ones to columns).\n- $\\operatorname{vec}\\left(\\mathcal{S}^{n}\\right)=\\left(\\left(\\mathcal{S}^{n+1}\\right)_{(1,2 n)}\\right)^{\\top} \\operatorname{vec}\\left(\\mathbf{A}^{n}\\right)$ for each $n \\in[N-1]$.\n\nUsing Lemma 3 we obtain\n\n$$\n\\begin{aligned}\n\\mathbb{E} y_{1}^{4}=\\mathbb{E}\\left\\langle\\left\\|\\mathbf{A}^{1}, \\mathbf{A}^{2}, \\cdots, \\mathbf{A}^{N}\\right\\|, \\boldsymbol{\\mathcal { X }}\\right\\rangle^{4} & =\\mathbb{E}\\left\\langle\\operatorname{vec}\\left(\\left(\\mathbf{A}^{1}\\right)^{\\top}\\right), \\operatorname{vec}\\left(\\mathbf{S}^{2}\\right)\\right\\rangle^{4}=3 R^{-\\frac{2}{N}} \\mathbb{E}\\left\\|\\operatorname{vec}\\left(\\mathbf{S}^{2}\\right)\\right\\|_{F}^{4} \\\\\n& =3 R^{-\\frac{2}{N}} \\mathbb{E}\\left\\|\\left(\\left(\\mathcal{S}^{3}\\right)_{(1,4)}\\right)^{\\top} \\operatorname{vec}\\left(\\mathbf{A}^{2}\\right)\\right\\|_{F}^{4}\n\\end{aligned}\n$$\n\nUsing successive applications of Lemma 4 it follows that\n\n$$\n\\begin{aligned}\n\\mathbb{E} y_{1}^{4}= & 3 R^{-\\frac{2}{N}} \\mathbb{E}\\left\\|\\left(\\left(\\mathcal{S}^{3}\\right)_{(1,4)}\\right)^{\\top} \\operatorname{vec}\\left(\\mathbf{A}^{2}\\right)\\right\\|_{F}^{4} \\\\\n\\leq & 3^{2} R^{-\\frac{4}{N}} \\mathbb{E}\\left\\|\\left(\\mathcal{S}^{3}\\right)_{(1,4)}\\right\\|_{F}^{4}=3^{2} R^{-\\frac{2}{N}} \\mathbb{E}\\left\\|\\operatorname{vec}\\left(\\mathcal{S}^{3}\\right)\\right\\|=3^{2} R^{-\\frac{4}{N}} \\mathbb{E}\\left\\|\\left(\\left(\\mathcal{S}^{4}\\right)_{(1,6)}\\right)^{\\top} \\operatorname{vec}\\left(\\mathbf{A}^{3}\\right)\\right\\|_{F}^{4} \\\\\n\\leq & 3^{3} R^{-\\frac{8}{N}} \\mathbb{E}\\left\\|\\left(\\mathcal{S}^{4}\\right)_{(1,6)}\\right\\|_{F}^{4}=3^{3} R^{-\\frac{8}{N}} \\mathbb{E}\\left\\|\\operatorname{vec}\\left(\\mathcal{S}^{4}\\right)\\right\\|_{F}^{4} \\\\\n\\leq & \\cdots \\\\\n\\leq & 3^{N-1} R^{-\\frac{2(N-1)}{N}} \\mathbb{E}\\left\\|\\operatorname{vec}\\left(\\mathcal{S}^{N}\\right)\\right\\|_{F}^{4}=3^{N-1} R^{-\\frac{2(N-1)}{N}} \\mathbb{E}\\left\\|\\left(\\mathcal{S}_{\\left(1, \\ldots, N-1\\right)}^{N}\\right)^{\\top}\\right\\|_{F}^{4} \\\\\n= & 3^{N-1} R^{-\\frac{2(N-1)}{N}} \\mathbb{E}\\left\\|\\left(\\boldsymbol{\\mathcal { X }_{( N }}\\right)^{\\top} \\mathbf{A}^{N} \\boldsymbol{\\mathcal { I }_{( 1 )}}\\right\\|_{F}^{4}=3^{N-1} R^{-\\frac{2(N-1)}{N}} \\mathbb{E}\\left\\|\\left(\\boldsymbol{\\mathcal { X }_{( N )}}\\right)^{\\top} \\mathbf{A}^{N}\\right\\|_{F}^{4} \\\\\n\\leq & 3^{N-1} R^{-2} R(R+2)\\|\\boldsymbol{\\mathcal { X }}\\|_{F}^{4} \\\\\n= & 3^{N-1}\\left(1+\\frac{2}{R}\\right)\\|\\boldsymbol{\\mathcal { X }}\\|_{F}^{4}\n\\end{aligned}\n$$\n\nwhere we used the equality $\\left\\|\\mathcal{T} \\mathcal{I}_{(1)}\\right\\|_{F}^{2}=\\|\\mathcal{T}\\|_{F}^{2}$ for any tensor $\\mathcal{T}$ (which follows from the fact that $\\mathcal{I}_{(1)}\\left(\\mathcal{I}_{(1)}\\right)^{\\top}=\\mathbf{I}$ ) for the penultimate equality.\nSimilar to proof of Theorem 1 for $f_{\\mathrm{TT}(R)}$ map, we obtain\n\n$$\n\\mathbb{E}\\|\\mathbf{y}\\|_{2}^{4}=\\sum_{i=1}^{k} \\mathbb{E} y_{i}^{4}+\\sum_{i \\neq j} \\mathbb{E} y_{i}^{2} y_{j}^{2} \\leq k\\left(3^{N-1}\\left(1+\\frac{2}{R}\\right)\\|\\boldsymbol{\\mathcal { X }}\\|_{F}^{4}\\right)+k(k-1)\\|\\boldsymbol{\\mathcal { X }}\\|_{F}^{4}\n$$\n\nFinally,\n\n$$\n\\begin{aligned}\n\\operatorname{Var}\\left(\\left\\|f_{\\mathrm{CP}(R)}(\\boldsymbol{\\mathcal { X } )}\\right\\|_{2}^{2}\\right) & =\\operatorname{Var}\\left(\\left\\|\\frac{1}{\\sqrt{k}} \\mathbf{y}\\right\\|_{2}^{2}\\right)=\\frac{1}{k^{2}} \\mathbb{E}\\left(\\|\\mathbf{y}\\|_{2}^{4}\\right)-\\frac{1}{k^{2}} \\mathbb{E}\\left(\\|\\mathbf{y}\\|_{2}^{2}\\right)^{2}=\\frac{1}{k^{2}} \\mathbb{E}\\|\\mathbf{y}\\|_{2}^{4}-\\|\\boldsymbol{\\mathcal { X }}\\|_{F}^{4} \\\\\n& \\leq \\frac{1}{k^{2}}\\left[k\\left(3^{N-1}\\left(1+\\frac{2}{R}\\right)\\|\\boldsymbol{\\mathcal { X }}\\|_{F}^{4}\\right)+k(k-1)\\|\\boldsymbol{\\mathcal { X }}\\|_{F}^{4}\\right]-\\|\\boldsymbol{\\mathcal { X }}\\|_{F}^{4} \\\\\n& \\leq \\frac{1}{k}\\left(3^{N-1}\\left(1+\\frac{2}{R}\\right)-1\\right)\\|\\boldsymbol{\\mathcal { X }}\\|_{F}^{4}\n\\end{aligned}\n$$"
    },
    {
      "markdown": "# A. 2 Proof of Theorem 2: CP case \n\nTheorem 2 for the map $f_{\\mathrm{CP}(R)}$ directly follows from the following concentration bound.\nTheorem. Let $\\boldsymbol{\\mathcal { X }} \\in \\mathbb{R}^{d_{1} \\times d_{2} \\times \\cdots \\times d_{N}}$. There exist absolute constants $C$ and $\\widetilde{K}>0$ such that the random projection map $f_{\\mathrm{CP}(R)}$ (see Definition 2) satisfies\n\n$$\n\\mathbb{P}\\left(\\left\\|\\int f_{\\mathrm{CP}(R)}(\\boldsymbol{\\mathcal { X }})\\right\\|_{2}^{2}-\\left\\|\\boldsymbol{\\mathcal { X }}\\right\\|_{F}^{2} \\mid \\geq \\varepsilon\\left\\|\\boldsymbol{\\mathcal { X }}\\right\\|_{F}^{2}\\right) \\leq C \\exp \\left[-C_{1} \\frac{(\\sqrt{k} \\varepsilon)^{\\frac{1}{N}}}{\\left(3^{N-1} \\widetilde{K}\\right)^{\\frac{1}{2 N}}(1+2 / R)^{\\frac{1}{2 N}}}\\right]\n$$\n\nProof. By CP part of Theorem 1, recall\n\n$$\n\\mathbb{E}\\left\\|f_{\\mathrm{CP}(R)}(\\boldsymbol{\\mathcal { X }})\\right\\|_{2}^{2}=\\left\\|\\boldsymbol{\\mathcal { X }}\\right\\|_{F}^{2}\n$$\n\nand\n\n$$\n\\operatorname{Var}\\left(\\left\\|f_{\\mathrm{CP}(R)}(\\boldsymbol{\\mathcal { X }})\\right\\|_{2}^{2}\\right) \\leq \\frac{1}{k}\\left(3^{N-1}\\left(1+\\frac{2}{R}\\right)-1\\right)\\left\\|\\boldsymbol{\\mathcal { X }}\\right\\|_{F}^{4}\n$$\n\nSince $\\left\\|f_{\\mathrm{CP}(R)}(\\boldsymbol{\\mathcal { X }})\\right\\|_{2}^{2}$ is an order $2 N$ polynomial of the entries of the matrices $\\mathbf{A}_{i}^{1}, \\cdots, \\mathbf{A}_{i}^{N}$ for $i \\in[k]$ we can apply Theorem 6 to obtain\n\n$$\n\\mathbb{P}\\left(\\left\\|\\int f_{\\mathrm{CP}(R)}(\\boldsymbol{\\mathcal { X }})\\right\\|_{2}^{2}-\\left\\|\\boldsymbol{\\mathcal { X }}\\right\\|_{F}^{2} \\mid \\geq \\lambda\\right) \\leq C \\exp \\left[-\\left(\\frac{\\lambda^{2}}{\\widetilde{K} \\operatorname{Var}\\left(\\left\\|f_{\\mathrm{CP}(R)}(\\boldsymbol{\\mathcal { X }})\\right\\|_{2}^{2}\\right)}\\right)^{\\frac{1}{2 N}}\\right]\n$$\n\nwhere $C=e^{2}$ and $\\widetilde{K}$ are absolute constants. Using the fact that\n\n$$\n\\operatorname{Var}\\left(\\left\\|f_{\\mathrm{CP}(R)}(\\boldsymbol{\\mathcal { X }})\\right\\|_{2}^{2}\\right) \\leq \\frac{3^{N-1}}{k}(1+2 / R)\\left\\|\\boldsymbol{\\mathcal { X }}\\right\\|_{F}^{4}\n$$\n\nand letting $\\lambda=\\varepsilon\\left\\|\\boldsymbol{\\mathcal { X }}\\right\\|_{F}^{2}$ we obtain\n\n$$\n\\begin{aligned}\n\\mathbb{P}\\left(\\left\\|\\int f_{\\mathrm{CP}(R)}(\\boldsymbol{\\mathcal { X }})\\right\\|_{2}^{2}-\\left\\|\\boldsymbol{\\mathcal { X }}\\right\\|_{F}^{2} \\mid \\geq \\varepsilon\\left\\|\\boldsymbol{\\mathcal { X }}\\right\\|_{F}^{2}\\right) & \\leq C \\exp \\left[-\\left(\\frac{k \\varepsilon^{2}\\left\\|\\boldsymbol{\\mathcal { X }}\\right\\|_{F}^{4}}{\\widetilde{K} 3^{N-1}(1+2 / R)\\left\\|\\boldsymbol{\\mathcal { X }}\\right\\|_{F}^{4}}\\right)^{\\frac{1}{2 N}}\\right] \\\\\n& \\leq C \\exp \\left[-\\frac{(\\sqrt{k} \\varepsilon)^{\\frac{1}{N}}}{\\left(3^{N-1} \\widetilde{K}\\right)^{\\frac{1}{2 N}}(1+2 / R)^{\\frac{1}{2 N}}}\\right]\n\\end{aligned}\n$$"
    },
    {
      "markdown": "# B Additional Experimental Results \n\n## B. 1 Pairwise Distance Estimation\n\n![img-2.jpeg](img-2.jpeg)\n\nFigure 3: Comparison of tensorized ranodm projections with Gaussian random projections on CIFAR-10 data for different values of the rank parameter: (left) rank 1, (middle) rank 3-10, (right) rank 5-25.\n\nWe compare the tensorized projection maps $f_{\\mathrm{TT}(R)}$ and $f_{\\mathrm{CP}(R)}$ with classical Gaussian RP on CIFAR-10 image data for different values of the rank parameter $R$. We reshape the first $\\mathrm{n}=50$ vectors (of size $32 \\times 32 \\times 4$ ) of CIFAR-10 to $4 \\times 4 \\times 4 \\times 4 \\times 4 \\times 3$ tensors, normalize them and compare the pairwise distance $\\frac{1}{n(n-1)} \\sum_{1 \\leq i \\neq j \\leq n} \\frac{\\left\\|f\\left(\\mathbf{x}_{i}\\right)-f\\left(\\mathbf{x}_{j}\\right)\\right\\|_{2}}{\\left\\|\\mathbf{x}_{i}-\\mathbf{x}_{j}\\right\\|_{2}}$ and standard deviation for different projection sizes $k$ over 100 trials. The results are reported in Figure 3 where we see that tensorized random projection maps perform competitively with classical Gaussian random projections.\n\n## B. 2 Time Evaluation\n\n![img-3.jpeg](img-3.jpeg)\n\nFigure 4: Comparison of embedding time between tensorized, Gaussian and very sparse Gaussian RP for the mediumorder case with different number of modes $(d=3, N \\in\\{8,11,12,13\\})$ when the input is given in the TT format (left) or CP format (right).\n\nWe report the average running time with respect to the input dimension $d^{N}$ for the medium-order case with different number of modes $(d=3, N \\in\\{8,11,12,13\\})$ in Figure 4, when the input tensor $\\mathcal{X}$ is either as a TT or CP tensor of rank 10. We can see that $f_{\\mathrm{TT}(R)}$ is more efficient when the input is in TT format. However, $f_{\\mathrm{CP}(R)}$ performs better when the input is in the CP format (though the computational gain of $f_{\\mathrm{CP}(R)}$ in this case is considerably smaller than the one of $f_{\\mathrm{TT}(R)}$ in the previous case). We can see that by increasing the dimension $f_{\\mathrm{TT}(R)}$ performs close to $f_{\\mathrm{CP}(R)}$ even when the input is in CP and it is faster than classical Gaussian RPs in both cases (which is not true for $f_{\\mathrm{CP}(100)}$ )."
    }
  ],
  "usage_info": {
    "pages_processed": 14,
    "doc_size_bytes": 1242464
  },
  "_metadata": {
    "model_version": "0.0.0",
    "llm_model": "mistral-ocr-2505",
    "query_data": {
      "pdf": "/Users/satyaortiz-gagne/CODE/paperext/data/cache/fulltext/ad7c1f61c129b78caf405b3558fa3153/fulltext.pdf"
    },
    "model_id": "parsepdf"
  }
}