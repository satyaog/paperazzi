{
  "pages": [
    {
      "markdown": "# RAINPROOF: An umbrella to shield text generators from Out-of-Distribution data \n\nMaxime Darrin*<br>ILLS $^{1}$<br>MILA - Quebec AI Institute<br>McGill University<br>Paris-Saclay University<br>Pablo Piantanida<br>ILLS $^{1}$<br>MILA - Quebec AI Institute<br>CNRS, CentraleSupélec<br>Paris-Saclay University<br>Equall, Paris\n\n\n#### Abstract\n\nImplementing effective control mechanisms to ensure the proper functioning and security of deployed NLP models, from translation to chatbots, is essential. A key ingredient to ensure safe system behaviour is Out-Of-Distribution (OOD) detection, which aims to detect whether an input sample is statistically far from the training distribution. Although OOD detection is a widely covered topic in classification tasks, most methods rely on hidden features output by the encoder. In this work, we focus on leveraging soft-probabilities in a black-box framework, i.e. we can access the soft-predictions but not the internal states of the model. Our contributions include: (i) RAINPROOF a Relative informAItioN Projection OOD detection framework; and (ii) a more operational evaluation setting for OOD detection. Surprisingly, we find that OOD detection is not necessarily aligned with task-specific measures. The OOD detector may filter out samples well processed by the model and keep samples that are not, leading to weaker performance. Our results show that RAINPROOF provides OOD detection methods more aligned with task-specific performance metrics than traditional OOD detectors.\n\n\n## 1 Introduction\n\nSignificant progress has been made in Natural Language Generation (NLG) in recent years with the development of powerful generic (e.g., GPT (Radford et al., 2018; Brown et al., 2020; Bahrini et al., 2023), LLAMA (Touvron et al., 2023) and its variants) and task-specific (e.g., Grover (Zellers et al., 2019), Pegasus (Zhang et al., 2020) and DialogGPT (Zhang et al., 2019b)) text generators. They power machine translation (MT) systems or chatbots that are exposed to the public, and their reliability is a prerequisite for adoption. Text generators are trained in the context of a so-called closed\n\n[^0]world (Fei and Liu, 2016), where training and test data are assumed to be drawn i.i.d. from a single distribution, known as the in-distribution. However, when deployed, these models operate in an open world (Parmar et al., 2021; Zhou, 2022) where the i.i.d. assumption is often violated. Changes in data distribution are detrimental and induce a drop in performance. It is necessary to develop tools to protect models from harmful distribution shifts as it is a clearly unresolved practical problem (Arora et al., 2021). For example, a trained translation model is not expected to be reliable when presented with another language (e.g. a Spanish model exposed to Catalan, or a Dutch model exposed to Afrikaans) or unexpected technical language (e.g., a colloquial translation model exposed to rare technical terms from the medical field). They also tend to be released behind API(OpenAI, 2023) ruling out many usual features-based OOD detection methods.\n\nMost work on Out-Of-Distribution (OOD) detection focus on classification, leaving OOD detection in (conditional) text generation settings mainly unexplored, even though it is among the most exposed applications. Existing solutions fall into two categories. The first one called training-aware methods (Zhu et al., 2022; Vernekar et al., 2019a,b), modifies the classifier training by exposing the neural network to OOD samples during training. The second one, called plug-in methods aims to distinguish regular samples in the in-distribution (IN) from OOD samples based on the model's behaviour on a new input. Plug-in methods include Maximum Softmax Probabilities (MSP) (Hendrycks and Gimpel, 2016) or Energy (Liu et al., 2020) or featurebased anomaly detectors that compute a per-class anomaly score (Ming et al., 2022; Ryu et al., 2017; Huang et al., 2020; Ren et al., 2021a). Although plug-in methods from classification settings seem attractive, their adaptation to text generation tasks is more involved. While text generation can be seen as a sequence of classification problems, i.e.,\n\n\n[^0]:    4axime.darrin@mila.quebec\n    ILLS - International Laboratory on Learning Systems\n    Mathématiques et Informatique Centralesupelec"
    },
    {
      "markdown": "chosing the next token at each step, the number of possible tokens is two orders of magnitude higher than usual classification setups.\n\nIn this work, we aim to develop new tools to build more reliable text generators which can be used in practical systems. To do so, we work under 4 constraints: (i) We do not assume we can access OOD samples; (ii) We suppose we are in a blackbox scenario: we do not assume we have access to the internal states of the model but only to the soft probability distributions it outputs; (iii) The detectors should be easy enough to use on top of any existing model to ensure adaptability; (iv) Not only should OOD detectors be able to filter OOD samples, but they also are expected to improve the average performance on the end-task the model has to perform.\n\nOur contributions. Our main contributions can be summarized as follows:\n\n1. A more operational benchmark for text generation OOD detection. We present LOFTER the Language Out of disTribution pErformance benchmaRk. Existing works on OOD detection for language modelling (Arora et al., 2021) focus on (i) the English language only, (ii) the GLUE benchmark, and (iii) measure performance solely in terms of OOD detection. LOFTER introduces more realistic data shifts in the generative setting that goes beyond English: language shifts induced by closely related language pairs (e.g., Spanish and Catalan or Dutch and Afrikaans (Xiao et al., 2020) ${ }^{1}$ ) and domain change (e.g., medical vs news data or vs dialogs). In addition, LOFTER comes with an updated evaluation setting: detectors' performance is jointly evaluated w.r.t the overall system's performance on the end task.\n2. A novel detector inspired by information projection. We present RAINPROOF: a Relative informAltioN Projection Out OF distribution detector. RAINPROOF is fully unsupervised. It is flexible and can be applied both when no reference samples (IN) are available (corresponding to scenario $\\mathrm{s}_{0}$ ) and when they are (corresponding to scenario $\\mathrm{s}_{1}$ ). RAINPROOF tackles $\\mathrm{s}_{0}$ by computing the models' predictions negentropy (Brillouin, 1953) and uses it as a measure of normality. For $\\mathrm{s}_{1}$, it relies upon its natural extension: the Information Projection\n[^0](Csiszar and Matus, 2003), which relies on a reference set to get a data-driven notion of normality.\n3. New insights on the operational value of OOD detectors. Our experiments on LOFTER show that OOD detectors may filter out samples that are well processed (i.e. well translated) by the model and keep samples that are not, leading to weaker performance. Our results show that RAINPROOF improves performance on the end task while removing most of the OOD samples.\n4. Code and reproductibility. We release a plug-and-play library built upon the Transformers library that implements our detectors and baselines ${ }^{23}$.\n\n## 2 Problem Statement \\& Related Works\n\n### 2.1 Notations \\& conditional text generation\n\nLet us denote $\\Omega$, a vocabulary of size $|\\Omega|$ and $\\Omega^{*}$ its Kleene closure ${ }^{4}$. We denote $\\mathcal{P}(\\Omega)=$ $\\left\\{\\mathbf{p} \\in[0,1]^{|\\Omega|}: \\sum_{i=1}^{|\\Omega|} \\mathbf{p}_{i}=1\\right\\}$ the set of probability distributions defined over $\\Omega$. Let $\\mathcal{D}_{\\text {train }}$ be the training set, composed of $N \\geqslant 1$ i.i.d. samples $\\left\\{\\left(\\mathbf{x}^{i}, \\mathbf{y}^{i}\\right)\\right\\}_{i=1}^{N} \\in(\\mathcal{X} \\times \\mathcal{Y})^{N}$ with probability law $\\mathbf{p}_{X Y}$. We denote $\\mathbf{p}_{X}$ and $\\mathbf{p}_{Y}$ the associated marginal laws of $\\mathbf{p}_{X Y}$. Each $\\mathbf{x}^{i}$ is a sequence of tokens, and $x_{j}^{i} \\in \\Omega$ the $j$ th token of the $i$ th sequence. $\\mathbf{x}_{\\leqslant t}^{i}=\\left\\{x_{1}^{i}, \\cdots, x_{t}^{i}\\right\\} \\in \\Omega^{*}$ denotes the prefix of length $t$. The same notations hold for $\\mathbf{y}$.\nConditional textual generation. In conditional textual generation, the goal is to model a probability distribution $\\mathbf{p}_{*}(\\mathbf{x}, \\mathbf{y})$ over variable-length text sequences $(\\mathbf{x}, \\mathbf{y})$ by finding $\\mathbf{p}_{\\theta} \\approx \\mathbf{p}_{*}(\\mathbf{x}, \\mathbf{y})$ for any $(\\mathbf{x}, \\mathbf{y})$. In this work, we assume to have access to a pretrained conditional language model $f_{\\theta}: \\mathcal{X} \\times \\mathcal{Y} \\rightarrow \\mathbf{R}^{|\\Omega|}$, where the output is the (unnormalized) logits scores. $f_{\\theta}$ parameterized $\\mathbf{p}_{\\theta}$, i.e., for any $(\\mathbf{x}, \\mathbf{y}), \\mathbf{p}_{\\theta}(\\mathbf{x}, \\mathbf{y})=\\operatorname{softmax}\\left(f_{\\theta}(\\mathbf{x}, \\mathbf{y}) / T\\right)$ where $T \\in \\mathbf{R}$ denotes the temperature. Given an input sequence $\\mathbf{x}$, the pretrained language $f_{\\theta}$ can recursively generate an output sequence $\\hat{\\mathbf{y}}$ by sampling $y_{t+1} \\sim \\mathbf{p}_{\\theta}^{T}\\left(\\cdot \\mid \\mathbf{x}, \\hat{\\mathbf{y}}_{\\leqslant t}\\right)$, for $t \\in[1,|\\mathbf{y}|]$. Note that $\\hat{y}_{0}$ is the start of sentence ( $<\\mathrm{SOS}>$ token). We denote by $\\mathcal{S}(\\mathbf{x})$, the set of normalized logits scores generated by the model when the initial input is $\\mathbf{x}$ i.e., $\\mathcal{S}(\\mathbf{x})=\\left\\{\\operatorname{softmax}\\left(f_{\\theta}\\left(\\mathbf{x}, \\hat{\\mathbf{y}}_{\\leqslant t}\\right)\\right)\\right\\}_{t=1}^{|\\mathbf{y}|}$.\n\n[^1]\n[^0]:    ${ }^{1}$ Afrikaans is a daughter language of Dutch. The Dutch sentence: \"Appelen zijn gewoonlijk groen, geel of rood\" corresponds to \"Appels is gewoonlik groen, geel of rooi.\"\n\n[^1]:    ${ }^{2}$ https://github.com/icannos/ToddBenchmark\n    ${ }^{3}$ This work was performed using HPC resources from GENCI-IDRIS (Grant 2022-AD011013945).\n    ${ }^{4}$ The Kleene closure corresponds to sequences of arbitrary size written with words in $\\Omega$. Formally: $\\Omega^{*}=\\bigcup_{i=0}^{\\infty} \\Omega^{i}$."
    },
    {
      "markdown": "Note that elements of $\\mathcal{S}(\\mathbf{x})$ are discrete probability distributions over $\\Omega$.\n\n### 2.2 Problem statement\n\nIn OOD detection, the goal is to find an anomaly score $a: \\mathcal{X} \\rightarrow \\mathbf{R}_{+}$that quantifies how far a sample is from the IN distribution. $\\mathbf{x}$ is classified as IN or OUT according to the score $a(\\mathbf{x})$. We then fix a threshold $\\gamma$ and classifies the test sample IN if $a(\\mathbf{x}) \\leqslant \\gamma$ or OOD if $a(\\mathbf{x})>\\gamma$. Formally, let us denote $g(\\cdot, \\gamma)$ the decision function, we take: $g(\\mathbf{x}, \\gamma)= \\begin{cases}1 & \\text { if } a(\\mathbf{x})>\\gamma \\\\ 0 & \\text { if } a(\\mathbf{x}) \\leqslant \\gamma .\\end{cases}$\nRemark 1. In our setting, OOD examples are not available. Tuning $\\gamma$ is a complex task, and it is usually calibrated using OOD samples. In our work, we decided not to rely on OOD samples but on the available training set to fix $\\gamma$ in a realistic setting. Indeed, even well-tailored datasets might contain significant shares of outliers (Meister et al., 2023). Therefore, we fix $\\gamma$ so that at least $80 \\%$ of the IN data pass the filtering procedure. See Sec. G. 3 for more details.\n\n### 2.3 Review of existing OOD detectors\n\nOOD detection for classification. Most works on OOD detection have focused on detectors for classifiers and rely either on internal representations (features-based detectors) or on the final soft probabilities produced by the classifier (softmax based detectors).\nFeatures-based detectors. They leverage latent representations to derive anomaly scores. The most well-known is the Mahanalobis distance (Lee et al., 2018a; Ren et al., 2021b), but there are other methods employing Grams matrices (Sastry and Oore, 2020), Fisher Rao distance (Gomes et al., 2022) or other statistical tests (Haroush et al., 2021). These methods require access to the latent representations of the models, which does not fit the black-box scenario. In addition, it is well known in classification that performing per-class OOD detection is key to get good performance (Lee et al., 2018b). This per-class approach is a priori impossible in text generation since it would have to be done per token or by some other unknown type of classes. We argue that it is necessary to find non-class-dependent solutions, especially when it comes to the Mahalanobis distance, which relies upon the hypothesis that the data are unimodal; we study the validity of this hypothesis and show that it is not true in a generative setting in Ap. A.\n\nSoftmax-based detectors. These detectors rely on the soft probabilities produced by the model. The MSP (Hendrycks and Gimpel, 2017; Hein et al., 2019; Liang et al., 2018; Hsu et al., 2020) uses the probability of the mode while others take into account the entire logit distribution (e.g., Energybased scores (Liu et al., 2020)). Due to the large vocabulary size, it is unclear how these methods generalize to sequence generation tasks.\nOOD detection for text generation. Little work has been done on OOD detection for text generation. Therefore, we will follow (Arora et al., 2021; Podolskiy et al., 2021) and rely on their baselines. We also generalize common OOD scores such as MSP or Energy by computing the average score along the sequence at each step of the text generation. We refer the reader to Sec. B. 7 for more details.\nQuality estimation as OOD detection metric. Quality Estimation Metrics are not designed to detect OOD samples but to assess the overall quality of generated samples. However, they are interesting baselines to consider, as OOD samples should lead to low-quality outputs. We will use COMET QE (Stewart et al., 2020) as a baseline to filter out low-quality results induced by OOD samples.\nRemark 2. Note that features-based detectors assume white-box access to internal representations, while softmax-based detectors rely solely on the final output. Our work operates in a black-box framework but also includes a comparison to the Mahalanobis distance for completeness.\n\n## 3 RAINPROOF OOD detector\n\n### 3.1 Background\n\nAn information measure $\\mathcal{I}: \\mathcal{P}(\\Omega) \\times \\mathcal{P}(\\Omega) \\rightarrow \\mathbf{R}$ quantifies the similarity between any pair of discrete distributions $\\mathbf{p}, \\mathbf{q} \\in \\mathcal{P}(\\Omega)$. Since $\\Omega$ is a finite set, we will adopt the following notations $\\mathbf{p}=\\left[\\mathbf{p}_{1}, \\cdots, \\mathbf{p}_{|\\Omega|}\\right]$ and $\\mathbf{q}=\\left[\\mathbf{q}_{1}, \\cdots, \\mathbf{q}_{|\\Omega|}\\right]$. While there exist information distances, it is, in general, difficult to build metrics that satisfy all the properties of a distance, thus we often rely on divergences that drop the symmetry property and the triangular inequality.\n\nIn what follows, we motivate the information measures we will use in this work.\n\nFirst, we rely on the Rényi divergences (Csiszár, 1967). Rényi divergences belong to the $f$ -divergences family and are parametrized by a parameter $\\alpha \\in \\mathbf{R}_{+}-\\{1\\}$. They are flexible"
    },
    {
      "markdown": "and include well-known divergences such as the Kullback-Leiber divergence (KL) (Kullback, 1959) (when $\\alpha \\rightarrow 1$ ) or the Hellinger distance (Hellinger, 1909) (when $\\alpha=0.5$ ). The Rényi divergence between $\\mathbf{p}$ and $\\mathbf{q}$ is defined as follows:\n\n$$\nD_{\\alpha}(\\mathbf{p} \\| \\mathbf{q})=\\frac{1}{\\alpha-1} \\log \\left(\\sum_{i=1}^{|\\Omega|} \\frac{\\mathbf{p}_{i}^{\\alpha}}{\\mathbf{q}_{i}^{\\alpha-1}}\\right)\n$$\n\nThe Rényi divergence is popular as $\\alpha$ allows weighting the relative influence of the distributions' tail.\n\nSecond, we investigate the Fisher-Rao distance (FR). FR is a distance on the Riemannian space formed by the parametric distributions, using the Fisher information matrix as its metric. It computes the geodesic distance between two discrete distributions (Rao, 1992) and is defined as follows:\n\n$$\n\\operatorname{FR}(\\mathbf{p} \\| \\mathbf{q})=\\frac{2}{\\pi} \\arccos \\sum_{i=1}^{|\\Omega|} \\sqrt{\\mathbf{p}_{i} \\times \\mathbf{q}_{i}}\n$$\n\nIt has recently found many applications (Picot et al., 2022; Colombo et al., 2022b).\n\n### 3.2 RAINPROOF for the no-reference scenario ( $\\mathrm{s}_{0}$ )\n\nAt inference time, the no-reference scenario ( $\\mathrm{s}_{0}$ ) does not assume the existence of a reference set of IN samples to decide whether a new input sample is OOD. Which include, for example, Softmax-based detectors such as MSP, Energy or the sequence log-likelihood ${ }^{5}$\n\nUnder these assumptions, our OOD detector RAINPROOF comprises three steps. For a given input $\\mathbf{x}$ with generated sentence $\\hat{\\mathbf{y}}$ :\n\n1. We first use $f_{\\theta}$ to extract the step-by-step sequence of soft distributions $\\mathcal{S}(\\mathbf{x})$.\n2. We then compute an anomaly score $\\left(a_{\\mathcal{I}}(\\mathbf{x})\\right)$ by averaging a step-by-step score provided by $\\mathcal{I}$. This step-by-step score is obtained by measuring the similarity between a reference distribution $\\mathbf{u} \\in$ $\\mathcal{P}(\\Omega)$ and one element of $\\mathcal{S}(\\mathbf{x})$. Formally,\n\n$$\na_{\\mathcal{I}}(\\mathbf{x})=\\frac{1}{|\\mathcal{S}(\\mathbf{x})|} \\sum_{\\mathbf{p} \\in \\mathcal{S}(\\mathbf{x})} \\mathcal{I}(\\mathbf{p} \\| \\mathbf{u})\n$$\n\nwhere $|\\mathcal{S}(\\mathbf{x})|=|\\hat{\\mathbf{y}}|$.\n\n[^0]3. The last step consists of thresholding the previous anomaly score $a_{\\mathcal{I}}(\\mathbf{x})$. If $a_{\\mathcal{I}}(\\mathbf{x})$ is over a given threshold $\\gamma$, we classify $\\mathbf{x}$ as an OOD example.\n\nInterpretation of Eq. 3. $a_{\\mathcal{I}}(\\mathbf{x})$ measures the average dissimilarity of the probability distribution of the next token to normality (as defined by $\\mathbf{u}$ ). $a_{\\mathcal{I}}(\\mathbf{x})$ also corresponds to the token average uncertainty of the model $f_{\\theta}$ to generate $\\hat{\\mathbf{y}}$ when the input is $\\mathbf{x}$. The intuition behind Eq. 3 is that the distributions produced by $f_{\\theta}$, when exposed to an OOD sample, should be far from normality and thus have a high score.\n\nChoice of $\\mathbf{u}$ and $\\mathcal{I}$. The uncertainty definition of Eq. 3 depends on the choice of both the reference distribution $\\mathbf{u}$ and the information measure $\\mathcal{I}$. A natural choice for $\\mathbf{u}$ is the uniform distribution, i.e., $\\mathbf{u}=\\left[\\frac{1}{|\\Omega|}, \\cdots, \\frac{1}{|\\Omega|}\\right]$ which we will use in this work. It is worth pointing out that $\\mathcal{I}(\\cdot \\| \\mathbf{u})$ yields the negentropy of a distribution (Brillouin, 1953). Other possible choices for $\\mathbf{u}$ include one hot or tf-idf distribution (Colombo et al., 2022b). For $\\mathcal{I}$, we rely on the Rényi divergence to obtain $a_{\\mathcal{D}_{\\alpha}}$ and the Fisher-Rao distance to obtain $a_{\\mathrm{FR}}$.\n\n### 3.3 RAINPROOF for the reference scenario ( $\\mathrm{s}_{1}$ )\n\nIn the reference scenario $\\left(\\mathrm{s}_{1}\\right)$, we assume that one has access to a reference set of $\\mathbf{I N}$ samples $\\mathcal{R}=$ $\\left\\{\\mathbf{x}^{i}:\\left(\\mathbf{x}^{i}, \\mathbf{y}^{i}\\right) \\in \\mathcal{D}_{\\text {train }}\\right\\}_{i=1}^{|\\mathcal{R}|}$ where $|\\mathcal{R}|$ is the size of the reference set. For example, the Mahalanobis distance works under this assumption. One of the weaknesses of Eq. 3 is that it imposes an ad-hoc choice when using $\\mathbf{u}$ (the uniform distribution). In $\\mathrm{s}_{1}$, we can leverage $\\mathcal{R}$, to obtain a data-driven notion normality.\n\nUnder $\\mathrm{s}_{1}$, our OOD detector RAINPROOF follows these four steps:\n\n1. (Offline) For each $\\mathbf{x}^{i} \\in \\mathcal{R}$, we generate $\\hat{\\mathbf{y}}^{i}$ and the associated sequence of probability distributions $\\left(\\mathcal{S}\\left(\\mathbf{x}^{i}\\right)\\right)$. Overall we thus generate $\\sum_{\\mathbf{x} \\in \\mathcal{R}}\\left|\\hat{\\mathbf{y}}^{1}\\right|$ probability distributions which could explode for long sequences ${ }^{6}$. To overcome this limitation, we rely on the bag of distributions of each sequence (Colombo et al., 2022b). We form the set of these\n[^1]\n[^0]:    ${ }^{5}$ The detector based on the log-likelihood of the sequence is defined as $a_{L}(\\mathbf{x})=-\\frac{1}{|\\hat{\\mathbf{y}}|} \\sum_{i=0}^{|\\hat{\\mathbf{y}}|-1} \\log \\mathbf{p}_{\\theta}\\left(\\hat{\\mathbf{y}}_{t+1} \\mid \\mathbf{x}, \\hat{\\mathbf{y}}_{\\leqslant t}\\right)$.\n\n[^1]:    ${ }^{6}$ It is also worth pointing out that projecting at each timestep would require a per-step reference set in addition to the computational time required to compute the projections, therefore we decided to aggregate the probability distributions over the sequence."
    },
    {
      "markdown": "bags of distributions:\n\n$$\n\\overline{\\mathcal{S}}^{*}=\\bigcup_{\\mathbf{x}^{\\prime} \\in \\mathcal{R}}\\left\\{\\frac{1}{\\left|\\mathcal{S}\\left(\\mathbf{x}^{\\prime}\\right)\\right|} \\sum_{\\mathbf{p} \\in \\mathcal{S}\\left(\\mathbf{x}^{\\prime}\\right)} \\mathbf{p}\\right\\}\n$$\n\n2. (Online) For a given input $\\mathbf{x}$ with generated sentence $\\hat{\\mathbf{y}}$, we compute its bag of distributions representation:\n\n$$\n\\overline{\\mathbf{p}}(\\mathbf{x})=\\frac{1}{|\\mathcal{S}(\\mathbf{x})|} \\sum_{\\mathbf{p} \\in \\mathcal{S}(\\mathbf{x})} \\mathbf{p}\n$$\n\n3. (Online) For $\\mathbf{x}$, we then compute an anomaly score $a_{\\mathcal{I}}^{*}(\\mathbf{x})$ by projecting $\\overline{\\mathbf{p}}(\\mathbf{x})$ on the set $\\overline{\\mathcal{S}}^{*}$. Formally, $a_{\\mathcal{I}}^{*}(\\mathbf{x})$ is defined as:\n\n$$\na_{\\mathcal{I}}^{*}(\\mathbf{x})=\\min _{\\mathbf{p} \\in \\overline{\\mathcal{S}}^{*}} \\mathcal{I}(\\mathbf{p} \\| \\overline{\\mathbf{p}}(\\mathbf{x}))\n$$\n\nWe denote $\\mathbf{p}^{*}(\\mathbf{x})=\\underset{\\mathbf{p} \\in \\overline{\\mathcal{S}}^{*}}{\\arg \\min } \\mathcal{I}(\\mathbf{p} \\| \\overline{\\mathbf{p}}(\\mathbf{x}))$.\n4. The last step consists of thresholding the previous anomaly score $a_{\\mathcal{I}}(\\mathbf{x})$. If $a_{\\mathcal{I}}(\\mathbf{x})$ is over a given threshold $\\gamma$, we classify $\\mathbf{x}$ as an OOD example.\n\nInterpretation of Eq. 6. $a_{\\mathcal{I}}(\\mathbf{x})$ relies on a Generalized Information Projection (Kullback, 1954; Csiszár, 1975, 1984) ${ }^{7}$ which measures the similarity between $\\overline{\\mathbf{p}}(\\mathbf{x})$ and the set $\\overline{\\mathcal{S}}^{*}$. Note that the closest element of $\\overline{\\mathcal{S}}^{*}$ in the sens of $\\mathcal{I}$ can give insights on the decision of the detector. It allows interpreting the decision of the detector as we will see in Tab. 6.\n\nChoice of $\\mathcal{I}$. Similarly to Sec. 3.2, we will rely on the Rényi divergence to define $a_{\\mathcal{R}_{0}}^{*}(\\mathbf{x})$ and the Fisher-Rao distance $a_{\\text {FR }}^{*}(\\mathbf{x})$.\n\n## 4 Results on LOFTER\n\n### 4.1 LOFTER: Language Out oF disTribution pErformance benchmaRk\n\nLOFTER for NMT. We consider a realistic setting involving both topic and language shifts. Language shifts correspond to exposing a model trained for a given language to another which is either linguistically close (e.g., Afrikaans for a system trained on Dutch) or missing in the training data (as it is the case for german in BLOOM (Scao et al., 2022)). It is an interesting setting because the differences between languages might not be obvious but still\n\n[^0]cause a significant drop in performance. For linguistically close languages, we selected closely related language pairs such as Catalan-Spanish, Portuguese-Spanish and Afrikaans-Dutch) coming from the Tatoeba dataset (Tiedemann, 2012) (see Tab. 8). Domain shifts can involve technical or rare terms or specific sentence constructions, which can affect the model's performance. We simulated such shifts from Tatoeba MT using news, law (EuroParl dataset), and medical texts (EMEA).\n\nLOFTER for dialogs. For conversational agents, we focused on a scenario where a goal-oriented agent, designed to handle a specific type of conversation (e.g., customer conversations, daily dialogue), is exposed to an unexpected conversation. In this case, it is crucial to interrupt the agent so it does not damage the user's trust with misplaced responses. We rely on the Multi WOZ dataset (Zang et al., 2020), a human-to-human dataset collected in the Wizard-of-Oz set-up (Kelley, 1984), for IN distribution data and its associated fine-tuned model. We simulated shifts using dialogue datasets from various sources, which are part of the SILICONE benchmark (Chapuis et al., 2020). Specifically, we use a goal-oriented dataset (i.e., Switchboard Dialog Act Corpus (SwDA) (Stolcke et al., 2000)), a multi-party meetings dataset (i.e., MRDA (Shriberg et al., 2004) and Multimodal EmotionLines Dataset MELD (Poria et al., 2018)), daily communication dialogs (i.e., DailyDialog DyDA (Li et al., 2017)), and scripted scenarii (i.e., IEMOCAP (Tripathi et al., 2018)). We refer the curious reader to Sec. B. 5 for more details on each dataset.\nModel Choices. We evaluated our methods on open-source and freely available language bilingual models (the Helsinki suite (Tiedemann and Thottingal, 2020)), on a BLOOM-based instructions model BLOOMZ (Muennighoff et al., 2022) (for which German is OOD). For dialogue tasks, we relied on the Dialog GPT (Zhang et al., 2019b) model finetuned on Multi WOZ, which acts as IN distribution. We consider the Helsinki models as they are used in production for lightweight applications. Additionally, they are specialized for a specific language pair and released with their associated training set, making them ideal candidates to study the impact of OOD in a controlled setting. ${ }^{8}$ Metrics. To evaluate the performance on the OOD task, we report the Area Under the Receiver Oper-\n\n[^1]\n[^0]:    ${ }^{7}$ The minimization problem of Eq. 6 finds numerous connections in the theory of large deviation (Sanov, 1958) or in statistical physics (Jaynes, 1957).\n\n[^1]:    ${ }^{8}$ Please note that for the likelihood detector, the translation model is additionally fine-tuned on the development set, ensuring a strong baseline."
    },
    {
      "markdown": "Table 1: Summary of the OOD detection performance of our detectors (Ours) compared to commonly used strong baselines (Bas.). We report the best detector for each scenario in bold and underline the best overall. The $\\downarrow$ indicates that for this score, the lower, the better; otherwise, the higher, the better. ${ }^{9}$\n\n|  |  |  | Language shifts |  | Domain shifts |  | Dialog shifts |  |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n|  |  |  | AUROC | FPR $\\downarrow$ | AUROC | FPR $\\downarrow$ | AUROC | FPR $\\downarrow$ |\n| Ours | Ours | $\\begin{gathered} a_{D_{n}} \\\\ a_{F R} \\end{gathered}$ | $\\mathbf{0 . 9 5}$ | $\\mathbf{0 . 2 5}$ | $\\mathbf{0 . 8 5}$ | $\\mathbf{0 . 6 2}$ | $\\mathbf{0 . 7 9}$ | $\\mathbf{0 . 6 4}$ |\n|  |  | 0.93 | 0.28 | 0.81 | 0.67 | 0.76 | 0.68 |\n|  |  | $a_{E}$ | 0.89 | 0.44 | 0.79 | 0.78 | 0.65 | 0.76 |\n|  | Bas. | $\\begin{gathered} a_{M S P} \\\\ a_{L} \\end{gathered}$ | 0.87 | 0.44 | 0.79 | 0.77 | 0.66 | 0.72 |\n|  |  | 0.78 | 0.79 | 0.73 | 0.88 | 0.65 | 0.95 |\n|  |  | $a_{C Q E}$ | 0.71 | 0.57 | 0.73 | 0.88 | X | X |\n| $\\begin{gathered} b_{1} \\\\ b_{2} \\end{gathered}$ | Ours | $\\begin{gathered} a_{D_{n}^{\\prime}} \\\\ a_{F R} \\end{gathered}$ | 0.88 | 0.34 | $\\mathbf{0 . 8 6}$ | $\\mathbf{0 . 5 0}$ | $\\mathbf{0 . 8 6}$ | $\\mathbf{0 . 5 2}$ |\n|  |  | 0.88 | 0.35 | 0.81 | 0.69 | 0.76 | 0.75 |\n|  | Bas. | $a_{M}$ | $\\mathbf{0 . 9 2}$ | $\\mathbf{0 . 2 6}$ | 0.78 | 0.59 | 0.84 | 0.55 |\n\nTable 2: Correlation between OOD scores and translation metrics BLEU, BERT-S and COMET.\n![img-0.jpeg](img-0.jpeg)\nating Characteristic AUROC and the False positive rate FPR $\\downarrow$. These methods have been widely employed in previous research on out-of-distribution (OOD) detection. An exhaustive description of the metrics can be found in Sec. 4.1.\n\n### 4.2 Experiments in MT\n\nResults on language shifts (Tab. 1). We find that our no-reference methods ( $a_{D_{n}}$ and $a_{F R}$ ) achieve better performance than common no-reference baselines but also outperform the reference-based baseline. In particular, $a_{D_{n}}$, by achieving an AUROC of 0.95 and FPR $\\downarrow$ of 0.25 , outperforms all considered methods. Moreover, while no-reference baselines only capture up to $45 \\%$ of the OOD samples on average, ours detect up to $55 \\%$. In addition, COMET QE, a quality estimation tool, performs poorly in pure OOD detection, suggesting that while OOD detection and quality estimation can be related, they are still different problems.\nResults on domain shifts (Tab. 1). We evaluate the OOD detection performance of RAINPROOF on domain shifts in Spanish (SPA) and German (DEU) with technical, medical data and parliamentary data. For $s_{0}$, we observe that $a_{D_{n}}$ and $a_{F R}$ outperform the strongest baselines (i.e., Energy, MSP and se-\n![img-1.jpeg](img-1.jpeg)\n\nFigure 1: Ablation study on RAINPROOF for $\\alpha$ and reference set size $(|\\mathcal{R}|)$ for dialogue shift detection. Smaller $\\alpha$ emphasizes the tail of the distribution, while $\\alpha=0$ counts common non-zero elements.\nquence likelihood) by several AUROC points. Interestingly enough, even our no-reference detectors outperform the reference-based baseline (i.e., $a_{M}$, a deeper study of this phenomenon is presented in Ap. A). While $a_{D_{n}}$ achieves similar AUROC performance to its information projection counterpart $a_{D_{n}^{\\prime}}$, the latter achieve better FPR $\\downarrow$. Once again, the COMET QE metric does not yield competitive performance for OOD detection.\n\n### 4.3 Experiments in dialogue generation\n\nResults on Dialogue shifts (Tab. 1). Dialogue shifts are understandably more difficult to detect, as shown in our experiments, as they are smaller than language shifts. Our no-reference detectors do not outperform the Mahalanobis baseline and achieve only 0.79 in AUROC. The best baseline is the Mahalanobis distance and achieves better performance on dialogue tasks than on NMT domain shifts, reaching an AUROC of 0.84 . However, our reference-based detector based on the Rényi information projection secures better AUROC (0.86) and better FPR $\\downarrow(0.52)$. Even if our detectors achieve decent results on this task, it is clear that dialogue shifts will require further work and investigation (see Ap. F), especially in the wake of LLMs.\n\n### 4.4 Ablations Study\n\nFig. 1 shows that RAINPROOF offers a crucial flexibility by utilizing the Rényi divergence with adjustable parameter alpha. RAINPROOF's detectors show improvement when considering the tail of the distributions. Notably, lower values of $\\alpha$ (close to 0 ) yield better results with the Rényi Information projection $a_{D_{n}^{\\prime}}$. This finding suggests that the tail of the distributions used in text generation contains contextual information and insights about the processed texts. These results are consistent"
    },
    {
      "markdown": "Table 3: Impact of OOD detectors on BLEU for IN data only, OOD data and the combination of both ALL. We report average BLEU (Abs.), BLEU gains (G.s) compared to $f_{\\theta}$ only, and removed subset share (R.Sh.). $\\gamma$ set to remove $20 \\%$ of IN dataset.\n![img-2.jpeg](img-2.jpeg)\n\nTable 4: Computation time (in sec). Off. (Onl.) stands for offline (resp. online) time.\n\n| Score | Off. | Onl. |\n| :--: | :--: | :--: |\n| $a_{D_{\\alpha}}$ | $\\times$ | $2.10^{-3} \\mathrm{~s}$ |\n| $a_{M S P}$ | $\\times$ | $1.10^{-1} \\mathrm{~s}$ |\n| $a_{M}$ | 40s | $3.10^{-3} \\mathrm{~s}$ |\n| $a_{D_{\\zeta}}$ | $\\times$ | $9.10^{-2} \\mathrm{~s}$ |\n\nwith recent research in automatic text generation evaluation (Colombo et al., 2022b). Interestingly, increasing the size of the reference set beyond 1.2 k has minimal influence. We provide an additional study of the impact of the temperature and parameter $\\alpha$ for the different OOD scores in Ap. E.\n\n## 5 A More Practical Evaluation\n\nFollowing previous work, we measure the performance of the detectors on the OOD detection task based on AUROC and FPR $\\downarrow$. However, this evaluation framework neglects the impact of the detector on the overall system's performance and the downstream task it performs. We identify three main evaluation criteria that are important in practice: (i) execution time, (ii) overall system performance in terms of the quality of the generated answers, and (iii) interpretability of the decision. Our study is conducted on NMT because due to the existence of relevant and widely adopted metrics for assessing the quality of a generated sentence (i.e., BLEU (Papineni et al., 2002) and BERT-S (Zhang et al., 2019a) and COMET (Stewart et al., 2020)).\n\n### 5.1 Execution time\n\nRuntime and memory costs. We report in Tab. 4 the runtime of all methods. Detectors for $\\mathrm{s}_{0}$ are faster than the ones for $\\mathrm{s}_{1}$. Unlike detectors using references, no-reference detectors do not require additional memory. They can be set up easily in a plug\\&play manner at virtually no costs.\n\n### 5.2 Effects of Filtering on Translation Quality\n\nIn this experiment, we investigate the impact of OOD filtering from the perspectives of quality estimation and selective generation.\nGlobal performance. In Tab. 3 and Tab. 5, we report the global performance of the system $\\left(f_{\\theta}\\right)$ with and without OOD detectors on IN samples, OOD samples, and all samples (ALL). In most cases, adding detectors increases the average quality of the returned answers on all three subsets but with varying efficacy. $a_{M S P}$ is a notable exception, and we provide a specific correlation analysis later. While the reference-based detectors tend to remove more OOD samples, the no-reference detectors demonstrate better performance regarding the remaining sentences' average BLEU. Thus, OOD detector evaluation should consider the final task performance. Overall, it is worth noting that directly adapting classical OOD detection methods (e.g., MSP or Energy) to the sequence generation problem leads to poor results in terms of performance gains (i.e., as measured by BLEU or BERT-S). $a_{D_{\\alpha}}$ removes up to $62 \\%$ of OOD samples (whereas the likelihood only removes $45 \\%$ ) and maintains or improves the average performance of the system on the end task. In other words, $a_{D_{\\alpha}}$ provides the best combination of OOD detection performance and system performance improvements.\nThreshold free analysis. In Tab. 2, we report the correlations between OOD scores and quality metrics on each data subset (IN and OUT distribution, and ALL combined). For the OOD detector to improve or maintain performance on the end task, its score must correlate with performance metrics similarly for each subset. We notice that it is not the case for the likelihood or $a_{\\text {MSP }}$. The highest likelihood on IN data corresponds to higher quality answers. Still, the opposite is true for OOD samples, meaning using the likelihood to remove OOD samples tends to remove OOD samples that are well handled by the model. By contrast, RAINPROOF scores correlate well and in the same way on both IN and OUT, allowing them to remove OOD samples while improving performance.\n\n### 5.3 Towards an interpretable decision\n\nAn important dimension of fostering adoption is the ability to verify the decision taken by the automatic system. RAINPROOF offers a step in this direction when used with references: for each input sample, RAINPROOF finds the closest sample"
    },
    {
      "markdown": "Table 5: Detailed impacts on NMT performance results per tasks (Domain- or Language-shifts) of the different detectors. We present results on the different parts of the data: IN data, OOD data and the combination of both, ALL. For each, we report the absolute average BLEU (Abs.), the average gains in BLEU (G.s.) compared to a setting without OOD filtering ( $f_{\\theta}$ only) and the share of the subset removed by the detector (R.Sh.). We provide more detailed results on each dataset in Ap. G. In addition, we performed this study using different thresholds see Sec. G. 3\n\n|  |  |  |  |  | Domain-shifts |  |  |  |  |  |  |  |  | Language shifts |  |  |  |  |  |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n|  |  |  |  |  | OOD |  |  | ALL |  |  |  | IN |  |  |  |  |  |  |  |\n|  |  |  | Abs. | G. | Rh. | Abs. | G. | Rh. | Abs. | G. | Rh. | Abs. | G. | Rh. | Abs. |  |  | Rh. | Abs. |\n|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n|  |  | $\\#$ | 46.9 | 40.0 | 0.0\\% | 43.3 | 40.0 | 0.0\\% | 46.2 | 40.0 | 0.0\\% | 44.5 | 40.0 | 0.0\\% | 43.3 | 40.0 | 0.0\\% | 46.2 | 40.0 |\n|  | Ours | $\\alpha_{D}$ | 49.9 | 43.0 | 18.2\\% | 46.8 | 43.5 | 23.0\\% | 50.0 | 43.8 | 19.9\\% | 64.3 | 43.8 | 17.0\\% | 47.2 | 44.3 | 42.5 | 59.7 | 49.6 |\n|  |  | $\\alpha_{D_{1}}$ | 49.0 | 42.1 | 20.0\\% | 46.2 | 43.0 | 22.2 | 48.0 | 41.9 | 27.8\\% | 62.2 | 42.7 | 19.8\\% | 53.6 | 43.3 | 42.4 | 50.6 | 43.2 |\n|  |  | $\\alpha_{E}$ | 49.4 | 42.6 | 20.0\\% | 45.5 | 42.3 | 18.0\\% | 48.9 | 42.8 | 19.7\\% | 63.9 | 43.4 | 19.9\\% | 54.4 | 44.1 | 42.6 | 55.2 | 45.1 |\n|  | Bas. | $\\alpha_{E}$ | 50.6 | 43.8 | 19.0\\% | 47.5 | 44.3 | 24.7\\% | 50.9 | 44.7 | 21.1\\% | 65.8 | 45.1 | 19.0\\% | 56.4 | 44.1 | 42.6 | 59.7 | 49.6 |\n|  |  | $\\alpha_{M S P}$ | 45.6 | 39.8 | 18.8\\% | 53.4 | 47.8 | 25.2 | 42.1 | 37.8 | 29.6\\% | 58.9 | 45.8 | 18.1\\% | 51.1 | 44.7 | 41.5 | 50.8 | 45.2 |\n|  | Ours | $\\alpha_{M S P}$ | 48.0 | 41.2 | 20.0\\% | 44.2 | 40.9 | 17.1\\% | 46.8 | 40.6 | 20.2\\% | 61.3 | 37.9 | 20.0\\% | 51.1 | 42.8 | 24.8\\% | 51.6 | 43.5 |\n|  |  | $\\alpha_{D 1}$ | 46.9 | 40.0 | 19.0\\% | 37.4 | 34.1 | 15.3 | 46.3 | 40.1 | 35.0\\% | 60.9 | 37.1 | 19.3\\% | 37.4 | 46.4 | 37.0 | 55.6 | 45.3 |\n|  | Ours | $\\alpha_{D 1}$ | 46.9 | 40.0 | 18.7\\% | 37.4 | 34.1 | 15.3 | 46.3 | 40.1 | 34.8\\% | 60.9 | 37.1 | 19.2\\% | 37.4 | 46.4 | 37.0 | 55.6 | 45.3 |\n|  | Bas. | $\\alpha_{D}$ | 46.7 | 40.1 | 20.0\\% | 43.1 | 39.1 | 16.5 | 45.4 | 40.7 | 36.3\\% | 60.6 | 37.1 | 20.0\\% | 43.4 | 42.3 | 39.1 | 55.3 | 45.2 |\n\nTable 6: Interpretability Analysis OOD source (S.), their ground-truth (GD.), their generation (Gen.) and projections onto the reference set.\n\n| S. | Abir a la nit viarem trebullar fies a les des. |  |\n| :-- | :-- | :-- |\n| Gd. | Last night we worked until 10 p.m. |  |\n| Gen. | Abir a la nit viarem trebullar fies a les des. BLEU 3.75 |  |\n| $\\mathbf{p}^{+}(\\mathbf{x})$ | Dar gain per lafbes. Score 1.23 |  |\n| S | Australia no és Austria. |  |\n| Gd | Australia isn't Austria. |  |\n| Gen. | Australia is not Austria. BLEU 21.86 |  |\n| $\\mathbf{p}^{+}(\\mathbf{x})$ | La vida no es fácil. Score 0.82 |  |\n\n(in the sense of the Information Projection) in the reference set to take its decision. Tab. 6 present examples of OOD samples along with their translation scores, projection scores, and projection on the reference set. Qualitative analysis shows that, in general, sentences close to the reference set and whose projection has a close meaning are better handled by $f_{\\theta}$. Therefore, one can visually interpret the prediction of RAINPROOF and validate it.\n\n## 6 RAINPROOF on LLM for NMT\n\nAs an alternative to NMT models, we can study the performance of instruction finetuned LLM on translation tasks. However, it is important to note that while LLMs are trained on enormous amounts of data, they still miss many languages. Typically, they are trained on around 100 languages (Conneau et al., 2019), this falls far short of the existing 7000 languages. In our test-bed experiments, we decided to rely on BLOOM models, which have not been specifically trained on German (DEU) data. Therefore, We can use German samples to simulate OOD detection in an instruction-following translation setting, specifically relying on BLOOMZ (Muennighoff et al., 2022). We prompt the model to trans-\n\nTable 7: OOD detection on BLOOMZ using German as an OOD language for the LLM.\n\n|  |  | ALROC | FPR $\\downarrow$ |  |\n| :--: | :--: | :--: | :--: | :--: |\n| $\\%$ | Ours | $\\alpha_{D_{1}}$ | 0.50 | 1.00 |\n|  |  | $\\alpha_{D_{2}}$ | 0.58 | 0.92 |\n|  |  | $\\alpha_{\\text {MSP }}$ | $\\mathbf{0 . 5 9}$ | $\\mathbf{0 . 9 0}$ |\n|  | Bas. | $\\alpha_{E}$ | 0.51 | 0.97 |\n|  |  | $\\alpha_{E}$ | 0.54 | 0.90 |\n|  | Ours | $\\alpha_{D_{1}}$ | $\\mathbf{0 . 7 1}$ | $\\mathbf{0 . 8 0}$ |\n|  |  | $\\alpha_{D_{2}}$ | 0.70 | 0.82 |\n|  | Bas. | $\\alpha_{M}$ | 0.66 | 0.89 |\n\nlate Tatoeba dataset samples into English, focusing on languages known to be within the distribution for BLOOMZ while attempting to separate the German samples from them. From Tab. 7, we observed that our no-reference methods perform comparably to the $\\alpha_{\\text {MSP }}$ baseline, but are outperformed by the Mahalanobis distance in this scenario. However, the information projection methods demonstrate substantial improvements over all the baselines.\n\n## 7 Conclusion\n\nThis work introduces a detection framework called RAINPROOF and a new benchmark called LOFTER for black-box OOD detection on text generator. We adopt an operational perspective by not only considering OOD performance but also task-specific metrics: despite the good results obtained in pure OOD detection, OOD filtering can harm the performance of the final system, as it is the case for $\\alpha_{M S P}$ or $\\alpha_{M}$. We found that RAINPROOF succeed in removing OOD while inducing significant gains in translation performance both on OOD samples and in general. In conclusion, this work paves the way for developing text-generation OOD detectors and calls for a global evaluation when benchmarking future OOD detectors."
    },
    {
      "markdown": "# 8 Limitations \n\nWhile this work does not bear significant ethical or impact hazards, it is worth pointing out that it is not a perfect, absolutely safe solution against OOD distribution samples. Preventing the processing of OOD samples is an important part of ensuring ML algorithms' safety and robustness but it cannot guarantee total safety nor avoid all OOD samples. In this work, we approach the problem of OOD detection from a performance standpoint: we argue that OOD detectors should increase performance metrics since they should remove risky samples. However, no one can give such guarantees, and the outputs of ML models should always be taken with caution, whatever safety measures or filters are in place. Additionally, we showed that our methods worked in a specific setting of language shifts or topic shifts, mainly on translation tasks. While our methods performed well for small language shifts (shifts induced by linguistically close languages) and showed promising results on detecting topic shifts, the latter task remains particularly hard. Further work should explore different types of distribution shifts in other newer settings such as different types of instructions or problems given to instruction-finetuned models."
    },
    {
      "markdown": "## References\n\nUdit Arora, William Huang, and He He. 2021. Types of out-of-distribution texts and how to detect them. arXiv preprint arXiv:2109.06827.\n\nAram Bahrini, Mohammadsadra Khamoshifar, Hossein Abbasimehr, Robert J. Riggs, Maryam Esmaeili, Rastin Mastali Majdabadkohne, and Morteza Pasehvar. 2023. Chatgpt: Applications, opportunities, and threats.\n\nLeon Brillouin. 1953. The negentropy principle of information. Journal of Applied Physics, 24(9):11521163.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.\n\nEmile Chapuis, Pierre Colombo, Matteo Manica, Matthieu Labeau, and Chloé Clavel. 2020. Hierarchical pre-training for sequence labelling in spoken dialog. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2636-2648, Online. Association for Computational Linguistics.\n\nPierre Colombo, Eduardo D. C. Gomes, Guillaume Staerman, Nathan Noiry, and Pablo Piantanida. 2022a. Beyond mahalanobis-based scores for textual ood detection.\n\nPierre Jean A Colombo, Chloé Clavel, and Pablo Piantanida. 2022b. Infolm: A new metric to evaluate summarization \\& data2text generation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 10554-10562.\n\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Unsupervised cross-lingual representation learning at scale. arXiv preprint arXiv:1911.02116.\nI. Csiszar and F. Matus. 2003. Information projections revisited. IEEE Transactions on Information Theory, 49(6):1474-1490.\n\nImre Csiszár. 1967. Information-type measures of difference of probability distributions and indirect observation. studia scientiarum Mathematicarum Hungarica, 2:229-318.\n\nImre Csiszár. 1975. I-divergence geometry of probability distributions and minimization problems. The annals of probability, pages 146-158.\n\nImre Csiszár. 1984. Sanov property, generalized iprojection and a conditional limit theorem. The Annals of Probability, pages 768-793.\n\nGeli Fei and Bing Liu. 2016. Breaking the closed world assumption in text classification. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 506-514.\n\nEduardo Dadalto Camara Gomes, Florence Alberge, Pierre Duhamel, and Pablo Piantanida. 2022. Igeood: An information geometry approach to out-of-distribution detection. arXiv preprint arXiv:2203.07798.\n\nMatan Haroush, Tzviel Frostig, Ruth Heller, and Daniel Soudry. 2021. A statistical framework for efficient out of distribution detection in deep neural networks.\n\nMatthias Hein, Maksym Andriushchenko, and Julian Bitterwolf. 2019. Why relu networks yield highconfidence predictions far away from the training data and how to mitigate the problem. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 41-50.\n\nErnst Hellinger. 1909. Neue begründung der theorie quadratischer formen von unendlichvielen veränderlichen. Journal für die reine und angewandte Mathematik, 1909(136):210-271.\n\nDan Hendrycks and Kevin Gimpel. 2016. A baseline for detecting misclassified and out-of-distribution examples in neural networks. arXiv preprint arXiv:1610.02136.\n\nDan Hendrycks and Kevin Gimpel. 2017. A baseline for detecting misclassified and out-of-distribution examples in neural networks. In International Conference on Learning Representations.\n\nYen-Chang Hsu, Yilin Shen, Hongxia Jin, and Zsolt Kira. 2020. Generalized odin: Detecting out-ofdistribution image without learning from out-ofdistribution data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10951-10960.\n\nHaiwen Huang, Zhihan Li, Lulu Wang, Sishuo Chen, Bin Dong, and Xinyu Zhou. 2020. Feature space singularity for out-of-distribution detection. arXiv preprint arXiv:2011.14654.\n\nEdwin T Jaynes. 1957. Information theory and statistical mechanics. Physical review, 106(4):620.\n\nJohn F Kelley. 1984. An iterative design methodology for user-friendly natural language office information applications. ACM Transactions on Information Systems (TOIS), 2(1):26-41.\n\nSolomon Kullback. 1954. Information theory and statistics. Courier Corporation.\n\nSolomon Kullback. 1959. Information Theory and Statistics. John Wiley."
    },
    {
      "markdown": "Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. 2018a. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages 7167-7177. Curran Associates, Inc.\n\nKimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. 2018b. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. Advances in neural information processing systems, 31.\n\nYanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, and Shuzi Niu. 2017. Dailydialog: A manually labelled multi-turn dialogue dataset. In Proceedings of The 8th International Joint Conference on Natural Language Processing (IJCNLP 2017).\n\nShiyu Liang, Yixuan Li, and R. Srikant. 2018. Enhancing the reliability of out-of-distribution image detection in neural networks. In International Conference on Learning Representations.\n\nWeitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li. 2020. Energy-based out-of-distribution detection. Advances in Neural Information Processing Systems.\n\nClara Meister, Tiago Pimentel, Gian Wiher, and Ryan Cotterell. 2023. Locally Typical Sampling. Transactions of the Association for Computational Linguistics, 11:102-121.\n\nYifei Ming, Yiyou Sun, Ousmane Dia, and Yixuan Li. 2022. Cider: Exploiting hyperspherical embeddings for out-of-distribution detection. arXiv preprint arXiv:2203.04450.\n\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al. 2022. Crosslingual generalization through multitask finetuning. arXiv preprint arXiv:2211.01786.\n\nNathan Ng, Kyra Yee, Alexei Baevski, Myle Ott, Michael Auli, and Sergey Edunov. 2020. Facebook fair's wmt19 news translation task submission. In Proc. of WMT.\n\nOpenAI. 2023. Gpt-4 technical report.\nKishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311-318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.\n\nJitendra Parmar, Satyendra Singh Chouhan, Vaskar Raychoudhury, and Santosh Singh Rathore. 2021. Openworld machine learning: Applications, challenges, and opportunities,\n\nMarine Picot, Francisco Messina, Malik Boudiaf, Fabrice Labeau, Ismail Ben Ayed, and Pablo Piantanida. 2022. Adversarial robustness via fisher-rao regularization. IEEE Transactions on Pattern Analysis and Machine Intelligence.\n\nAlexander Podolskiy, Dmitry Lipin, Andrey Bout, Ekaterina Artemova, and Irina Piontkovskaya. 2021. Revisiting mahalanobis distance for transformerbased out-of-domain detection. arXiv preprint arXiv:2101.03778.\n\nSoujanya Poria, Devamanyu Hazarika, Navonil Majumder, Gautam Naik, Erik Cambria, and Rada Mihalcea. 2018. Meld: A multimodal multi-party dataset for emotion recognition in conversations. arXiv preprint arXiv:1810.02508.\n\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by generative pre-training.\n\nC Radhakrishna Rao. 1992. Information and the accuracy attainable in the estimation of statistical parameters. In Breakthroughs in statistics, pages 235-247. Springer.\n\nJie Ren, Stanislav Fort, Jeremiah Liu, Abhijit Guha Roy, Shreyas Padhy, and Balaji Lakshminarayanan. 2021a. A simple fix to mahalanobis distance for improving near-ood detection. arXiv preprint arXiv:2106.09022.\n\nJie Ren, Stanislav Fort, Jeremiah Liu, Abhijit Guha Roy, Shreyas Padhy, and Balaji Lakshminarayanan. 2021b. A simple fix to mahalanobis distance for improving near-ood detection.\n\nSeonghan Ryu, Seokhwan Kim, Junhwi Choi, Hwanjo Yu, and Gary Geunbae Lee. 2017. Neural sentence embedding using only in-domain sentences for out-of-domain sentence detection in dialog systems. Pattern Recognition Letters, 88:26-32.\n\nIvan N Sanov. 1958. On the probability of large deviations of random variables. United States Air Force, Office of Scientific Research.\n\nChandramouli Shama Sastry and Sageev Oore. 2020. Detecting out-of-distribution examples with Gram matrices. In Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 8491-8501. PMLR.\n\nTeven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. 2022. Bloom: A 176bparameter open-access multilingual language model. arXiv preprint arXiv:2211.05100.\n\nElizabeth Shriberg, Raj Dhillon, Sonali Bhagat, Jeremy Ang, and Hannah Carvey. 2004. The icsi meeting recorder dialog act (mrda) corpus. Technical report, INTERNATIONAL COMPUTER SCIENCE INST BERKELEY CA."
    },
    {
      "markdown": "Craig Stewart, Ricardo Rei, Catarina Farinha, and Alon Lavie. 2020. COMET - deploying a new state-of-the-art MT evaluation metric in production. In Proceedings of the 14th Conference of the Association for Machine Translation in the Americas (Volume 2: User Track), pages 78-109, Virtual. Association for Machine Translation in the Americas.\n\nAndreas Stolcke, Klaus Ries, Noah Coccaro, Elizabeth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul Taylor, Rachel Martin, Marie Meteer, and Carol Van Ess-Dykema. 2000. Dialogue act modeling for automatic tagging and recognition of conversational speech. Computational Linguistics, 26(3):339-371.\n\nNLLB Team, Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. 2022. No language left behind: Scaling humancentered machine translation.\n\nJörg Tiedemann and Santhosh Thottingal. 2020. OPUSMT - Building open translation services for the World. In Proceedings of the 22nd Annual Conferenec of the European Association for Machine Translation (EAMT), Lisbon, Portugal.\n\nJörg Tiedemann. 2012. Parallel data, tools and interfaces in opus. In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC'12), Istanbul, Turkey. European Language Resources Association (ELRA).\n\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models.\n\nSamarth Tripathi, Sarthak Tripathi, and Homayoon Beigi. 2018. Multi-modal emotion recognition on iemocap dataset using deep learning. arXiv preprint arXiv:1804.05788.\n\nSachin Vernekar, Ashish Gaurav, Vahdat Abdelzad, Taylor Denouden, Rick Salay, and Krzysztof Czarnecki. 2019a. Out-of-distribution detection in classifiers via generation.\n\nSachin Vernekar, Ashish Gaurav, Taylor Denouden, Buu Phan, Vahdat Abdelzad, Rick Salay, and Krzysztof Czarnecki. 2019b. Analysis of confident-classifiers for out-of-distribution detection. arXiv preprint arXiv:1904.12220.\n\nTim Z. Xiao, Aidan N. Gomez, and Yarin Gal. 2020. Wat zei je? detecting out-of-distribution translations with variational transformers.\n\nXiaoxue Zang, Abhinav Rastogi, Srinivas Sunkara, Raghav Gupta, Jianguo Zhang, and Jindong Chen. 2020. Multiwoz 2.2: A dialogue dataset with additional annotation corrections and state tracking baselines. In Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI, ACL 2020, pages 109-117.\n\nRowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. 2019. Defending against neural fake news. Advances in neural information processing systems, 32.\n\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter Liu. 2020. Pegasus: Pre-training with extracted gap-sentences for abstractive summarization. In International Conference on Machine Learning, pages 11328-11339. PMLR.\n\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019a. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675.\n\nYizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, and Bill Dolan. 2019b. Dialogpt: Large-scale generative pre-training for conversational response generation. arXiv preprint arXiv:1911.00536.\n\nWenxuan Zhou, Fangyu Liu, and Muhao Chen. 2021. Contrastive out-of-distribution detection for pretrained transformers. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.\n\nZhi-Hua Zhou. 2022. Open-environment machine learning. National Science Review, 9(8):nwac123.\n\nQiuyu Zhu, Guohui Zheng, and Yingying Yan. 2022. Effective out-of-distribution detection in classifier based on pedcc-loss."
    },
    {
      "markdown": "## A Examining the Limitations of Mahalanobis-Based OOD Detector for Text Generation\n\n![img-3.jpeg](img-3.jpeg)\n(a) deu date on fra model.\n(b) spa date on fra model.\n\nFigure 2: PCA reduction of encoder's hidden features for IN and OUT distribution samples, with Mahalanobis distance mean (green cross). The plot reveals the multimodal nature of the distributions.\n\nThe main drawback of the Mahalanobis distance is assuming a single-mode distribution. In text classification, this is mitigated by fitting one Mahalanobis scorer per class. However, in text generation, this assumption is flawed as there are multiple modes as illustrated in Fig. 2). PCA of Fig. 2 illustrate a failure case of the Mahalanobis distance in the case of OOD detection.\n\n## B Experimental setting\n\nIn this section, we dive into the details and definitions of our experimental setting. First, we present our OOD detection performance metrics (Sec. B.1), then we provide a couple samples for one of the small language shifts (Sec. B.4). We also discuss the choices of pre-trained model (Sec. B.6) and how we adapted common OOD detectors to the text generation case (Sec. B.7).\n\n## B. 1 Additionnal details on metrics\n\nOOD Detection is usually an unbalanced binary classification problem where the class of interest is OUT. Let us denote $Z$ the random variable corresponding to actually being out of distribution. We can assess the performance of our OOD detectors focusing on the False alarm rate and on the True detection rate. The False alarm rate or False positive rate (FPR) is the proportion of samples misclassified as OUT. For a score threshold $\\gamma$, we have $\\operatorname{FPR}=\\operatorname{Pr}(a(\\mathbf{x})>\\gamma \\mid Z=0)$. The True detection rate or True positive rate (TPR) is the proportion of OOD samples that are detected by the method. It is given by $\\operatorname{TPR}=\\operatorname{Pr}(a(\\mathbf{x})>\\gamma \\mid Z=$ 1).\n\nIn order to evaluate the performance of our methods we will focus and report mainly the AUROC and the FPR $\\downarrow$, we provide more detailed metrics and experiments in Sec. B.1.\n\nArea Under the Receiver Operating Characteristic curve (AUROC). The Receiver Operating Characteristic curve is curve obtained by plotting the True positive rate against the False positive rate. The area under this curve is the probability that an in-distribution example $\\mathbf{X}_{i n}$ has an anomaly score higher than an OOD sample $\\mathbf{x}_{\\text {out }}$ : AUROC $=\\operatorname{Pr}\\left(a\\left(\\mathbf{x}_{i n}\\right)>a\\left(\\mathbf{x}_{\\text {out }}\\right)\\right)$. It is given by $\\gamma \\mapsto$ $(\\operatorname{Pr}(a(\\mathbf{x})>\\gamma \\mid Z=0), \\operatorname{Pr}(a(\\mathbf{x})>\\gamma \\mid Z=1))$.\n\nFalse Positive Rate at $95 \\%$ True Positive Rate (FPR $\\downarrow$ ). We accept to allow only a given false positive rate $r$ corresponding to a defined level of safety and we want to know what share of positive samples we actually catch under this constraint. It leads to select a threshold $\\gamma_{r}$ such that the corresponding TPR equals $r$. At this threshold, one then computes: $\\operatorname{Pr}(a(\\mathbf{x})>\\gamma_{r} \\mid Z=0)$ with $\\gamma_{r}$ s.t. $\\operatorname{TPR}\\left(\\gamma_{r}\\right)=r$. $r$ is chosen depending on the difficulty of the task at hand and the required level of safety.\n\nFor the sake of brevity, we present only AUROCand FPR $\\downarrow$ metrics in our aggregated results but we also used Detection error and Area Under the Precision-Recall curve metrics and those are presented in our full results section (Ap. F).\n\nDetection error. It is simply the probability of miss-classification for a given True positive rate.\n\nArea Under the Precision-Recall curve (AUPR-IN/AUPR-OUT). The Precision-Recall curve plots the recall (true detection rate) against the precision (actual proportion of OOD amongst the predicted OOD). The area under this curve $\\gamma \\mapsto$ $(\\operatorname{Pr}(Z=1 \\mid s(\\mathbf{X}) \\leqslant \\gamma), \\operatorname{Pr}(s(\\mathbf{X}) \\leqslant \\gamma \\mid Z=1))$ captures the trade-off between precision and recall made by the model. A high value represents a high precision and a high recall i.e. the detector captures most of the positive samples while having few False positives."
    },
    {
      "markdown": "B. 2 Language pairs\n\n| Model | IN data | OUT data |\n| :--: | :--: | :--: |\n| Language shift |  |  |\n| DEU-ENG | Tatoeba DEU | News FR |\n| DEU-ENG | Tatoeba DEU | Tatoeba NLD |\n| SPA-ENG | Tatoeba SPA | News FR |\n| SPA-ENG | Tatoeba SPA | Tatoeba CAT |\n| SPA-ENG | Tatoeba SPA | Tatoeba POR |\n| NLD-ENG | Tatoeba SPA | AFR |\n| Domain shift |  |  |\n| DEU-ENG | Tatoeba DEU | EMEA DEU |\n| DEU-ENG | Tatoeba DEU | Eurpar1 DEU |\n| DEU-ENG | Tatoeba DEU | EMEA DEU |\n| DEU-ENG | Tatoeba DEU | Eurpar1 DEU |\n\nTable 8: Summary of models and studied shifts.\n\n## B. 3 Dataset sizes\n\n| Dataset Name | Size |\n| :-- | :--: |\n| Tatoeba AFR | 1373 |\n| Tattoeba CAT | 1630 |\n| Tatoeba DEU | 3000 |\n| Tatoeba NLD | 3000 |\n| Tatoeba POR | 3000 |\n| Tatoebamt ES | 3000 |\n| newscommentary DE | 3000 |\n| newscommentary ES | 3000 |\n| newscommentary FR | 6000 |\n| newscommentary NL | 3000 |\n| amazonreviewmulti DE | 3000 |\n| amazonreviewmulti ES | 3000 |\n| dailydialog default | 3000 |\n| europarlbilingual DE | 3000 |\n| europarlbilingual ES | 3000 |\n| EMEA DE | 3000 |\n| EMEA ES | 3000 |\n| EMEA NL | 3000 |\n| multiwoz | 3000 |\n| silicone dydae | 3000 |\n| silicone iemocap | 805 |\n| silicone maptask | 2963 |\n| silicone melds | 1109 |\n| silicone mrda | 3000 |\n| silicone oasis | 1513 |\n| silicone sem | 485 |\n| silicone swda | 3000 |\n\nTable 9: Number of samples in each (test) datasets\n\n## B. 4 Samples\n\nIn Tab. 10 we provide examples of small shifts in translation between Spanish and Catalan and its impact on a spanish to english translation model.\n\n## B. 5 Dialog datasets\n\nSwitchboard Dialog Act Corpus (SwDA) is a corpus of telephonic conversations. The corpus provides labels, topic and speaker information (Stolcke et al., 2000).\nICSI MRDA Corpus (MRDA) contains transcript 75 h of naturally occuring meetings involving more than 50 people (Shriberg et al., 2004).\nDaylyDialog Act Corpus (DyDA) contains daily common communications between people, covering topic such as small talk, meteo or daily activities (Li et al., 2017).\nInteractive Emotional Dyadic Motion Capture IEMOCAP)(Tripathi et al., 2018) consists of transcripts of improvisations or scripted scenarii supposed to outline the expression of emotions.\n\n## B. 6 Choices of models\n\nTo perform our experiments we needed models that were already well installed and deployed and that would also support OOD settings. For translation tasks, we needed specialized models for a notion of OOD to be easily defined. It would be indeed more hazardous to define a notion of OOD language when working with a multilingual model. The same is true for conversational models.\nNeural Machine Translation model. We benchmark our OOD method on translation models provided by Helsinkiy NLP (Tiedemann and Thottingal, 2020) on several pairs of languages with large and small shifts. We extended the experiment to detect domain shifts. These models are indeed specialized in each language pair and are widely recognised in the neural machine translation field. For our experiments we used the testing set provided along these models, so we can consider that they have been fine-tuned over the same distribution.\nConversational model. We used a dialogGPT (Zhang et al., 2019b) model fine-tuned on the Multi WOZ dataset as chat bot model. The finetuning on daily dialogue-type tasks ensures that the model is specialized, thus allowing us to get a good definition of samples not being in its range of expertise. Moreover, the choice of the architecture, DialogGPT, guarantees that our results are valid on a very common architecture."
    },
    {
      "markdown": "| Source sentence | Expected translation | Translation | BLEU |\n| :-- | :-- | :-- | --: |\n| A en Tom li agrada la tecnología. | Tom likes technology. | Tom li likes technology. | 42.73 |\n| Aci está la tesa bossa. | Here is your bag. | Aci está la tesa bossa. | 8.12 |\n| Això et pesarà en perfil. | That'll put you in danger. | Això et pesarà en perfil. | 8.12 |\n| A Londres hi han molto parcs bonics. | There are many beautiful parks in London. | To London hi han molto parcs bonics. | 6.57 |\n| Aquest pa és moñ déficiós. | This bread is very delicious. | Aquest pa és moñ déficiós. | 8.12 |\n| A tots els meus amics els agraden els videojocs. | All my friends like playing videogames. | A tots els meus amics els agrade els videojocs. | 4.20 |\n| Açö és un peix. | This is a fish. | Auauauauauauauauauaa ... uuauauauauauauauauauauaua | 0.00 |\n| Mòñes felicitats! | Congratulations! | Mòñes congrats! | 27.52 |\n| Bon any nou! | Happy New Year! | Bon any nou! | 15.97 |\n| Aquell que memnix, robarl. | He that will lie, will steal. | The one who's mindless, he'll steal. | 12.22 |\n| Jo sor, qui té la clas. | I'm the one who has the key. | Jo sor, qui te la clas. | 5.69 |\n| En Tom sart a treballar cada matí a dos quarts de set. | Tom leaves for work at 6:30 every morning. | In Tom sart to pull each matí to two quarts of set. | 3.67 |\n| Ell m'ha dit que la seva casa era crebturicada. | He told me that his house was haunted. | Ell m'ha dit that the seva house was haunted. | 27.78 |\n| Aquest és el físic on va nitser el meu pare. | This is the place where my father was born. | Aquest is the físic on va nitser el meu pare. | 8.30 |\n\nTable 10: Example of behaviours of a language model trained to handle Spanish inputs on Catalan inputs.\n\n## B. 7 Generalization of existing OOD detectors to Sequence Generation\n\nIn this section, we extend classical OOD detection score to the conditional text generation settting. Common OOD detectors were built for classification tasks and we need to adapt them to conditional text generation. Our task can be viewed as a sequence of classification problems with a very large number of classes (the size of the vocabulary). We chose the most naive approach which consists of averaging the OOD scores over the sequence. We experimented with other aggregation such as the $\\mathrm{min} / \\mathrm{max}$ or the standard deviation without getting interesting results.\nLikelihood Score The most naive approach to build a OOD score is to rely solely on the loglikelihood of the sequence. For a conditioning $\\mathbf{x}$ we define the log-likelyhood score by $a_{L}(\\mathbf{x})=$ $-\\sum_{t=0}^{|\\hat{\\mathbf{y}}|-1} \\log \\mathbf{p}_{\\theta}\\left(\\hat{\\mathbf{y}}_{t+1} \\mid \\mathbf{x}, \\hat{\\mathbf{y}}_{\\leqslant t}\\right)$. The likelihood is the same as the perplexity.\nAverage Maximum Softmax Probability score The maximum softmax probability (Hendrycks and Gimpel, 2017) takes the probability of the mode of the categorical distribution as score of OOD. We extend thise definition in the case of sequence of probability distribution by averaging this score along the sequence. For a given conditioning $\\mathbf{x}$, we define the average MSP score $a_{\\mathrm{MSP}}(\\mathbf{x})=$ $\\frac{1}{|\\hat{\\mathbf{y}}|} \\sum_{t=1}^{|\\hat{\\mathbf{y}}|} \\max _{i \\in[0, K]}\\left[\\mathbf{p}_{\\theta}^{T}\\left(i \\mid \\mathbf{x}, \\hat{\\mathbf{y}}_{\\leqslant t}\\right)\\right)$. While it is closely linked to uncertainty measures it discards most of the information contained in the probability distribution. It discards the whole probability distribution. We claim that much more information can be retrieve by studying the whole distribution.\nAverage Energy score We extend the definition of the energy score described in (Liu et al., 2020) to a sequence of probability distributions by averaging the score along the sequence. For a\ngiven conditioning $\\mathbf{x}$ and a temperature $T$ we define the average energy of the sequence: $a_{E}(\\mathbf{x}) \\triangleq$ $-\\frac{T}{|\\hat{\\mathbf{y}}|} \\sum_{t=1}^{|\\hat{\\mathbf{y}}|} \\log \\sum_{i}^{|t|}\\left[e^{f_{\\theta}\\left(\\mathbf{x}, \\hat{\\mathbf{y}}_{\\leqslant t}\\right)_{i} / T}\\right.$. It corresponds to the normalization term of the softmax function applied on the logits. While it takes into account the whole distribution, it only takes into account the amount of unormalized mass before normalization without attention to how this mass is distributed along the features.\nMahalanobis distance Following (Lee et al., 2018a; Colombo et al., 2022a) compute the Mahalanobis matrice based on the samples of a given reference set $\\mathcal{R}$. In our case we are using encoderdecoder models we use the output of the last hidden layer of the encoder as embedding. Let's denote $\\phi(\\mathbf{x})$ this embedding for a conditioning $\\mathbf{x}$. Let $\\mu$ and $\\Sigma$ be respectively, the mean and the covariance of these embedding on the reference set. We define $a_{\\mathrm{M}}(\\mathbf{x})=(1+(\\phi(\\mathbf{x})-\\mu)^{\\top} \\Sigma^{-1}(\\phi(\\mathbf{x})-\\mu))^{-1}$.\n\n## B. 8 Computational budget\n\nWe had a budget of 20000 h on NVIDIA V100 GPU. While this is an important number it was used to compute the benchmarks over many pairs and languages. In practice our OOD detectors do not require much addition computation overhead since they only rely on the probability distributions already output by the models.\n\n## B. 9 Towards an interpretable decision\n\nAn important dimension of fostering adoption is the ability to verify the decision taken by the automatic system. RAINPROOF offers a step in this direction when used with references: for each input sample, RAINPROOF finds the closest sample (in the sense of the Information Projection) in the reference set to take its decision. We present in Tab. 11 some OOD samples along with their translation scores, projection scores, and their projection on the refer-"
    },
    {
      "markdown": "Table 11: OOD inputs, their translations and projections onto the reference set. The first 2 are far from the reference set and not well translated whereas the next 2 are very close to the reference set and well translated. We can, for that matter, notice that the projection is quite close to the input sentence grammatically speaking.\n\n| Source | Able a la siti vlasno toftullat faes a les des. |  |\n| :--: | :--: | :--: |\n| Ground truth | Laa eight we roubad auld 10 p.m. |  |\n| Generated | Able a la siti vlasno toftullat faes a les des. | RLD:3.75 |\n| $\\mathrm{p}^{C}(\\mathrm{~m})$ | Dar gans per laftes. |  |\n| Source | Aspecta cola e'ba eshezeat i no té bon gant. |  |\n| Ground truth | This cola faes les dos and doesn't work any good. |  |\n| Generated | This tail e'ba eshezeat i no tas bon gant. | RLD:4.09 |\n| $\\mathrm{p}^{\\mathrm{C}}(\\mathrm{m})$ | Esta cachara es de sé. |  |\n| source | Aspecta és una carta molt estranya. |  |\n| Ground truth | This is a very strange letter. |  |\n| Generated | This is a molt estranya cand. | RLD:26.27 |\n| $\\mathrm{p}^{\\mathrm{C}}(\\mathrm{m})$ | Este carro es chiquete. |  |\n| source | Australia no és Austria. |  |\n| Ground truth | Australia (se) Austria. |  |\n| Generated | Australia es not Austria. | RLD:21.86 |\n| $\\mathrm{p}^{\\mathrm{C}}(\\mathrm{m})$ | La vida no es fácil. |  |\n\nence set. We notice that, in general, sentences that are close to the reference set, and whose projection has a close meaning, are better handled by $f_{\\theta}$. Therefore, one can visually interpret the prediction of RAINPROOF, and validate it. This observation further validates our method.\n\n## C Scaling to larger models\n\nIn order to validate our results we perform experiments on larger and general-purpose models such as BloomZ (Muennighoff et al., 2022), NLLB (Team et al., 2022) and the Facebook WMT16 submission (Ng et al., 2020).\n\n|  |  |  | AUROC | FPR $\\downarrow$ | AUPR-IN | AUPR-OUT | Err |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| $s_{0}$ | Ours | $a_{D_{s}}$ | 0.72 | 0.80 | 0.54 | 0.79 | 0.51 |\n|  | Bas. | $a_{C_{2 E}}$ | 0.65 | 0.95 | 0.74 | 0.59 | 0.52 |\n|  |  | $a_{L}$ | 0.47 | 0.89 | 0.59 | 0.50 | 0.43 |\n| $s_{1}$ | Ours. | $a_{D_{s}}$ | 0.74 | 0.87 | 0.80 | 0.68 | 0.34 |\n|  |  | $a_{F R_{s}}$ | 0.74 | 0.88 | 0.80 | 0.67 | 0.35 |\n|  | Bas. | $a_{C}$ | 0.70 | 0.90 | 0.80 | 0.52 | 0.38 |\n|  |  | $a_{D}$ | 0.64 | 0.89 | 0.54 | 0.67 | 0.53 |\n|  |  | $a_{S f}$ | 0.62 | 0.81 | 0.66 | 0.55 | 0.38 |\n\nTable 12: Performance in OOD detection on large translation model Facebook WMT-19 submission. (RUSDEU).\n\n|  |  | AUROC | FPR $\\downarrow$ | AUPR-IN | AUPR-OUT | Err |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| $s_{0}$ | Ours | $a_{F R}$ | 0.57 | 0.93 | 0.50 | 0.64 | 0.50 |\n|  |  | $a_{D_{s}}$ | 0.58 | 0.92 | 0.52 | 0.63 | 0.51 |\n|  | Bas. | $a_{M S P}$ | 0.59 | 0.90 | 0.64 | 0.52 | 0.42 |\n|  |  | $a_{E}$ | 0.51 | 0.97 | 0.47 | 0.55 | 0.57 |\n|  |  | $a_{L}$ | 0.54 | 0.90 | 0.44 | 0.62 | 0.53 |\n| $s_{1}$ | $s_{1}$ | $a_{D_{s}}$ | 0.71 | 0.82 | 0.73 | 0.64 | 0.39 |\n|  |  | $a_{F R^{*}}$ | 0.70 | 0.82 | 0.73 | 0.64 | 0.40 |\n|  | Bas. | $a_{S f}$ | 0.66 | 0.89 | 0.74 | 0.56 | 0.42 |\n\nTable 13: OOD detection on BloomZ using German as an OOD language for the instruction model.\n\n## C. 1 Negative results on NLLB\n\nBy the very definition of the No-Language-LeftBehind model, it should be particularly hard to find OOD language to benchmark on. The model still requires special token to be set in the sequence to define the source and target languages. We tried to apply our OOD detection methods to situations where the presented language does not correspond to the source language set by the special token. We found that in this scenario the likelihood was by far the best discriminator of OOD samples. It can be explained by the fact that our inputs are not actually OOD, they are just not consistent with the source language token, but the model is still well calibrated overall on these inputs.\n\n|  |  |  | AUROC | FPR $\\downarrow$ | AUPR-IN | AUPR-OUT | Err |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| $s_{0}$ | Ours | $a_{F R}$ | 0.50 | 1.00 | 0.75 | 0.75 | 0.51 |\n|  |  | $a_{D_{s}}$ | 0.57 | 0.95 | 0.60 | 0.62 | 0.50 |\n|  | Bas. | $a_{M S P}$ | 0.71 | 0.80 | 0.71 | 0.68 | 0.42 |\n|  |  | $a_{E}$ | 0.53 | 0.97 | 0.54 | 0.50 | 0.51 |\n|  |  | $a_{L}$ | 0.80 | 0.59 | 0.78 | 0.79 | 0.32 |\n| $s_{1}$ | Ours | $a_{F R^{*}}$ | 0.54 | 0.93 | 0.57 | 0.49 | 0.46 |\n|  | Bas. | $a_{S f}$ | 0.62 | 0.89 | 0.67 | 0.54 | 0.42 |\n\nTable 14: Performance in OOD detection for NLLB.\n\n## D Additional OOD features-based baselines\n\nTo further support the point that features-based detectors have important flaws when it comes to text generation we compare our best performing OOD score to SOTA OOD detectors in text such as the DataDepth $\\left(a_{D}\\right)$ (Colombo et al., 2022a) and the Maximum Cosine Projection $\\left(a_{C}\\right)$ (Zhou et al., 2021)."
    },
    {
      "markdown": "|  |  |  | AUROC | FPR $\\downarrow$ | AUPR-IN | AUPR-OUT | Err |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| $s_{0}$ | Ours | $a_{D_{n}}$ | 0.85 | 0.66 | 0.69 | 0.91 | 0.47 |\n|  | Bas. | $a_{C Q E}$ | 0.76 | 0.51 | 0.78 | 0.71 | 0.20 |\n|  |  | $a_{L}$ | 0.81 | 0.55 | 0.86 | 0.69 | 0.20 |\n| $s_{1}$ | Bas. | $a_{C}$ | 0.61 | 0.95 | 0.82 | 0.36 | 0.32 |\n|  |  | $a_{D}$ | 0.57 | 0.93 | 0.35 | 0.75 | 0.67 |\n|  |  | $a_{M}$ | 0.70 | 0.80 | 0.46 | 0.84 | 0.58 |\n\nTable 15: Comparison of our best detector $a_{D_{n}}$ against SOTA features based-ood detectors on close language shifts.\n\n## E Parameters tuning\n\nDetectors depend on their anomaly score to make decisions, and these scores can be parametric. First of all, soft probability-based scores depend on the soft probability distribution and its scaling. Therefore, the temperature is a crucial parameter to tune to get the most performance. While a small temperature makes the distribution pickier, a higher value spreads the probability mass along the classes. Moreover, the Rényi divergence depends on a factor $\\alpha$. We provide here further results and analysis of those parameters on our results.\n\nIn Fig. 3, we analyse the impact of the temperature and $\\alpha$ parameter for our Renyi-Negentropy score. Consistently with results for the information projection we find that the tail of the distribution is important to ensure good detection of OOD samples for all language shifts. A temperature higher than 2 and lower values of $\\alpha$ yield the best results. We recommend using $\\alpha=0.5$ with a temperature of 2 .\n\nWe found that our $a_{D_{n}}$ score, the Rényi negentropy is more stable concerning the temperature and the considered datasets and shifts than the energybased OOD score and the MSP score. Indeed, in Fig. 4, we show that the baselines do not behave consistently across datasets when the temperature changes. This is a problem when deploying these scores in production. Indeed, we cannot fit a temperature for each possible type of shift or OOD samples. By contrast, there exist sets of parameters (temperature and $\\alpha$ ) for which our negentropybased scores perform consistently across different shifts.\n\n## F Performance of our detectors in OOD detection\n\n## F. 1 Importance of tails' distributions\n\n![img-4.jpeg](img-4.jpeg)\n\nFigure 5: Impact of $\\alpha$ on the performance of the Rényi information projection for dialog shifts detection. A smaller $\\alpha$ increases the weight of the tail of the distribution. An $\\alpha$ of 0 would consist in counting the number of the common non zero elements.\n\nOur results show that, when it comes to domain shift (domain shifts in translation or dialog shifts), reference-based detectors are required to obtain good results. They also show that, the more these detectors take into account the tail of the distributions, the better they are, as displayed in Fig. 5. We find that low values of $\\alpha$ (near 0 ) yields better results with the Rényi Information projection $a_{D_{n}^{*}}$. It suggests that the tail of the distributions used during text generation carries context information and insights on the processed texts. Such results are consistent with findings of recent works in the context of automatic evaluation of text generation (Colombo et al., 2022b).\n\n## F. 2 Summary of our results\n\nIn Fig. 6 we present the different performance levels of all the detectors we studied. We can see that in every task our detectors outperform the baselines but also that in dialog shift, while the Mahalanobis distance outperform clearly our detectors for $\\mathrm{s}_{\\mathrm{E}}$, they still outperform baselines for their scenario by far.\n\n## F. 3 Detailed results of OOD detection performances\n\nIn this section, we present the performances of our OOD detectors on each detailed tasks, i.e. for each pair of IN and OOD data with all the considered metrics. Our metrics outperform other OOD detectors baselines in almost all scenarios."
    },
    {
      "markdown": "![img-5.jpeg](img-5.jpeg)\n\nFigure 3: Effect of the temperature and $\\alpha$ parameter for $a_{D_{\\alpha}}$ on the performance on OOD detection in terms of AUROC.\n\nTable 16: Detailed results of the performances of our OOD detectors on different language shifts. The first language of the pair is the reference language of the model and the second one is the studied shift.\n\n| Scenario |  | Score | AIPP-IN | AIPP-GIT | AUROC | EHR | $\\Pi$ | FPR | precision | recall | Scenario | Score | AIPP-IN | AIPP-GIT | AUROC | EHR | $\\Pi$ | FPR | precision | recall |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n|  | Ours | $a_{D_{\\alpha}}$ | 0.99 | 1.00 | 1.00 | 0.02 | 0.83 | 0.01 | 0.71 | 1.00 |  | Ours | $a_{D_{\\alpha}}$ | 0.80 | 0.97 | 0.91 | 0.23 | 0.67 | 0.41 | 0.54 | 1.00 |\n|  |  | eqn | 0.98 | 1.00 | 0.95 | 0.03 | 0.85 | 0.02 | 0.71 | 1.00 |  |  | eqn | 0.86 | 0.96 | 0.91 | 0.37 | 0.67 | 0.48 | 0.54 | 0.87 |\n|  | Resol |  | 0.96 | 0.96 | 0.97 | 0.05 | 0.82 | 0.05 | 0.71 | 1.00 |  | Resol |  | 0.97 | 0.97 | 0.99 | 0.75 | 0.70 | 0.35 | 0.88 | 0.23 | 1.00 |\n|  |  |  | 0.61 | 0.84 | 0.78 | 0.53 | 0.63 | 0.76 | 0.60 | 0.65 |  |  |  | 0.97 | 0.97 | 0.99 | 0.75 | 0.70 | 0.35 | 0.88 | 0.23 | 1.00 |\n|  |  | $\\alpha_{O D T}$ | 0.96 | 0.99 | 0.98 | 0.09 | 0.82 | 0.05 | 0.71 | 1.00 |  | $\\alpha_{O D T}$ | 0.94 | 0.96 | 0.73 | 0.70 | 0.41 | 0.87 | 0.37 | 1.00 |  |  |\n|  |  |  | 0.90 | 0.76 | 0.67 | 0.60 | 0.34 | 0.87 | 0.47 | 0.29 | 0.29 |  |  | 0.97 | 0.77 | 0.76 | 0.51 | 0.80 | 0.35 | 1.00 | 0.73 | 0.37 |\n|  | Ours |  | 1.00 | 1.00 | 1.00 | 0.02 | 0.83 | 0.01 | 0.71 | 1.00 |  |  |  | 0.98 | 0.98 | 0.97 | 0.70 | 0.73 | 0.50 | 0.91 | 0.44 | 0.57 |\n|  |  |  | 0.96 | 1.00 | 0.99 | 0.02 | 0.83 | 0.01 | 0.71 | 0.99 |  |  |  |  | 0.98 | 0.98 | 0.97 | 0.61 | 0.93 | 0.59 | 0.48 |\n|  |  | $\\alpha_{O D T}$ | 1.00 | 1.00 | 1.00 | 0.02 | 0.83 | 0.00 | 0.71 | 0.99 |  |  |  |  | 0.98 | 0.98 | 0.89 | 0.77 | 0.74 | 0.54 | 0.92 | 0.47 | 0.65 |\n|  |  | $\\alpha_{O D T}$ | 0.99 | 0.99 | 0.99 | 0.02 | 0.83 | 0.00 | 0.71 | 0.99 |  |  |  |  | 0.99 | 0.99 | 0.97 | 0.65 | 0.92 | 0.50 | 0.92 | 0.50 |\n|  |  | $\\alpha_{O D T}$ | 1.00 | 1.00 | 1.00 | 0.02 | 0.83 | 0.00 | 0.71 | 0.99 |  |  |  |  | 0.99 | 0.99 | 0.97 | 0.65 | 0.92 | 0.41 | 0.52 |  |\n|  |  |  | 0.96 | 0.98 | 0.99 | 0.02 | 0.83 | 0.01 | 0.71 | 0.99 |  |  |  |  | 0.98 | 0.98 | 0.96 | 0.76 | 0.90 | 0.99 | 0.86 | 0.66 |\n|  | Resol |  | 0.99 | 0.99 | 0.99 | 0.02 | 0.83 | 0.01 | 0.71 | 0.99 |  | Resol |  | 0.99 | 0.99 | 0.99 | 0.99 | 0.99 | 0.02 | 0.83 | 0.01 | 0.71 | 1.00 |\n|  |  |  | 0.96 | 0.99 | 0.99 | 0.02 | 0.83 | 0.01 | 0.71 | 0.99 |  |  |  |  | 0.99 | 0.99 | 0.99 | 0.99 | 0.99 | 0.02 | 0.83 | 0.01 | 0.71 |\n|  |  |  | 0.94 | 0.99 | 0.99 | 0.02 | 0.83 | 0.01 | 0.71 | 0.99 |  |  |  |  | 0.99 | 0.99 | 0.99 | 0.99 | 0.99 | 0.02 | 0.83 | 0.01 |  |\n|  |  |  | 0.92 | 0.99 | 0.99 | 0.02 | 0.83 | 0.01 | 0.71 | 0.99 |  | Resol |  | 0.99 | 0.99 | 0.99 | 0.99 | 0.99 | 0.99 | 0.02 | 0.83 | 0.01 | 0.71 |\n|  |  |  | 0.88 | 0.91 | 0.87 | 0.73 | 0.63 | 0.47 | 0.62 | 0.64 |  |  |  |  | 0.98 | 0.98 | 0.98 | 0.98 | 0.98 | 0.02 | 0.84 | 0.01 |  |\n| (a) deu-nld |  |  |  |  |  |  |  |  |  |  |  | (b) spa-cat |  |  |  |  |  |  |  |  |  |  |  |\n|  |  |  | AIPP-IN | AIPP-GIT | AUROC | EHR | $\\Pi$ | FPR | precision | recall |  |  |  | AIPP-IN | AIPP-GIT | AUROC | EHR | $\\Pi$ | FPR | precision | recall |  |  |\n|  | Ours | $a_{D_{\\alpha}}$ | 0.67 | 0.95 | 0.87 | 0.35 | 0.53 | 0.67 | 0.45 | 0.88 |  | Ours | $a_{D_{\\alpha}}$ | 0.99 | 1.00 | 1.00 | 0.82 | 0.83 | 0.81 | 0.71 | 1.00 |  |\n|  |  | eqn | 0.64 | 0.94 | 0.83 | 0.58 | 0.55 | 0.70 | 0.45 | 0.72 |  |  |  | eqn | 0.97 | 0.99 | 0.99 | 0.04 | 0.83 | 0.04 | 0.71 | 1.00 |  |\n|  |  |  | 0.65 | 0.94 | 0.84 | 0.62 | 0.57 | 0.74 | 0.46 | 1.00 |  |  |  | 0.97 | 0.97 | 0.98 | 0.04 | 0.82 | 0.03 | 0.71 | 1.00 |  |\n|  | Resol |  | 0.64 | 0.94 | 0.84 | 0.63 | 0.31 | 0.79 | 0.19 | 1.00 |  | Resol |  | 0.98 | 0.98 | 0.99 | 0.99 | 0.04 | 0.83 | 0.03 | 0.71 | 1.00 |  |\n|  |  |  | 0.63 | 0.94 | 0.83 | 0.62 | 0.31 | 0.75 | 0.19 | 1.00 |  |  |  | 0.98 | 0.98 | 0.99 | 0.99 | 0.04 | 0.83 | 0.03 | 0.71 | 1.00 |  |\n|  |  |  | 0.59 | 0.85 | 0.58 | 0.76 | 0.37 | 0.92 | 0.38 | 0.38 |  |  |  |  | 0.98 | 0.98 | 0.99 | 0.02 | 0.83 | 0.00 | 0.71 | 0.99 |  |\n|  | Resol |  | 0.58 | 0.94 | 0.79 | 0.47 | 0.47 | 0.57 | 0.40 | 0.57 |  |  |  |  | 0.98 | 0.98 | 0.99 | 0.02 | 0.83 | 0.00 | 0.71 | 1.00 |  |\n|  |  |  | 0.57 | 0.94 | 0.75 | 0.49 | 0.40 | 0.59 | 0.35 | 0.47 |  |  |  |  | 0.98 | 0.98 | 1.00 | 0.99 | 0.02 | 0.83 | 0.01 | 0.71 | 1.00 |  |\n|  |  |  | 0.56 | 0.93 | 0.74 | 0.61 | 0.45 | 0.74 | 0.39 | 0.55 |  |  |  |  | 0.98 | 0.98 | 0.99 | 0.01 | 0.83 | 0.00 | 0.71 | 1.00 |  |\n|  |  |  | 0.55 | 0.82 | 0.55 | 0.80 | 0.00 | 0.97 | 0.00 | 0.00 |  |  |  |  | 0.98 | 0.98 | 1.00 | 1.00 | 0.02 | 0.83 | 0.00 | 0.71 | 0.99 |  |\n|  |  |  | 0.47 | 0.94 | 0.78 | 0.47 | 0.46 | 0.57 | 0.39 | 0.56 |  |  |  |  | 0.98 | 0.98 | 1.00 | 0.02 | 0.83 | 0.00 | 0.71 | 1.00 |  |\n|  |  |  | 0.52 | 0.90 | 0.75 | 0.74 | 0.00 | 0.90 | 0.00 | 0.00 |  |  |  |  | 0.98 | 0.98 | 0.99 | 0.02 | 0.83 | 0.00 | 0.71 | 0.99 |  |\n|  | Resol |  | 0.52 | 0.86 | 0.58 | 0.77 | 0.29 | 0.94 | 0.22 | 0.40 |  | Resol |  | 0.98 | 0.98 | 0.99 | 0.99 | 0.02 | 0.83 | 0.00 | 0.71 | 0.99 |  |\n|  |  |  | 0.55 | 0.92 | 0.74 | 0.61 | 0.45 | 0.74 | 0.39 | 0.55 |  |  |  |  | 0.98 | 0.98 | 0.99 | 0.02 | 0.83 | 0.00 | 0.71 | 1.00 |  |\n|  |  |  | 0.52 | 0.82 | 0.57 | 0.80 | 0.00 | 0.97 | 0.00 | 0.00 |  |  |  |  | 0.98 | 0.98 | 0.99 | 0.02 | 0.83 | 0.00 | 0.71 | 0.99 |  |\n|  |  |  | 0.47 | 0.94 | 0.78 | 0.47 | 0.46 | 0.57 | 0.39 | 0.56 |  |  |  |  | 0.98 | 0.98 | 0.99 | 0.02 | 0.83 | 0.00 | 0.71 | 1.00 |  |\n|  |  |  | 0.52 | 0.90 | 0.75 | 0.74 | 0.00 | 0.90 | 0.00 | 0.00 |  |  |  |  | 0.98 | 0.98 | 0.99 | 0.02 | 0.83 | 0.00 | 0.71 | 0.99 |  |\n|  | Resol |  | 0.52 | 0.86 | 0.58 | 0.77 | 0.29 | 0.94 | 0.22 | 0.40 |  | Resol |  | 0.98 | 0.98 | 0.99 | 0.02 | 0.83 | 0.00 | 0.71 | 0.99 |  |\n|  |  |  | 0.55 | 0.92 | 0.74 | 0.61 | 0.45 | 0.74 | 0.39 | 0.55 |  |  |  |  | 0.98 | 0.98 | 0.99 | 0.02 | 0.83 | 0.00 | 0.71 | 1.00 |  |\n|  |  |  | 0.50 | 0.82 | 0.55 | 0.61 | 0.45 | 0.74 | 0.39 | 0.55 |  |  |  |  | 0.98 | 0.98 | 0.99 | 0.02 | 0.83 | 0.00 | 0.71 | 1.00 |  |\n|  |  |  | 0.45 | 0.87 | 0.58 | 0.60 | 0.50 | 0.83 | 0.01 | 0.71 | 1.00 |  |  |  |  | 0.98 | 0.98 | 0.99 | 0.02 | 0.83 | 0.00 | 0.71 | 1.00 |  |\n|  |  |  | 0.42 | 0.90 | 0.69 | 0.67 | 0.07 | 0.83 | 0.07 | 0.71 | 0.99 |  |  |  |  | 0.98 | 0.98 | 0.99 | 0.02 | 0.83 | 0.00 | 0.61 | 0.94 |  |\n|  |  |  | 0.39 | 0.90 | 0.69 | 0.67 | 0.07 | 0.83 | 0.07 | 0.71 | 0.99 |  |  |  |  | 0.98 | 0.98 | 0.99 | 0.02 | 0.83 | 0.00 | 0.61 | 0.94 |  |\n|  |  |  | 0.36 | 0.98 | 0.69 | 0.68 | 0.83 | 0.00 | 0.71 | 0.99 |  |  |  |  |  | 0.98 | 0.98 | 0.99 | 0.02 | 0.83 | 0.00 | 0.61 | 0.94 |  |\n|  |  |  | 0.35 | 0.83 | 0.71 | 0.53 | 0.53 | 0.77 | 0.54 | 0.52 |  |  |  |  | 0.98 | 0.98 | 0.99 | 0.99 | 0.02 | 0.83 | 0.00 | 0.61 | 0.94 |  |\n|  |  |  | 0.32 | 0.85 | 0.71 | 0.53 | 0.53 | 0.77 | 0.54 | 0.52 |  |  |  |  | 0.98 | 0.98 | 0.99 | 0.99 | 0.02 | 0.83 | 0.00 | 0.61 | 0.94 |  |\n|  |  |  | 0.30 | 0.89 | 0.69 | 0.68 | 0.82 | 0.04 | 0.71 | 1.00 |  |  |  |  | 0.98 | 0.98 | 0.99 | 0.99 | 0.02 | 0.83 | 0.00 | 0.61 | 0.94 |  |\n|  |  |  | 0.28 | 0.95 | 0.67 | 0.74 | 0.78 | 0.50 | 0.55 | 0.50 | 0.50 |  |  |  |  | 0.98 | 0.98 | 0.99 | 0.02 | 0.83 | 0.00 | 0.61 | 0.94 |  |\n|  |  |  | 0.26 | 0.90 | 0.68 | 0.69 | 0.03 | 0.82 | 0.01 | 0.71 | 0.98 |  |  |  |  | 0.98 | 0.98 | 0.99 | 0.02 | 0.83 | 0.00 | 0.61 | 0.94 |  |\n|  |  |  | 0.24 | 0.81 | 0.71 | 0.57 | 0.65 | 0.00 | 0.96 | 0.00 | 0.00 |  |  |  |  | 0.98 | 0.98 | 0.99 | 0.02 | 0.83 | 0.00 | 0.61 | 0.94 |  |\n|  |  |  | 0.22 | 0.80 | 0.99 | 0.87 | 0.07 | 0.83 | 0.07 | 0.71 | 0.99 |  |  |  |  | 0.98 | 0.98 | 0.99 | 0.02 | 0.83 | 0.00 | 0.61 | 0.94 |  |\n|  |  |  | 0.20 | 0.80 | 1.00 | 1.00 | 0.02 | 0.83 | 0.00 | 0.71 | 1.00 |  |  |  |  | 0.98 | 0.98 | 0.99 | 0.02 | 0.83 | 0.00 | 0.61 | 0.94 |  |\n|  |  |  | 0.18 | 0.99 | 0.88 | 0.85 | 0.03 | 0.91 | 0.71 | 1.00 |  |  |  |  |  | 0.98 | 0.98 | 0.99 | 0.02 | 0.83 | 0.00 | 0.61 | 0.94 |  |\n|  |  |  | 0.16 | 0.99 | 1.00 | 0.02 | 0.83 | 0.01 | 0.71 | 1.00 |  |  |  |  |  | 0.98 | 0.98 | 0.99 | 0.02 | 0.83 | 0.00 | 0.61 | 0.94 |  |\n|  |  |  | 0.14 | 0.99 | 1.00 | 0.99 | 0.02 | 0.83 | 0.01 | 0.71 | 0.98 |  |  |  |  |  | 0.98 | 0.98 | 0.99 | 0.02 | 0.83 | 0.00 | 0.61 | 0.94 |  |\n|  |  |  | 0.12 | 0.97 | 0.95 | 0.13 | 0.82 | 0.17 | 0.71 | 0.97 |  |  |  |  |  | 0.98 | 0.98 | 0.99 | 0.02 | 0.83 | 0.00 | 0.61 | 0.94 |  |\n|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n\n(f) spa-por"
    },
    {
      "markdown": "Table 17: Detailed results of the performances of our OOD detectors on different domain shifts. For Spanish (spa) and German (de), we present two domains shifts: Technical medical (EMEA) data and legal parlementary texts (parl) against common language emboddied by the Tatoeba dataset (tat).\n\n| Scenario |  | Score | AIPP-IN | AIPP-GIT | AIROC | EIR | IT | FPB | precision | recall | Scenario | Score | AIPP-IN | AIPP-GIT | AIROC | EIR | IT | FPB | precision | recall |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n|  | Ours | $a_{i j_{j}}$ | 0.90 | 0.76 | 0.86 | 0.43 | 0.81 | 0.82 | 0.80 | 1.00 |  | Ours | $a_{i j_{j}}$ | 0.75 | 0.75 | 0.76 | 0.41 | 0.67 | 0.76 | 0.67 | 1.00 |\n|  |  | $\\mathrm{a}_{\\text {jij }}$ | 0.87 | 0.72 | 0.81 | 0.48 | 0.71 | 0.86 | 0.78 | 0.75 |  |  |  | 0.86 | 0.87 | 0.70 | 0.44 | 0.45 | 0.84 | 0.64 |\n|  |  | $a_{i j}$ | 0.88 | 0.75 | 0.83 | 0.48 | 0.78 | 0.93 | 0.79 | 1.00 |  | Baseline | $a_{i j}$ | 0.79 | 0.79 | 0.64 | 0.67 | 0.83 | 0.69 | 1.00 |\n|  |  |  | 0.86 | 0.73 | 0.82 | 0.48 | 0.76 | 0.91 | 0.77 | 0.74 |  |  |  | 0.88 | 0.81 | 0.68 | 0.49 | 0.67 | 0.94 | 0.90 | 1.00 |\n|  | Ours | $a_{i j j}$ | 0.89 | 0.76 | 0.85 | 0.44 | 0.79 | 0.84 | 0.80 | 1.00 |  |  |  | 0.79 | 0.79 | 0.71 | 0.45 | 0.67 | 0.86 | 0.90 | 1.00 |\n|  |  | $a_{i j j}$ | 0.90 | 0.88 | 0.90 | 0.25 | 0.82 | 0.85 | 0.81 | 0.86 |  |  |  | 0.90 | 0.86 | 0.65 | 0.68 | 0.44 | 0.68 | 0.84 | 0.83 | 0.00 |\n|  | Ours | $a_{i j j}$ | 0.89 | 0.83 | 0.88 | 0.33 | 0.82 | 0.62 | 0.80 | 0.83 |  |  |  | 0.87 | 0.63 | 0.67 | 0.48 | 0.68 | 0.90 | 0.00 | 0.00 |\n|  |  | $a_{i j j}=$ | 0.88 | 0.73 | 0.84 | 0.47 | 0.78 | 0.89 | 0.79 | 0.77 |  |  |  | 0.87 | 0.67 | 0.70 | 0.44 | 0.68 | 0.82 | 0.00 | 0.00 |\n|  |  | $a_{i j j}=$ | 0.87 | 0.73 | 0.83 | 0.48 | 0.77 | 0.91 | 0.79 | 0.75 |  |  |  | 0.86 | 0.66 | 0.65 | 0.69 | 0.45 | 0.68 | 0.83 | 0.00 | 0.00 |\n|  | Ours | $a_{i j j}=$ | 0.85 | 0.70 | 0.80 | 0.50 | 0.74 | 0.94 | 0.77 | 0.72 |  |  |  | 0.82 | 0.64 | 0.65 | 0.45 | 0.68 | 0.85 | 0.00 | 0.00 |\n|  |  | $a_{i j j}=$ | 0.86 | 0.67 | 0.80 | 0.52 | 0.75 | 0.98 | 0.78 | 0.72 |  |  |  | 0.85 | 0.65 | 0.67 | 0.69 | 0.43 | 0.68 | 0.80 | 0.00 | 0.00 |\n|  | Baseline | \\% | 0.88 | 0.89 | 0.90 | 0.25 | 0.90 | 0.46 | 0.66 | 0.00 |  | Baseline |  | 0.57 | 0.61 | 0.60 | 0.46 | 0.27 | 0.87 | 0.47 | 0.19 |\n|  |  |  | 0.87 | 0.90 | 0.89 | 0.22 | 0.81 | 0.39 | 0.80 | 0.81 |  |  |  | 0.82 | 0.66 | 0.66 | 0.44 | 0.68 | 0.83 | 0.00 | 0.00 |\n|  |  |  |  |  |  |  |  |  |  |  |  | (a) deu:news-EMEA |  |  |  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |  |  |  | (b) spa:news-parl |  |  |  |  |  |  |  |  |  |  |\n| Scenario |  | Score | AIPP-IN | AIPP-GIT | AIROC | EIR | IT | FPB | precision | recall | Scenario | Score | AIPP-IN | AIPP-GIT | AIROC | EIR | IT | FPB | precision | recall |\n|  | Ours | $a_{i j_{j}}$ | 0.75 | 0.75 | 0.71 | 0.41 | 0.67 | 0.78 | 0.66 | 1.00 |  | Ours | $a_{i j_{j}}$ | 0.92 | 0.81 | 0.89 | 0.37 | 0.83 | 0.70 | 0.81 | 1.00 |\n|  |  | $a_{i j j}$ | 0.81 | 0.65 | 0.65 | 0.45 | 0.42 | 0.84 | 0.61 | 0.52 |  |  |  | 0.89 | 0.75 | 0.85 | 0.44 | 0.79 | 0.82 | 0.00 | 0.78 |\n|  | Baseline | $a_{i j}$ | 0.75 | 0.75 | 0.68 | 0.45 | 0.67 | 0.85 | 0.66 | 1.00 |  | Baseline | $a_{i j}$ | 0.90 | 0.77 | 0.86 | 0.44 | 0.80 | 0.82 | 0.00 | 1.00 |\n|  |  | $a_{i j}$ | 0.63 | 0.58 | 0.64 | 0.51 | 0.67 | 0.96 | 0.50 | 1.00 |  |  |  | 0.86 | 0.73 | 0.82 | 0.47 | 0.76 | 0.89 | 0.77 | 0.74 |\n|  |  | $a_{i j j}=$ | 0.75 | 0.75 | 0.68 | 0.46 | 0.67 | 0.86 | 0.51 | 1.00 |  |  |  | 0.90 | 0.80 | 0.89 | 0.87 | 0.41 | 0.81 | 0.77 | 0.80 | 1.00 |\n|  | Ours | $a_{i j j}$ | 0.69 | 0.66 | 0.68 | 0.43 | 0.30 | 0.81 | 0.80 | 0.22 |  | Ours | $a_{i j_{j}}$ | 0.88 | 0.85 | 0.88 | 0.29 | 0.81 | 0.34 | 0.80 | 0.82 |\n|  |  | $a_{i j j_{j}}=$ | 0.66 | 0.64 | 0.68 | 0.46 | 0.00 | 0.88 | 0.00 | 0.00 |  |  |  | 0.89 | 0.85 | 0.88 | 0.32 | 0.81 | 0.59 | 0.80 | 0.82 |\n|  |  | $a_{i j j}=$ | 0.65 | 0.65 | 0.68 | 0.45 | 0.00 | 0.86 | 0.00 | 0.00 |  |  |  | 0.90 | 0.77 | 0.86 | 0.44 | 0.79 | 0.83 | 0.80 | 0.79 |\n|  |  | $a_{i j j}=$ | 0.65 | 0.65 | 0.68 | 0.45 | 0.00 | 0.85 | 0.00 | 0.00 |  |  |  | 0.88 | 0.75 | 0.84 | 0.45 | 0.77 | 0.85 | 0.79 | 0.75 |\n|  |  | $a_{i j j}=$ | 0.63 | 0.64 | 0.66 | 0.45 | 0.00 | 0.86 | 0.00 | 0.00 |  |  |  | 0.81 | 0.66 | 0.76 | 0.50 | 0.70 | 0.94 | 0.76 | 0.65 |\n|  | Ours | $a_{i j j}=$ | 0.64 | 0.64 | 0.67 | 0.45 | 0.00 | 0.86 | 0.00 | 0.00 |  | Ours |  | 0.87 | 0.70 | 0.81 | 0.49 | 0.75 | 0.94 | 0.78 | 0.72 |\n|  |  | $a_{i j j}=$ | 0.52 | 0.66 | 0.59 | 0.41 | 0.40 | 0.78 | 0.52 | 0.53 |  | Baseline |  | 0.67 | 0.59 | 0.64 | 0.49 | 0.68 | 0.94 | 0.68 | 0.60 |\n|  |  |  | 0.58 | 0.61 | 0.62 | 0.47 | 0.00 | 0.89 | 0.00 | 0.00 |  |  |  | 0.61 | 0.61 | 0.83 | 0.51 | 0.75 | 0.58 | 0.78 | 0.72 |\n|  |  |  |  |  |  |  |  |  |  |  |  | (c) deu:news-parl |  |  |  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |  |  |  | (d) spa:news-EMEA |  |  |  |  |  |  |  |  |  |  |\n\n![img-6.jpeg](img-6.jpeg)\n(a) MSP\n![img-7.jpeg](img-7.jpeg)\n(b) Energy score\n\nFigure 4: Impact of the temperature used to compute the energy $\\left(a_{E}\\right)$ and MSP $\\left(a_{\\text {MSP }}\\right)$ OOD scores in terms of AUROC.\n\n## F. 4 ROC AUC curves\n\n## F.4.1 Language shifts\n\nIn Fig. 7 and Fig. 8 we present the ROC-AUC curves of our different detectors for language shifts in translation.\n\n## F.4.2 Domain shifts\n\nIn Fig. 9 and Fig. 10 we present the ROC-AUC curves of our different detectors for topic shifts in translation.\n\n## F.4.3 Dialog shifts\n\nIn Fig. 11 and Fig. 12 we present the ROC-AUC curves of our different detectors for topic shifts in a dialog setting.\n\n## G NTM performance\n\nSurprisingly we show that common OOD detectors tend to exclude samples that the model well handles and keep some that are not leading to decreasing overall performance in terms of translation metrics. Moreover, it seems this phenomenon is more dominant in reference-based detectors. We show that our uncertainty-based detectors mostly avoid that downfall and provide good OOD detection and improved translation performances.\n\n## G. 1 Absolute performances\n\nIt is clear (somewhat expected) that NMT models do not perform as well on OOD data as we can see in Tab. 19b. However, we find that our OOD detectors are able to remove most of the worst-case samples and keep enough well-translated samples so that with correct filtering our method actually allows the model to achieve somewhat acceptable BLEU scores.\n\n## G. 2 Gains\n\nIn Tab. 20 we give the detailed gain in translation performance based on the BLEU score.\n\n## G. 3 Choice of threshold\n\nWe believe that the choice of the threshold for OOD detection should not require OOD samples because we do not want to assume we have access to all kind"
    },
    {
      "markdown": "![img-8.jpeg](img-8.jpeg)\n\nFigure 6: Trade-offs between AUROCand FPR $\\downarrow$ for each tasks and metrics\n\nTable 18: Detailed performance results of our OOD detectors on dialog shift against the Multi WOZ dataset as reference set.\n\n| Scenario |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | "
    },
    {
      "markdown": "![img-9.jpeg](img-9.jpeg)\n\nFigure 7: ROCAUC curves for our uncertainty-based metrics compared to common baselines for language shift detection. Baselines are represented in dashed lines.\n![img-10.jpeg](img-10.jpeg)\n\nFigure 8: ROC-AUC curves for our reference-based metrics compared to common baselines for language shift detection. Baselines are represented in dashed lines."
    },
    {
      "markdown": "![img-11.jpeg](img-11.jpeg)\n\nFigure 9: ROC-AUC curves for our uncertainty-based metrics compared to common baselines for domain shift detection. baselines are represented in dashed lines.\n![img-12.jpeg](img-12.jpeg)\n\nFigure 10: ROC-AUC curves for our reference-based metrics compared to common baselines for domain shift detection. baselines are represented in dashed lines."
    },
    {
      "markdown": "![img-13.jpeg](img-13.jpeg)\n\nFigure 11: ROC-AUC curves for our uncertainty-based metrics compared to common baselines for dialog shift detection. baselines are represented in dashed lines.\n![img-14.jpeg](img-14.jpeg)\n\nFigure 12: ROC-AUC curves for our reference-based metrics compared to common baselines for dialog shift detection. baselines are represented in dashed lines."
    },
    {
      "markdown": "| Scenario |  | Score | spa-cat | spa-por | nld-afr | spa:tat-parl | deu:news-parl | spa:tat-EMEA | deu:news-EMEA |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n|  |  | $\\times$ | 59.63 | 59.63 | 62.38 | 59.63 | 34.07 | 59.63 | 34.07 |\n| $s_{0}$ | Ours | $a_{D_{N}}$ | 63.82 | 62.41 | 67.77 | 64.78 | 36.68 | 63.82 | 36.69 |\n|  |  | $a_{F R}$ | 62.41 | 62.41 | 65.89 | 63.54 | 36.10 | 63.54 | 36.19 |\n|  | Baselines | $a_{E}$ | 62.99 | 62.99 | 65.80 | 62.99 | 35.87 | 62.99 | 35.87 |\n|  |  | $a_{L}$ | 64.52 | 64.52 | 67.78 | 64.52 | 36.72 | 64.52 | 36.72 |\n|  |  | $a_{\\text {MSP }}$ | 58.15 | 58.15 | 60.31 | 58.15 | 33.00 | 58.15 | 33.00 |\n|  |  | $a_{\\text {CGE }}$ | 60.36 | 60.36 | 63.19 | 60.36 | 35.68 | 60.36 | 35.68 |\n| $s_{1}$ | Ours | $a_{D_{2}^{\\prime}}$ | 59.99 | 59.99 | 62.82 | 59.99 | 33.82 | 59.99 | 33.82 |\n|  |  | $a_{F R^{\\prime}}$ | 59.98 | 59.98 | 62.83 | 59.98 | 33.78 | 59.98 | 33.78 |\n|  | Baselines | $a_{C}$ | 60.80 | 60.80 | 62.67 | 60.80 | 34.48 | 60.80 | 34.48 |\n|  |  | $a_{M}$ | 59.63 | 59.63 | 62.61 | 59.63 | 33.85 | 59.63 | 33.85 |\n\n(a) IN\n\n| Scenario |  | Score | spa-cat | spa-por | nld-afr | spa:tat-parl | deu:news-parl | spa:tat-EMEA | deu:news-EMEA |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n|  |  | $\\times$ | 15.73 | 15.40 | 23.79 | 33.17 | 28.36 | 39.48 | 52.96 |\n| $s_{0}$ | Ours | $a_{D_{N}}$ | 22.06 | 38.04 | 36.51 | 41.80 | 33.64 | 64.11 | 54.60 |\n|  |  | $a_{F R}$ | 26.41 | 37.74 | 33.86 | 38.17 | 32.10 | 62.29 | 55.02 |\n|  | Baselines | $a_{E}$ | 14.44 | 12.92 | 27.64 | 35.11 | 31.97 | 60.52 | 54.48 |\n|  |  | $a_{L}$ | 16.62 | 15.40 | 35.90 | 40.23 | 34.18 | 61.32 | 54.41 |\n|  |  | $a_{\\text {MSP }}$ | 17.36 | 18.97 | 23.78 | 30.81 | 26.46 | 61.61 | 54.64 |\n|  |  | $a_{\\text {CGE }}$ | 17.61 | 16.70 | 29.02 | 33.18 | 29.36 | 59.66 | 55.81 |\n| $s_{1}$ | Ours | $a_{D_{2}^{\\prime}}$ | 20.16 | 23.33 | 30.74 | 34.16 | 27.38 | 44.45 | 44.62 |\n|  |  | $a_{F R^{\\prime}}$ | 20.15 | 23.35 | 30.61 | 34.16 | 27.39 | 44.45 | 43.61 |\n|  | Baselines | $a_{C}$ | 16.84 | 16.72 | 23.67 | 33.32 | 29.11 | 59.55 | 52.19 |\n|  |  | $a_{M}$ | 16.76 | 19.37 | 25.64 | 30.83 | 27.22 | 59.73 | 54.71 |\n\n(b) OOD\n\n| Scenario |  | Score | spa-cat | spa-por | nld-afr | spa:tat-parl | deu:news-parl | spa:tat-EMEA | deu:news-EMEA |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n|  |  | $\\times$ | 50.25 | 44.89 | 55.20 | 50.81 | 31.21 | 59.55 | 43.12 |\n| $s_{0}$ | Ours | $a_{D_{N}}$ | 61.50 | 61.35 | 65.66 | 59.61 | 35.35 | 63.91 | 46.32 |\n|  |  | $a_{F R}$ | 60.80 | 61.35 | 63.40 | 56.70 | 34.34 | 63.10 | 46.11 |\n|  | Baselines | $a_{E}$ | 54.86 | 48.72 | 61.99 | 53.66 | 34.19 | 62.06 | 45.78 |\n|  |  | $a_{L}$ | 58.75 | 54.92 | 65.56 | 58.32 | 35.64 | 63.35 | 46.22 |\n|  |  | $a_{\\text {MSP }}$ | 51.26 | 48.32 | 52.96 | 48.85 | 29.50 | 56.86 | 33.37 |\n|  |  | $a_{\\text {CGE }}$ | 51.69 | 45.67 | 57.53 | 49.91 | 32.88 | 60.09 | 44.38 |\n| $s_{1}$ | Ours | $a_{D_{2}^{\\prime}}$ | 54.33 | 53.38 | 59.22 | 58.34 | 30.37 | 59.52 | 36.91 |\n|  |  | $a_{F R^{\\prime}}$ | 54.33 | 53.39 | 59.18 | 58.34 | 30.37 | 59.52 | 36.89 |\n|  | Baselines | $a_{C}$ | 53.27 | 48.88 | 55.85 | 50.82 | 31.77 | 60.34 | 43.84 |\n|  |  | $a_{M}$ | 53.55 | 53.99 | 58.40 | 53.80 | 30.42 | 59.63 | 37.86 |\n\n(c) ALL\n\nTable 19: Absolue translation performances in terms of BLEU on the different subset (IN, OOD, ALL) of each dataset of our translation OOD performance benchmark."
    },
    {
      "markdown": "| Scenario |  | Score | spa-cat | spa-por | nld-afr | spactat-parl | deu:news-parl | spactat-EMEA | deu:news-EMEA |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n|  |  |  |  |  |  |  |  | 44.19 | $+2.62$ |\n| $S_{0}$ | Ours | $\\begin{aligned} & a_{D_{n}} \\\\ & a_{F B} \\end{aligned}$ | $\\begin{aligned} & +2.78 \\\\ & +3.36 \\end{aligned}$ | $\\begin{aligned} & +2.78 \\\\ & +3.36 \\end{aligned}$ | $\\begin{aligned} & +3.50 \\\\ & +3.42 \\end{aligned}$ | $\\begin{aligned} & +3.91 \\\\ & +3.36 \\end{aligned}$ | $\\begin{aligned} & +4.07 \\\\ & +1.79 \\end{aligned}$ | $\\begin{aligned} & +3.91 \\\\ & +3.36 \\end{aligned}$ | $\\begin{aligned} & +4.11 \\\\ & +1.79 \\end{aligned}$ |\n|  | Baselines | $\\begin{aligned} & a_{E} \\\\ & a_{L} \\end{aligned}$ | $\\begin{aligned} & +4.89 \\\\ & +4.81 \\end{aligned}$ | $\\begin{aligned} & +4.89 \\\\ & +4.88 \\end{aligned}$ | $\\begin{aligned} & +5.40 \\\\ & -2.07 \\end{aligned}$ | $\\begin{aligned} & +4.89 \\\\ & -1.48 \\end{aligned}$ | $\\begin{aligned} & +2.65 \\\\ & -1.07 \\end{aligned}$ | $\\begin{aligned} & +4.89 \\\\ & -1.48 \\end{aligned}$ | $\\begin{aligned} & +2.65 \\\\ & -1.07 \\end{aligned}$ |\n|  |  | $\\begin{aligned} & a_{C Q E} \\\\ & a_{D_{2}} \\end{aligned}$ | $\\begin{aligned} & +0.73 \\\\ & +0.36 \\end{aligned}$ | $\\begin{aligned} & +0.73 \\\\ & +0.36 \\end{aligned}$ | $\\begin{aligned} & +0.80 \\\\ & +0.44 \\end{aligned}$ | $\\begin{aligned} & +0.73 \\\\ & +0.36 \\end{aligned}$ | $\\begin{aligned} & +1.61 \\\\ & +0.36 \\end{aligned}$ | $\\begin{aligned} & +0.73 \\\\ & +0.36 \\end{aligned}$ | $\\begin{aligned} & +1.61 \\\\ & +0.26 \\end{aligned}$ |\n| $S_{1}$ | Ours | $\\begin{aligned} & a_{D_{2}} \\\\ & a_{F B^{*}} \\end{aligned}$ | $\\begin{aligned} & +0.36 \\\\ & +0.35 \\end{aligned}$ | $\\begin{aligned} & +0.36 \\\\ & +0.35 \\end{aligned}$ | $\\begin{aligned} & +0.45 \\\\ & +0.45 \\end{aligned}$ | $\\begin{aligned} & +0.35 \\\\ & +0.35 \\end{aligned}$ | $\\begin{aligned} & -0.29 \\\\ & -0.36 \\end{aligned}$ | $\\begin{aligned} & +0.35 \\\\ & +0.35 \\end{aligned}$ | $\\begin{aligned} & -0.29 \\\\ & -0.22 \\end{aligned}$ |\n|  | Baselines | $\\begin{aligned} & a_{C} \\\\ & a_{M} \\end{aligned}$ | $\\begin{aligned} & +1.17 \\\\ & -0.00 \\end{aligned}$ | $\\begin{aligned} & +1.17 \\\\ & +0.00 \\end{aligned}$ | $\\begin{aligned} & +0.29 \\\\ & +0.23 \\end{aligned}$ | $\\begin{aligned} & +1.17 \\\\ & -0.00 \\end{aligned}$ | $\\begin{aligned} & +0.40 \\\\ & -0.22 \\end{aligned}$ | $\\begin{aligned} & +1.17 \\\\ & -0.00 \\end{aligned}$ | $\\begin{aligned} & +0.40 \\\\ & -0.22 \\end{aligned}$ |\n| (a) IN |  |  |  |  |  |  |  |  |  |\n| Scenario |  | Score | spa-cat | spa-por | nld-afr | spactat-parl | deu:news-parl | spactat-EMEA | deu:news-EMEA |\n| $S_{0}$ | Ours | $\\begin{aligned} & a_{D_{n}} \\\\ & a_{F B} \\end{aligned}$ | $\\begin{aligned} & +2.35 \\\\ & +10.68 \\end{aligned}$ | $\\begin{aligned} & +2.35 \\\\ & +22.34 \\end{aligned}$ | $\\begin{aligned} & +1.57 \\\\ & +10.07 \\end{aligned}$ | $\\begin{aligned} & +2.75 \\\\ & +3.00 \\end{aligned}$ | $\\begin{aligned} & +5.28 \\\\ & +3.75 \\end{aligned}$ | $\\begin{aligned} & +4.74 \\\\ & +2.91 \\end{aligned}$ | $\\begin{aligned} & +2.44 \\\\ & +2.85 \\end{aligned}$ |\n|  | Baselines | $\\begin{aligned} & a_{E} \\\\ & a_{L} \\end{aligned}$ | $\\begin{aligned} & -1.29 \\\\ & -0.89 \\end{aligned}$ | $\\begin{aligned} & -2.41 \\\\ & -0.70 \\end{aligned}$ | $\\begin{aligned} & +3.85 \\\\ & +12.12 \\end{aligned}$ | $\\begin{aligned} & +1.94 \\\\ & +7.06 \\end{aligned}$ | $\\begin{aligned} & +3.61 \\\\ & +5.82 \\end{aligned}$ | $\\begin{aligned} & +1.14 \\\\ & +1.94 \\end{aligned}$ | $\\begin{aligned} & +2.31 \\\\ & +2.24 \\end{aligned}$ |\n|  |  | $\\begin{aligned} & a_{M S P} \\\\ & a_{C Q E} \\end{aligned}$ | $\\begin{aligned} & +1.63 \\\\ & +1.88 \\end{aligned}$ | $\\begin{aligned} & +3.57 \\\\ & +1.50 \\end{aligned}$ | $\\begin{aligned} & -0.01 \\\\ & -1.23 \\end{aligned}$ | $\\begin{aligned} & -2.36 \\\\ & -0.01 \\end{aligned}$ | $\\begin{aligned} & -1.90 \\\\ & +1.66 \\end{aligned}$ | $\\begin{aligned} & -1.87 \\\\ & +0.29 \\end{aligned}$ | $\\begin{aligned} & +1.52 \\\\ & +2.21 \\end{aligned}$ |\n| $S_{1}$ | Ours | $\\begin{aligned} & a_{D_{2}} \\\\ & a_{F B^{*}} \\end{aligned}$ | $\\begin{aligned} & +4.43 \\\\ & +4.42 \\end{aligned}$ | $\\begin{aligned} & +2.63 \\\\ & +7.95 \\end{aligned}$ | $\\begin{aligned} & +6.95 \\\\ & +6.82 \\end{aligned}$ | $\\begin{aligned} & +0.98 \\\\ & +0.98 \\end{aligned}$ | $\\begin{aligned} & -0.98 \\\\ & -0.97 \\end{aligned}$ | $\\begin{aligned} & -14.92 \\\\ & -14.92 \\end{aligned}$ | $\\begin{aligned} & -8.54 \\\\ & -8.56 \\end{aligned}$ |\n|  | Baselines | $\\begin{aligned} & a_{C} \\\\ & a_{M} \\end{aligned}$ | $\\begin{aligned} & +1.11 \\\\ & +1.93 \\end{aligned}$ | $\\begin{aligned} & +1.32 \\\\ & +3.97 \\end{aligned}$ | $\\begin{aligned} & -0.12 \\\\ & +1.86 \\end{aligned}$ | $\\begin{aligned} & +0.14 \\\\ & +2.84 \\end{aligned}$ | $\\begin{aligned} & +0.75 \\\\ & -1.15 \\end{aligned}$ | $\\begin{aligned} & +0.17 \\\\ & +0.36 \\end{aligned}$ | $\\begin{aligned} & +0.02 \\\\ & +2.55 \\end{aligned}$ |\n| (b) OOD |  |  |  |  |  |  |  |  |  |\n| Scenario |  | Score | spa-cat | spa-por | nld-afr | spactat-parl | deu:news-parl | spactat-EMEA | deu:news-EMEA |\n| $S_{0}$ | Ours | $\\begin{aligned} & a_{D_{n}} \\\\ & a_{F B} \\end{aligned}$ | $\\begin{aligned} & +11.24 \\\\ & +10.55 \\end{aligned}$ | $\\begin{aligned} & +11.24 \\\\ & +16.40 \\end{aligned}$ | $\\begin{aligned} & +10.46 \\\\ & +8.20 \\end{aligned}$ | $\\begin{aligned} & +8.80 \\\\ & +5.89 \\end{aligned}$ | $\\begin{aligned} & +4.14 \\\\ & +3.13 \\end{aligned}$ | $\\begin{aligned} & +4.37 \\\\ & +3.56 \\end{aligned}$ | $\\begin{aligned} & +2.20 \\\\ & +2.99 \\end{aligned}$ |\n|  | Baselines | $\\begin{aligned} & a_{E} \\\\ & a_{L} \\end{aligned}$ | $\\begin{aligned} & +4.61 \\\\ & +8.50 \\end{aligned}$ | $\\begin{aligned} & +3.80 \\\\ & +10.04 \\end{aligned}$ | $\\begin{aligned} & +6.80 \\\\ & +10.36 \\end{aligned}$ | $\\begin{aligned} & +2.85 \\\\ & +7.51 \\end{aligned}$ | $\\begin{aligned} & +2.98 \\\\ & +1.47 \\end{aligned}$ | $\\begin{aligned} & +2.52 \\\\ & +3.80 \\end{aligned}$ | $\\begin{aligned} & +2.66 \\\\ & +3.10 \\end{aligned}$ |\n|  |  | $\\begin{aligned} & a_{M S P} \\\\ & a_{C Q E} \\end{aligned}$ | $\\begin{aligned} & +1.61 \\\\ & +1.44 \\end{aligned}$ | $\\begin{aligned} & +2.12 \\\\ & +0.78 \\end{aligned}$ | $\\begin{aligned} & +2.84 \\\\ & +2.33 \\end{aligned}$ | $\\begin{aligned} & -2.16 \\\\ & -0.90 \\end{aligned}$ | $\\begin{aligned} & -1.72 \\\\ & +1.67 \\end{aligned}$ | $\\begin{aligned} & -2.68 \\\\ & -0.55 \\end{aligned}$ | $\\begin{aligned} & -0.74 \\\\ & +1.26 \\end{aligned}$ |\n| $S_{1}$ | Ours | $\\begin{aligned} & a_{D_{2}} \\\\ & a_{F B^{*}} \\end{aligned}$ | $\\begin{aligned} & +4.07 \\\\ & +4.08 \\end{aligned}$ | $\\begin{aligned} & +8.49 \\\\ & +8.50 \\end{aligned}$ | $\\begin{aligned} & +4.03 \\\\ & +3.98 \\end{aligned}$ | $\\begin{aligned} & +7.53 \\\\ & +7.52 \\end{aligned}$ | $\\begin{aligned} & -0.85 \\\\ & -0.85 \\end{aligned}$ | $\\begin{aligned} & -0.02 \\\\ & -0.03 \\end{aligned}$ | $\\begin{aligned} & -6.20 \\\\ & -6.23 \\end{aligned}$ |\n|  | Baselines | $\\begin{aligned} & a_{C} \\\\ & a_{M} \\end{aligned}$ | $\\begin{aligned} & +3.02 \\\\ & +3.30 \\end{aligned}$ | $\\begin{aligned} & +3.02 \\\\ & +9.10 \\end{aligned}$ | $\\begin{aligned} & +0.65 \\\\ & +3.29 \\end{aligned}$ | $\\begin{aligned} & -0.19 \\\\ & -2.99 \\end{aligned}$ | $\\begin{aligned} & +0.56 \\\\ & -0.20 \\end{aligned}$ | $\\begin{aligned} & +0.80 \\\\ & -0.28 \\end{aligned}$ | $\\begin{aligned} & +0.72 \\\\ & -5.26 \\end{aligned}$ |\n\n(c) ALL\n\nTable 20: Detailed impact of the OOD filtering on the different subset for each task."
    },
    {
      "markdown": "|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| Scenario |  | Dataset | ALL | IN | OOD | ALL | IN | OOD | ALL | IN | OOD | ALL | IN | OOD | ALL | IN | OOD | ALL | IN | OOD | ALL | IN | OOD |\n|  |  | Score |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n|  | Ours | $\\begin{gathered} n_{1} \\\\ n_{2} \\end{gathered}$ | 22% | 18% | 21% | 34% | 26% | 26% | 30% | 20% | 20% | 31% | 20% | 20% | 29% | 29% | 28% | 19% | 18% | 20% | 14% | 20% | 7% |\n|  |  |  | 258\\% | 20% | 24% | 44% | 20% | 20% | 26% | 17% | 24% | 24% | 17% | 20% | 28% | 20% | 37% | 16% | 17% | 12% | 15% | 19% | 10% |\n|  |  | $\\begin{gathered} n_{3} \\\\ n_{4} \\end{gathered}$ | 24% | 20% | 27% | 25% | 20% | 28% | 28% | 20% | 20% | 20% | 20% | 30% | 30% | 20% | 40% | 15% | 20% | 4% | 14% | 20% | 9% |\n|  | Baseline | $\\begin{gathered} n_{1} \\\\ n_{2} \\end{gathered}$ | 27% | 19% | 26% | 35% | 19% | 20% | 30% | 20% | 27% | 27% | 19% | 25% | 30% | 19% | 40% | 14% | 19% | 4% | 12% | 19% | 6% |\n|  |  |  | 22% | 18% | 18% | 27% | 18% | 19% | 17% | 19% | 20% | 16% | 18% | 12% | 14% | 20% | 8% | 41% | 18% | 20% | 20% | 20% |  |\n|  | Ours | $\\begin{gathered} n_{10} \\\\ n_{20} \\end{gathered}$ | 21% | 20% | 27% | 29% | 20% | 26% | 22% | 20% | 21% | 15% | 20% | 26% | 28% | 20% | 37% | 14% | 20% | 14% | 20% | 14% | 20% |\n|  |  |  | 26% | 19% | 24% | 34% | 19% | 26% | 20% | 20% | 25% | 19% | 20% | 22% | 12% | 19% | 6% | 45% | 18% | 20% | 20% | 19% |  |\n|  |  | $\\begin{gathered} n_{2} \\\\ n_{3} \\end{gathered}$ | 20% | 19% | 27% | 35% | 19% | 25% | 25% | 19% | 20% | 20% | 19% | 22% | 12% | 18% | 6% | 45% | 18% | 20% | 20% | 18% | 18% |\n|  | Baseline | $\\begin{gathered} n_{1} \\\\ n_{2} \\end{gathered}$ | 21% | 17% | 21% | 24% | 17% | 18% | 17% | 16% | 22% | 12% | 17% | 22% | 10% | 11% | 10% | 12% | 12% | 12% | 12% | 11% | 11% |\n|  |  |  | 22% | 20% | 24% | 28% | 20% | 24% | 27% | 20% | 25% | 25% | 20% | 27% | 17% | 20% | 14% | 20% | 20% | 20% | 20% | 20% | 20% |\n\nTable 21: Share of the datasets removed when taking $\\gamma$ so that we keep $80 \\%$ of the IN distribution.\n![img-15.jpeg](img-15.jpeg)\n\nFigure 13: Gain in translation performances when filtering OOD samples with our method on different datasets and language pairs.\n\nTable 22: Correlation between OOD scores and translation metrics BLEU and BERT-S on domain shifts datasets.\n![img-16.jpeg](img-16.jpeg)\nof different OOD samples that might occur. Therefore we choose to fix the False Positive Rate of our detector by constraining the amount of known IN distribution samples that are classified as OOD.\n\n## H Negative results\n\n## H. 1 Different aggregation of OOD metrics\n\nMost of our detectors are initially classification OOD detectors that we adapted for text generation by averaging them over the generated sequences and using this aggregated score as a score for the whole sequence. We experimented with other aggregations such as the standard deviation or the min/max along the sequence. If the standard deviation gave relatively good results they were still less interesting that the naive average.\n\n## H. 2 Negentropy of bag of distributions\n\nWe introduced in Sec. 3.3 the bag of distributions as a way to aggregate a sequence of probability distribution and compare it to a set of reference using information projections Sec. 3.3. A natural idea would be to apply the Negentropy methods (Sec. 3.2) to these aggregated distributions.\n\nMore formally given a sequence of probability distribution $\\mathcal{S}_{\\theta}(\\mathbf{x})=\\left\\{p_{\\theta}^{T}\\left(\\mathbf{x}, \\hat{\\mathbf{y}}_{\\leqslant t}\\right)\\right\\}_{t=1}^{n}$ we would compute its bag of distributions:\n\n$$\n\\hat{p}_{\\theta}(x) \\triangleq \\frac{1}{|y|} \\sum_{t=1}^{|y|} p_{\\theta}\\left(x, y_{\\leqslant t}\\right)\n$$\n\nAnd then compute as novelty score:\n\n$$\nJ_{D}(\\mathbf{p})=D(\\mathbf{p} \\| \\mathcal{U})\n$$\n\nFurther experiments have shown that this process was unable to discriminate OOD samples or improve performance translation. We suspect that the uncertainty at each step is key to capture the behavior of the language model and that this uncertainty information is lost when averaging probability distribution along the sequence."
    },
    {
      "markdown": "Table 23: Detailed impacts on NMT performance results per tasks (Domain- or Language-shifts) of the different OOD detectors with a threshold defined to keep $99 \\%$ of the IN data. We present results on the different part of the data: IN data, OOD data and the combination of both, ALL. For each we report the absolute average BLEU score (Abs.), the average gains in BLEU (G.s.) compared to a setting without OOD filtering ( $f_{\\theta}$ only) and the share of the subset removed by the detector (R.Sh.).\n\n|  |  |  |  |  |  | Domain shifts |  |  |  |  |  |  | Language shifts |  |  |  |  |  |  |  |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n|  |  |  |  | IN |  |  | OOD |  |  | ALL |  | IN |  | OOD |  |  | ALL |  |  |  |\n|  |  |  | Abs. |  | R.Sh | Abs. | G.s | R.Sh | Abs. | G.s | R.Sh | Abs. | G.s | R.Sh | Abs. | G.s | R.Sh | Abs. | G.s | R.Sh |\n|  |  | $\\bar{x}$ | 47.1 | $+0.0$ | $0.0 \\%$ | 43.4 | $+0.0$ | $0.0 \\%$ | 45.3 | $+0.0$ | $0.0 \\%$ | 60.5 | $+0.0$ | $0.0 \\%$ | 12.1 | $+0.0$ | $0.0 \\%$ | 43.9 | $+0.0 \\%$ | $0.0 \\%$ |\n|  | Ours | $\\alpha_{D_{1}}$ | 47.3 | $+0.2$ | $1.0 \\%$ | 44.2 | $+0.8$ | $5.5 \\%$ | 45.8 | $+0.5$ | $3.2 \\%$ | 60.7 | $+0.2$ | $1.0 \\%$ | 22.8 | $+3.7$ | 34.9\\% | 49.2 | $+5.4$ | $14.6 \\%$ |\n|  |  | $\\alpha_{D_{2}}$ | 47.3 | $+0.2$ | $1.0 \\%$ | 44.1 | $+0.7$ | $7.2 \\%$ | 45.7 | $+0.4$ | $4.1 \\%$ | 60.7 | $+0.2$ | $1.0 \\%$ | 22.3 | $+4.2$ | 37.0\\% | 49.7 | $+5.9$ | $15.8 \\%$ |\n| $s_{0}$ |  | $\\alpha_{E}$ | 47.3 | $+0.2$ | $1.0 \\%$ | 44.0 | $+0.5$ | $1.9 \\%$ | 45.6 | $+0.4$ | $1.4 \\%$ | 60.9 | $+0.4$ | $1.0 \\%$ | 12.2 | $+0.6$ | 17.6\\% | 46.0 | $+2.1$ | $7.3 \\%$ |\n|  | Res. | $\\alpha_{L}$ | 47.3 | $+0.2$ | $0.9 \\%$ | 44.0 | $+0.6$ | $1.9 \\%$ | 45.6 | $+0.4$ | $1.4 \\%$ | 60.8 | $+0.4$ | $0.9 \\%$ | 12.1 | $+0.6$ | 18.4\\% | 46.2 | $+2.3$ | $7.6 \\%$ |\n|  |  | $\\alpha_{M N P}$ | 47.0 | $-0.1$ | $1.0 \\%$ | 40.5 | $+0.8$ | $14.7 \\%$ | 43.5 | $+1.2$ | $7.8 \\%$ | 60.4 | $-0.1$ | $1.0 \\%$ | 11.5 | $+0.3$ | 4.3\\% | 44.3 | $+0.2$ | $2.5 \\%$ |\n|  | Ours | $\\alpha_{D_{2}}$ | 47.0 | $-0.1$ | $0.9 \\%$ | 40.5 | $+1.1$ | $26.5 \\%$ | 43.9 | $+1.4$ | $13.7 \\%$ | 60.5 | $-0.1$ | $0.9 \\%$ | 12.3 | $+4.1$ | 10.5\\% | 45.3 | $+1.4$ | $4.8 \\%$ |\n| $s_{1}$ |  | $\\alpha_{D_{2}}$ | 47.0 | $-0.1$ | $0.9 \\%$ | 40.5 | $+1.1$ | $26.6 \\%$ | 43.9 | $+1.3$ | $13.8 \\%$ | 60.5 | $-0.1$ | $0.9 \\%$ | 12.3 | $+4.1$ | 10.5\\% | 45.3 | $+1.4$ | $4.8 \\%$ |\n|  | Res. | $\\alpha_{E}$ | 47.0 | $-0.1$ | $1.0 \\%$ | 41.6 | $+1.5$ | $18.1 \\%$ | 44.6 | $-0.7$ | $9.6 \\%$ | 60.5 | $-0.1$ | $1.0 \\%$ | 11.4 | $+0.3$ | 12.1\\% | 45.3 | $+1.4$ | $5.9 \\%$ |"
    }
  ],
  "usage_info": {
    "pages_processed": 27,
    "doc_size_bytes": 1340190
  },
  "_metadata": {
    "model_version": "0.0.0",
    "llm_model": "mistral-ocr-2505",
    "query_data": {
      "pdf": "/Users/satyaortiz-gagne/CODE/paperext/data/cache/openreview/_4F4CDK9Mo.pdf"
    },
    "model_id": "parsepdf"
  }
}