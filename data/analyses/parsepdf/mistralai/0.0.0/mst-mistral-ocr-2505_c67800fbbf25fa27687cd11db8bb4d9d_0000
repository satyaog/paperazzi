{
  "pages": [
    {
      "markdown": "# Metrics Reloaded: Recommendations for image analysis validation \n\nLENA MAIER-HEIN ${ }^{-1}$, German Cancer Research Center (DKFZ) Heidelberg, Division of Intelligent Medical Systems and HI Helmholtz Imaging, Germany, Faculty of Mathematics and Computer Science and Medical Faculty, Heidelberg University, Heidelberg, Germany, and National Center for Tumor Diseases (NCT), NCT Heidelberg, a partnership between DKFZ and University Medical Center Heidelberg, Germany\nANNIKA REINKE ${ }^{-1}$, German Cancer Research Center (DKFZ) Heidelberg, Division of Intelligent Medical Systems and HI Helmholtz Imaging, Germany and Faculty of Mathematics and Computer Science, Heidelberg University, Heidelberg, Germany\nPATRICK GODAU, German Cancer Research Center (DKFZ) Heidelberg, Division of Intelligent Medical Systems, Germany, Faculty of Mathematics and Computer Science, Heidelberg University, Heidelberg, Germany, and National Center for Tumor Diseases (NCT), NCT Heidelberg, a partnership between DKFZ and University Medical Center Heidelberg, Germany\nMINU D. TIZABI, German Cancer Research Center (DKFZ) Heidelberg, Division of Intelligent Medical Systems, Germany and National Center for Tumor Diseases (NCT), NCT Heidelberg, a partnership between DKFZ and University Medical Center Heidelberg, Germany\nFLORIAN BUETTNER, German Cancer Consortium (DKTK), partner site Frankfurt/Mainz, a partnership between DKFZ and UCT Frankfurt-Marburg, Germany, German Cancer Research Center (DKFZ) Heidelberg, Germany, Goethe University Frankfurt, Department of Medicine, Germany, Goethe University Frankfurt, Department of Informatics, Germany, and Frankfurt Cancer Insititute, Germany\nEVANGELIA CHRISTODOULOU, German Cancer Research Center (DKFZ) Heidelberg, Division of Intelligent Medical Systems, Germany\nBEN GLOCKER, Department of Computing, Imperial College London, London, UK\nFABIAN ISENSEE, German Cancer Research Center (DKFZ) Heidelberg, Division of Medical Image Computing and HI Applied Computer Vision Lab, Germany\nJENS KLEESIEK, Institute for AI in Medicine, University Medicine Essen, Essen, Germany\nMICHAL KOZUBEK, Centre for Biomedical Image Analysis and Faculty of Informatics, Masaryk University, Brno, Czech Republic\nMAURICIO REYES, ARTORG Center for Biomedical Engineering Research, University of Bern, Bern, Switzerland and Department of Radiation Oncology, University Hospital Bern, University of Bern, Bern, Switzerland\nMICHAEL A. RIEGLER, Simula Metropolitan Center for Digital Engineering, Oslo, Norway and UiT The Arctic University of Norway, romsø, Norway\nMANUEL WIESENFARTH, German Cancer Research Center (DKFZ) Heidelberg, Division of Biostatistics, Germany\nA. EMRE KAVUR, German Cancer Research Center (DKFZ) Heidelberg, Division of Intelligent Medical Systems, Division of Medical Image Computing, HI Applied Computer Vision Lab, Germany\nCAROLE H. SUDRE, MRC Unit for Lifelong Health and Ageing at UCL and Centre for Medical Image Computing, Department of Computer Science, University College London, London, UK and School of Biomedical Engineering and Imaging Science, King's College London, London, UK\nMICHAEL BAUMGARTNER, German Cancer Research Center (DKFZ) Heidelberg, Division of Medical Image Computing, Germany\nMATTHIAS EISENMANN, German Cancer Research Center (DKFZ) Heidelberg, Division of Intelligent Medical Systems, Germany\nDOREEN HECKMANN-NÖTZEL, German Cancer Research Center (DKFZ) Heidelberg, Division of Intelligent Medical Systems, Germany and National Center for Tumor Diseases (NCT), NCT Heidelberg, a partnership between DKFZ and University Medical Center Heidelberg, Germany\nTIM RÄDSCH, German Cancer Research Center (DKFZ) Heidelberg, Division of Intelligent Medical Systems and HI Helmholtz Imaging, Germany\nLAURA ACION, Instituto de Cálculo, CONICET - Universidad de Buenos Aires, Buenos Aires, Argentina\nMICHELA ANTONELLI, School of Biomedical Engineering and Imaging Science, King's College London, London, UK and Centre for Medical Image Computing, University College London, London, UK"
    },
    {
      "markdown": "TAL ARBEL, Centre for Intelligent Machines and MILA (Québec Artificial Intelligence Institute), McGill University, Montréal, Canada\nSPYRIDON BAKAS, Division of Computational Pathology, Dept of Pathology \\& Laboratory Medicine, Indiana University School of Medicine, IU Health Information and Translational Sciences Building, Indianapolis, USA and Center for Biomedical Image Computing and Analytics (CBICA), University of Pennsylvania, Richards Medical Research Laboratories FL7, Philadelphia, PA, USA\nARRIEL BENIS, Department of Digital Medical Technologies, Holon Institute of Technology, Holon, Israel and European Federation for Medical Informatics, Le Mont-sur-Lausanne, Switzerland\nMATTHEW B. BLASCHKO, Center for Processing Speech and Images, Department of Electrical Engineering, KU Leuven, Kasteelpark Arenberg 10 - box 2441, 3001 Leuven, Belgium\nM. JORGE CARDOSO, School of Biomedical Engineering and Imaging Science, King's College London, London, UK VERONIKA CHEPLYGINA, Department of Computer Science, IT University of Copenhagen, Copenhagen, Denmark\nBETH A. CIMINI, Imaging Platform, Broad Institute of MIT and Harvard, Cambridge, MA, USA\nGARY S. COLLINS, Centre for Statistics in Medicine, University of Oxford, Oxford, UK\nKEYVAN FARAHANI, Center for Biomedical Informatics and Information Technology, National Cancer Institute, Bethesda, MD, USA\nLUCIANA FERRER, Instituto de Investigación en Ciencias de la Computación (ICC), CONICET-UBA, Ciudad Universitaria, Ciudad Autónoma de Buenos Aires, Argentina\nADRIAN GALDRAN, Universitat Pompeu Fabra, Barcelona, Spain and University of Adelaide, Adelaide, Australia\nBRAM VAN GINNEKEN, Fraunhofer MEVIS, Bremen, Germany and Radboud Institute for Health Sciences, Radboud University Medical Center, Nijmegen, The Netherlands\nROBERT HAASE, Now with: Center for Scalable Data Analytics and Artificial Intelligence (ScaDS.AI), Leipzig University, Leipzig, Germany, Technische Universität (TU) Dresden, DFG Cluster of Excellence \"Physics of Life\", Dresden, Germany, and Center for Systems Biology, Dresden, Germany\nDANIEL A. HASHIMOTO, Department of Surgery, Perelman School of Medicine, Philadelphia, PA, USA and General Robotics Automation Sensing and Perception Laboratory, School of Engineering and Applied Science, University of Pennsylvania, Philadelphia, PA, USA\nMICHAEL M. HOFFMAN, Princess Margaret Cancer Centre, University Health Network, Toronto, Canada, Department of Medical Biophysics, University of Toronto, Toronto, Canada, Department of Computer Science, University of Toronto, Toronto, Canada, and Vector Institute for Artificial Intelligence, Toronto, Canada\nMEREL HUISMAN, Department of Radiology and Nuclear Medicine, Radboud University Medical Center, Nijmegen, The Netherlands\nPIERRE JANNIN, Laboratoire Traitement du Signal et de l’Image - UMR_S 1099, Université de Rennes 1, Rennes, France and INSERM, Paris Cedex, France\nCHARLES E. KAHN, Department of Radiology and Institute for Biomedical Informatics, University of Pennsylvania, Philadelphia, PA, USA\nDAGMAR KAINMUELLER, Max-Delbrück Center for Molecular Medicine in the Helmholtz Association (MDC), Biomedical Image Analysis and HI Helmholtz Imaging, Berlin, Germany and University of Potsdam, Digital Engineering Faculty, Potsdam, Germany\nBERNHARD KAINZ, Department of Computing, Faculty of Engineering, Imperial College London, London, UK and Department AIBE, Friedrich-Alexander-Universität (FAU), Erlangen-Nürnberg, Germany\nALEXANDROS KARARGYRIS, IHU Strasbourg, Strasbourg, France\nALAN KARTHIKESALINGAM, Google Health DeepMind, London, UK\nHANNES KENNGOTT, Department of General, Visceral and Transplantation Surgery, Heidelberg University Hospital, Heidelberg, Germany\nFLORIAN KOFLER, Helmholtz AI, München, Germany\nANNETTE KOPP-SCHNEIDER, German Cancer Research Center (DKFZ) Heidelberg, Division of Biostatistics, Germany\nANNA KRESHUK, Cell Biology and Biophysics Unit, European Molecular Biology Laboratory (EMBL), Heidelberg, Germany\nTAHSIN KURC, Department of Biomedical Informatics, Stony Brook University, Stony Brook, NY, USA\nBENNETT A. LANDMAN, Electrical Engineering, Vanderbilt University, Nashville, TN, USA\nGEERT LITJENS, Department of Pathology, Radboud University Medical Center, Nijmegen, The Netherlands"
    },
    {
      "markdown": "AMIN MADANI, Department of Surgery, University Health Network, Philadelphia, PA, Canada\nKLAUS MAIER-HEIN, German Cancer Research Center (DKFZ) Heidelberg, Division of Medical Image Computing and HI Helmholtz Imaging,, Germany and Pattern Analysis and Learning Group, Department of Radiation Oncology, Heidelberg University Hospital, Heidelberg, Germany\nANNE L. MARTEL, Physical Sciences, Sunnybrook Research Institute, Toronto, Canada and Department of Medical Biophysics, University of Toronto, Toronto, Canada\nPETER MATTSON, Google, Mountain View, CA 94043, USA\nERIK MEIJERING, School of Computer Science and Engineering, University of New South Wales, Sydney, Kensington, Australia\nBJOERN MENZE, Department of Quantitative Biomedicine, University of Zurich, Zurich, Switzerland\nKAREL G.M. MOONS, Julius Center for Health Sciences and Primary Care, UMC Utrecht, Utrecht University, Utrecht, The Netherlands\nHENNING MÜLLER, Information Systems Institute, University of Applied Sciences Western Switzerland (HES-SO), Sierre, Switzerland and Medical Faculty, University of Geneva, Geneva, Switzerland\nBRENNAN NICHYPORUK, MILA (Québec Artificial Intelligence Institute), Montréal, Canada\nFELIX NICKEL, Department of General, Visceral and Thoracic Surgery, University Medical Center Hamburg-Eppendorf, Hamburg, Germany\nJENS PETERSEN, German Cancer Research Center (DKFZ) Heidelberg, Division of Medical Image Computing, Germany NASIR RAJPOOT, Tissue Image Analytics Laboratory, Department of Computer Science, University of Warwick, Coventry, UK\nNICOLA RIEKE, NVIDIA GmbH, München, Germany\nJULIO SAEZ-RODRIGUEZ, Institute for Computational Biomedicine, Heidelberg University, Heidelberg, Germany and Faculty of Medicine, Heidelberg University Hospital, Heidelberg, Germany\nCLARA I. SÁNCHEZ, Informatics Institute, Faculty of Science, University of Amsterdam, Amsterdam, The Netherlands SHRAVYA SHETTY, Google Health, Google, Palo Alto, CA, USA\nMAARTEN VAN SMEDEN, Julius Center for Health Sciences and Primary Care, University Medical Center Utrecht, Utrecht, The Netherlands\nRONALD M. SUMMERS, National Institutes of Health Clinical Center, Bethesda, MD, USA\nABDEL A. TAHA, Institute of Information Systems Engineering, TU Wien, Vienna, Austria\nALEKSEI TIULPIN, Research Unit of Health Sciences and Technology, Faculty of Medicine, University of Oulu, Oulu, Finland and Neurocenter Oulu, Oulu University Hospital, Oulu, Finland\nSOTIRIOS A. TSAFTARIS, School of Engineering, The University of Edinburgh, Edinburgh, Scotland\nBEN VAN CALSTER, Department of Development and Regeneration and EPI-centre, KU Leuven, Leuven, Belgium and Department of Biomedical Data Sciences, Leiden University Medical Center, Leiden, The Netherlands\nGAËL VAROQUAUX, Parietal project team, INRIA Saclay-Île de France, Palaiseau, France\nPAUL F. JÄGER*, German Cancer Research Center (DKFZ) Heidelberg, Interactive Machine Learning Group and HI Helmholtz Imaging, Germany\n\n[^0]\n[^0]:    *Corresponding authors: Lena Maier-Hein, l.maier-hein@dkfz-heidelberg.de; Annika Reinke: a.reinke@dkfzheidelberg.de; Paul F. Jäger: p.jaeger@dkfz-heidelberg.de.\n    ${ }^{3}$ Shared first authors: Lena Maier-Hein and Annika Reinke"
    },
    {
      "markdown": "#### Abstract\n\nIncreasing evidence shows that flaws in machine learning (ML) algorithm validation are an underestimated global problem. Particularly in automatic biomedical image analysis, chosen performance metrics often do not reflect the domain interest, thus failing to adequately measure scientific progress and hindering translation of ML techniques into practice. To overcome this, we created Metrics Reloaded, a comprehensive framework guiding researchers in the problem-aware selection of metrics. It was developed by a large international consortium in a multi-stage Delphi process and is based on the novel concept of a problem fingerprint - a structured representation of the given problem that captures all aspects that are relevant for metric selection, from the domain interest to the properties of the target structure(s), data set and algorithm output. Based on the problem fingerprint, users are guided through the process of choosing and applying appropriate validation metrics while being made aware of potential pitfalls. Metrics Reloaded targets image analysis problems that can be interpreted as a classification task at image, object or pixel level, namely image-level classification, object detection, semantic segmentation, and instance segmentation tasks. To improve the user experience, we implemented the framework in the Metrics Reloaded online tool. Following the convergence of ML methodology across application domains, Metrics Reloaded fosters the convergence of validation methodology. Its applicability is demonstrated for various biomedical use cases.\n\n\nKeywords: Challenges, Competitions, Validation, Model Validation, Evaluation, Metrics, Metric Selection, Good Scientific Practice, Image Processing, Image Analysis, Computer Vision, Classification, Segmentation, Instance Segmentation, Semantic Segmentation, Detection, Localization, Medical Imaging, Biological Imaging"
    },
    {
      "markdown": "# MAIN \n\nAutomatic image processing with machine learning (ML) is gaining increasing traction in biological and medical imaging research and practice. Research has predominantly focused on the development of new image processing algorithms. The critical issue of reliable and objective performance assessment of these algorithms, however, remains largely unexplored. While the suitability of new medical treatments is typically directly assessed via well-interpretable and clinically meaningful measures, such as survival and complication rates, algorithm performance in image processing is commonly assessed with validation metrics ${ }^{1}$ that should serve as proxies for the domain interest. In consequence, the impact of validation metrics cannot be overstated; first, they are the basis for deciding on the practical (e.g. clinical) suitability of a method and are thus a key component for translation into biomedical practice. In fact, validation that is not conducted according to relevant metrics could be one major reason for why many artificial intelligence (AI) developments in medical imaging fail to reach clinical practice [75, 139]. In other words, the numbers presented in journals and conference proceedings do not reflect how successful a system will be when applied in practice. Second, metrics guide the scientific progress in the field; flawed metric use can lead to entirely futile resource investment and infeasible research directions while obscuring true scientific advancements.\n\nDespite the importance of metrics, an increasing body of work shows that the metrics used in common practice often do not adequately reflect the underlying biomedical problems, diminishing the validity of the investigated methods [38, 54, 63, 78, 80, 97, 101, 152, 155]. This especially holds true for challenges, internationally respected competitions that over the last few years have become the de facto standard for comparative performance assessment of image processing methods. These challenges are often published in prestigious journals [29, 132, 153] and receive tremendous attention from both the scientific community and industry. Among a number of shortcomings in design and quality control that were recently unveiled by a multi-center initiative [97], the choice of inappropriate metrics stood out as a core problem. Compared to other areas of AI research, choosing the right metric is particularly challenging in image processing because the suitability of a metric depends on various factors. As a foundation for the present work, we identified three core categories related to pitfalls in metric selection (see Fig. 1a):\n\nInappropriate choice of the problem category: The chosen metrics do not always reflect the biomedical need. For example, object detection problems are often framed as segmentation tasks, resulting in the use of metrics that do not account for the potentially critical localization of all objects in the scene [26, 69] (Fig. 1a, top left).\nPoor metric selection: Certain characteristics of a given biomedical problem render particular metrics inadequate. Mathematical metric properties are often neglected, for example, when using the Dice Similarity Coefficient (DSC) in the presence of particularly small structures (Fig. 1a, top right).\nPoor metric application: Even if a metric is well-suited for a given problem in principle, pitfalls can occur when applying that metric to a specific data set. For example, a common flaw pertains to ignoring hierarchical data structure, as in data from multiple hospitals or a variable number of images per patient (Fig. 1a, bottom), when aggregating metric values.\n\n[^0]\n[^0]:    ${ }^{1}$ Not to be confused with distance metrics in the pure mathematical sense."
    },
    {
      "markdown": "(a) VARIOUS PITFALLS RELATED TO CHOICE OF VALIDATION METRIC\n![img-0.jpeg](img-0.jpeg)\n\nFig. 1. Contributions of the Metrics Reloaded framework. a) Motivation: Common problems related to metrics typically arise from (top left) inappropriate choice of the problem category (here: object detection confused with semantic segmentation), (top right) poor metric selection (here: neglecting the small size of structures) and (bottom) poor metric application (here: inappropriate aggregation scheme). Pitfalls are highlighted by lightning bolts, $\\varnothing$ refers to the average DSC values. Green metric values correspond to a good metric value, whereas red values correspond to a poor value. Green check marks indicate desirable behavior of metrics, red crosses indicate undesirable behavior. b) Metrics Reloaded addresses these pitfalls. (1) To enable the selection of metrics that match the domain interest, the framework is based on the new concept of problem fingerprinting, i.e., the generation of a structured representation of the given biomedical problem that captures all properties that are relevant for metric selection. Based on the problem fingerprint, Metrics Reloaded guides the user through the process of metric selection and application while raising awareness of relevant pitfalls. (2) An instantiation of the framework for common biomedical use cases demonstrates its broad applicability. (3) A publicly available online tool facilitates application of the framework."
    },
    {
      "markdown": "These problems are magnified by the fact that common practice often grows historically, and poor standards may be propagated between generations of scientists and in prominent publications. To dismantle such historically grown poor practices and leverage distributed knowledge from various subfields of image processing, we established the multidisciplinary Metrics Reloaded ${ }^{2}$ consortium. This consortium comprises international experts from the fields of medical image analysis, biological image analysis, medical guideline development, general ML, different medical disciplines, statistics and epidemiology, representing a large number of biomedical imaging initiatives and societies.\n\nThe mission of Metrics Reloaded is to foster reliable algorithm validation through problem-aware, standardized choice of metrics with the long-term goal of (1) enabling the reliable tracking of scientific progress and (2) aiding to bridge the current chasm between ML research and translation into biomedical imaging practice.\n\nBased on a kickoff workshop held in December 2020, the Metrics Reloaded framework (Fig. 1b and Fig. 2) was developed using a multi-stage Delphi process [21] for consensus building. Its primary purpose is to enable users to make educated decisions on which metrics to choose for a driving biomedical problem. The foundation of the metric selection process is the new concept of problem fingerprinting (Fig. 3). Abstracting from a specific domain, problem fingerprinting is the generation of a structured representation of the given biomedical problem that captures all properties relevant for metric selection. As depicted in Fig. 3, the properties captured by the fingerprint comprise domain interest-related properties, such as the particular importance of structure boundary, volume or center, target structure-related properties, such as the shape complexity or the size of structures relative to the image grid size, data set-related properties, such as class imbalance, as well as algorithm output-related properties, such as the theoretical possibility of the algorithm output not containing any target structure.\n\nBased on the problem fingerprint, the user is then, in a transparent and understandable manner, guided through the process of selecting an appropriate set of metrics while being made aware of potential pitfalls related to the specific characteristics of the underlying biomedical problem. The Metrics Reloaded framework currently supports problems in which categorical target variables are to be predicted based on a given $n$-dimensional input image (possibly enhanced with context information) at pixel, object or image level, as illustrated in Fig. 4. It thus supports problems that can be assigned to one of the following four problem categories: image-level classification (image level), object detection (object level), semantic segmentation (pixel level), or instance segmentation (pixel level). Designed to be imaging modality-independent, Metrics Reloaded can be suited for application in various image analysis domains even beyond the field of biomedicine.\n\nHere, we present the key contributions of our work in detail, namely (1) the Metrics Reloaded framework for problem-aware metric selection along with the key findings and design decisions that guided its development (Fig. 2), (2) the application of the framework to common biomedical use cases, showcasing its broad applicability (selection shown in Fig. 5) and (3) the open online tool that has been implemented to improve the user experience with our framework.\n\n[^0]\n[^0]:    ${ }^{2}$ We thank the Intelligent Medical Systems (IMSY) lab members Nina Sautter, Patricia Vieten and Tim Adler for the suggestion of the name, inspired by the Matrix movies."
    },
    {
      "markdown": "![img-1.jpeg](img-1.jpeg)\n\nFig. 2. Metrics Reloaded recommendation framework from a user perspective. In step 1 - problem fingerprinting, the given biomedical image analysis problem is mapped to the appropriate image problem category, namely image-level classification (ImLC), semantic segmentation (SemS), object detection (ObD), or instance segmentation (InS) (Fig. 4). The problem category and further characteristics of the given biomedical problem relevant for metric selection are then captured in a problem fingerprint (Fig. 3). In step 2 - metric selection, the user follows the respective coloured path of the chosen problem category (ImLC $\\rightarrow$, SemS $\\rightarrow, \\mathrm{ObD} \\rightarrow$, or $\\mathrm{InS} \\rightarrow$ ) to select a suitable pool of metrics from the Metrics Reloaded pools shown in green. When a tree branches, the fingerprint items determine which exact path to take. Finally, in step 3 - metric application, the user is supported in applying the metrics to a given data set. During the traversal of the decision tree, the user goes through subprocesses, indicated by the $\\mathrm{m}$-symbol, which are provided in Extended Data Figs. 1 - 9 and represent relevant steps in the metric selection process. Ambiguities related to metric selection are resolved via decision guides (Suppl. Note 2.7) that help users make an educated decision when multiple options are possible. A comprehensive textual description of the recommendations for all four problem categories as well as for the selection of corresponding calibration metrics (if any) is provided in Suppl. Note 2.2 - Suppl. Note 2.6. An overview of the symbols used in the process diagram is provided in Fig. 5.1. Condensed versions of the mappings for every category can be found in Suppl. Note 2.2 for image-level classification, Suppl. Note 2.3 for semantic segmentation, Suppl. Note 2.4 for object detection, and Suppl. Note 2.5 for instance segmentation."
    },
    {
      "markdown": "![img-2.jpeg](img-2.jpeg)\n\nFig. 3. Relevant properties of a driving biomedical image analysis problem are captured by the problem fingerprint (selection for semantic segmentation shown here). The fingerprint comprises a set of items, each of which represents a specific property of the problem, is either binary or categorical, and must be instantiated by the user. Besides the problem category, the fingerprint comprises domain interest-related, target structure-related, data set-related and algorithm output-related properties. A comprehensive version of the fingerprints for all problem categories can be found in Figs. SN 1.7-SN 1.9 (image-level classification), Figs. SN 1.10 /SN 1.11 (semantic segmentation), Figs. SN 1.12-SN 1.14 (object detection) and Figs. SN 1.15SN 1.17 (instance segmentation). Used abbreviations: Prediction (Pred), Reference (Ref)."
    },
    {
      "markdown": "# RESULTS \n\n## Metrics Reloaded framework\n\nMetrics Reloaded is the result of a multi-stage Delphi process, comprising five international workshops, nine surveys, numerous expert group meetings, and crowdsourced feedback processes, all conducted between 2020 and 2022. As a foundation of the recommendation framework, we identified common and rare pitfalls related to metrics in the field of biomedical image analysis using a community-powered process, detailed in this work's sister publication [127]. We found that common practice is often not well-justified, and poor practices may even be propagated from one generation of scientists to the next. Importantly, many pitfalls generalize not only across the four problem categories that our framework addresses but also across domains (Fig. 4). This is because the source of the pitfall, such as class imbalance, uncertainties in the reference, or poor image resolution, can occur irrespective of a specific modality or application.\n\nFollowing the convergence of AI methodology across domains and problem categories, we therefore argue for the analogous convergence of validation methodology.\n![img-3.jpeg](img-3.jpeg)\n\nFig. 4. Metrics Reloaded fosters the convergence of validation methodology across modalities, application domains and classification scales. The framework considers problems in which categorical target variables are to be predicted at image, object and/or pixel level, resulting (from top to bottom) in imagelevel classification, object detection, instance segmentation or semantic segmentation problems. These problem categories are relevant across modalities (here computed tomography (CT), microscopy and endoscopy) and application domains. From left to right: annotation of (left) benign and malignant lesions in CT images [5], (middle) different cell types in microscopy images [96], and (right) medical instruments in laparoscopy images [98]."
    },
    {
      "markdown": "# Cross-domain approach enables integration of distributed knowledge \n\nTo break historically grown poor practices, we followed a multidisciplinary cross-domain approach that enabled us to critically question common practice in different communities and integrate distributed knowledge in one common framework. To this end, we formed an international multidisciplinary consortium of 73 experts from various biomedical image analysis-related fields. Furthermore, we crowdsourced metric pitfalls and feedback on our approach in a social media campaign. Ultimately, a total of 156 researchers contributed to this work, including 84 mentioned in the acknowledgements. Consideration of the different knowledge and perspectives on metrics led to the following key design decisions for Metrics Reloaded:\n\nEncapsulating domain knowledge: The questions asked to select a suitable metric are mostly similar regardless of image modality or application: Are the classes balanced? Is there a specific preference for the positive or negative class? What is the accuracy of the reference annotation? Is the structure boundary or volume of relevance for the target application? Importantly, while answering these questions requires domain expertise, the consequences in terms of metric selection can largely be regarded as domain-independent. Our approach is thus to abstract from the specific image modality and domain of a given problem by capturing the properties relevant for metric selection in a problem fingerprint (Fig. 3).\nExploiting synergies across classification scales: Similar considerations apply with regard to metric choice for classification, detection and segmentation tasks, as they can all be regarded as classification tasks at different scales (Fig. 4). The similarities between the categories, however, can also lead to problems when the wrong category is chosen (see Fig. 1a, top left). Therefore, we (1) address all four problem categories in one common framework (Fig. 2) and (2) cover the selection of the problem category itself in our framework (Fig. 1).\n\nSetting new standards: As the development and implementation of recommendations that go beyond the state of the art often requires critical mass, we involved stakeholders of various communities and societies in our consortium. Notably, out crowdsourcing-based approach led to a pool of metric candidates (Tab. 2.1) that not only includes commonly applied metrics, but also metrics that have to date received little attention in biomedical image analysis.\nAbstracting from inference methodology: Metrics should be chosen based solely on the driving biomedical problem and not be affected by algorithm design choices. For example, the error functions applied in common neural network architectures do not justify the use of corresponding metrics (e.g. validating with DSC to match the Dice loss used for training a neural network). Instead, the domain interest should guide the choice of metric, which, in turn, can guide the choice of the loss term.\nExploiting complementary metric strengths: A single metric typically cannot cover the complex requirements of the driving biomedical problem [126]. To account for the complementary strengths and weakness of metrics, we generally recommend the usage of multiple complementary metrics to validate image analysis problems. As detailed in our recommendations (Suppl. Note 2), we specifically recommend the selection of metrics from different families.\nValidation by consensus building and community feedback: A major challenge for research on metrics is its validation, due to the lack of methods capable of quantitatively assessing the superiority of a given metric set over another. Following the spirit of large consortia formed to develop reporting guidelines (e.g., CONSORT [137], TRIPOD [109], STARD [19]), we built the validation of our framework on three main pillars: (1) Delphi"
    },
    {
      "markdown": "processes to challenge and refine the proposals of the expert groups that worked on individual components of the framework, (2) community feedback obtained by broadcasting the framework via society mailing lists and social media platforms and (3) and instantiation of the framework to a range of different biological and medical use cases.\nInvolving and educating users: Choosing adequate validation metrics is a complex process. Rather than providing a black box recommendation, Metrics Reloaded guides the user through the process of metric selection while raising awareness on pitfalls that may occur. In cases in which the tradeoffs between different choices must be considered, decision guides (Suppl. Note 2.7) assist in deciding between competing metrics while respecting individual preferences.\n\n# Problem fingerprints encapsulate relevant domain knowledge \n\nTo encapsulate relevant domain knowledge in a common format and then enable a modality-agnostic metric recommendation approach that generalizes over domains, we developed the concept of problem fingerprinting, illustrated in Fig. 3. As a foundation, we crowdsourced all properties of a driving biomedical problem that are potentially relevant for metric selection via surveys issued to the consortium (see Suppl. Methods). This process resulted in a list of binary and categorical variables (fingerprint items) that must be instantiated by a user to trigger the Metrics Reloaded recommendation process. Common issues often relate to selecting metrics from the wrong problem category, as illustrated in Fig. 1a (top left). To avoid such issues, problem fingerprinting begins with mapping a given problem with all its intrinsic and data set-related properties to the corresponding problem category via the category mapping shown in Fig. 1. The problem category is a fingerprint item itself.\n\nIn the following, we will refer to all fingerprint items with the notation FP.X.Y, where $Y$ is a numerical identifier and the index $X$ represents one of the following families:\n\nFP1 - Problem category refers to the problem category generated by S1 (Extended Data Fig. 1).\nFP2 - Domain interest-related properties reflect user preferences and are highly dependent on the target application. A semantic image segmentation that serves as the foundation for radiotherapy planning, for example, would require exact contours (FP2.1 Particular importance of structure boundaries $=$ TRUE). On the other hand, for a cell segmentation problem that serves as prerequisite for cell tracking, the object centers may be much more important (FP2.3 = TRUE). Both problems could be tackled with identical network architectures, but the validation metrics should be different.\nFP3 - Target structure-related properties represent inherent properties of target structure(s) (if any), such as the size, size variability and the shape. Here, the term target structures can refer to any object/structure of interest, such as cells, vessels, medical instruments or tumors.\nFP4 - Data set-related properties capture properties inherent to the provided data to which the metric is applied. They primarily relate to class prevalences, uncertainties of the reference annotations, and whether the data structure is hierarchical.\nFP5 - Algorithm output-related properties encode properties of the output, such as the availability of predicted class scores.\n\nNote that not all properties are relevant for all problem categories. For example, the shape and size of target structures is highly relevant for segmentation problems but irrelevant for image classification problems. The complete problem category-specific fingerprints are provided in Suppl. Note 1.3."
    },
    {
      "markdown": "# Metrics Reloaded addresses all three types of metric pitfalls \n\nMetrics Reloaded was designed to address all three types of metric pitfalls identified in [127] and illustrated in Fig. 1a. More specifically, each of the three steps shown in Fig. 2 addresses one type of pitfall:\n\nStep 1 - Fingerprinting. A user should begin by reading the general instructions of the recommendation framework, provided in Suppl. Note 1.1. Next, the user should convert the driving biomedical problem to a problem fingerprint. This step is not only a prerequisite for applying the framework across application domains and classification scales, but also specifically addresses the inappropriate choice of the problem category via the integrated category mapping. Once the user's domain knowledge has been encapsulated in the problem fingerprint, the actual metric selection is conducted according to a domain- and modality-agnostic process.\n\nStep 2 - Metric Selection. A Delphi process yielded the Metrics Reloaded pool of reference-based validation metrics shown in Tab. 2.1. Notable, this pool contains metrics that are currently not widely known in some biomedical image analysis communities. A prominent example is the Net Benefit (NB) [163] metric, popular in clinical prediction tasks and designed to determine whether basing decisions on a method would do more good than harm. A diagnostic test, for example, may lead to early identification and treatment of a disease, but typically will also cause a number of patients without disease being subjected to unnecessary further interventions. NB allows to consider such tradeoffs by putting benefits and harms on the same scale so that they can be directly compared. Another example is the Expected Cost (EC) metric [90], which can be seen as a generalization of Accuracy with many desirable added features, but is not well-known in the biomedical image analysis communities [51]. Based on the Metrics Reloaded pool, the metric recommendation is performed with a Business Process Model and Notation (BPMN)-inspired flowchart (see Fig. 5.1), in which conditional operations are based on one or multiple fingerprint properties (Fig. 2). The main flowchart has three substeps, each addressing the complementary strengths and weaknesses of common metrics. First, common reference-based metrics, which are based on the comparison of the algorithm output to a reference annotation, are selected. Next, the pool of standard metrics can be complemented with custom metrics to address applicationspecific complementary properties. Finally, non-reference-based metrics assessing speed, memory consumption or carbon footprint, for example, can be added to the metric pool(s). In this paper, we focus on the step of selecting reference-based metrics, because this is where synergies across modalities and scales can be exploited.\n\nThese synergies are showcased by the substantial overlap between the different paths that, depending on the problem category, are taken through the mapping during metric selection. All paths comprise several subprocesses $S$ (indicated by the $\\boxplus$-symbol), each of which holds a subsidiary decision tree representing one specific step of the selection process. Traversal of a subprocess typically leads to the addition of a metric to the problem-specific metric pool. In multi-class prediction problems, dedicated metric pools for each class may need to be generated as relevant properties may differ from class to class. A three-dimensional semantic segmentation problem, for example, could require the simultaneous segmentation of both tubular and non-tubular structures (e.g., liver vessels and tissue). These require different metrics for validation. Although this is a corner case, our framework addresses this issue in principle. In ambiguous cases, i.e., when the user can choose between two options in one step of the decision tree, a corresponding decision guide details the tradeoffs that need to be considered (Suppl. Note 2.7). For example, the Intersection over"
    },
    {
      "markdown": "Union (IoU) and the DSC are mathematically closely related. The concrete choice typically boils down to a simple user or community preference.\n\nFig. 2 along with the corresponding Subprocesses S1-S9 (Extended Data Figs. 1 - 9) captures the core contribution of this paper, namely the consensus recommendation of the Metrics Reloaded consortium according to the final Delphi process. For all ten components, the required Delphi consensus threshold ( $>75 \\%$ agreement) was met. In all cases of disagreement, which ranged from $0 \\%$ to $7 \\%$ for Fig. 2 and S1-S9, each remaining point of criticism was respectively only raised by a single person. The following paragraphs present a summary of the four different colored paths through Step 2 - Metric Selection of the recommendation tree (Fig. 2) for the task of selecting reference-based metrics from the Metrics Reloaded pool of common metrics. More comprehensive textual descriptions can be found in SUPPL. NOTE 2.\n\nImage-level Classification (ImLC). Image-level classification is conceptually the most straightforward problem category, as the task is simply to assign one of multiple possible labels to an entire image (see Suppl. Note 2.2). The validation metrics are designed to measure two key properties: discrimination and calibration.\n\nDiscrimination refers to the ability of a classifier to discriminate between two or more classes. This can be achieved with counting metrics that operate on the cardinalities of a fixed confusion matrix (i.e., the true/false positives/negatives in the binary classification case). Prominent examples are Sensitivity, Specificity or $\\mathrm{F}_{1}$ Score for binary settings and Matthews Correlation Coefficient (MCC) for multi-class settings. Converting predicted class scores to a fixed confusion matrix (in the binary case by setting a potentially arbitrary cutoff) can, however, be regarded as problematic in the context of performance assessment [127]. Multi-threshold metrics, such as Area under the Receiver Operating Characteristic Curve (AUROC), are therefore based on varying the cutoff, which enables the explicit analysis of the tradeoff between competing properties such as Sensitivity and Specificity.\n\nWhile most research in biomedical image analysis focuses on the discrimination capabilities of classifiers, a complementary important property is the calibration of a model. An uncertainty-aware model should yield predicted class scores that represent the true likelihood of events [56], as detailed in Suppl. Note 2.6. Overoptimistic or underoptimistic classifiers can be especially problematic in prediction tasks where a clinical decision may be made based on the risk of the patient of developing a certain condition. Metrics Reloaded hence provides recommendations for validating the algorithm performance both in terms of discrimination and calibration. We recommend the following process for classification problems (blue path in Fig. 2; detailed description in Suppl. Note 2.2):\n\n1: Select multi-class metric (if any): Multi-class metrics have the unique advantage of capturing the performance of an algorithm for all classes in a single value. With the ability of taking into account all entries of the multi-class confusion matrix, they provide a holistic measure of performance without the need for customized class-aggregation schemes. We recommend using a multi-class metric if a decision rule applied to the predicted class scores is available (FP2.6). In certain use cases, especially in the presence of ordinal data, there is an unequal severity of class confusions (FP2.5.2), meaning that different costs should be applied to different misclassifications reflected by the confusion matrix. In such cases, we generally recommend EC as metric. Otherwise, depending on the specific scenario, Accuracy, Balanced Accuracy (BA) and MCC may be viable alternatives. The concrete choice of metric depends primarily on the prevalences (e.g. frequencies) of classes in the provided validation set and the target population (FP4.1/2), as detailed in Subprocess S2 (Extended Data Fig. 2) and the"
    },
    {
      "markdown": "corresponding textual description in Suppl. Note 2.2.\nAs class-specific analyses are not possible with multi-class metrics, which can potentially hide poor performance on individual classes, we recommend an additional validation with per-class counting metrics (optional) and multi-threshold metrics (always recommended).\n2: Select per-class counting metric (if any): If a decision rule applied to the predicted class scores is available (FP2.6), a per-class counting metric, such as the $F_{\\beta}$ Score, should be selected. Each class of interest is separately assessed, preferably in a \"one-versus-rest\" fashion. The choice depends primarily on the decision rule and the distribution of classes (FP4.2). Details can be found in Subprocess S3 for selecting per-class counting metrics (Extended Data Fig. 3).\n3: Select multi-threshold metric (if any): Counting metrics reduce the potentially complex output of a classifier (the continuous class scores) to a single value (the predicted class), such that they can work with a fixed confusion matrix. To compensate for this loss of information and obtain a more comprehensive picture of a classifier's discriminatory performance, multithreshold metrics work with a dynamic confusion matrix reflecting a range of possible thresholds applied to the predicted class scores. While we recommend the popular, wellinterpretable and prevalence-independent AUROC as the default multi-threshold metric for classification, Average Precision (AP) can be more suitable in the case of high class balance because it incorporates predicted values, as detailed in Subprocess S4 for selecting multi-threshold metrics (Extended Data Fig. 4).\n4: Select calibration metric (if any): If calibration assessment is requested (FP2.7), one or multiple calibration metrics should be added to the metric pool as detailed in Subprocess S5 for selecting calibration metrics (Extended Data Fig. 5).\n\nSemantic segmentation (SemS). In semantic segmentation, classification occurs at pixel level. However, it is not advisable to simply apply the standard classification metrics to the entire collection of pixels in a data set for two reasons. Firstly, pixels of the same image are highly correlated. Hence, to respect the hierarchical data structure, metric values should first be computed per image and then be aggregated over the set of images. Note in this context that the commonly used DSC is mathematically identical to the popular $\\mathrm{F}_{1}$ Score applied at pixel level. Secondly, in segmentation problems, the user typically has an inherent interest in structure boundaries, centers or volumes of structures (FP2.1, FP2.2, FP2.3). The family of boundary-based metrics (subset of distance-based metrics) therefore requires the extraction of structure boundaries from the binary segmentation masks as a foundation for segmentation assessment. Based on these considerations and given all the complementary strengths and weaknesses of common segmentation metrics [127], we recommend the following process for segmentation problems (yellow path in Fig. 2; detailed description in Suppl. Note 2.3):\n\n1: Select overlap-based metric (if any): In segmentation problems, counting metrics such as the DSC or IoU measure the overlap between the reference annotation and the algorithm prediction. As they can be considered the de facto standard for assessing segmentation quality and are well-interpretable, we recommend using them by default unless the target structures are consistently small, relative to the grid size (FP3.1), and the reference may be noisy (FP4.3.1). Depending on the specific properties of the problems, we recommend the DSC or IoU (default recommendation), the $\\mathrm{F}_{\\beta}$ Score (preferred when there is a preference for either False Positive (FP) or False Negative (FN)) or the centerline Dice Similarity Coefficient (clDice) (for tubular structures). Details can be found in Subprocess S6 for selecting overlap-based metrics (Extended Data Fig. 6)."
    },
    {
      "markdown": "2: Select boundary-based metric (if any): Key weaknesses of overlap-based metrics include shape unawareness and limitations when dealing with small structures or high size variability [127]. Our general recommendation is therefore to complement an overlap-based metric with a boundary-based metric. If annotation imprecisions should be compensated for (FP2.5.7), our default recommendation is the Normalized Surface Distance (NSD). Otherwise, the fundamental user preference guiding metric selection is whether errors should be penalized by existence or distance (FP2.5.6), as detailed in Subprocess S7 for selecting boundary-based metrics (Extended Data Fig. 7).\n\nObject detection ( $O b D$ ). Object detection problems differ from segmentation problems in several key features with respect to metric selection. Firstly, they involve distinguishing different instances of the same class and thus require the step of locating objects and assigning them to the corresponding reference object. Secondly, the granularity of localization is comparatively rough, which is why no boundary-based metrics are required (otherwise the problem would be phrased as an instance segmentation problem). Finally, and crucially important from a mathematical perspective, the absence of True Negatives (TNs) in object detection problems renders many popular classification metrics (e.g. Accuracy, Specificity, AUROC) invalid. In binary problems, for example, suitable counting metrics can only be based on three of the four entries of the confusion matrix. Based on these considerations and taking into account all the complementary strengths and weaknesses of existing metrics [127], we propose the following steps for object detection problems (green path in Fig. 2; detailed description in Suppl. Note 2.4):\n\n1: Select localization criterion: An essential part of the validation is to decide whether a prediction matches a reference object. To this end, (1) the location of both the reference objects and the predicted objects must be adequately represented (e.g., by masks, bounding boxes or center points), and (2) a metric for deciding on a match (e.g. Mask IoU) must be chosen. As detailed in Subprocess S8 for selecting the localization criterion (Extended Data Fig. 8), our recommendation considers both the granularity of the provided reference (FP4.4) and the required granularity of the localization (FP2.4).\n2: Select assignment strategy: As the localization does not necessarily lead to unambiguous matchings, an assignment strategy needs to be chosen to potentially resolve ambiguities that occurred during localization. As detailed in Subprocess S9 for selecting the assignment strategy (Extended Data Fig. 9), the recommended strategy depends on the availability of continuous class scores (FP5.1) as well as on whether double assignments should be punished (FP2.5.8).\n3: Select classification metric(s) (if any): Once objects have been located and assigned to reference objects, generation of a confusion matrix (without TN) is possible. The final step therefore simply comprises choosing suitable classification metrics for validation. Several subfields of biomedical image analysis have converged to choosing solely a counting metric, such as the $\\mathrm{F}_{\\beta}$ Score, as primary metric in object detection problems. We follow this recommendation when no continuous class scores are available for the detected objects (FP5.1). Otherwise, we disagree with the practice of basing performance assessment solely on a single, potentially suboptimal cutoff on the continuous class scores. Instead, we follow the recommendations for image-level classification and propose complementing a counting metric (Subprocess S3, Extended Data Fig. 3) with a multi-threshold metric (Subprocess S4, Extended Data Fig. 4) to obtain a more holistic picture of performance. As multi-threshold metric, we recommend AP or Free-Response Receiver Operating Characteristic (FROC) Score, depending on whether an easy interpretation (FROC Score) or a standardized metric (AP) is"
    },
    {
      "markdown": "preferred. The choice of per-class counting metric depends primarily on the decision rule (FP2.6).\n\nNote that the previous description implicitly assumed single-class problems, but generalization to multi-class problems is straightforward by applying the validation per-class. It is further worth mentioning that metric application is not straightforward in object detection problems as the number of objects in an image may be extremely small, or even zero, compared to the number of pixels in an image. Special considerations with respect to aggregation must therefore be made, as detailed in Suppl. Note 2.4.\n\nInstance segmentation (InS). Instance segmentation delivers the tasks of object detection and semantic segmentation at the same time. Thus, the pitfalls and recommendations for instance segmentation problems are closely related to those for segmentation and object detection [127]. This is directly reflected in our metric selection process (purple path in Fig. 2; detailed description in Suppl. Note 2.5):\n\n1: Select object detection metric(s): To overcome problems related to instance unawareness (Fig. 1a, top left), we recommend selection of a set of detection metrics to explicitly measure detection performance. To this end, we recommend almost the exact process as for object detection with two exceptions. Firstly, given the fine granularity of both the output and the reference annotation, our recommendation for the localization strategy differs, as detailed in Subprocess S8 (Extended Data Fig. 8). Secondly, as depicted in S3 (Extended Data Fig. 3), we recommend the Panoptic Quality (PQ) [77] as an alternative to the F $\\beta$ Score. This metric is especially suited for instance segmentation, as it combines the assessment of overall detection performance and segmentation quality of successfully matched (True Positive (TP)) instances in a single score.\n2: Select segmentation metric(s) (if any): In a second step, metrics to explicitly assess the segmentation quality for the TP instances may be selected. Here, we follow the exact same process as in semantic segmentation (Subprocesses S6, Extended Data Fig. 6 and S7, Extended Data Fig. 7). The primary difference is that the segmentation metrics are applied per-instance.\n\nImportantly, the development process of the Metrics Reloaded framework was designed such that the pitfalls identified in the sister publication of this work [128] are comprehensively addressed. Tab. 1 makes the recommendations and design decisions corresponding to specific pitfalls explicit.\n\nTable 1. Metrics Reloaded addresses common and rare pitfalls in metric selection, as compiled in [128]. The first column lists all pitfall sources captured by the published taxonomy that relate to either the inadequate choice of the problem category or poor metric selection. The second column summarizes how Metrics Reloaded addresses these pitfalls. The notation FPX.Y refers to a fingerprint item (Suppl. Note 1.3).\n\n| Source of Pitfall | Addressed in Metrics Reloaded by |\n| :--: | :--: |\n| Inadequate choice of the problem category |  |\n| Wrong choice of problem category. | Problem category mapping (Subprocess S1, Extended Data Fig. 4) as a prerequisite for metric selection. |\n| Disregard of the domain interest |  |\n| Importance of structure boundaries | FP2.1 - Particular importance of structure boundaries; recommendation to complement common overlap-based segmentation metrics with boundary-based metrics (Fig. 2, Suppl. Note 2.3) if the property holds. |\n| Importance of structure volume | FP2.2 - Particular importance of structure volume; recommendation to complement common overlapbased and boundary-based segmentation metrics with volume-based metrics (see Suppl. Note 2.3) if the property holds. |"
    },
    {
      "markdown": "Importance of structure center(line)\n\n| Importance of confidence awareness | FP2.3 - Particular importance of structure center(line); recommendation of the centerline Dice Similarity Coefficient (clDice) as alternative to the common Dice Similarity Coefficient (DSC) or Intersection over Union (IoU) in segmentation problems (Subprocess S6, Extended Data Fig. 6) and recommendation of center point-based localization criterion in object detection (Subprocess S8, Extended Data Fig. 8) if the property holds. |\n| :--: | :--: |\n| Importance of confidence awareness | FP2.7.1 - Calibration assessment requested; dedicated recommendations on calibration (Suppl. Note 2.6). |\n| Importance of comparability across data sets | FP4.2 - Provided class prevalences reflect the population of interest; used in the Subprocesses S2-S4 (Extended Data Figs. 2-4); general focus on prevalence dependency of metrics in the framework. |\n| Unequal severity of class confusions | FP2.5 - Penalization of errors; recommendation of the so far uncommon metric EC as classification metric (Subprocess S2, Extended Data Fig. 2); setting $\\beta$ in the $\\mathrm{F}_{\\beta}$ Score according to preference for FP (oversegmentation) and FN (undersegmentation) (see DG9.3 in Suppl. Note 2.7.2). |\n| Importance of cost-benefitanalysis | FP2.6 - Decision rule applied to predicted class scores: incorporation of a decision rule that is based on cost-benefit analysis; recommendation of the so far uncommon metrics Net Benefit (NB) (Fig. SN 3.11) and Expected Cost (EC) (Fig. SN 3.6). |\n\n# Disregard of target structure properties \n\nSmall structure sizes FP3.1 - Small size of structures relative to pixel size; recommendation to consider the problem an object detection problem (Suppl. Note 2.4); complementation of overlap-based segmentation metrics with boundary-based metrics in the case of small structures with noisy reference (Subprocess S6, Extended Data Fig. 6); recommendation of lower object detection localization threshold in case of small sizes (see DG8.3 in Suppl. Note 2.7.7).\nHigh variability of structure sizes FP3.2 - High variability of structure sizes; recommendation of lower object detection localization threshold (see DG8.3 in Suppl. Note 2.7.7) and size stratification (Suppl. Note 2.4) in case of size variability.\nComplex structure shapes FP3.3 - Target structures feature tubular shape; recommendation of the clDice as alternative to the common DSC in segmentation problems (Subprocess S6, Extended Data Fig. 6) and recommendation of Point inside Mask/Box/Approx as localization criterion in object detection if the property holds (Subprocess S8, Extended Data Fig. 8).\nOccurrence of overlapping or touching structures FP3.5 - Possibility of overlapping or touching target structures; explicit recommendation to phrase problem as instance segmentation rather than semantic segmentation problem (Suppl. Note 2.3); recommendation of higher object detection localization threshold in case of small sizes (see DG8.3 in Suppl. Note 2.7.7).\nOccurrence of disconnected structures FP3.6 - Possibility of disconnected target structure(s); recommendation of appropriate localization criterion for object detection (DG8.2 in Suppl. Note 2.7.7).\n\n## Disregard of data set properties\n\nHigh class imbalance FP4.1 - High class imbalance and FP2.5.5 - Compensation for class imbalances requested; compensation of class imbalance via prevalence-independent metrics such as EC and BA.\nSmall test set size\nResummendation of confidence intervals for all metrics.\nImperfect reference standard: $\\quad$ FP4.3.1 - High inter-rater variability and FP2.5.7 - Compensation for annotation imprecisions requested; default recommendation of the so far rather uncommon metric Normalized Surface Distance (NSD) to assess the quality of boundaries.\nNo\n\n## Imperfect reference standard: Spatial outliers in reference\n\nFP4.3.2 - Possibility of spatial outliers in reference annotation and FP2.5.6 - Handling of spatial outliers; recommendation of outlier-robust metrics, such as NSD in case no distance-based penalization of outliers is requested in segmentation problems.\nOccurrence of cases with an\nFP4.6 - Possibility of reference without target structure(s); recommendations for aggregation in the case of empty references according to Suppl. Note 2.4 and Extended Data Tab. 1.\n\n## Disregard of algorithm output properties\n\n| Possibility of empty prediction | FP5.2 - Possibility of algorithm output not containing the target structure(s); selection of appropriate <br> aggregation strategy in object detection (Suppl. Note 2.4). |\n| :-- | :-- |\n| Possibility of overlapping pre- <br> dictions | FP5.4 - Possibility of overlapping predictions; recommendation of an assignment strategy based on <br> IoU + 0.5 if overlapping predictions are not possible and no predicted class scores are available. |\n| Lack of predicted class scores | FP5.1 - Availability of predicted class scores; leveraging class scores for optimizing decision regions <br> (FP2.6) and assessing calibration quality (FP2.7). |\n\nOnce common reference-based metrics have been selected and, where necessary, complemented by application-specific metrics, the user proceeds with the application of the metrics to the given problem.\n\nStep 3 - Metric Application. Although the application of a metric to a given data set may appear straightforward, numerous pitfalls can occur [127]. Our recommendations for addressing them are provided in Extended Data Tab. 1 1. Following the taxonomy provided in the sister publication of this work [128], they are categorized in recommendations related to metric implementation,"
    },
    {
      "markdown": "aggregation, ranking, interpretation, and reporting. While several aspects are covered in related work (e.g. [168]), an important contribution of the present work is the metric-specific summary of recommendations captured in the Metric Cheat Sheets (Suppl. Note 3.1). A further major contribution is our implementation of all Metrics Reloaded metrics in the open-source framework Medical Open Network for Artificial Intelligence (MONAI), available at https://github.com/Project-MONAI/ MetricsReloaded (see Suppl. Methods).\n\n# Metrics Reloaded is broadly applicable in biomedical image analysis \n\nTo validate the Metrics Reloaded framework, we used it to generate recommendations for common use cases in biomedical image processing (see SUPPL. NOTE 4). The traversal through the decision tree of our framework is detailed for eight selected use cases corresponding to the four different problem categories (Fig. 5):\n\nImage-level classification (Figs. SN 4.5 - SN 4.8): frame-based sperm motility classification from time-lapse microscopy video of human spermatozoa (ImLC-1) and disease classification in dermoscopic images (ImLC-2).\nSemantic segmentation (Figs. SN 4.9 - SN 4.10): embryo segmentation in microscopy images (SemS-1) and liver segmentation in Computed Tomography (CT) images (SemS-2).\nObject detection (Figs. SN 4.6 - SN 4.7, SN 4.11 - SN 4.12): cell detection and tracking during the autophagy process in time-lapse microscopy (ObD-1) and multiple sclerosis (MS) lesion detection in multi-modal brain magnetic resonance imaging (MRI) images (ObD-2).\nInstance segmentation (Figs. SN 4.6 - SN 4.7, SN 4.9 - SN 4.12): instance segmentation of neurons from the fruit fly in 3D multi-color light microscopy images (InS-1) and surgical instrument instance segmentation in colonoscopy videos (InS-2).\n\nThe resulting metric recommendations (Fig. 5) demonstrate that a common framework across domains is sensible. In the showcased examples, shared properties of problems from different domains result in almost identical recommendations. In the semantic segmentation use cases, for example, the specific image modality is irrelevant for metric selection. What matters is the fact that a single object with a large size relative to the grid size should be segmented - properties that are captured by the proposed fingerprint. In SUPPL. NOTE 4, we present recommendations for several other biomedical use cases.\n\n## The Metrics Reloaded online tool allows user-friendly metric selection\n\nSelecting appropriate validation metrics while considering all potential pitfalls that may occur is a highly complex process, as demonstrated by the large number of figures in this paper. Some of the complexity, however, also results from the fact that the figures need to capture all possibilities at once. For example, many of the figures could be simplified substantially for problems based on only two classes. To leverage this potential and to improve the general user experience with our framework, we developed the Metrics Reloaded online tool, which is currently available as a beta version with restricted access (see Suppl. Methods). The tool captures our framework in a user-centric manner and can serve as a trustworthy common access point for image analysis validation."
    },
    {
      "markdown": "![img-4.jpeg](img-4.jpeg)\n\nFig. 5. Instantiation of the framework with recommendations for concrete biomedical questions. From top to bottom: (1) Image classification for the examples of sperm motility classification [62] and disease classification in dermoscopic images [33, 154]. (2) Semantic segmentation of large objects for the examples of embryo segmentation from microscopy [147] and liver segmentation in computed tomography (CT) images [2, 141]. (3) Detection of multiple and arbitrarily located objects for the examples of cell detection and tracking during the autophagy process [111, 174] and MS lesion detection in multi-modal brain MRI images [36, 79]. (4) Instance segmentation of tubular objects for the examples of instance segmentation of neurons from the fruit fly [100, 108, 151] and surgical instrument instance segmentation [98]. The corresponding traversals through the decision trees are shown inSUPPL. NOTE 4. An overview of the recommended metrics can be found in Suppl. Note 3.1, including relevant information for each metric."
    },
    {
      "markdown": "# DISCUSSION \n\nConventional scientific practice often grows through historical accretion, leading to standards that are not always well-justified. This holds particularly true for the validation standards in biomedical image analysis.\n\nThe present work represents the first comprehensive investigation and, importantly, constructive set of recommendations challenging the state of the art in biomedical image analysis algorithm validation with a specific focus on metrics. With the intention of revisiting - literally \"re-searching\" - common validation practices and developing better standards, we brought together experts from traditionally disjunct fields to leverage distributed knowledge. Our international consortium of more than 70 experts from the fields of biomedical image analysis, machine learning, statistics, epidemiology, biology, and medicine, representing a large number of relevant biomedical imaging initiatives and societies, developed the Metrics Reloaded framework that offers guidelines and tools to choose performance metrics in a problem-aware manner. The expert consortium was primarily compiled in a way to cover the required expertise from various fields but also consisted of researchers of different countries, (academic) ages, roles, and backgrounds (details can be found in the Methods). Importantly, Metrics Reloaded comprehensively addresses all pitfalls related to metric selection (Tab. 1) and application (Extended Data Tab. 1) that were identified in this work's sister publication [128].\n\nToday, the acceptance of a paper often relies on demonstrating a method's superior performance with respect to related work according to common validation metrics [17]. Using appropriate validation metrics is not only fundamental for identifying new promising solutions and catalyzing scientific progress, but also key to the successful translation of ML methods into practice. In fact, the importance of using metrics relevant to the end user has recently been prominently highlighted in the context of structured assessment of the (real-world) \"readiness\" of AI technology at various points in its development cycle [88] as well as regulatory science enabling the clinical deployment of AI [91]. By significantly easing the choice of appropriate validation metrics, we expect the Metrics Reloaded framework to increase the quality of research in the field of biomedical image analysis and catalyze faster initial translation of AI methods into (clinical) practice, as well as open up entirely new avenues of quality control and continuous performance monitoring of AI-based applications.\n\nMetrics Reloaded is the result of a 2.5-year long process involving numerous workshops, surveys, and expert group meetings. Many controversial debates were conducted during this time. Even deciding on the exact scope of the paper was anything but trivial. Our consortium eventually agreed on focusing on biomedical classification problems with categorical reference data and thus exploiting synergies across classification scales. Generating and handling fuzzy reference data (e.g., from multiple observers) is a topic of its own [95, 146] and was decided to be out of scope for this work. Furthermore, the inclusion of calibration metrics in addition to discrimination metrics was originally not intended because calibration is a complex topic in itself, and the corresponding field is relatively young and currently highly dynamic. This decision was reversed due to high demand from the community, expressed through crowdsourced feedback on the framework.\n\nExtensive discussions also evolved around the inclusion criteria for metrics, considering the tradeoff between established (potentially flawed) and new (not yet stress-tested) metrics. Our strategy for arriving at the Metrics Reloaded recommendations balanced this tradeoff by using common metrics as a starting point and making adaptations where needed. For example, Weighted Cohen's Kappa (WCK), originally designed for assessing inter-rater agreement, is the state-of-the-art metric used in the medical imaging community when handling ordinal data. Unlike other"
    },
    {
      "markdown": "common multi-class metrics, such as (Balanced) Accuracy or MCC, it allows the user to specify different costs for different class confusions, thereby addressing the ordinal rating. However, our consortium deemed the (not widely known) metric EC generally more appropriate due to its favorable mathematical properties. Importantly, our framework does not intend to impose recommendations or act as a \"black box\" ; instead, it enables users to make educated decisions while considering ambiguities and tradeoffs that may occur. This is reflected by our use of decision guides (Suppl. Note 2.7), which actively involve users in the decision-making process (for the example above, for instance, see DG2.1).\n\nAn important further challenge that our consortium faced was how to best provide recommendations in case multiple questions are asked for a single given data set. For example, a clinician's ultimate interest may lie in assessing whether tumor progress has occurred in a patient. While this would be phrased as an image-level classification task (given two images as input), an interesting surrogate task could be seen in a segmentation task assessing the quality of tumor delineation and providing explainability for the results. Metrics Reloaded addresses the general challenge of multiple different driving biomedical questions corresponding to one data set pragmatically by generating a recommendation separately for each question. The same holds true for multi-label problems, for example, when multiple different types of abnormalities potentially co-occur in the same image/patient.\n\nAnother key challenge we faced was the validation of our framework due to the lack of ground truth \"best metrics\" to be applied for a given use case. Our solution builds upon three pillars. Firstly, we adopted established consensus building approaches utilized for developing widely used guidelines such as CONSORT [137], TRIPOD [109], or STARD [19]). Secondly, we challenged our initial recommendation framework by acquiring feedback via a social media campaign. Finally, we instantiated the final framework to a range of different biological and medical use cases. Our approach showcases the benefit of crowdsourcing as a means of expanding the horizon beyond the knowledge peculiar to specific scientific communities. The most prominent change effected in response to the social media feedback was the inclusion of the aforementioned EC, a powerful metric from the speech recognition community. Furthermore, upon popular demand, we added recommendations on assessing the interpretability of model outputs, now captured by Subprocess S5 (Extended Data Fig. 5).\n\nAfter many highly controversial debates, the consortium ultimately converged on a consensus recommendation, as indicated by the high agreement in the final Delphi process (median agreement with the Subprocesses: $93 \\%$ ). While some subsprocesses (S1, S7, S8) were unanimously agreed on without a single negative vote, several issues were raised by individual researchers. While most of them were minor (e.g., concerning wording), a major debate revolved around calibration metrics. Some members, for example, questioned the value of stand-alone calibration metrics altogether. The reason for this view is the critically important misconception that the predicted class scores of a well-calibrated model express the true posterior probability of an input belonging to a certain class [122] - e.g., a patient's risk for a certain condition based on an image. As this is not the case, several researchers argued for basing calibration assessment solely on proper scoring rules (such as the Brier Score (BS)), which assess the quality of the posteriors better than the stand-alone calibration metrics. We have addressed all these considerations in our recommendation framework including a detailed rationale for our recommendations, provided in Suppl. Note 2.6.\n\nWhile we believe our framework to cover the vast majority of biomedical image analysis use cases, suggesting a comprehensive set of metrics for every possible biomedical problem may be out of its scope. The focus of our framework lies in correcting poor practices related to the selection of"
    },
    {
      "markdown": "common metrics. However, in some use cases, common reference-based metrics - as a matter of principle - be unsuitable. In fact, the use of application-specific metrics may be required in some cases. A prominent example are instance segmentation problems in which the matching of reference and predicted instances is infeasible, causing overlap-based localization criteria to fail. Metrics such as Rand Index (RI) [125] and Variation of Information (Vol) [107] address this issue by avoiding one-to-one correspondences between predicted and reference instances. To make our framework applicable to such specific use cases, we integrated the step of choosing application-specific metrics in the main workflow (Fig. 2). Examples of such application-specific metrics can be found in related work $[40,47]$.\n\nMetrics Reloaded primarily provides guidance for the selection of metrics that measure some notion of the \"correctness\" of an algorithm's predictions on a set of test cases. It should be noted that holistic algorithm performance assessment also includes other aspects. One of them is robustness. For example, the accuracy of an algorithm for detecting disease in medical scans should ideally be the same across different hospitals that may use different acquisition protocols or scanners from different manufacturers. Recent work, however, shows that even the exact same models with nearly identical test set performance in terms of predictive accuracy may behave very differently on data from different distributions [41].\n\nReliability is another important algorithmic property to be taken into account during validation. A reliable algorithm should have the ability to communicate its confidence and raise a flag when the uncertainty is high and the prediction should be discarded [136]. For calibrated models, this can be achieved via the predicted class scores, although other methods based on dedicated model outputs trained to express the confidence or on density estimation techniques are similarly popular. Importantly, an algorithm with reliable uncertainty estimates or increased robustness to distribution shift might not always be the best performing in terms of predictive performance [68]. For safe use of classification systems in practice, careful balancing of the tradeoff between robustness and reliability over accuracy might be necessary.\n\nSo far, Metrics Reloaded focuses on common reference-based methods that compare model outputs to corresponding reference annotations. We made this design choice due to our hypothesis that reference-based metrics can be chosen in a modality- and application-agnostic manner using the concept of problem fingerprinting. As indicated by the step of choosing potential non-referencebased metrics (Fig. 2), however, it should be noted that validation and evaluation of algorithms should go far beyond purely technical performance [43, 149]. In this context, Jannin introduced a global concept of \"responsible research\" to encompass all possible high-level assessment aspects of a digital technology [70], including environmental, ethical, economical, social and societal aspects. For example, there are increasing efforts specifically devoted to the estimation of energy consumption and greenhouse gas emission of machine learning algorithms [86, 120, 144]. For these considerations, we would like to point the reader to available tools such as the Green Algorithms calculator [87] or Carbontracker [169].\n\nIt must further be noted that while Metrics Reloaded places a focus on the selection of metrics, adequate application is also important. Detailed failure case analysis [131] and performance assessment on relevant subgroups, for example, have been highlighted as critical components for better understanding when and where an algorithm may fail [27, 117]. Given that learning-based algorithms rely on the availability of historical data sets for training, there is a real risk that any existing biases in the data may be picked up and replicated or even exacerbated when an algorithm makes predictions [1,52]. This is of particular concern in the context of systemic biases in healthcare, such as the scarcity of representative data from underserved populations and often higher"
    },
    {
      "markdown": "error rates in diagnostic labels in particular subgroups [65, 118]. Relevant meta information such as patient demographics, including biological sex and ethnicity, needs to be accessible for the test sets such that potentially disparate performance across subgroups can be detected [105]. Here, it is important to make use of adequate aggregations over the validation metrics as disparities in minority groups might otherwise be missed.\n\nFinally, it must be noted that our framework addresses metric choice in the context of technical validation of biomedical algorithms. For translation of an algorithm into, for example, clinical routine, this validation may be followed by a (clinical) validation step assessing its performance compared to conventional, non-algorithm-based care according to patient-related outcome measures, such as overall survival [119].\n\nA key remaining challenge for Metric Reloaded is its dissemination such that it will substantially contribute to raising the quality of biomedical imaging research. To encourage widespread adherence to new standards, entry barriers should be as low as possible. While the framework with its vast number of subprocesses may seem very complex at first, it is important to note that from a user perspective only a fraction of the framework is relevant for a given task, making the framework more tangible. This is notably illustrated by the Metric Reloaded online tool, which substantially simplifies the metric selection procedure. As is common in scientific guideline and recommendation development, we intend to regularly update our framework to reflect current developments in the field, such as the inclusion of new metrics or biomedical use cases. This is intended to include an expansion of the framework's scope to further problem categories, such as regression and reconstruction. In order to accommodate future developments in a fast and efficient manner, we envision our consortium building consensus through accelerated Delphi rounds organized by the Metric Reloaded core team. Once consensus is obtained, changes will be implemented in both the framework and online tool and highlighted so that users can easily identify changes to the previous version, which will ensure full transparency and comparability of results. In this way, we envision the Metrics Reloaded framework and online tool as a dynamic resource reliably reflecting the current state of the art at any given time point in the future, for years to come.\n\nOf note, while the provided recommendations originate from the biomedical image analysis community, many aspects generalize to imaging research as a whole. Particularly the recommendations derived for individual fingerprints (e.g., implications of class imbalance) hold across domains, although it is possible that for different domains the existing fingerprints would need to be complemented by further features that this community is not aware of.\n\nIn conclusion, the Metrics Reloaded framework provides biomedical image analysis researchers with the first systematic guidance on choosing validation metrics across different imaging tasks in a problem-aware manner. Through its reliance on methodology that can be generalized, we envision the Metrics Reloaded framework to spark a scientific debate and hopefully lead to similar efforts being undertaken in other areas of imaging research, thereby raising research quality on a much larger scale than originally anticipated. In this context, our framework and the process by which it was developed could serve as a blueprint for broader efforts aimed at providing reliable recommendations and enforcing adherence to good practices in imaging research.\n\n# CODE AVAILABILITY STATEMENT \n\nWe provide reference implementations for all Metrics Reloaded metrics within the MONAI opensource framework. They are accessible at https://github.com/Project-MONAI/MetricsReloaded."
    },
    {
      "markdown": "# ACKNOWLEDGEMENTS \n\nThis work was initiated by the Helmholtz Association of German Research Centers in the scope of the Helmholtz Imaging Incubator (HI), the MICCAI Special Interest Group on biomedical image analysis challenges and the benchmarking working group of the MONAI initiative. It received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No. [101002198], NEURAL SPICING). It was further supported in part by the Intramural Research Program of the National Institutes of Health (NIH) Clinical Center as well as by the National Cancer Institute (NCI) and the National Institute of Neurological Disorders and Stroke (NINDS) of the NIH, under award numbers NCI:U01CA242871, NCI:U24CA279629, and NINDS:R01NS042645. The content of this publication is solely the responsibility of the authors and does not represent the official views of the NIH. T.A. acknowledges the Canada Institute for Advanced Research (CIFAR) AI Chairs program, the Natural Sciences and Engineering Research Council of Canada. F.B. was co-funded by the European Union (ERC, TAIPO, 101088594). Views and opinions expressed are however those of the authors only and do not necessarily reflect those of the European Union or the European Research Council. Neither the European Union nor the granting authority can be held responsible for them. V.C. acknowledges funding from NovoNordisk Foundation (NNF21OC0068816) and Independent Research Council Denmark (1134-00017B). B.A.C. was supported by NIH grant P41 GM135019 and grant 2020-225720 from the Chan Zuckerberg Initiative DAF, an advised fund of the Silicon Valley Community Foundation. G.S.C. was supported by Cancer Research UK (programme grant: C49297/A27294). M.M.H. is supported by the Natural Sciences and Engineering Research Council of Canada (RGPIN-2022-05134). A.Kara. is supported by French State Funds managed by the \"Agence Nationale de la Recherche (ANR)\" - \"Investissements d'Avenir\" (Investments for the Future), Grant ANR-10-IAHU-02 (IHU Strasbourg). M.K. was supported by the Ministry of Education, Youth and Sports of the Czech Republic (Project LM2018129). T.K. was supported in part by 4UH3-CA22502103, 1U24CA180924-01A1, 3U24CA215109-02, and 1UG3-CA225-021-01 grants from the National Institutes of Health. G.L. receives research funding from the Dutch Research Council, the Dutch Cancer Association, HealthHolland, the European Research Council, the European Union, and the Innovative Medicine Initiative. C.H.S. is supported by an Alzheimer's Society Junior Fellowship (AS-JF-17-011). M.R is supported by Innosuisse grant number 31274.1 and Swiss National Science Foundation Grant Number 205320_212939. R.M.S. is supported by the Intramural Research Program of the NIH Clinical Center. A.T. acknowledges support from Academy of Finland (Profi6 336449 funding program), University of Oulu strategic funding, Finnish Foundation for Cardiovascular Research, Wellbeing Services County of North Ostrobothnia (VTR project K62716), and Terttu foundation. S.A.T. acknowledges the support of Canon Medical and the Royal Academy of Engineering and the Research Chairs and Senior Research Fellowships scheme (grant RCSRF1819\\8\\25). B.V.C. was supported by Research Foundation Flanders (FWO grant G097322N) and Internal Funds KU Leuven (grant C24M/20/064).\n\nWe thank Nina Sautter, Patricia Vieten and Tim Adler for proposing the name for the project.\nWe would like to thank Peter Bankhead, Fred Hamprecht, Hannes Kenngott, David Moher, and Bram Stieltjes for fruitful discussions on the framework.\n\nWe thank Susanne Steger for the data protection supervision and Anke Trotter for the hosting of the surveys."
    },
    {
      "markdown": "We would like to thank Lisa Mais for instantiating the use case for instance segmentation of neurons from the fruit fly in 3D multicolor light microscopy images. We would further like to thank the Janelia FlyLight Project Team for providing us with example images for this use case.\n\nWe would like to thank the following people for testing the metric mappings, reviewing the recommendations and performing metric-centric testing: Tim Adler, Christoph Bender, Ahmad Bin Qasim, Kris Dreher, Niklas Holzwarth, Marco Hübner, Dominik Michael, Lucas-Raphael Müller, Maike Rees, Tom Rix, Melanie Schellenberg, Silvia Seidlitz, Jan Sellner, Akriti Srivastava, Fabian Wolf, Amine El Yamlahi, Silvia D. Almeida, Michael Baumgartner, Dimitrios Bounias, Till Bungert, Maximilian Fischer, Lukas Klein, Gregor Köhler, Bálint Kovács, Carsten Lueth, Tobias Norajitra, Constantin Ulrich, Tassilo Wald, Iuliia Alekseenko, Xiao Liu, Andrea Marheim Storås, Vajira Thambawita.\n\nWe would like to thank the following people for taking our social media community survey and providing helpful feedback for improving the framework: Yamashita Akemi, Roi Anteby, Callum Arthurs, Pieter De Backer, Henry Badgery, Matthew Baugh, Jose Bernal, Matthew Blaschko, Dimitrios Bounias, Felipe Campos Kitamura, Jacob Carse, Chen Chen, Ivo Flipse, Nicolas Gaggion, Camila González, Pedro M. Gordaliza, Tim Horeman, Leo Joskowicz, Abin Jose, Amith Kamath, Brendan Kelly, Yannick Kirchhoff, Levin Arne Kobelke, Lars Krämer, Mira Krendel, John LaMaster, Thomas de Lange, Joël L. Lavanchy, Jianning Li, Carsten Lüth, Lisa Mais, Andrea Marheim Storås, Vishwesh Nath, Cian Scannell, Constantin Pape, M.P. Schijven, Alberto Selvanetti, Bella Specktor Fadida, Roger Staff, Jeremy Tan, Eric Tkaczyk, Rodrigo Tripodi Calumby, Athanasios Vlontzos, Weitong Zhang, Can Zhao, Jiayi Zhu.\n\n# COMPETING INTERESTS \n\nThe authors declare the following competing interests: Under his terms of employment, M.B.B. is entitled to stock options in Mona.health, a KU Leuven spinoff. F.B. is an employee of Siemens AG (Munich, Germany). F.B. reports funding from Merck (Darmstadt, Germany). B.v.G. is a shareholder of Thirona (Nijmegen, NL). B.G. was an employee of HeartFlow Inc (California, USA) and Kheiron Medical Technologies Ltd (London, UK). M.M.H. received an Nvidia GPU Grant. B.K. is a consultant for ThinkSono Ldt (London, UK). G.L. is on the advisory board of Canon Healthcare IT (Minnetonka, USA) and is a shareholder of Aiosyn BV (Nijmegen, NL). N.R. is an employee of Nvidia GmbH (Munich, Germany). J.S.-R. reports funding from GSK (Heidelberg, Germany), Pfizer (New York, USA) and Sanofi (Paris, France) and fees from Travere Therapeutics (California, USA), Stadapharm (Bad Vilbel, Germany), Astex Therapeutics (Cambridge, UK), Pfizer (New York, USA), and Grunenthal (Aachen, Germany). R.M.S. receives patent royalties from iCAD (New Hampshire, USA), ScanMed (Nebraska, USA), Philips (Amsterdam, NL), Translation Holdings (Alabama, USA) and PingAn (Shenzhen, China); his lab received research support from PingAn through a Cooperative Research and Development Agreement. S.A.T. receives financial support from Canon Medical Research Europe (Edinburgh, Scotland)."
    },
    {
      "markdown": "# REFERENCES \n\n[1] Adewole S Adamson and Avery Smith. Machine learning and health care disparities in dermatology, 2018.\n[2] Michela Antonelli, Annika Reinke, Spyridon Bakas, Keyvan Farahani, Annette Kopp-Schneider, Bennett A Landman, Geert Litjens, Bjoern Menze, Olaf Ronneberger, Ronald M Summers, et al. The medical segmentation decathlon. Nature Communications, 13(1):1-13, 2022.\n[3] Guilherme Aresta, Teresa Araújo, Scotty Kwok, Sai Saketh Chennamsetty, Mohammed Safwan, Varghese Alex, Bahram Marami, Marcel Prastawa, Monica Chan, Michael Donovan, et al. Bach: Grand challenge on breast cancer histology images. Medical image analysis, 56:122-139, 2019.\n[4] S. G. Armato III, G. McLennan, L. Bidaut, M. F. McNitt-Gray, C. R. Meyer, A. P. Reeves, B. Zhao, D. R. Aberle, C. I. Henschke, E. A. Hoffman, et al. Data from lidc-idri [data set]. The Cancer Imaging Archive, 2015.\n[5] Samuel G Armato III, Geoffrey McLennan, Luc Bidaut, Michael F McNitt-Gray, Charles R Meyer, Anthony P Reeves, Binsheng Zhao, Denise R Aberle, Claudia I Henschke, Eric A Hoffman, et al. The lung image database consortium (lidc) and image database resource initiative (idri): a completed reference database of lung nodules on ct scans. Medical physics, 38(2):915-931, 2011.\n[6] Saeid Asgari Taghanaki, Kumar Abhishek, Joseph Paul Cohen, Julien Cohen-Adad, and Ghassan Hamarneh. Deep semantic segmentation of natural and medical images: a review. Artificial Intelligence Review, 54(1):137-178, 2021.\n[7] John Attia. Moving beyond sensitivity and specificity: using likelihood ratios to help interpret diagnostic tests. Australian prescriber, 26(5):111-113, 2003.\n[8] Marc Aubreville, Nikolas Stathonikos, Christof A Bertram, Robert Klopleisch, Natalie ter Hoeve, Francesco Ciompi, Frauke Wilm, Christian Marzahl, Taryn A Donovan, Andreas Maier, et al. Mitosis domain generalization in histopathology images-the midog challenge. arXiv preprint arXiv:2204.03742, 2022.\n[9] MA Badgeley, JR Zech, L Oakden-Rayner, BS Glicksberg, M Liu, W Gale, MV McConnell, B Percha, TM Snyder, and JT Dudley. Deep learning predicts hip fracture using confounding patient and healthcare variables. npj digit med. 2019; 2: 31, 2019.\n[10] Andriy I Bandos, Howard E Rockette, Tao Song, and David Gur. Area under the free-response roc curve (froc) and a related summary index. Biometrics, 65(1):247-256, 2009.\n[11] Christian F Baumgartner, Konstantinos Kamnitsas, Jacqueline Matthew, Tara P Fletcher, Sandra Smith, Lisa M Koch, Bernhard Kainz, and Daniel Rueckert. Sononet: real-time detection and localisation of fetal standard scan planes in freehand ultrasound. IEEE transactions on medical imaging, 36(11):2204-2215, 2017.\n[12] Babak Ehteshami Bejnordi, Mitko Veta, Paul Johannes Van Diest, Bram Van Ginneken, Nico Karssemeijer, Geert Litjens, Jeroen AWM Van Der Laak, Meyke Hermsen, Quirine F Manson, Maschenka Balkenhol, et al. Diagnostic assessment of deep learning algorithms for detection of lymph node metastases in women with breast cancer. Jama, 318(22):2199-2210, 2017.\n[13] Miroslav Beneš and Barbara Zitová. Performance evaluation of image segmentation algorithms on microscopic image data. Journal of microscopy, 257(1):65-85, 2015.\n[14] Jorge Bernal, Aymeric Histace, Marc Masana, Quentin Angermann, Cristina Sánchez-Montes, Cristina Rodríguez de Miguel, Maroua Hammami, Ana García-Rodríguez, Henry Córdova, Olivier Romain, et al. Gtcreator: a flexible annotation tool for image-based datasets. International journal of computer assisted radiology and surgery, 14(2): 191-201, 2019.\n[15] Olivier Bernard, Alain Lalande, Clement Zotti, Frederick Cervenansky, Xin Yang, Pheng-Ann Heng, Irem Cetin, Karim Lekadir, Oscar Camara, Miguel Angel Gonzalez Ballester, et al. Deep learning techniques for automatic mri cardiac multi-structures segmentation and diagnosis: is the problem solved? IEEE transactions on medical imaging, 37(11): $2514-2525,2018$.\n[16] Sebastian Bickelhaupt, Paul Ferdinand Jaeger, Frederik Bernd Laun, Wolfgang Lederer, Heidi Daniel, Tristan Anselm Kuder, Lorenz Wuesthof, Daniel Paech, David Bonekamp, Alexander Radbruch, et al. Radiomics based on adapted diffusion kurtosis imaging helps to clarify most mammographic findings suspicious for cancer. Radiology, 287(3): $761-770,2018$.\n[17] Abeba Birhane, Pratyusha Kalluri, Dallas Card, William Agnew, Ravit Dotan, and Michelle Bao. The values encoded in machine learning research. arXiv, June 2021.\n[18] Christopher M Bishop and Nasser M Nasrabadi. Pattern recognition and machine learning, volume 4. Springer, 2006.\n[19] Patrick M Bossuyt, Johannes B Reitsma, David E Bruns, Constantine A Gatsonis, Paul P Glasziou, Les M Irwig, Jeroen G Lijmer, David Moher, Drummond Rennie, Henrica CW De Vet, et al. Towards complete and accurate reporting of studies of diagnostic accuracy: the stard initiative. Annals of internal medicine, 138(1):40-44, 2003.\n[20] Glenn W Brier et al. Verification of forecasts expressed in terms of probability. Monthly weather review, 78(1):1-3, 1950 ."
    },
    {
      "markdown": "[21] Bernice B Brown. Delphi process: a methodology used for the elicitation of opinions of experts. Technical report, Rand Corp Santa Monica CA, 1968.\n[22] Niko Brümmer and Johan Du Preez. Application-independent evaluation of speaker detection. Computer Speech \\& Language, 20(2-3):230-275, 2006.\n[23] Samuel Budd, Prachi Patkee, Ana Baburamani, Mary Rutherford, Emma C Robinson, and Bernhard Kainz. Surface agnostic metrics for cortical volume segmentation and regression. In Machine Learning in Clinical Neuroimaging and Radiogenomics in Neuro-Oncology, pages 3-12. Springer, 2020.\n[24] Juan C Caicedo, Allen Goodman, Kyle W Karhohs, Beth A Cimini, Jeanelle Ackerman, Marzieh Haghighi, CherKeng Heng, Tim Becker, Minh Doan, Claire McQuin, et al. Nucleus segmentation across imaging experiments: the 2018 data science bowl. Nature methods, 16(12):1247-1253, 2019.\n[25] Juan C Caicedo, Jonathan Roth, Allen Goodman, Tim Becker, Kyle W Karhohs, Matthieu Broisin, Csaba Molnar, Claire McQuin, Shantanu Singh, Fabian J Theis, et al. Evaluation of deep learning strategies for nucleus segmentation in fluorescence images. Cytometry Part A, 95(9):952-965, 2019.\n[26] Aaron Carass, Snehashis Roy, Adrian Gherman, Jacob C Reinhold, Andrew Jesson, Tal Arbel, Oskar Maier, Heinz Handels, Mohsen Ghafoorian, Bram Platel, et al. Evaluating white matter lesion segmentations with refined sørensendice analysis. Scientific reports, 10(1):1-19, 2020.\n[27] Danton S Char, Nigam H Shah, and David Magnus. Implementing machine learning in health care - addressing ethical challenges. N. Engl. J. Med., 378(11):981-983, March 2018.\n[28] Bowen Cheng, Ross Girshick, Piotr Dollár, Alexander C Berg, and Alexander Kirillov. Boundary iou: Improving object-centric image segmentation evaluation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15334-15342, 2021.\n[29] Nicolas Chenouard, Ihor Smal, Fabrice De Chaumont, Martin Maška, Ivo F Sbalzarini, Yuanhao Gong, Janick Cardinale, Craig Carthel, Stefano Coraluppi, Mark Winter, et al. Objective comparison of particle tracking methods. Nature methods, 11(3):281-289, 2014.\n[30] Nancy Chinchor. Muc-4 evaluation metrics. In Proceedings of the 4th Conference on Message Understanding, MUC4 '92, page 22-29, USA, 1992. Association for Computational Linguistics. ISBN 1558602739. doi: 10.3115/1072064.1072067. URL https://doi.org/10.3115/1072064.1072067.\n[31] Neil T Clancy, Geoffrey Jones, Lena Maier-Hein, Daniel S Elson, and Danail Stoyanov. Surgical spectral imaging. Medical image analysis, 63:101699, 2020.\n[32] Kenneth Clark, Bruce Vendt, Kirk Smith, John Freymann, Justin Kirby, Paul Koppel, Stephen Moore, Stanley Phillips, David Maffitt, Michael Pringle, et al. The cancer imaging archive (tcia): maintaining and operating a public information repository. Journal of digital imaging, 26(6):1045-1057, 2013.\n[33] Noel Codella, Veronica Rotemberg, Philipp Tschandl, M Emre Celebi, Stephen Dusza, David Gutman, Brian Helba, Aadi Kalloo, Konstantinos Liopyris, Michael Marchetti, et al. Skin lesion analysis toward melanoma detection 2018: A challenge hosted by the international skin imaging collaboration (isic). arXiv preprint arXiv:1902.03368, 2019.\n[34] Jacob Cohen. A coefficient of agreement for nominal scales. Educational and psychological measurement, 20(1):37-46, 1960 .\n[35] Gary S Collins, Paula Dhiman, Constanza L Andaur Navarro, Jie Ma, Lotty Hooft, Johannes B Reitsma, Patricia Logullo, Andrew L Beam, Lily Peng, Ben Van Calster, et al. Protocol for development of a reporting guideline (tripod-ai) and risk of bias tool (probast-ai) for diagnostic and prognostic prediction model studies based on artificial intelligence. BMJ open, 11(7):e048008, 2021.\n[36] Olivier Commowick, Audrey Istace, Michael Kain, Baptiste Laurent, Florent Leray, Mathieu Simon, Sorina Camarasu Pop, Pascal Girard, Roxana Ameli, Jean-Christophe Ferré, et al. Objective evaluation of multiple sclerosis lesion segmentation using a data management and processing infrastructure. Scientific reports, 8(1):1-17, 2018.\n[37] CONSORT-AI and SPIRIT-AI Steering Group. Reporting guidelines for clinical trials evaluating artificial intelligence interventions are needed. Nat. Med., 25(10):1467-1468, October 2019.\n[38] Paulo Correia and Fernando Pereira. Video object relevance metrics for overall segmentation quality evaluation. EURASIP Journal on Advances in Signal Processing, 2006:1-11, 2006.\n[39] George Cybenko, Dianne P O'Leary, and Jorma Rissanen. The Mathematics of Information Coding, Extraction and Distribution, volume 107. Springer Science \\& Business Media, 1998.\n[40] Marc-Alexandre Côté, Gabriel Girard, Arnaud Boré, Eleftherios Garyfallidis, Jean-Christophe Houde, and Maxime Descoteaux. Tractometer: towards validation of tractography pipelines. Medical Image Analysis, 17(7):844-857, October 2013. ISSN 1361-8423. doi: 10.1016/j.media.2013.03.009.\n[41] A D'Amour, K Heller, D Moldovan, B Adlam, and others. Underspecification presents challenges for credibility in modern machine learning. arXiv preprint arXiv, 2020.\n[42] Jesse Davis and Mark Goadrich. The relationship between precision-recall and roc curves. In Proceedings of the 23rd international conference on Machine learning, pages 233-240, 2006."
    },
    {
      "markdown": "[43] Université de Montréal. The Declaration - Montreal Responsible AI, 2017. URL https://www.montrealdeclarationresponsibleai.com/the-declaration.\n[44] Morris H DeGroot and Stephen E Fienberg. The comparison and evaluation of forecasters. Journal of the Royal Statistical Society: Series D (The Statistician), 32(1-2):12-22, 1983.\n[45] Lee R Dice. Measures of the amount of ecologic association between species. Ecology, 26(3):297-302, 1945.\n[46] James M Dolezal, Andrew Srisuwananukorn, Dmitry Karpeyev, Siddhi Ramesh, Sara Kochanny, Brittany Cody, Aaron S Mansfield, Sagar Rakshit, Radhika Bansal, Melanie C Bois, et al. Uncertainty-informed deep learning models enable high-confidence predictions for digital histopathology. Nature communications, 13(1):6572, 2022.\n[47] David G Ellis, Carlos M Alvarez, and Michele R Aizenberg. Qualitative criteria for feasible cranial implant designs. In Cranial Implant Design Challenge, pages 8-18. Springer, 2021.\n[48] Mark Everingham, Andrew Zisserman, Christopher KI Williams, Luc Van Gool, Moray Allan, Christopher M Bishop, Olivier Chapelle, Navneet Dalal, Thomas Deselaers, Gyuri Dorkó, et al. The 2005 pascal visual object classes challenge. In Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment: First PASCAL Machine Learning Challenges Workshop, MLCW 2005, Southampton, UK, April 11-13, 2005, Revised Selected Papers, pages 117-176. Springer, 2006.\n[49] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. International journal of computer vision, 88(2):303-338, 2010.\n[50] Mark Everingham, SM Ali Eslami, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes challenge: A retrospective. International journal of computer vision, 111(1):98-136, 2015.\n[51] Luciana Ferrer. Analysis and comparison of classification metrics. arXiv preprint arXiv:2209.05355, 2022. The document discusses common performance metrics used in machine learning classification, and introduces the expected cost (EC) metric. It compares these metrics and argues that EC is superior due to its generality, simplicity, and intuitive nature. Additionally, it highlights the potential of EC in measuring calibration and optimal decision-making using class posteriors.\n[52] Robert Geirhos, Jörn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix A Wichmann. Shortcut learning in deep neural networks. Nature Machine Intelligence, 2(11):665-673, November 2020.\n[53] Tilmann Gneiting and Adrian E Raftery. Strictly proper scoring rules, prediction, and estimation. Journal of the American statistical Association, 102(477):359-378, 2007.\n[54] Mark J Gooding, Annamarie J Smith, Maira Tariq, Paul Aljabar, Devis Peressutti, Judith van der Stoep, Bart Reymen, Daisy Emans, Djoya Hattu, Judith van Loon, et al. Comparative evaluation of autocontouring in clinical practice: a practical method using the turing test. Medical physics, 45(11):5105-5115, 2018.\n[55] Margherita Grandini, Enrico Bagli, and Giorgio Visani. Metrics for multi-class classification: an overview. arXiv preprint arXiv:2008.05756, 2020.\n[56] Sebastian Gregor Gruber and Florian Buettner. Better uncertainty calibration via proper scores for classification and beyond. In Advances in Neural Information Processing Systems, 2022.\n[57] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On Calibration of Modern Neural Networks. ICML, page 10, 2017.\n[58] Kartik Gupta, Amir Rahimi, Thalaiyasingam Ajanthan, Thomas Mensink, Cristian Sminchisescu, and Richard Hartley. Calibration of neural networks using splines. arXiv preprint arXiv:2006.12800, 2020.\n[59] Metin N Gurcan, Anant Madabhushi, and Nasir Rajpoot. Pattern recognition in histopathological images: An icpr 2010 contest. In International Conference on Pattern Recognition, pages 226-234. Springer, 2010.\n[60] James A Hanley and Barbara J McNeil. The meaning and use of the area under a receiver operating characteristic (roc) curve. Radiology, 143(1):29-36, 1982.\n[61] Trevor Hastie, Robert Tibshirani, Jerome H Friedman, and Jerome H Friedman. The elements of statistical learning: data mining, inference, and prediction, volume 2. Springer, 2009.\n[62] Trine B Haugen, Steven A Hicks, Jorunn M Andersen, Oliwia Witczak, Hugo L Hammer, Rune Borgli, Pål Halvorsen, and Michael Riegler. Visem: A multimodal video dataset of human spermatozoa. In Proceedings of the 10th ACM Multimedia Systems Conference, pages 261-266, 2019.\n[63] Katrin Honauer, Lena Maier-Hein, and Daniel Kondermann. The hci stereo metrics: Geometry-aware performance analysis of stereo algorithms. In Proceedings of the IEEE International Conference on Computer Vision, pages 2120-2128, 2015.\n[64] Daniel P Huttenlocher, Gregory A. Klanderman, and William J Rucklidge. Comparing images using the hausdorff distance. IEEE Transactions on pattern analysis and machine intelligence, 15(9):850-863, 1993.\n[65] Hussein Ibrahim, Xiaoxuan Liu, Nevine Zariffa, Andrew D Morris, and Alastair K Denniston. Health data poverty: an assailable barrier to equitable digital health care. Lancet Digit Health, 3(4):e260-e265, April 2021."
    },
    {
      "markdown": "[66] BSEN ISO 9000. Quality management systems: Fundamentals and vocabulary. London: British Standards Institution, 2000.\n[67] Paul Jaccard. The distribution of the flora in the alpine zone. 1. New phytologist, 11(2):37-50, 1912.\n[68] Paul F Jaeger, Carsten T Lüth, Lukas Klein, and Till J Bungert. A call to reflect on evaluation practices for failure detection in image classification. International Conference on Learning Representations, 2023.\n[69] Paul Ferdinand Jäger. Challenges and opportunities of end-to-end learning in medical image classification. Karlsruher Institut für Technologie, 2020.\n[70] Pierre Jannin. Towards responsible research in digital technology for health care. arXiv, September 2021.\n[71] Pierre Jannin, Christophe Grova, and Calvin R Maurer. Model for defining and reporting reference-based validation protocols in medical image processing. International Journal of Computer Assisted Radiology and Surgery, 1(2):63-73, 2006.\n[72] Liang Jin, Jiancheng Yang, Kaiming Kuang, Bingbing Ni, Yiyi Gao, Yingli Sun, Pan Gao, Weiling Ma, Mingyu Tan, Hui Kang, et al. Deep-learning-assisted detection and segmentation of rib fractures from ct scans: Development and validation of fracnet. EBioMedicine, 62:103106, 2020.\n[73] Thierry Judge, Olivier Bernard, Mihaela Porumb, Agisilaos Chartsias, Arian Beqiri, and Pierre-Marc Jodoin. -reliable uncertainty estimation for medical image segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 492-502. Springer, 2022.\n[74] Feng Kang, Rong Jin, and Rahul Sukthankar. Correlated label propagation with application to multi-label learning. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06), volume 2, pages 1719-1726. IEEE, 2006.\n[75] Christopher J Kelly, Alan Karthikesalingam, Mustafa Suleyman, Greg Corrado, and Dominic King. Key challenges for delivering clinical impact with artificial intelligence. BMC medicine, 17:1-9, 2019.\n[76] Daanish Ali Khan, Linhong Li, Ninghao Sha, Zhuoran Liu, Abelino Jimenez, Bhiksha Raj, and Rita Singh. Nondeterminism in neural networks for adversarial robustness. arXiv preprint arXiv:1905.10906, 2019.\n[77] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, and Piotr Dollár. Panoptic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9404-9413, 2019.\n[78] Florian Kofler, Ivan Ezhov, Fabian Isensee, Christoph Berger, Maximilian Korner, Johannes Paetzold, Hongwei Li, Suprosanna Shit, Richard McKinley, Spyridon Bakas, et al. Are we using appropriate segmentation metrics? Identifying correlates of human expert perception for CNN training beyond rolling the DICE coefficient. arXiv preprint arXiv:2103.06205v1, 2021.\n[79] Florian Kofler, Suprosanna Shit, Ivan Ezhov, Lucas Fidon, Rami Al-Maskari, Hongwei Li, Harsharan Bhatia, Timo Loehr, Marie Piraud, Ali Erturk, et al. blob loss: instance imbalance aware loss functions for semantic segmentation. arXiv preprint arXiv:2205.08209, 2022.\n[80] Ender Konukoglu, Ben Glocker, Dong Hye Ye, Antonio Criminisi, and Kilian M Pohl. Discriminative segmentationbased evaluation through shape dissimilarity. IEEE transactions on medical imaging, 31(12):2278-2289, 2012.\n[81] Jan Kottner, Laurent Audigé, Stig Brorson, Allan Donner, Byron J Gajewski, Asbjørn Hróbjartsson, Chris Roberts, Mohamed Shoukri, and David L Streiner. Guidelines for reporting reliability and agreement studies (grras) were proposed. International journal of nursing studies, 48(6):661-671, 2011.\n[82] Harold W Kuhn. The hungarian method for the assignment problem. Naval research logistics quarterly, 2(1-2):83-97, 1955.\n[83] Meelis Kull, Miquel Perello Nieto, Markus Kängsepp, Telmo Silva Filho, Hao Song, and Peter Flach. Beyond temperature scaling: Obtaining well-calibrated multi-class probabilities with dirichlet calibration. Advances in neural information processing systems, 32, 2019.\n[84] Ananya Kumar, Percy S Liang, and Tengyu Ma. Verified uncertainty calibration. Advances in Neural Information Processing Systems, 32, 2019.\n[85] Fabian Kuppers, Jan Kronenberger, Amirhossein Shantia, and Anselm Haselhoff. Multivariate confidence calibration for object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 326-327, 2020.\n[86] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. Quantifying the carbon emissions of machine learning. arXiv preprint arXiv:1910.09700, 2019.\n[87] Loïc Lannelongue, Jason Grealey, and Michael Inouye. Green algorithms: quantifying the carbon footprint of computation. Advanced science, 8(12):2100707, 2021.\n[88] Alexander Lavin, Ciarán M Gilligan-Lee, Alessya Visnjic, Siddha Ganju, Dava Newman, Sujoy Ganguly, Danny Lange, Atilim Güneş Baydin, Amit Sharma, Adam Gibson, et al. Technology readiness levels for machine learning systems. Nature Communications, 13(1):1-19, 2022.\n[89] EPV Le, Y Wang, Yuan Huang, Sarah Hickman, and FJ Gilbert. Artificial intelligence in breast imaging. Clinical radiology, 74(5):357-366, 2019."
    },
    {
      "markdown": "[90] David A van Leeuwen and Niko Brümmer. An introduction to application-independent evaluation of speaker recognition systems. In Speaker classification I, pages 330-353. Springer, 2007.\n[91] Jochen K Lennerz, Ursula Green, Drew FK Williamson, and Faisal Mahmood. A unifying force for the realization of medical ai. npj Digital Medicine, 5(1):1-3, 2022.\n[92] Zeju Li, Konstantinos Kamnitsas, Mobarakol Islam, Chen Chen, and Ben Glocker. Estimating model performance under domain shifts with class-specific confidence scores. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 693-703. Springer, 2022.\n[93] Kung-Yee Liang and Scott L Zeger. Longitudinal data analysis using generalized linear models. Biometrika, 73(1): $13-22,1986$.\n[94] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740-755. Springer, 2014.\n[95] Xiaoqi Liu, Kelsey Parks, Inga Saknite, Tahsin Reasat, Austin D Cronin, Lee E Wheless, Benoit M Dawant, and Eric R Tkaczyk. Baseline photos and confident annotation improve automated detection of cutaneous graft-versus-host disease. Clinical hematology international, 3(3):108, 2021.\n[96] Vebjorn Ljosa, Katherine L Sokolnicki, and Anne E Carpenter. Annotated high-throughput microscopy image sets for validation. Nature methods, 9(7):637-637, 2012.\n[97] Lena Maier-Hein, Matthias Eisenmann, Annika Reinke, Sinan Onogur, Marko Stankovic, Patrick Scholz, Tal Arbel, Hrvoje Bogunovic, Andrew P Bradley, Aaron Carass, et al. Why rankings of biomedical image analysis competitions should be interpreted with care. Nature communications, 9(1):1-13, 2018. With this comprehensive analysis of biomedical image analysis competitions (challenges), the authors initiated a shift in how such challenges are designed, performed, and reported in the biomedical domain. Its concepts and guidelines have been adopted by reputed organizations such as MICCAI.\n[98] Lena Maier-Hein, Martin Wagner, Tobias Ross, Annika Reinke, Sebastian Bodenstedt, Peter M Full, Hellena Hempe, Diana Mindroc-Filimon, Patrick Scholz, Thuy Nuong Tran, et al. Heidelberg colorectal data set for surgical data science in the sensor operating room. Scientific data, 8(1):1-11, 2021.\n[99] Lena Maier-Hein, Annika Reinke, Evangelia Christodoulou, Ben Glocker, Patrick Godau, Fabian Isensee, Jens Kleesiek, Michal Kozubek, Mauricio Reyes, Michael A Riegler, et al. Metrics reloaded: Pitfalls and recommendations for image analysis validation. arXiv preprint arXiv:2206.01653, 2022.\n[100] Lisa Mais, Peter Hirsch, and Dagmar Kainmueller. Patchperpix for instance segmentation. In European Conference on Computer Vision, pages 288-304. Springer, 2020.\n[101] Ran Margolin, Lihi Zelnik-Manor, and Ayellet Tal. How to evaluate foreground maps? In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 248-255, 2014.\n[102] Martin Maška, Vladimír Ulman, David Svoboda, Pavel Matula, Petr Matula, Cristina Ederra, Ainhoa Urbiola, Tomás España, Subramanian Venkatesan, Deepak MW Balak, et al. A benchmark for comparison of cell tracking algorithms. Bioinformatics, 30(11):1609-1617, 2014.\n[103] Brian W Matthews. Comparison of the predicted and observed secondary structure of t4 phage lysozyme. Biochimica et Biophysica Acta (BBA)-Protein Structure, 405(2):442-451, 1975.\n[104] Pavel Matula, Martin Maška, Dmitry V Sorokin, Petr Matula, Carlos Ortiz-de Solórzano, and Michal Kozubek. Cell tracking accuracy measurement based on comparison of acyclic oriented graphs. PloS one, 10(12):e0144959, 2015.\n[105] Melissa D McCradden, James A Anderson, Elizabeth A Stephenson, Erik Drysdale, Lauren Erdman, Anna Goldenberg, and Randi Zlotnik Shaul. A research ethics framework for the clinical translation of healthcare machine learning. Am. J. Bioeth., pages 1-15, January 2022.\n[106] Alireza Mehrtash, William M Wells, Clare M Tempany, Purang Abolmaesumi, and Tina Kapur. Confidence calibration and predictive uncertainty estimation for deep medical image segmentation. IEEE transactions on medical imaging, 39 (12):3868-3878, 2020.\n[107] Marina Meilă. Comparing clusterings by the variation of information. In Learning theory and kernel machines, pages 173-187. Springer, 2003.\n[108] G. Meissner, A. Nern, Z. Dorman, DePasquale G.M., K. Forster, T. Gibney, Hausenfluck J.H., Y. He, N. Iyer, J. Jeter, et al. A searchable image resource of drosophila gal4-driver expression patterns with single neuron resolution. BioRxiv, page 2020.05.29.080473, 2022.\n[109] Karel GM Moons, Douglas G Altman, Johannes B Reitsma, John PA Ioannidis, Petra Macaskill, Ewout W Steyerberg, Andrew J Vickers, David F Ransohoff, and Gary S Collins. Transparent reporting of a multivariable prediction model for individual prognosis or diagnosis (tripod): explanation and elaboration. Annals of internal medicine, 162(1): W1-W73, 2015.\n[110] Mahdi Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht. Obtaining well calibrated probabilities using bayesian binning. In Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015."
    },
    {
      "markdown": "[111] Yukiko Nagao, Mika Sakamoto, Takumi Chinen, Yasushi Okada, and Daisuke Takao. Robust classification of cell cycle phase and biological feature extraction by image-based deep learning. Molecular biology of the cell, 31(13):1346-1354, 2020.\n[112] Ying-Hwey Nai, Bernice W Teo, Nadya L Tan, Sophie O'Doherty, Mary C Stephenson, Yee Liang Thian, Edmund Chiong, and Anthonin Reilhac. Comparison of metrics for the evaluation of medical segmentations using prostate mri dataset. Computers in Biology and Medicine, 134:104497, 2021.\n[113] Prashant Nasa, Ravi Jain, and Deven Juneja. Delphi methodology in healthcare research: how to decide its appropriateness. World Journal of Methodology, 11(4):116, 2021.\n[114] Vishwesh Nath, Kurt G Schilling, Prasanna Parvathaneni, Yuankai Huo, Justin A Blaber, Allison E Hainline, Muhamed Barakovic, David Romascano, Jonathan Rafael-Patino, Matteo Frigo, et al. Tractography reproducibility challenge with empirical data (traced): the 2017 ismrm diffusion study group challenge. Journal of Magnetic Resonance Imaging, $51(1): 234-249,2020$.\n[115] Lukas Neumann, Andrew Zisserman, and Andrea Vedaldi. Relaxed softmax: Efficient confidence auto-calibration for safe pedestrian detection. 2018 NIPS Workshop on Machine Learning for Intelligent Transportation Systems, 2018.\n[116] Stanislav Nikolov, Sam Blackwell, Alexei Zverovitch, Ruheena Mendes, Michelle Livne, Jeffrey De Fauw, Yojan Patel, Clemens Meyer, Harry Askham, Bernadino Romera-Paredes, et al. Clinically applicable segmentation of head and neck anatomy for radiotherapy: deep learning algorithm development and validation study. Journal of Medical Internet Research, 23(7):e26151, 2021.\n[117] Luke Oakden-Rayner, Jared Dunnmon, Gustavo Carneiro, and Christopher Ré. Hidden stratification causes clinically meaningful failures in machine learning for medical imaging. Proc ACM Conf Health Inference Learn (2020), 2020: 151-159, April 2020.\n[118] Ziad Obermeyer, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. Dissecting racial bias in an algorithm used to manage the health of populations. Science, 366(6464):447-453, October 2019.\n[119] Seong Ho Park, Kyunghwa Han, Hye Young Jang, Ji Eun Park, June-Goo Lee, Dong Wook Kim, and Jaesoon Choi. Methods for Clinical Evaluation of Artificial Intelligence Algorithms for Medical Diagnosis. Radiology, 306(1):20-31, January 2023. ISSN 0033-8419. doi: 10.1148/radiol.220182. URL https://pubs.rsna.org/doi/10.1148/radiol.220182. Publisher: Radiological Society of North America.\n[120] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. Carbon emissions and large neural network training. arXiv, April 2021.\n[121] Stephen G Pauker and Jerome P Kassirer. Therapeutic decision making: a cost-benefit analysis. New England Journal of Medicine, 293(5):229-234, 1975.\n[122] Alexandre Perez-Lebel, Marine Le Morvan, and Gaël Varoquaux. Beyond calibration: estimating the grouping loss of modern neural networks. International Conference on Learning Representations, 2023.\n[123] Teodora Popordanoska, Raphael Sayer, and Matthew B Blaschko. A consistent and differentiable lp canonical calibration error estimator. In Advances in Neural Information Processing Systems, 2022.\n[124] Joaquin Quinonero-Candela, Carl Edward Rasmussen, Fabian Sinz, Olivier Bousquet, and Bernhard Schölkopf. Evaluating predictive uncertainty challenge. In Machine Learning Challenges Workshop, pages 1-27. Springer, 2005.\n[125] William M Rand. Objective criteria for the evaluation of clustering methods. Journal of the American Statistical association, 66(336):846-850, 1971.\n[126] Annika Reinke, Matthias Eisenmann, Sinan Onogur, Marko Stankovic, Patrick Scholz, Peter M Full, Hrvoje Bogunovic, Bennett A Landman, Oskar Maier, Bjoern Menze, et al. How to exploit weaknesses in biomedical challenge design and organization. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 388-395. Springer, 2018.\n[127] Annika Reinke, Matthias Eisenmann, Minu D Tizabi, Carole H Sudre, Tim Rädsch, Michela Antonelli, Tal Arbel, Spyridon Bakas, M Jorge Cardoso, Veronika Cheplygina, et al. Common limitations of image processing metrics: A picture story. arXiv preprint arXiv:2104.05642, 2021.\n[128] Annika Reinke, Minu D Tizabi, Michael Baumgartner, Matthias Eisenmann, Doreen Heckmann-Nötzel, A Emre Kavur, Tim Rädsch, Carole H Sudre, Laura Acion, Michela Antonelli, et al. Understanding metric-related pitfalls in image analysis validation. Nature methods, pages 1-13, 2024.\n[129] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems, 28, 2015.\n[130] Richard D Riley, Joie Ensor, Kym IE Snell, Thomas PA Debray, Doug G Altman, Karel GM Moons, and Gary S Collins. External validation of clinical prediction models using big datasets from e-health records or ipd meta-analysis: opportunities and challenges. bmj, 353, 2016.\n[131] Tobias Roß, Pierangela Bruno, Annika Reinke, Manuel Wiesenfarth, Lisa Koeppel, Peter M Full, Bünyamin Pekdemir, Patrick Godau, Darya Trofimova, Fabian Isensee, et al. How can we learn (more) from challenges? a statistical approach to driving future algorithm development. arXiv preprint arXiv:2106.09302, 2021."
    },
    {
      "markdown": "[132] Daniel Sage, Hagai Kirshner, Thomas Pengo, Nico Stuurman, Junhong Min, Suliana Manley, and Michael Unser. Quantitative evaluation of software packages for single-molecule localization microscopy. Nature methods, 12(8): $717-724,2015$.\n[133] Frank W Samuelson and Nicholas Petrick. Comparing image detection algorithms using resampling. In 3rd IEEE International Symposium on Biomedical Imaging: Nano to Macro, 2006., pages 1312-1315. IEEE, 2006.\n[134] Cristina Sánchez-Montes, Francisco Javier Sánchez, Jorge Bernal, Henry Córdova, María López-Cerón, Miriam Cuatrecasas, Cristina Rodríguez De Miguel, Ana García-Rodríguez, Rodrigo Garcés-Durán, María Pellisé, et al. Computer-aided prediction of polyp histology on white light colonoscopy using surface pattern analysis. Endoscopy, 51(03):261-265, 2019.\n[135] Jörg Sander, Bob D de Vos, Jelmer M Wolterink, and Ivana Išgum. Towards increased trustworthiness of deep learning segmentation methods on cardiac mri. In Medical imaging 2019: image Processing, volume 10949, pages 324-330. SPIE, 2019.\n[136] Peter Schulam and Suchi Saria. Can you trust this prediction? auditing pointwise reliability after learning. In Kamalika Chaudhuri and Masashi Sugiyama, editors, Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics, volume 89 of Proceedings of Machine Learning Research, pages 1022-1031. PMLR, 2019.\n[137] Kenneth F Schulz, Douglas G Altman, David Moher, and CONSORT Group*. Consort 2010 statement: updated guidelines for reporting parallel group randomized trials. Annals of internal medicine, 152(11):726-732, 2010.\n[138] Arnaud Arindra Adiyoso Setio, Alberto Traverso, Thomas De Bel, Moira SN Berens, Cas Van Den Bogaard, Piergiorgio Cerello, Hao Chen, Qi Dou, Maria Evelina Fantacci, Bram Geurts, et al. Validation, comparison, and combination of algorithms for automatic detection of pulmonary nodules in computed tomography images: the luna16 challenge. Medical image analysis, 42:1-13, 2017.\n[139] Nigam H Shah, Arnold Milstein, and Steven C Bagley. Making machine learning models clinically useful. Jama, 322 (14):1351-1352, 2019.\n[140] Suprosanna Shit, Johannes C Paetzold, Anjany Sekuboyina, Ivan Ezhov, Alexander Unger, Andrey Zhylka, Josien PW Pluim, Ulrich Bauer, and Bjoern H Menze. cldice-a novel topology-preserving loss function for tubular structure segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1656016569, 2021.\n[141] Amber L Simpson, Michela Antonelli, Spyridon Bakas, Michel Bilello, Keyvan Farahani, Bram Van Ginneken, Annette Kopp-Schneider, Bennett A Landman, Geert Litjens, Bjoern Menze, et al. A large annotated medical image dataset for the development and evaluation of segmentation algorithms. arXiv preprint arXiv:1902.09063, 2019.\n[142] Viknesh Sounderajah, Hutan Ashrafian, Ravi Aggarwal, Jeffrey De Fauw, Alastair K Denniston, Felix Greaves, Alan Karthikesalingam, Dominic King, Xiaoxuan Liu, Sheraz R Markar, Matthew D F McInnes, Trishan Panch, Jonathan Pearson-Stuttard, Daniel S W Ting, Robert M Golub, David Moher, Patrick M Bossuyt, and Ara Darzi. Developing specific reporting guidelines for diagnostic accuracy studies assessing AI interventions: The STARD-AI steering group. Nat. Med., 26(6):807-808, June 2020.\n[143] Ewout W Steyerberg, Andrew J Vickers, Nancy R Cook, Thomas Gerds, Mithat Gonen, Nancy Obuchowski, Michael J Pencina, and Michael W Kattan. Assessing the performance of prediction models: a framework for some traditional and novel measures. Epidemiology (Cambridge, Mass.), 21(1):128, 2010.\n[144] Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in NLP. arXiv, June 2019.\n[145] Cecilia Summers and Michael J Dinneen. Nondeterminism and instability in neural network optimization. In International Conference on Machine Learning, pages 9913-9922. PMLR, 2021.\n[146] Abdel Aziz Taha and Allan Hanbury. Metrics for evaluating 3d medical image segmentation: analysis, selection, and tool. BMC medical imaging, 15(1):1-28, 2015. The paper discusses the importance of effective metrics for evaluating the accuracy of 3D medical image segmentation algorithms. The authors analyze existing metrics, propose a selection methodology, and develop a tool to aid researchers in choosing appropriate evaluation metrics based on the specific characteristics of the segmentation task.\n[147] Anna Targosz, Piotr Przystałka, Ryszard Wiaderkiewicz, and Grzegorz Mrugacz. Semantic segmentation of human oocyte images using deep neural networks. BioMedical Engineering OnLine, 20(1):40, 2021.\n[148] Alaa Tharwat. Classification assessment methods. Applied Computing and Informatics, 2020.\n[149] The Institute for Ethical Ai and Machine Learning. The institute for ethical AI \\& machine learning. https://ethical. institute/principles.html, 2018. Accessed: 2022-5-21.\n[150] Kimberley M Timmins, Irene C van der Schaaf, Edwin Bennink, Ynte M Ruigrok, Xingle An, Michael Baumgartner, Pascal Bourdon, Riccardo De Feo, Tommaso Di Noto, Florian Dubost, et al. Comparing methods of detecting and segmenting unruptured intracranial aneurysms on tof-mras: The adam challenge. Neuroimage, 238:118216, 2021.\n[151] Laszlo Tirian and Barry J Dickson. The vt gal4, lexa, and split-gal4 driver line collections for targeted expression in the drosophila nervous system. BioRxiv, page 198648, 2017."
    },
    {
      "markdown": "[152] Thuy N Tran, Tim Adler, Amine Yamlahi, Evangelia Christodoulou, Patrick Godau, Annika Reinke, Minu D Tizabi, Peter Sauer, Tillmann Persicke, Jörg G. Albert, and Lena Maier-Hein. Sources of performance variability in deep learning-based polyp detection. arXiv preprint arXiv:2211.09708, 2022.\n[153] Vladimir Ulman, Martin Maška, Klas EG Magnusson, Olaf Ronneberger, Carsten Haubold, Nathalie Harder, Pavel Matula, Petr Matula, David Svoboda, Miroslav Radojevic, et al. An objective comparison of cell-tracking algorithms. Nature methods, 14(12):1141-1152, 2017.\n[154] Richard Usatine and Rachel Manci. Dermoscopedia, 2021. https://dermoscopedia.org/File:DF_chinese_dms.JPG.\n[155] Femke Vaassen, Colien Hazelaar, Ana Vaniqui, Mark Gooding, Brent van der Heyden, Richard Canters, and Wouter van Elmpt. Evaluation of measures for assessing time-saving of automatic organ-at-risk segmentation in radiotherapy. Physics and Imaging in Radiation Oncology, 13:1-6, 2020.\n[156] Juozas Vaicenavicius, David Widmann, Carl Andersson, Fredrik Lindsten, Jacob Roll, and Thomas Schön. Evaluating model calibration in classification. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 3459-3467. PMLR, 2019.\n[157] Ben Van Calster, Daan Nieboer, Yvonne Vergouwe, Bavo De Cock, Michael J Pencina, and Ewout W Steyerberg. A calibration hierarchy for risk models was defined: from utopia to empirical data. Journal of clinical epidemiology, 74: $167-176,2016$.\n[158] Ben Van Calster, David J McLernon, Maarten Van Smeden, Laure Wynants, and Ewout W Steyerberg. Calibration: the achilles heel of predictive analytics. BMC medicine, 17(1):1-7, 2019.\n[159] Bram Van Ginneken, Samuel G Armato III, Bartjan de Hoop, Saskia van Amelsvoort-van de Vorst, Thomas Duindam, Meindert Niemeijer, Keelin Murphy, Arnold Schilham, Alessandra Retico, Maria Evelina Fantacci, et al. Comparing and combining algorithms for computer-aided detection of pulmonary nodules in computed tomography scans: the anode09 study. Medical image analysis, 14(6):707-722, 2010.\n[160] Kirsten Van Hoorde, Sabine Van Huffel, Dirk Timmerman, Tom Bourne, and Ben Van Calster. A spline-based tool to assess and visualize the calibration of multiclass risk predictions. Journal of biomedical informatics, 54:283-293, 2015.\n[161] C Van Rijsbergen. Information retrieval: theory and practice. In Proceedings of the Joint IBM/University of Newcastle upon Tyne Seminar on Data Base Systems, volume 79, 1979.\n[162] Andrew J Vickers and Elena B Elkin. Decision curve analysis: a novel method for evaluating prediction models. Medical Decision Making, 26(6):565-574, 2006.\n[163] Andrew J Vickers, Ben Van Calster, and Ewout W Steyerberg. Net benefit approaches to the evaluation of prediction models, molecular markers, and diagnostic tests. bmj, 352, 2016.\n[164] David S Wack, Michael G Dwyer, Niels Bergsland, Carol Di Perri, Laura Ranza, Sara Hussein, Deepa Ramasamy, Guy Poloni, and Robert Zivadinov. Improved assessment of multiple sclerosis lesion segmentation agreement via detection and outline error estimates. BMC medical imaging, 12(1):1-10, 2012.\n[165] Matthijs J Warrens. Some paradoxical results for the quadratically weighted kappa. Psychometrika, 77(2):315-323, 2012.\n[166] Jonathan Wenger, Hedvig Kjellström, and Rudolph Triebel. Non-parametric calibration for classification. In International Conference on Artificial Intelligence and Statistics, pages 178-190. PMLR, 2020.\n[167] David Widmann, Fredrik Lindsten, and Dave Zachariah. Calibration tests in multi-class classification: A unifying framework. Advances in Neural Information Processing Systems, 32, 2019.\n[168] Manuel Wiesenfarth, Annika Reinke, Bennett A Landman, Matthias Eisenmann, Laura Aguilera Saiz, M Jorge Cardoso, Lena Maier-Hein, and Annette Kopp-Schneider. Methods and open-source toolkit for analyzing and visualizing challenge results. Scientific reports, 11(1):1-15, 2021.\n[169] Lasse F Wolff Anthony, Benjamin Kanding, and Raghavendra Selvan. Carbontracker: Tracking and predicting the carbon footprint of training deep learning models. arXiv, July 2020.\n[170] Laure Wynants, Maarten Van Smeden, David J McLernon, Dirk Timmerman, Ewout W Steyerberg, and Ben Van Calster. Three myths about risk thresholds for prediction models. BMC medicine, 17(1):1-7, 2019.\n[171] Yinchong Yang and Florian Buettner. Multi-output gaussian processes for uncertainty-aware recommender systems. In Uncertainty in Artificial Intelligence, pages 1505-1514. PMLR, 2021.\n[172] Varduhi Yeghiazaryan and Irina Voiculescu. An overview of current evaluation methods used in medical image segmentation. Department of Computer Science, University of Oxford, 2015.\n[173] Varduhi Yeghiazaryan and Irina D Voiculescu. Family of boundary overlap metrics for the evaluation of medical image segmentation. Journal of Medical Imaging, 5(1):015006, 2018.\n[174] Ying Zhang, Yubin Xie, Wenzhong Liu, Wankun Deng, Di Peng, Chenwei Wang, Haodong Xu, Chen Ruan, Yongjie Deng, Yaping Guo, et al. Deepphagy: a deep learning framework for quantitatively measuring autophagy activity in saccharomyces cerevisiae. Autophagy, 16(4):626-640, 2020.\n[175] Qiuming Zhu. On the performance of matthews correlation coefficient (mcc) for imbalanced dataset. Pattern Recognit. Lett., 136:71-80, 2020."
    },
    {
      "markdown": "# ACRONYMS \n\nAI artificial intelligence\nAP Average Precision\nASSD Average Symmetric Surface Distance\nAUC Area under the curve\nAUROC Area under the Receiver Operating Characteristic Curve\nBA Balanced Accuracy\nBM Bookmaker Informedness\nBPMN Business Process Model and Notation\nBS Brier Score\nBSS Brier Skill Score\nCE Calibration Error\nCK Cohen's Kappa\nclDice centerline Dice Similarity Coefficient\nCT computed tomography\nCWCE Class-wise Calibration Error\nDSC Dice Similarity Coefficient\nEC Expected Cost\nECE Expected Calibration Error\n$\\mathbf{E C E}^{\\mathbf{K D E}}$ Expected Calibration Error Kernel Density Estimate\nECI Expected Calibration Index\nECN normalized EC\nEQUATOR Enhancing the QUAlity and Transparency Of health Research\nER Error Rate\nFN False Negative\nFP False Positive\nFPPI False Positives per Image\nFDR False Discovery Rate\nFOR False Omission Rate\nFROC Free-Response Receiver Operating Characteristic\nHD Hausdorff Distance\nHD95 Hausdorff Distance 95th Percentile\nImLC Image-level Classification\nInS Instance Segmentation\nIoU Intersection over Union\nIoR Intersection over Reference\nJ Youden Index\nKCE Kernel Calibration Error\nLR+ Positive Likelihood Ratio\nmAP Mean Average Precision\nMASD Mean Average Surface Distance\nMCC Matthews Correlation Coefficient\nMICCAI Medical Image Computing and Computer Assisted Interventions\nMK Markedness\nML machine learning\nMONAI Medical Open Network for Artificial Intelligence\nMRI magnetic resonance imaging"
    },
    {
      "markdown": "MS multiple sclerosis\nNaN 'Not a Number'\nNB Net Benefit\nNLL Negative Log Likelihood\nNPV Negative Predictive Value\nNSD Normalized Surface Distance\nObD Object Detection\nO:E ratio Observed:Expected ratio\nPPV Positive Predictive Value\nPQ Panoptic Quality\nPHI Protected Health Information\nPR Precision-Recall\nPSR Proper Scoring Rule\nRBS Root Brier Score\nRI Rand Index\nROC Receiver Operating Characteristic\nROI Region of Interest\nSemS Semantic Segmentation\nTN True Negative\nTNR True Negative Rate\nTOF-MRA time-of-flight magnetic resonance angiography\nTP True Positive\nTPR True Positive Rate\nVoI Variation of Information\nWCK Weighted Cohen's Kappa\nWSI whole slide imaging\n$\\mathrm{X}^{t h}$ Percentile HD X ${ }^{t h}$ Percentile Hausdorff Distance"
    },
    {
      "markdown": "# EXTENDED DATA \n\n![img-5.jpeg](img-5.jpeg)\n\nExtended Data Fig. 1. Subprocess S1 for selecting a problem category. The Category Mapping maps a given research problem to the appropriate problem category with the goal of grouping problems by similarity of validation. The leaf nodes represent the categories: image-level classification, object detection, instance segmentation, or semantic segmentation. FP2.1 refers to fingerprint 2.1 (see Fig. SN 1.10). An overview of the symbols used in the process diagram is provided in Fig. SN 5.1."
    },
    {
      "markdown": "![img-6.jpeg](img-6.jpeg)\n\nExtended Data Fig. 2. Subprocess S2 for selecting multi-class metrics (if any). Applies to: image-level classification (ImLC). In the case of presence of class imbalance and no compensation of class imbalance being requested, one should follow the \"No\" branch. Decision guides are provided in Suppl. Note 2.7.1. A detailed description of the subprocess is given in Suppl. Note 2.2."
    },
    {
      "markdown": "![img-7.jpeg](img-7.jpeg)\n\nExtended Data Fig. 3. Subprocess S3 for selecting a per-class counting metric (if any). Applies to: image-level classification (ImLC), object detection (ObD), and instance segmentation (InS). Decision guides are provided in Suppl. Note 2.7.2. A detailed description of the subprocess is given in Suppl. Notes 2.2, 2.4 and 2.5 ."
    },
    {
      "markdown": "![img-8.jpeg](img-8.jpeg)\n\nExtended Data Fig. 4. Subprocess S4 for selecting a multi-threshold metric (if any). Applies to: imagelevel classification (ImLC), object detection (ObD), and instance segmentation (InS). Decision guides are provided in Suppl. Note 2.7.3. A detailed description of the subprocess is given in Suppl. Notes 2.2, 2.4 and 2.5."
    },
    {
      "markdown": "![img-9.jpeg](img-9.jpeg)\n\nExtended Data Fig. 5. Subprocess S5 for selecting a calibration metric (if any). Applies to: imagelevel classification (ImLC). Decision guides are provided in Suppl. Note 2.7.4. A detailed description of the subprocess is given in Suppl. Note 2.6. Further suggested calibration metrics include the calibration loss [22], calibration slope [143], Expected Calibration Index (ECI) [160] and Observed:Expected ratio (O:E ratio) [130]."
    },
    {
      "markdown": "![img-10.jpeg](img-10.jpeg)\n\nExtended Data Fig. 6. Subprocess S6 for selecting overlap-based segmentation metrics (if any). Applies to semantic segmentation (SemS) and instance segmentation (InS). Decision guides are provided in Suppl. Note 2.7.5. A detailed description of the subprocess is given in Suppl. Notes 2.3 and 2.5."
    },
    {
      "markdown": "![img-11.jpeg](img-11.jpeg)\n\nExtended Data Fig. 7. Subprocess S7 for selecting a boundary-based segmentation metric (if any). Applies to: semantic segmentation (SemS) and instance segmentation (InS). Decision guides are provided in Suppl. Note 2.7.6. A detailed description of the subprocess is given in Suppl. Notes 2.3 and 2.5."
    },
    {
      "markdown": "![img-12.jpeg](img-12.jpeg)\n\nExtended Data Fig. 8. Subprocess S8 for selecting the localization criterion. Applies to: object detection (ObD) and instance segmentation (InS). Definitions of the localization criteria can be found in [127]. Decision guides are provided in Suppl. Note 2.7.7. A detailed description of the subprocess is given in Suppl. Notes 2.4 and 2.5 ."
    },
    {
      "markdown": "![img-13.jpeg](img-13.jpeg)\n\nExtended Data Fig. 9. Subprocess S9 for selecting the assignment strategy. Applies to: object detection (ObD) and instance segmentation (InS). Assignment strategies are defined in [127]. Decision guides are provided in Suppl. Note 2.7.8. A detailed description of the subprocess is given in Suppl. Notes 2.4 and 2.5."
    },
    {
      "markdown": "Extended Data Tab. 1. Recommendations for metric application addressing the pitfalls collected in [128]. The first column comprises all sources of pitfalls captured by the published taxonomy that relate to the application of (already selected) metrics. The second column provides the Metrics Reloaded recommendation. The notation FPXY refers to a fingerprint item (Suppl. Note 1.3).\n\nSource of Pitfall Recommendation\n\n# Metric implementation \n\n| Non-standardized metric defini- | Use reference implementations provided at https://github.com/Project-MONAI/ |\n| :-- | :-- |\n| tion and undefined corner cases | MetricsReloaded. |\n| Discretization issues | Use unbiased estimates of properties of interest if possible (Suppl. Note 2.6). |\n| Metric-specific issues including | Read metric-specific recommendations in the cheat sheets (Suppl. Note 3.1). |\n| sensitivity to hyperparameters |  |\n\n## Aggregation\n\nHierarchical label/class structure\nMulti-class problem\n\nNon-independence of test cases (FP4.5)\nRisk of bias\nPossibility of invalid prediction (FPS.3)\n\nAddress the potential correlation between classes when aggregating [74].\nComplement validation with multi-class metrics such as Expected Cost (EC) or Matthews Correlation Coefficient (MCC) with per-class validation (Fig. 2); perform weighted class aggregation if FP2.5.1 Unequal interest across classes holds.\nRespect the hierarchical data structure when aggregating metrics [93].\nLeverage metadata (e.g. on imaging device/protocol/center) to reveal potential algorithmic bias [9].\nFollow category-specific aggregation strategy detailed in Suppl. Note SUPPL. NOTE 2 .\n\nRanking\nMetric relationships\nRanking uncertainties\n\nProvide information beyond plain tables that make possible uncertainties in rankings explicit as detailed in [168].\n\n## Reporting\n\nNon-determinism of algorithms Consider multiple test set runs to address the variability of results resulting from non-determinism $[76,145]$.\nUninformative visualization Include a visualization of the raw metric values [168] and report the full confusion matrix unless FP2.6 = no decision rule applied holds.\n\n## Interpretation\n\nLow resolution\nLack of lower/upper bounds\nInsufficient domain relevance of metric score differences\n\nRead metric-related recommendations to obtain awareness of the pitfall (Suppl. Note 3.1).\nRead metric-related recommendations to obtain awareness of the pitfall (Suppl. Note 3.1).\nReport on the quality of the reference (e.g. intra-rater and inter-rater variability) [81]. Choose the number of decimal places such that they reflect both relevance and uncertainties of the reference. More than one decimal number is often not useful given the typically high inter-rater variability."
    },
    {
      "markdown": "# SUPPLEMENTARY INFORMATION \n\n## Content of Supplementary Notes\n\nOur metric recommendations are detailed in the following sections.\n![img-14.jpeg](img-14.jpeg)\n\n## SUPPL. METHODS\n\nDelphi process, expert consortium, reference implementation, web-based tool\n![img-15.jpeg](img-15.jpeg)\n\nStep 1 - Problem Fingerprinting (SUPPL. NOTE 1)\nGeneral instructions (Suppl. Note 1.1), problem category mapping (Suppl. Note 1.2), generation of the problem fingerprint (Suppl. Note 1.3)\n\nStep 2 - Metric Selection (SUPPL. NOTE 2)\nMetrics Reloaded pool of reference-based metrics (Suppl. Note 2.1), recommendations for metric selection (Suppl. Notes 2.2-2.6), decision guides (Suppl. Note 2.7)\n![img-16.jpeg](img-16.jpeg)\n\nStep 3 - Metric Application (SUPPL. NOTE 3)\nMetric cheat sheets (Suppl. Note 3.1)\n![img-17.jpeg](img-17.jpeg)\n\n## Recommendations for selected use cases (SUPPL. NOTE 4)\n\n## Terminology and Notation (SUPPL. NOTE 5)\n\nSymbols (Suppl. Note 5.1), acronyms (Suppl. Note 5.3), glossary (Suppl. Note 5.4)\n\nOverview of relevant content for Image-level Classification (ImLC), Semantic Segmentation (SemS), Object Detection (ObD) and Instance Segmentation (InS) problems.\n\n|  | ImLC | SemS | ObD | InS |\n| :--: | :--: | :--: | :--: | :--: |\n| Problem fingerprint <br> (SUPPL. NOTE 1) | Figs. SN 1.7 - SN 1.9 | $\\begin{gathered} \\text { Figs. SN } 1.10- \\\\ \\text { SN } 1.11 \\end{gathered}$ | $\\begin{gathered} \\text { Figs. SN } 1.12- \\\\ \\text { SN } 1.14 \\end{gathered}$ | $\\begin{gathered} \\text { Figs. SN } 1.15- \\\\ \\text { SN } 1.17 \\end{gathered}$ |\n| Recommendations for metric selection (SUPPL. NOTE 2) | Suppl. Note 2.2; <br> Calibration: Suppl. Note 2.6 | Suppl. Note 2.3 | Suppl. Note 2.4 | Suppl. Note 2.5 |\n| Metric cheat sheets (SUPPL. NOTE 3) | Suppl. Note 3.1 | Suppl. Note 3.1 | Suppl. Note 3.1 | Suppl. Note 3.1 |\n| Instantiation for common use cases (SUPPL. NOTE 4) | Fig. SN 4.1 | Fig. SN 4.2 | Fig. SN 4.3 | Fig. SN 4.4 |"
    },
    {
      "markdown": "# SUPPLEMENTARY METHODS \n\n## Delphi process\n\nWe compiled the recommendations provided by the Metrics Reloaded framework by assembling an international expert consortium which then underwent a multi-stage Delphi process. A Delphi process is a structured group communication process that serves to gather opinions from an expert panel via a series of individual interrogations, usually in the form of questionnaires, interspersed with feedback from the respondents [21]. The technique is widely used for establishing consensus among experts in medicine, particularly in the development of best practices in areas where evidence may be limited, conflicting, or absent [113]. The initial panel participating in our Delphi process comprised 30 international biomedical image analysis experts representing 25 institutions. Member selection was initially based on membership in one of the three initiatives that triggered this research, namely the Biomedical Image Analysis Challenges (BIAS) initiative, the Medical Open Network for Artificial Intelligence (MONAI) Working Group for Evaluation, Reproducibility and Benchmarks, and the Medical Image Computing and Computer Assisted Interventions (MICCAI) Special Interest Group for Challenges (previously MICCAI board working group). To reflect as broad a range of imaging domains as possible and expand the available expertise, the number of consortium members was gradually increased from the initial 30 to a final number of 73 . The members provided a wide range of expertise ranging from biology, medicine, epidemiology and biomedical image analysis all the way to statistics, mathematics and computer science. Furthermore, leading members of major standardization initiatives were included, such as the Enhancing the QUAlity and Transparency Of health Research (EQUATOR) network, from which imaging and clinical guidelines have originated, including CONSORT/CONSORT-AI [37, 137], TRIPOD/TRIPODAI [35, 109], STARD/STARD-AI [19, 142], and others.\n\nOverall, the process comprised six distinct stages and encompassed five workshops and nine surveys before a final Delphi consensus voting was performed. Each survey was developed by the Metrics Reloaded core team and taken by the remaining members of the consortium; in other words, the researchers that designed the surveys did not take part in them. Upon completion, the core team then analyzed the results, discussed them with team members where necessary, and integrated the feedback, thus iteratively refining the framework. The main stages of the compilation and consensus building process are detailed in the following:\n\n1. Initialization. A kickoff workshop was held in December 2020 with the primary goal of deciding on the concrete scope of the recommendation framework. Prior to the workshop, an initial survey had been conducted with a focus on gathering relevant literature as well as theoretical and practical failure cases of metrics in the broader scope of classification, segmentation and detection. Based on the discussions at the workshop, a series of three surveys was issued whose responses resulted in (1) a joint terminology (see Suppl. Note 5.4), (2) inclusion criteria for the paper, namely the decision to cover classification tasks at image/object and pixel level (Fig. 4), (3) a shortlist of relevant metrics for each category (whose subsequent refinement resulted in Tab. 2.1), and (4) an initial set of fingerprint items, whose refinement resulted in the final fingerprints presented in Suppl. Note 1.3. It was further decided by the consortium that the choice of problem category should be part of the recommendation itself (covered in Subprocess S1, Extended Data Fig. 1).\n2. Compilation of first draft of recommendations in expert groups. The primary purpose of the second Delphi workshop in June 2021 was the formation of expert groups that should coordinate individual task forces. Five expert groups were initially formed; three dedicated to the problem"
    },
    {
      "markdown": "categories addressed in the framework (one for image-level classification, one for semantic segmentation and one for object detection and instance segmentation), plus a biomedical expert group and a cross-topic expert group. The task of the expert groups corresponding to the problem categories was to develop recommendations for their respective categories that address the pitfalls compiled in the sister publication [128] (now captured in Fig. 2 and the nine subprocesses in Extended Data Figs. 1 - 9). The task of the cross-topic group was to identify and tackle metric-related issues going beyond pure metric selection, such as metric aggregation, reporting and implementation, statistical considerations, rankings, and biases (now captured in Extended Data Tab. 1). The task of the biomedical expert group was to ensure that the recommendation framework would satisfy the needs of domain experts, such as clinicians, and to identify relevant biomedical scenarios (now captured in SUPPL. NOTE 4). Group-specific surveys were issued to support the work of the task forces. To give the experts enough freedom, no specific restrictions were imposed with respect to how the individual groups would arrive at their recommendations. In the following third Delphi workshop, held in October 2021, the expert group leaders discussed their preliminary results with the entire core team.\n3. Consolidation of recommendations by Metrics Reloaded core team. Once the expert groups had finalized their initial recommendation drafts for the problem categories, the Metrics Reloaded core team consolidated and harmonized the recommendations in close collaboration with the groups. In the fourth Delphi workshop in January 2022, the resulting decision trees capturing the core recommendations (Fig. 2; S1-S4, S6-S9) were presented and discussed.\n4. Revision by Metrics Reloaded consortium. The decision trees were then subjected to internal tests by members of the consortium and their teams. The Metrics Reloaded core team incorporated the survey-based feedback in close collaboration with the expert groups. The first draft of the entire framework was then presented and discussed at the fifth Delphi workshop in March 2022.\n5. Crowdsourcing of feedback. Finally, community feedback was obtained via a social media campaign. The recommendation framework was released on arXiv [99], and a survey link was sent by the Metrics Reloaded core team to various mailing lists, as well as posted on the social media platforms LinkedIn, with the hashtag \\#imageanalysis, and Twitter, where we tagged various relevant accounts, e.g., @MICCAIStudents, @WomenInMICCAI, @midl_conference, @ELLISforEurope, @ProjectMONAI, and @naturemethods. The original tweet received more than 42,000 impressions. All co-authors were asked to distribute the survey among their colleagues and societies. Furthermore, the survey link was added to the released arXiv version. The survey was initially opened at the beginning of June 2022 and closed at the end of July 2022. The community had the choice between submitting one-click feedback or detailed feedback by answering questions on the comprehensiveness and usefulness of our approach, the specific mappings, as well as voicing concerns or questions. In addition, we specifically asked for which biomedical use cases the framework should be instantiated. All contributors were given the choice to be included in the acknowledgements. total of 186 researchers participated in the survey. Of those, 82 provided feedback in the form of free text answers. 58 participants chose to give detailed feedback rather than one-click feedback. A total of 46 researchers wished to be mentioned in the acknowledgements and provided their names. Contributors who provided substantial feedback were invited into the consortium (seven in total). The social media survey was used as a basis to select biomedical use cases for which the framework was instantiated. Based on the feedback, we designed the metric cheat sheets (see SUPPL. NOTE 4). The implementation of the web toolkit was highly encouraged by several survey participants. Moreover, in response to the feedback, an additional expert group on the topic of"
    },
    {
      "markdown": "calibration was established with newly recruited consortium members, which led to the generation of the calibration recommendations captured in S5 (Extended Data Fig. 5). A revised framework, with the community feedback integrated (e.g., including new classification metrics, such as the EC), was presented to the consortium in another survey, based on which the Metrics Reloaded core team compiled the final recommendations (captured in Fig. 2 and S1-S9) that served as basis for the final Delphi-based consensus building.\n6. Final Delphi consensus building. In the final stage, an accelerated final Delphi process was initiated to vote for the ten core components of the recommendation framework (Fig. 2 and Subprocesses S1-S9). In response to the consortium's comments, final modifications to the calibration recommendations were made. After two rounds of revisions to S5, the final recommendation received strong support (only one member disagreed). For all other nine components, the first round had already resulted in a very strong consensus (disagreement $0 \\%-7 \\%$ ). Minor modifications, primarily concerning formatting and style, were communicated to the entire consortium whose members were then given the opportunity to veto any of the changes, which none of the consortium made use of.\n\n# Expert consortium \n\nhe expert consortium consisted of a total of 73 researchers ( $73 \\%$ male, $27 \\%$ female) from a total of 65 institutions. The majority of experts ( $52 \\%$ ) were professors, followed by postdoctoral researchers (37\\%). The median h-index of the consortium was 34 (mean: 27; minimum: 6; maximum: 113) and the median academic age was 18 years (mean: 19; minimum: 3; max: 42). Experts were from 18 countries and 5 continents. $66 \\%$ of experts had a technical, $7 \\%$ a clinical, $3 \\%$ a biological, and $24 \\%$ a mixed background. From the 65 institutions, we could identify the number of employees for $88 \\%$ of institutions. From those, the majority of institutions had a size between 1,000 and 10,000 employees ( $58 \\%$ ), followed by even larger institutions between 10,000 and 100,000 employees ( $25 \\%$ ), and smaller institutions below 1,000 employees ( $16 \\%$ ). Only a small portion of institutions were above 100,000 employees ( $2 \\%$ ).\n\n## Reference implementations\n\nTo overcome pitfalls related to metric implementation [128], we provide reference implementations for all Metrics Reloaded metrics within the MONAI open-source framework. They are accessible at https://github.com/Project-MONAI/MetricsReloaded.\n\n## Web-based tool\n\nThe recommendation framework was implemented as a web-based tool, which guides the users through the entire recommendation processes of Fig. 2. The core advantage of the tool compared to the decision trees depicted in S1-S9 is the fact that the tool automatically restricts the visualization only to the relevant information that is required in each specific step and for the specific use case. It further provides comprehensive profiles of all metrics contained in the Metric Reloaded pool.\n\nThe Metric Reloaded tool is available at https://metrics-reloaded.dkfz.de."
    },
    {
      "markdown": "# SUPPL. NOTE 1 STEP 1 - PROBLEM FINGERPRINTING \n\nThe first step in the framework Step 1: Problem fingerprinting (Fig. 2) requires the user to read the general instructions provided in Suppl. Note 1.1, perform the problem category mapping according to Suppl. Note 1.2, and generate the corresponding problem fingerprint as detailed in Suppl. Note 1.3.\n\n### 1.1 General Instructions\n\nUsers of the Metrics Reloaded framework should read the following instructions prior to metric selection.\n\nInclusion criteria. The Metrics Reloaded framework currently considers problems in which categorical target variables are to be predicted based on a given $n$-dimensional input image. Hence, it covers a broad range of imaging modalities from classical 2D/3D modalities, such as fluorescence, computed tomography (CT) or X-ray imaging, to novel, for example spectral, imaging modalities that yield high-dimensional output per pixel [31]. Classification can occur at pixel, object or image level, resulting in the four problem problem categories covered by the framework and depicted in Fig. 4:\n\nImage-level classification refers to the assignment of one or multiple category labels to the entire image or fixed regions/predefined locations within an image.\nSemantic segmentation refers to the assignment of one or multiple category labels to each pixel. For many segmentation problems, object boundaries are generated in addition to the pixel-wise classification images, which enables the computation of distance-based metrics, such as the Normalized Surface Distance (NSD).\nObject detection refers to the localization and categorization of an unknown number of structures.\nInstance segmentation refers to the localization and delineation of each distinct structure of a particular class. It can be regarded as delivering the tasks of object detection and semantic segmentation at the same time. In contrast to object detection, instance segmentation also involves the accurate marking of the structure boundary. In contrast to semantic segmentation, it distinguishes different structures of the same class.\n\nNotably, the four different categories are mathematically closely related (Fig. 4) as they typically rely on the generation of confusion matrices as a foundation of metric computation. Application examples for all categories can be found in Fig. 5. Importantly, Metrics Reloaded does not require an entire image to be provided as input for the validation. For example, the classification of a Region of Interest (ROI) within a medical image may be required. In this example, the framework would proceed with the ROI as input as if it was an entire image. Furthermore, the shape of the image/input does not need to be rectangular. Finally, context information may be provided along with the input. For example, medical images may be processed along with clinical data to arrive at a diagnosis; video frames may be processed along with preceeding video snippets. Ultimately, only the algorithm output must correspond to an $n$-dimensional image.\n\nPhrasing of the biomedical task. The recommendation framework has been designed in a way to support the metric selection and application process for one specific driving biomedical question. In practice, multiple questions are often addressed with one given data set. For example, a clinician may have the ultimate interest of diagnosing brain cancer in a patient based on a given magnetic resonance imaging (MRI) data set. While this would be phrased as an image-level classification"
    },
    {
      "markdown": "task, an interesting surrogate task could be that of segmentation to assess the quality of tumor delineation. In the case of multiple different driving biomedical questions, a recommendation is generated separately for each question. This specifically holds true for multi-label problems, in which multiple labels can simultaneously be assigned to the same image/object/pixel (e.g., multiple sclerosis and brain tumor both assigned to the same magnetic resonance image). In such a case, the problem should be converted to multiple binary problems, for which the framework is traversed individually.\n\nMatching reference annotations. The metric selection process begins with the step of mapping a given problem with all its intrinsic and data set-related properties to the corresponding problem category via the category mapping shown in Fig. 1. Our framework assumes that the reference annotations of the given data set meet the requirements of the identified problem. Expected formats for both the reference annotations and the algorithm output are provided in Suppl. Note 5.2.\n\nModel-agnostic metric recommendation. Metrics should be chosen based solely on the driving biomedical problem and not be affected by algorithm design choices. For example, the error functions applied in common neural network architectures do not justify the use of corresponding metrics (e.g., validating with Dice Similarity Coefficient (DSC) to match the Dice loss used for training a neural network). Instead, the domain interest should guide the choice of metric, which, in turn, can guide the choice of the loss term.\n\nDealing with multiple classes. Multi-class metrics, such as Accuracy or Matthews Correlation Coefficient (MCC), have the unique advantage that they capture the performance of an algorithm for all classes in a single score without the need for customized class-aggregation schemes. On the other hand, they do now allow for detailed class-specific analyses. Metrics Reloaded therefore generally recommends performing a per-class validation for all classes (in addition to potential multi-class validation). Specifically in segmentation problems, problem properties may differ from class to class (e.g. the size or size variability of target structures). In these rare cases, the problem fingerprint needs to be generated separately for each class and several subprocesses (denoted by the $\\omega$-symbol in the framework overview shown in Fig. 2) need to be traversed separately for each class. Although not common in current validation practice, this may - in theory - lead to different validation metrics for different classes. We speak of class-specific metric pools in this case, which are generated in addition to the multi-class metric pool.\n\nPrimary and secondary metrics. In general, biomedical interest cannot be captured with a single metric. The framework has therefore been designed to recommend multiple complementary metrics for a given task. We assume two main use cases for our framework. In comparative benchmarking studies (e.g., competitive challenges), multiple algorithms or algorithm variants are compared on identical data sets. This requires the ranking of the competing algorithms according to performance. Typically, multiple complementary validation metrics are applied in this use case, resulting in either multiple rankings or a merged ranking that takes all or several metric values into account. We refer to the metrics that contribute to the (primary) ranking(s) as primary metrics. While our framework focuses on the recommendation of primary metrics, users are invited to complement them with secondary metrics according to their specific needs. Secondary metrics can additionally be applied for comprehensive reporting, for example because they reflect complementary properties of interest (e.g., compute time, carbon footprint), or for providing performance measures that are comparable across publications. The computer vision community, for instance, typically reports the Intersection over Union (IoU) rather than the DSC. The second use case of metrics addressed"
    },
    {
      "markdown": "by our framework are validation studies centered around a single algorithm that focus on comprehensive diagnostics rather than comparative assessment. In this case, it is often desired to report as many complementary metrics as possible in order to comprehensively analyze the properties of an algorithm. Users interested in this second use case can ignore the discrimination between primary and secondary metrics.\n\nDecision rule applied to predicted class scores. A classification system in practice operates by making decisions. Converting the raw continuous model outputs - the predicted class scores into discrete decisions is achieved by determining an appropriate decision rule. Common options are detailed in Suppl. Note 1.3 ( $\\rightarrow$ FP2.6: Decision rule applied to predicted class scores). While identifying the optimal decision rule for a classification system is beyond the scope of this work, it is important to know that the choice affects the selection of adequate validation metrics.\n\nIn binary tasks, defining an optimal decision rule boils down to determining a suitable cutoff (i.e., threshold) on predicted class scores (Fig. SN 1.1). In contrast, identifying an optimal decision rule for multi-class problems is generally more complex. A common, intuitive workaround for this challenge is to determine an individual decision rule for each predicted class score. However, this strategy implies that multiple decisions are made for the same input, thus fundamentally changing the task to multi-label classification (in this framework, multi-label classification is handled as separate binary tasks, as detailed in Suppl. Note 1.1 - inclusion criteria). Instead, in practice, a multi-class system requires a single global decision rule for all classes, which amounts to identifying optimal global 'decision regions'. The most common global decision rule is to simply select the class associated with the highest predicted class score, which is typically implemented as an 'argmax' operation and is also referred to as a 'maximum a posteriori' decision. Bayesian decision theory, however, shows that this argmax rule is only the optimal choice in case of equal severity of class confusions (FP2.5.2=False) and no compensation for class imbalances being requested (FP2.5.5=False). If one of these requirements is not fulfilled, a cost-dependent variation of the argmax-rule should be employed (see equation 44 in [51]). Further, the argmax decision rule assumes that predicted class scores are calibrated (see Section 2.6 for details on calibration). Fig. SN 1.1 showcases a hypothetical example of how argmax can be a suboptimal decision rule in combination with miscalibrated model outputs. While a variety of calibration metrics is discussed in Section 2.6, it should be noted that Expected Cost (EC) features a framework to directly validate the effect of a decision rule on the quality of associated decisions. Moreover, any measured negative effect can be associated with the miscalibration of scores, thus guiding users to enhance their decision making.\n\nA further potential pitfall associated with the global decision rule of a classifier can occur when the validation of a multi-class problem is primarily based on multi-threshold metrics. This is because multi-threshold metrics, which do not rely on a decision rule, may conceal the fact that in practice, the optimal global decision rule will not be identified. Thus, the resulting metric scores may overestimate the decision-making performance of a model in practice.\n\nFinally, an important consideration for identifying a decision rule of a classifier is that any data-based optimization or search must be performed on a separate data split different from the validation data. This consideration includes any configuration of re-calibration methods."
    },
    {
      "markdown": "![img-18.jpeg](img-18.jpeg)\n\nExtended Data Fig. SN 1.1. Argmax decision rule for converting predicted class scores to a categorical label. Choosing the class with the highest predicted class score does not necessarily result in the best decisionmaking, as for example measured by Accuracy.\n\nNotation. The notation for our recommendations has been based on Business Process Model and Notation (BPMN) ${ }^{3}$. The individual components used in the recommendation diagrams are explained in Fig. SN 5.1. Please note that we do not strictly follow BPMN to improve clarity of presentation.\n\nTerminology. Terminology may differ substantially across communities. For example, the statistics community prefers the term Positive Predictive Value (PPV) over Precision, as the latter can be confused with the mathematical precision (repeatability) term. In the medical domain, the term validation is used for an independent assessment (untouched test set) of an algorithm, while the machine learning community commonly uses a validation set for hyperparameter tuning. To avoid confusion resulting from unclear terminology, we follow the general terminology of [127] and have included a glossary in the Suppl. Note 5.4.\n\n[^0]\n[^0]:    ${ }^{3}$ https://www.omg.org/spec/BPMN/"
    },
    {
      "markdown": "# 1.2 Problem Category Mapping \n\nThe problem fingerprinting (Step 1 in Fig. 2; see Sec. 1.3) begins with the step of mapping a given problem with all its intrinsic and data set-related properties to the corresponding problem category via the category mapping shown in Fig. 1. This step is crucial for avoiding pitfalls related to the inappropriate choice of the problem category, as detailed in the sister publication of this work [128]. Specifically, when multiple instances of the same structure type can occur in an image, it is typically advisable to phrase the underlying problem as an object detection or instance segmentation problem rather than a semantic segmentation problem (Figs. 1 and SN 1.2).\n![img-19.jpeg](img-19.jpeg)\n\nPrediction (SemS)\n![img-20.jpeg](img-20.jpeg)\n\nPrediction (InS)\n![img-21.jpeg](img-21.jpeg)\n\nPI missed\nPoor segmentation of R2\n\nExtended Data Fig. SN 1.2. Boundary-based metrics in semantic/instance segmentation problems. If multiple structures of the same type can be seen within the same image (here: reference objects $R 1$ and $R 2$ ), it is generally advisable to phrase the problem as instance segmentation (InS; right) rather than semantic segmentation (SemS; left). This way, issues with boundary-based metrics resulting from comparing a given structure boundary to the boundary of the wrong instance in the reference can be avoided. In the provided example, the distance of the red boundary pixel to the reference, as measured by a boundary-based metric in SemS problems, would be zero, because different instances of the same structure cannot be distinguished. This problem is overcome by phrasing the problem as InS. In this case, (only) the boundary of the matched instance (here: R2) is considered for distance computation."
    },
    {
      "markdown": "# 1.3 Generation of the problem fingerprint \n\nMetrics Reloaded is based on the novel concept of problem fingerprinting - the generation of a structured representation of the given problem that captures all aspects that are relevant for metric selection. Specifically, the fingerprint comprises a set of items, each of which represents a specific property of the problem, is either binary or categorical, and must be instantiated by the user. In the following, we will refer to all fingerprint items with the notation FPX.Y, where Y is a numerical identifier and the index $X$ represents one of the following families: general, domain interest-related, target structure-related, data set-related, algorithm output-related.\n\nFingerprint generation begins with the aforementioned mapping of the underlying problem with its intrinsic and data set-related properties to the corresponding problem category via the category mapping (Subprocess S1) shown in Extended Data 1. Next, the user needs to instantiate the category-specific fingerprint items provided in Figs. SN 1.7-SN 1.9 (image-level classification), Figs. SN 1.10/SN 1.11 (semantic segmentation), Figs. SN 1.12-SN 1.14 (object detection), and Figs. SN 1.15-SN 1.17 (instance segmentation).\n\nInstantiating fingerprint items may not always be straightforward due to their binary/categorical nature. Therefore, the Metrics Reloaded tool comprises a \"Why are we asking this question?\" button in each branch based on a fingerprint that may not be straightforward to instantiate. In case of ongoing doubt, the user may traverse all appropriate branches originating from the questions.\n\nImportantly, some fingerprint items require particularly careful consideration and/or are not sufficiently self-explanatory. These are the following:\n\nFP2.6: Decision rule applied to predicted class scores. Modern algorithms output (continuous) predicted class scores. To classify cases in an actual biomedical application (i.e., to make actual decisions), however, applying a decision rule to the scores is required; this amounts to setting a cutoff value in the binary classification case. The deciding factor for whether or not to apply a decision rule during validation should be how much focus is to be put on the quality of the actual decisions of a classification system versus the general quality of its continuous predictions. While some communities have converged to decision rule-based validation (e.g., cell instance segmentation [24]), recent clinical initiatives advocate for decision rule-agnostic validation, arguing that decision rules are often over-optimized on a specific data set, associated results are not transferable across study cohorts (e.g., with differing disease prevalence) and clinical applications (e.g., with differing costbenefit trade-offs for patients), and continuous \"risk scores\" might be beneficial for communicating results with patients [12, 109, 163]. One study goes so far as to call out the common practice of imposing decision rules on continuous predictions as 'dichotomania' [170]. We handle this controversy in current practices by making validation with specific decision rules applied optional (for all tasks except semantic segmentation) and encoding user preferences in this fingerprint. The fingerprint offers the following decision rule strategies (Fig. SN 1.3):\n\nTarget-value based (for binary image-level classification problems) Sometimes, the underlying problem provides a specific target metric value to be reached (e.g., Sensitivity of 0.95), requiring a corresponding cutoff value. In this case, we use the notation Metric@(TargetMetric $=$ TargetValue), for example, Specificity@(Sensitivity $=0.95$ ), denoting the Specificity for a Sensitivity matching the target value (here: 0.95 ). Importantly, this cutoff needs to be configured on a separate and dedicated data split.\nOptimization-based If no specific target value is provided, a data-based decision rule can also be identified by optimizing a primary metric (e.g., $\\mathrm{F}_{1}$ Score) using a dedicated data"
    },
    {
      "markdown": "set for decision rule configuration. Notably, simple (one-dimensional) cutoff scans are only possible in binary tasks, while identifying decision rules in multiple classes represents a computationally and technically complex process.\nArgmax-based An alternative widely used strategy is to simply apply a decision rule based on the 'argmax' operation, which boils down to a cutoff of 0.5 in binary classification problems. The underlying hypothesis for this strategy is that the highest class score resembles the highest probability for the associated class being the true class. In Bayesian theory, this decision rule defines a Bayes classifier, and the theory further shows that the underlying hypothesis only holds for equal severity of class confusions (FP2.5.2=False) and when the class scores are calibrated. Detailed considerations for this decision rule strategy are provided in Sec. 1.1.\nCost-benefit-based If the predicted class scores are calibrated (see FP2.7), and task-related error costs or a risk cutoff (the latter only for binary classification tasks, e.g., \"only treat patients with cancer risk $>10 \\%$ \") are provided, one can apply this decision rule directly to the scores without data-driven optimization. Notably, in binary classification tasks, cost-benefitbased cutoffs often correspond to a cost ratio of True Positive (TP) versus False Positive (FP) (e.g., not more than 10 FP per 1 TP should be treated), while for cost-based cutoffs the explicit costs for both errors FP and False Negative (FN) are defined (see DG3.2, Suppl. Note 2.7.2). Cost-based decision rules are further extendable to multi-class problems [51].\nNo decision rule applied A complementary strategy is to abstain from validating algorithms under a certain decision rule and exclusively report results on multi-threshold metrics (averaging over various cutoffs) instead.\n![img-22.jpeg](img-22.jpeg)\n\nExtended Data Fig. SN 1.3. Illustration of strategies for identifying a decision rule applied to predicted class scores."
    },
    {
      "markdown": "FP2.4 Desired granularity of localization. Selecting a localization criterion operating on a lower/ coarser resolution with regard to provided reference annotations effectively discards spatial information and should be well motivated by the given task (see Fig. SN 1.4). For instance, Box Intersection over Union (IoU) is sometimes employed despite access to pixel-mask annotations (FP4.4) because associated models (object detectors) are considered simpler approaches compared to instance segmentation models. Such simplification may cause problems if structures are not well-approximated by a box shape - especially for 3D shapes, boxes usually constitute poor approximations - or if structures can overlap (FP3.5), causing multi-component masks (see Fig. SN 1.5).\n![img-23.jpeg](img-23.jpeg)\n\nExtended Data Fig. SN 1.4. Selection of a localization criterion that discards spatial information should be well motivated by the given task."
    },
    {
      "markdown": "Box $\\operatorname{loU}=0.3$ : True positive (TP)\nBox loU $\\leq 0.3$ : False positive (FP)\n![img-24.jpeg](img-24.jpeg)\n\nExtended Data Fig. SN 1.5. Bounding boxes are not well-suited for representing complex (top) and disconnected (bottom) shapes. Specifically, they are not well-suited for capturing multi-component structures. Predictions 1 and 2 both end up in a True Positive (TP) detection, as the Box Intersection over Union (loU) is larger than the cutoff 0.3 . However, Prediction 1 does not hit the real objects at all.\n\nFP2.5.5: Compensation for class imbalances. While Accuracy is the de facto standard metric in multi-class settings with balanced class frequencies and error costs, this metric is prone to several pitfalls when class imbalances are present. To give an example, consider the following confusion matrix for a binary classification task: $T P=0, F P=1, F N=1, T N=10,000$, which leads to an accuracy of $\\approx 1$. Three pitfalls pertain to this metric score, which at the same time represent the three reasons why users may want to compensate for the underlying effects caused by the class imbalance:"
    },
    {
      "markdown": "Misleading metric values due to missing reference value for naive classifier: In the provided example, the near-perfect score hides the fact that the same performance could have been achieved by a naive system always predicting the dominant class. Generally, in balanced scenarios, the Accuracy of a naive classifier is known to be \"1/number of classes\", which serves as an important anchor when interpreting the metric scores. However, when class imbalances are present, no such interpretation can be made and the naive reference depends on the class prevalences.\n\nMisleading metric values due to unequal contribution of classes to the metric score: In the provided example, the near-perfect score hides the fact that all samples of the positive class (here: one sample) were misclassified. While all classes contribute similarly to the Accuracy metric in balanced scenarios, frequent classes dominate the performance value in imbalanced settings. While $0 \\%(0 / 1)$ of the rare cases have been classified correctly, the metric achieves an almost perfect score due to the very good performance on the dominant class. Other prevalence-independent metrics, such as Balanced Accuracy (BA), are based on the equal contribution of each class irrespective of prevalence.\n\nMisleading metric values due to missing consideration of predictive values: In the provided example, the near-perfect score hides the fact that the positive predictive value of this system is 0 . Generally, in balanced scenarios, high accuracy scores imply high predictive values (Positive Predictive Value (PPV) and Negative Predictive Value (NPV)), which are important indicators of the utility of a classification system in practice. This is not necessarily the case in imbalanced scenarios, as seen in the provided example, where the PPV is 0 despite a high Accuracy. To compensate for this effect, alternative metrics such as Matthews Correlation Coefficient (MCC) can be considered, which explicitly assess the predictive performance of a classifier.\n\nFP4.2 Class prevalences reflect the population of interest. Class prevalences and their differences across data sets are highly important, although this aspect is often ignored in common validation practice. This can best be explained with the example of diagnostic tests, for example image-based disease classification. While several metrics, such as Sensitivity and Specificity, are independent of class frequencies and measure the inherent properties of the test, other metrics, such as Accuracy, measure the test performance for the specific prevalence of the test set. This is not problematic if the class prevalences of the provided test set reflect the population of interest, but can lead to problems otherwise (see Fig. SN 1.6). This fingerprint should hence be set to true if either the validation interest is constrained to the data set at hand (no future comparison to data sets with different class prevalences is desired) or no variation of prevalences is expected in other cohorts and upon application of the method."
    },
    {
      "markdown": "Inherent properties of a method: Sensitivity $=0.90$, Specificity $=0.80$\n![img-25.jpeg](img-25.jpeg)\n\n* Prevalence-corrected\n\n| Accuracy $=0.85$ | $<$ | Accuracy $=0.89$ | Prevalence |\n| :--: | :--: | :--: | :--: |\n| $M C C=0.70$ | $>>$ | $M C C=0.56$ | dependent |\n\nExtended Data Fig. SN 1.6. Effect of prevalence dependency. An algorithm with specific inherent properties (here: Sensitivity of 0.9 and Specificity of 0.8 ) may perform completely differently on different data sets if the prevalences differ (here: $50 \\%$ (left) and $90 \\%$ (right)) and prevalence-dependent metrics are used for validation (here: Accuracy and Matthews Correlation Coefficient (MCC)). In contrast, prevalence-independent metrics (here: Balanced Accuracy (BA) and the prevalence-corrected Expected Cost (EC)) can be used to compare validation results across different data sets. Used abbreviations: True Positive (TP), False Negative (FN), False Positive (FP) and True Negative (TN)."
    },
    {
      "markdown": "![img-26.jpeg](img-26.jpeg)\n\nExtended Data Fig. SN 1.7. Fingerprint for image-level classification (Part 1). In the case of binary fingerprint items, the blue column shows examples for which the property is true while the red column shows counterexamples. Categorical fingerprint items are only shown in blue. Suppl. Note 1.3 provides more detailed explanations of selected fingerprint items. Used abbreviations: True Positive (TP), False Positive (FP), False Negative (FN), True Negative (TN)."
    },
    {
      "markdown": "![img-27.jpeg](img-27.jpeg)\n\nExtended Data Fig. SN 1.8. Fingerprint for image-level classification (Part 2). In the case of binary fingerprint items, the blue column shows examples for which the property is true while the red column shows counterexamples. Categorical fingerprint items are only shown in blue. Suppl. Note 1.3 provides more detailed explanations of selected fingerprint items. Used abbreviations: True Positive (TP), False Positive (FP), False Negative (FN), True Negative (TN)."
    },
    {
      "markdown": "![img-28.jpeg](img-28.jpeg)\n\nExtended Data Fig. 5N 1.9. Fingerprint for image-level classification (Part 3). In the case of binary fingerprint items, the blue column shows examples for which the property is true while the red column shows counterexamples. Categorical fingerprint items are only shown in blue. Suppl. Note 1.3 provides more detailed explanations of selected fingerprint items. Used abbreviations: Reference (Ref), Prediction (Pred), True Positive (TP), False Positive (FP), False Negative (FN), True Negative (TN)."
    },
    {
      "markdown": "![img-29.jpeg](img-29.jpeg)\n\nExtended Data Fig. SN 1.10. Fingerprint for semantic segmentation (Part 1). In the case of binary fingerprint items, the blue column shows examples for which the property is true while the red column shows counterexamples. Categorical fingerprint items are only shown in blue. Suppl. Note 1.3 provides more detailed explanations of selected fingerprint items. Used abbreviations: Reference (Ref), Prediction (Pred), True Positive (TP), False Positive (FP), False Negative (FN), True Negative (TN)."
    },
    {
      "markdown": "# SEMANTIC SEGMENTATION (Items) PART 2 \n\n| Fingerprint ID and name | Fingerprint illustration | Fingerprint description |\n| :--: | :--: | :--: |\n| Target structure-related properties |  |  |\n| 3.1 Small size of structures relative to pixel size | - | Structures of the provided class are only a few pixels in size. Example: multiple sclerosis lesions in magnetic resonance imaging (MRI) scans. |\n| 3.2 High variability of structure sizes (within an image and/or across images) | ![img-30.jpeg](img-30.jpeg) | The target structures vary substantially in size, such that some structures are several times the size of others. <br> Example: polyps in colonoscopy screening, where some polys are several times the size of others. <br> Counterexample: large organs, such as the liver or the kidneys, which are relatively comparable in size across individuals. |\n| 3.3 Target structures feature tubular shape |  | The target structures feature a tubular shape. <br> Examples: vessels, neurons, microtubules. |\n| 3.4 Possibility of multiple labels per unit (pixel or image) | ![img-31.jpeg](img-31.jpeg) | One pixel may be assigned to multiple reference categories. Example: one pixel assigned to two labels 'tumor core' and 'tumor'. |\n| 3.5 Possibility of overlapping or touching target structures (e.g., medical instruments or cells) | ![img-32.jpeg](img-32.jpeg) | Different instances of a class can overlap or touch each other. Examples: overlapping cells or organisms, such as B8BC010 (worms in a dish); overlapping medical instruments in laparoscopy. |\n\n![img-33.jpeg](img-33.jpeg)\n\nExtended Data Fig. SN 1.11. Fingerprint for semantic segmentation (Part 2). In the case of binary fingerprint items, the blue column shows examples for which the property is true while the red column shows counterexamples. Categorical fingerprint items are only shown in blue. Suppl. Note 1.3 provides more detailed explanations of selected fingerprint items."
    },
    {
      "markdown": "![img-34.jpeg](img-34.jpeg)\n\nExtended Data Fig. SN 1.12. Fingerprint for object detection (Part 1). In the case of binary fingerprint items, the blue column shows examples for which the property is true while the red column shows counterexamples. Categorical fingerprint items are only shown in blue. Suppl. Note 1.3 provides more detailed explanations of selected fingerprint items. Used abbreviations: True Positive (TP), False Positive (FP), False Negative (FN), True Negative (TN)."
    },
    {
      "markdown": "![img-35.jpeg](img-35.jpeg)\n\nExtended Data Fig. SN 1.13. Fingerprint for object detection (Part 2). In the case of binary fingerprint items, the blue column shows examples for which the property is true while the red column shows counterexamples. Categorical fingerprint items are only shown in blue. Suppl. Note 1.3 provides more detailed explanations of selected fingerprint items. Used abbreviations: True Positive (TP), False Positive (FP), False Negative (FN), True Negative (TN)."
    },
    {
      "markdown": "# OBJECT DETECTION (ORD) PART 3 \n\n| Fingerprint ID and name | Fingerprint illustration | Fingerprint description |\n| :--: | :--: | :--: |\n| Data set-related properties |  |  |\n| 4.1 Presence of class imbalance | ![img-36.jpeg](img-36.jpeg) | The class prevalences differ substantially. <br> Example: In a screening application, the positive class (e.g., cancer) may occur extremely rarely. In this case, prevalence-dependent metrics, such as Accuracy, may be extremely misleading. |\n| 4.3 Uncertainties in the reference | The reference is typically only an approximation of the (forever unknown) ground truth. Various sources and types of errors exist, which require special treatment in the context of metric selection. |  |\n| 4.3.1 High inter-/intra-rater variability | ![img-37.jpeg](img-37.jpeg) | The reference can be assumed to be noisy due to high inter-rater variability. |\n| 4.4 Granularity of provided reference annotations <br> Options: <br> - Only position <br> - Rough outline <br> - Exact outline | ![img-38.jpeg](img-38.jpeg) | The granularity of the reference can vary in object detection problems. We distinguish three main categories: <br> Only position: Given an in-dimensional image, the object is represented by its position, encoded in $n$ degrees of freedom (e.g. xy/xyz coordinates of center point). Rough outline: A rough outline of the object is provided, typically given by simple geometric objects such as bounding boxes or ellipsoids. <br> Exact outline: The object is outlined exactly. |\n| 4.5 Non-independence of test cases | ![img-39.jpeg](img-39.jpeg) | The test cases are hierarchically structured, indicating non-independence of test cases. <br> Examples: multiple images of the same patient, hospital or video. |\n| 4.6 Possibility of reference without target structure(s) | ![img-40.jpeg](img-40.jpeg) | There are test cases in which the reference for at least one class is empty. |\n| Algorithm output-related properties |  |  |\n| 5.1 Availability of predicted class scores | ![img-41.jpeg](img-41.jpeg) | Modern algorithms in biomedical image classification output continuous class scores, which are often interpreted as predicted class probabilities. These scores contain relevant information about the performance of a model and are thus crucial for comprehensive and meaningful validation. <br> In object detection, predicted class probabilities are typically available for each detected object. <br> If no predicted class probabilities are available, this property is set to false. |\n| 5.2 Possibility of algorithm output not containing the target structure(s) | ![img-42.jpeg](img-42.jpeg) | The algorithm may yield outputs in which not all classes are present. |\n| 5.3 Possibility of invalid algorithm output (e.g., Prediction is NaN) | ![img-43.jpeg](img-43.jpeg) | The files representing the algorithm output can contain invalid output. Note that an invalid prediction differs from an empty prediction. |\n| 5.4 Possibility of overlapping predictions | ![img-44.jpeg](img-44.jpeg) | Predictions of the algorithm can potentially overlap. |\n\nExtended Data Fig. SN 1.14. Fingerprint for object detection (Part 3). In the case of binary fingerprint items, the blue column shows examples for which the property is true while the red column shows counterexamples. Categorical fingerprint items are only shown in blue. Suppl. Note 1.3 provides more detailed explanations of selected fingerprint items. Used abbreviations: Reference (Ref), Prediction (Pred)."
    },
    {
      "markdown": "![img-45.jpeg](img-45.jpeg)\n\nExtended Data Fig. SN 1.15. Fingerprint for instance segmentation (Part 1). In the case of binary fingerprint items, the blue column shows examples for which the property is true while the red column shows counterexamples. Categorical fingerprint items are only shown in blue. Suppl. Note 1.3 provides more detailed explanations of selected fingerprint items. Used abbreviations: True Positive (TP), False Positive (FP), False Negative (FN), True Negative (TN)."
    },
    {
      "markdown": "![img-46.jpeg](img-46.jpeg)\n\nExtended Data Fig. SN 1.16. Fingerprint for instance segmentation (Part 2). In the case of binary fingerprint items, the blue column shows examples for which the property is true while the red column shows counterexamples. Categorical fingerprint items are only shown in blue. Suppl. Note 1.3 provides more detailed explanations of selected fingerprint items. Used abbreviations: Reference (Ref), True Positive (TP), False Positive (FP), False Negative (FN), True Negative (TN)."
    },
    {
      "markdown": "![img-47.jpeg](img-47.jpeg)\n\nExtended Data Fig. SN 1.17. Fingerprint for instance segmentation (Part 3). In the case of binary fingerprint items, the blue column shows examples for which the property is true while the red column shows counterexamples. Categorical fingerprint items are only shown in blue. Suppl. Note 1.3 provides more detailed explanations of selected fingerprint items. Used abbreviations: Reference (Ref), Prediction (Pred)."
    },
    {
      "markdown": "# SUPPL. NOTE 2 STEP 2 - METRIC SELECTION \n\nAs a foundation for the metric selection process, the Metrics Reloaded consortium compiled a set of common reference-based validation metrics (Suppl. Note 2.1). The framework leverages the problem fingerprints to guide the user through the process of selecting an appropriate set of category-specific reference-based validation metrics while being made aware of potential pitfalls related to individual choices. A bird's eye perspective of the process is shown in Fig. 2. A detailed explanation for the selection of reference-based metrics is provided separately for all four problem categories in Suppl. Notes 2.2-2.5. Details on selecting appropriate calibration metrics, if desired, are given in Suppl. Note 2.6. The corresponding formal decision trees (subprocesses) along with corresponding decision guides are shown in Extended Data Figs. 1 - 9 and Suppl. Note 2.7, respectively.\n\n### 2.1 Metrics Reloaded pool of reference-based metrics\n\nThe Metrics Reloaded pool of common reference-based validation metrics is shown in Tab. SN 2.2. Most of these metrics are directly or indirectly based on the cardinalities of the confusion matrix (i.e., the true (T)/false (F) positives (P)/negatives (N) in binary problems). For the purpose of metric recommendation, we follow the terminology in the sister publication of this work [127] and classify the metrics into counting metrics that operate directly on a single fixed confusion matrix and express the metric value as a function of the cardinalities, multi-threshold metrics that operate on a dynamic confusion matrix, and distance-based metrics, designed to measure differences between boundaries, volumes, center (line)s or shapes [127]. In addition to these, our framework considers metrics designed to measure calibration capabilities of models.\n\nImportantly, many popular counting metrics are closely related. In Fig. SN 2.1, we categorize the relationship as follows.\n(1) Synonyms: For some metrics, various terms exist. Popular examples are:\n\n- Recall $=$ Sensitivity $=$ True Positive Rate $(\\mathrm{TPR})=$ Hit rate\n- Positive Predictive Value (PPV) = Precision\n- Dice Similarity Coefficient (DSC) $=\\mathrm{F}_{1}$ Score (at pixel level)\n(2) Mutually computable from each other: Some metrics are directly computable from each other without further information. Popular examples are:\n- Accuracy $=1$ - Error Rate (ER)\n- $\\mathrm{DSC}=(2$ \"Intersection over Union (IoU))/(1 + IoU)\n- Balanced Accuracy $=($ Bookmaker Informedness $(\\mathrm{BM})+1) / 2$\n(3) Generalization/Instantiation: Some metrics are an instantiation of others. Popular examples are:\n- DSC is an instantiation of the $\\mathrm{F}_{\\beta}$ Score for $\\beta=1$\n- Accuracy is a specialization of Expected Cost (EC), where costs are chosen as \"0-1-costs\", meaning $c_{i i}=0$ and $c_{i j}=1$ otherwise.\n- Balanced Accuracy (BA) is a specialization of EC, where costs are chosen such that $c_{i i}=0$ and $c_{i j}=\\frac{1}{C P_{i}}$ with $P_{i}$ reflecting the class prevalence of class $i$ and $C$ denoting the number of classes.\n(4) Mutually computable under certain conditions: Assuming a simple problem setup, additional metrics coincide. Popular examples are:\n- Assuming perfect class balance in a binary problem, BA = Accuracy = Cohen's Kappa (CK)\n- Assuming $\\beta=1$ allows to compute IoU and Jaccard index from $\\mathrm{F}_{\\beta}$ Score"
    },
    {
      "markdown": "(5) Other notable relationship: Some metrics share another notable relationship. These are detailed in the metric cheat sheets (Suppl. Note 3.1)\n\nCheat sheets for all metrics, comprising basic information such as definition and links to reference implementations, relationships to other metrics, and Metrics Reloaded recommendations for their usage, can be found in Suppl. Note 3.1."
    },
    {
      "markdown": "Extended Data Tab. SN 2.2. Overview of recommended reference-based metrics. For each metric, name, acronym, synonyms, reference to the definition and illustration, range and corresponding problem categories are provided. The direction of the arrow in the 'range' column indicates whether higher (up) or lower scores (down) are better. A detailed introduction and discussion of all metrics can be found in the sister publication of this work [127]. ImLC: image-level classification; SemS: semantic segmentation; ObD: object detection; InS: instance segmentation.\n\n| Metric | Acronym | Synonyms | Definition | Range |  | Recommended for SemS |  | InS |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| Counting Metrics |  |  |  |  |  |  |  |  |\n| Accuracy |  |  | [33, 148] | $[0,1] \\uparrow$ | x |  |  |  |\n| Balanced Accuracy | BA |  | [33, 148] | $[0,1] \\uparrow$ | x |  |  |  |\n| Weighted Cohen's Kappa | WCK | Cohen's Kappa Coefficient, Kappa Statistic, Kappa Score | [34] | $[-1,1] \\uparrow$ | x |  |  |  |\n| centerline Dice Similarity Coefficient | clDice |  | [140] | $[0,1] \\uparrow$ |  | x |  | x |\n| Dice Similarity Coefficient | DSC | Sørensen-Dice Coefficient, $\\mathrm{F}_{1}$ Score, Balanced F Score | [45] | $[0,1] \\uparrow$ |  | x |  | x |\n| Expected Cost | EC |  | [18, 51] | $[-60,66] \\downarrow$ | x |  |  |  |\n| $F_{d j}$ Score |  |  | [36] | $[0,1] \\uparrow$ | x | x | x | x |\n| False Positives per Image ${ }^{\\mathrm{T}}$ | FPFI |  | [10, 139] | $[0,66] \\downarrow$ |  |  | x | x |\n| Intersection over Union | IoU | Jaccard Index, Tanimoto Coefficient | [62] | $[0,1] \\uparrow$ |  | x |  | x |\n| Matthews Correlation Coefficient | MCC | Phi Coefficient | [103] | $[-1,1] \\uparrow$ | x |  |  |  |\n| Panoptic Quality | PQ |  | [77] | $[0,1] \\uparrow$ |  |  |  | x |\n| Net Benefit | NB |  | [163] | $[-60,66] \\uparrow$ | x |  |  |  |\n| Negative Predictive Value ${ }^{*}$ |  |  | $[16,148]$ | $[0,1] \\uparrow$ | x |  |  |  |\n| Positive Likelihood Ratio | LR+ | Likelihood Ratio Positive, Likelihood Ratio for Positive Results | [7] | $[0,66] \\uparrow$ | x |  |  |  |\n| Positive Predictive Value ${ }^{*}$ | PPV | Precision | $[16,33,148]$ | $[0,1] \\uparrow$ | x |  | x | x |\n| Sensitivity ${ }^{*}$ |  | Recall, Hit Rate, True Positive Rate (TPR) | $[16,33,148]$ | $[0,1] \\uparrow$ | x |  | x | x |\n| Specificity ${ }^{*}$ |  | Selectivity, True Negative Rate (TNR) | $[16,33,148]$ | $[0,1] \\uparrow$ | x |  |  |  |\n\nMulti-threshold Metrics\n\n| Area under the Receiver Operating <br> Characteristic Curve | AUROC | Area under the curve (AUC), AUC Re- <br> ceiver Operating Characteristic (ROC), <br> C-Index, C-Statistics | $[60]$ | $[0,1] \\uparrow$ | x |  |  |  |\n| :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- |\n| Average Precision | AP |  | $[94]$ | $[0,1] \\uparrow$ | x | x | x |  |\n| Free-Response Receiver Operating <br> Characteristic Score | FROC Score |  | $[10,159]$ | $[0,1] \\uparrow$ |  | x | x |  |\n\nDistance-based Metrics\n\n| Average Symmetric Surface Distance | ASSD |  | $[172]$ | $[0,66] \\downarrow$ |  | x |  | x |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| Boundary Intersection over Union | Boundary IoU |  |  | [28] | $[0,1] \\uparrow$ |  | x |  | x |\n| Hausdorff Distance | HD | Hausdorff <br> $\\begin{array}{ll}\\text { Metric, } \\\\ \\text { preir-Hausdorff Distance, Maximum }\\end{array}$ Symmetric Surface Distance |  | [64] | $[0,66] \\downarrow$ |  | x |  | x |\n| Mean Average Surface Distance | MASD |  |  | [13] | $[0,66] \\downarrow$ |  | x |  | x |\n| Normalized Surface Distance | NSD | Normalized Surface Dice, Surface Distance, Surface Dice |  | [116] | $[0,1] \\uparrow$ |  | x |  | x |\n| $\\mathrm{X}^{\\text {th }}$ Percentile Hausdorff Distance | $\\mathrm{X}^{\\text {th }}$ Percentile HD |  |  | [64] | $[0,66] \\downarrow$ |  | x |  | x |\n\nCalibration Metrics\n\n| Brier Score | BS |  | $[20]$ | $[0,1] \\downarrow$ | x |  | x | x |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| Class-wise Calibration Error | CWCE |  | [83, 84] | $[0,1] \\downarrow$ | x |  | x | x |\n| Expected Calibration Error | ECE |  | [37, 110] | $[0,1] \\downarrow$ | x |  | x | x |\n| Expected Calibration Error Kernel Density Estimate | ECE ${ }^{\\text {KDE }}$ |  | [123] | $[0,1] \\downarrow$ | x |  | x | x |\n| Kernel Calibration Error | KCE |  | [36, 167] | $[0,1] \\downarrow$ | x |  | x | x |\n| Negative Log Likelihood | NLL | Cross Entropy Loss | [39] | $[0,66] \\downarrow$ | x |  |  |  |\n| Root Brier Score | RBS |  | $[56]$ | $[0,1] \\downarrow$ | x |  | x | x |\n\n[^0]\n[^0]:    ${ }^{*}$ This metric is best used in combination with another metric using a predefined target value (see \"Target value-based cutoff\" in the definition of 192.6: Cutoff on predicted class scores (Suppl. Note 1.2)."
    },
    {
      "markdown": "![img-48.jpeg](img-48.jpeg)\n\nExtended Data Fig. SN 2.1. Common counting metrics and their relation to each other. The depicted metrics comprise the counting metrics of the Metrics Reloaded pool (Tab. SN 2.2) as well as closely related metrics, namely Error Rate (ER), Bookmaker Informedness (BM), Markedness (MK), and Cohen's Kappa (CK) with synonyms. Panoptic Quality (PQ), centerline Dice Similarity Coefficient (clDice), and False Positives per Image (FPPI) have been excluded as they rely on more information than solely the confusion matrix."
    },
    {
      "markdown": "# 2.2 Recommendations for Image-level Classification \n\n## Essentials\n\nFPX.Y refers to a fingerprint item detailed in Figs. SN 1.7-SN 1.9.\nSX refers to a subprocess in Extended Data Figs. 2-5.\nDGX.Y refers to a decision guide in Suppl. Notes 2.7.1-2.7.4.\n\nThis section provides recommendations for selecting common reference-based metrics for imagelevel classification problems. As depicted in Fig. 2, these common metrics can then be complemented by application-specific metrics as well as non-reference-based metrics (assessing run time or carbon footprint, for example).\n\nImage-level classification refers to the process of assigning one or multiple labels (classes) to an image. Modern algorithms usually output predicted class scores between 0 and 1 for every image and class, which are often interpreted as the probability of the image belonging to a specific class. In binary classification, a threshold can be applied to convert the continuous scores to a classification decision (e.g. cancer $=$ true for values above 0.5 ). In multi-class classification, the class associated with the highest predicted score is often selected as the final prediction ('argmax' operation). The most common strategies for converting predicted class scores into discrete decisions are captured in the fingerprint FP2.6 Decision rule applied to predicted class scores and are detailed in Suppl. Note 1.3.\n\nComparing the algorithm predictions with the reference labels enables the generation of a confusion matrix, which captures the number of correct class assignments on the diagonal for each class and the numbers for all possible class confusions in the remaining cells. In the binary case, these numbers, here referred to as the cardinalities, are simply the true/false positives/negatives arranged in a $2 \\times 2$ matrix. Counting metrics operate on this matrix by relating the cardinalities of different matrix entries [127]. They can be classified into multi-class counting metrics that operate on the full, potentially multi-class confusion matrix, such as Accuracy, Matthews Correlation Coefficient (MCC) and Expected Cost (EC), and per-class counting metrics that validate the performance of a particular class of interest defined as the positive class (e.g. with a one-vs-rest comparison for multi-class scenarios), such as the $\\mathrm{F}_{\\beta}$ Score. Per-class validation is typically recommended (see below) to obtain an in-depth understanding of the performance of each individual class, as multi-class metrics may potentially hide poor performance of individual classes. All counting metrics differ exclusively in which cardinalities of the confusion matrix they use and how they are combined."
    },
    {
      "markdown": "![img-49.jpeg](img-49.jpeg)\n\nExtended Data Fig. SN 2.2. Metrics Reloaded recommendation framework for image-level classification at a glance.\n\nCounting metrics in general reflect the fact that systems in practice need to define a strategy for converting the predicted class scores (if available) into actual decisions. Choosing a decision rule for the generation of a confusion matrix, however, is not necessarily straightforward, and counting metrics may fail to capture the full capacities of a classifier by restricting performance analysis to a single working point on the decision curve [128] (Fig. SN 1.3). Multi-threshold metrics (Fig. SN 2.9 such as Area under the Receiver Operating Characteristic Curve (AUROC) overcome the limitation of a potentially arbitrary threshold by calculating metric scores based on a range of thresholds. They are commonly only defined for binary classification (again, one-versus-rest validation can be performed) and relate basic complementary properties, such as Sensitivity and Specificity in the case of AUROC, or Sensitivity and Positive Predictive Value (PPV) in the case of Average Precision (AP), to each other. The metric value is then obtained by computing the area under the resulting curve [128].\n\nWhile both counting metrics and multi-threshold metrics measure the discrimination capabilities of a classifier, they do not assess whether the predicted class scores reflect the true probability of cases belonging to the predicted class. An orthogonal class of metrics has therefore been designed to assess the interpretability of classifier outputs. As detailed in Suppl. Note2.6, these calibration metrics can roughly be categorized in metrics that assess discrimination and calibration quality together, such as the Brier Score (BS), and those that assess only calibration, such as the Expected Calibration Error (ECE)).\n\nTaking into account these considerations as well as the complementary strengths and weaknesses of classification metrics, we recommend the following process for selecting reference-based classification metrics (blue path in Fig. 2 and Fig. SN 2.2):\n\n1: Select multi-class counting metric (if any): Multi-class counting metrics have the unique advantage that they capture the performance of an algorithm for all classes in a single value. With the ability to take into account all entries of the multi-class confusion matrix, they provide a holistic measure of performance without the need for customized class-aggregation schemes. We therefore recommend the selection of a multi-class counting metric based on Subprocess S2 (Extended Data Fig. 2) if a decision rule should be applied to the predicted class scores (FP2.6). In some use cases and especially in the presence of ordinal data, there may be an unequal severity of class confusions (FP2.5.2 = TRUE), implying that different costs to"
    },
    {
      "markdown": "be applied to different errors reflected by the confusion matrix must be available (FP2.5.4 = TRUE). In this case, the only viable options are Weighted Cohen's Kappa (WCK) (Fig. SN 3.18) and EC (Fig. SN 3.6). While WCK is widely used, it comes with severe drawbacks (see Suppl. Note 2.7.1 for details), such as high prevalence dependency and 'paradoxical results' [165] for the most common variant based on quadratic weights. For this reason, the consortium recommends EC as the default choice for the described scenario. In the case of equal costs, Accuracy (Fig. SN 3.2) is the most widely used multi-class metric, but we recommend it in only one specific scenario: when the class prevalences in the data set reflect those in the target population (FP4.2) and potential class imbalances should not be compensated for. In the more general case, the decision boils down to either picking one of the prevalence-independent metrics EC or Balanced Accuracy (BA) (Fig. SN 3.3), which is specifically recommended if the class prevalences do not reflect the target population, or MCC (Fig. SN 3.10), which has the important property that it requires not only the class-specific Sensitivities (i.e. Sensitivity and Specificity in the binary case) but also the corresponding predictive values (PPV and Negative Predictive Value (NPV)) to be high (see Fig. SN 2.3). Irrespective of the metric choice, we recommend additionally reporting the whole confusion matrix in the case of a reasonable number of classes.\n2: Select per-class counting metric (if any): As detailed class-specific analyses are not possible with multi-class counting metrics, which may potentially hide the poor performance of individual classes, we recommend an additional per-class validation with metrics selected according to Subprocess S3 (Extended Data Fig. 3). To this end, class-specific metric pools are generated. The choice of metric depends primarily on the decision rule applied to the predicted class scores (FP2.6; see Suppl. Note 1.3 for a detailed explanation). If a target value-based strategy is preferred, the decision rule applied to the predicted class scores is optimized such that a specific target value (e.g. Sensitivity $=0.95$; see Fig. SN 3.16) is achieved (see Fig. SN 1.3). Complementary metrics, such as Specificity (Fig. SN 3.17), can then be reported for this fixed value of the target metric (see decision guide 3.1 in Suppl. Note 2.7.2). In this case, the target metric is only reported for the specified target class. If a cost-benefit-based strategy is chosen (only recommended for binary classification tasks), we recommend selecting either Net Benefit (NB) (explicit risk-centric view; Fig. SN 3.11) or EC (cost-centric view; Fig. SN 3.6) (see decision guide 3.2 in Suppl. Note 2.7.2). In contrast, in the case of optimization-based or argmax-based decision rules, the metric choice should be made between Sensitivity, Positive Likelihood Ratio (LR+) (Fig. SN 3.14), and F- $\\beta$ Score (Fig. SN 3.7) (see decision guide 3.3 and 3.4 in Suppl. Note 2.7.2).\n3: Select multi-threshold metric: To obtain a more comprehensive picture of the discrimination performance of a classifier, we always recommend the selection of a multi-threshold metric according to Subprocess S4 (Extended Data Fig. 4), irrespective of the decision rule. Multithreshold metrics are again applied per class. A particular strength of AUROC (Fig. SN 3.19) is the fact that it is well-interpretable, as the value simply reflects the probability of a sample from the positive class being assigned a higher predicted class score compared to a sample from the negative class. Furthermore, it is prevalence-independent and therefore well-suited for comparison of performance across different data sets. AP (Fig. SN 3.20), on the other hand, is a prevalence-dependent metric, which comes with the advantage that predictive values are considered. This may be a crucial property in class-imbalanced scenarios where the focus is to be put on the rare class while AUROC scores are dominated by the frequent class and may lead to overly optimistic interpretation.\n4: Select calibration metric (if any): If the calibration of a method should be assessed in addition to its discrimination capabilities (FP2.7.1), one or multiple calibration metrics should"
    },
    {
      "markdown": "be chosen based on Subprocess S5 (Extended Data Fig. 5). Details on this process are provided in Suppl. Note 2.6."
    },
    {
      "markdown": "![img-50.jpeg](img-50.jpeg)\n\nExtended Data Fig. SN 2.3. Failure of prevalence-independent metrics in a screening scenario with high class imbalance. Intuitively, the system is substantially better than random guessing because almost all positive cases have been retrieved out of a large database. At the same time, it is not perfect because only about half of the retrieved cases are correctly classified. However, common popular metrics either indicate near-perfect performance (Sensitivity and Specificity close to 1) or random performance (normalized EC (ECN) about 1). Only the $F_{1}$ Score and Matthews Correlation Coefficient (MCC) reflect the intuitive scoring of \"quite good, but not perfect\" because they incorporate predictive values."
    },
    {
      "markdown": "# 2.3 Recommendations for Semantic segmentation \n\n## Essentials\n\nFPX.Y refers to a fingerprint item detailed in Figs. SN 1.10-SN 1.11.\nSX refers to a subprocess in Extended Data Figs. 6-7.\nDGX.Y refers to a decision guide in Apps. 2.7.5-2.7.6.\n\nThis section provides recommendations for selecting common reference-based metrics for semantic segmentation problems. As depicted in Fig. 2, these common metrics can then be complemented by application-specific metrics as well as non-reference-based metrics (assessing run time or carbon footprint, for example).\n\nSemantic segmentation is commonly defined as the process of partitioning an image into multiple segments/regions. To this end, one or multiple labels are assigned to each pixel such that pixels with the same label share certain characteristics. Semantic segmentation can therefore also be regarded as pixel-level classification. As in image-classification problems, predicted class probabilities are typically calculated for each pixel deciding on the class affiliation based on a threshold over the class scores [6]. In semantic segmentation problems, the pixel-level classification is typically followed by a post-processing step, in which boundary pixels are identified.\n\nThe most common semantic segmentation metrics (e.g. Dice Similarity Coefficient (DSC) and Intersection over Union (IoU)) are per-class counting metrics - here referred to as overlap-based metrics, which measure the overlap between the reference annotation and the prediction of the algorithm. They can be considered the de facto standard for assessing segmentation quality and are well-interpretable.\n\nA key weakness of overlap-based metrics is their shape and contour unawareness [127]. A second class of metrics, the distance-based metrics, therefore explicitly assess certain spatial characteristics such as the quality of structure centers or boundaries. Note that in scenarios in which multiple structures of the same type are present within the same image (e.g., in multiple sclerosis (MS) lesion segmentation), a potential pitfall is related to comparing a given structure boundary to the boundary of the wrong instance in the reference (Fig. SN 1.2). Similar issues arise in the case of completely missed instances. In such scenarios, we explicitly recommend reconsideration to phrase the problem as an instance segmentation problem. If semantic segmentation remains the chosen category, we advise against the use of distance-based metrics, as these are not designed for cases where confusion of boundaries across different instances can occur."
    },
    {
      "markdown": "![img-51.jpeg](img-51.jpeg)\n\nExtended Data Fig. SN 2.4. Metrics Reloaded recommendation framework for semantic segmentation at a glance.\n\nBased on the complementary strengths and weaknesses of common segmentation metrics [127], we recommend the following process for segmentation problems (orange path in Fig. 2 and Fig. SN 2.4):\n\n1: Select overlap-based metric (if any): We recommend selecting an overlap-based metric by default unless the target structures are consistently small (FP3.1) and the reference may be noisy (FP4.3.1). As detailed in Subprocess S6 for selecting overlap-based metrics (Extended Data Fig. 6), our default recommendation is the DSC (Fig. SN 3.5), which is almost identical to the IoU (Fig. SN 3.9). The $\\mathrm{F}_{\\beta}$ Score (Fig. SN 3.7), which can be seen as a generalization of the $\\mathrm{DSC}=\\mathrm{F}_{1}$ Score, is an alternative if there is a preference for either False Positive (FP) or False Negative (FN). In the specific case of tubular structures (FP3.3), the centerline Dice Similarity Coefficient (clDice) (Fig. SN 3.4) as an increasingly used variant of the DSC can also be applied (optionally in addition to the DSC).\n2: Select boundary-based metric (if any): To compensate for the weakness of overlap-based metrics, specifically their shape unawareness and limitations when dealing with small structures or high size variability [127], our general recommendation is to complement an overlapbased metric with a boundary-based metric according to Subprocess S7 (Extended Data Fig. 7). If annotation imprecisions should be compensated for, our default recommendation is the Normalized Surface Distance (NSD) (Fig. SN 3.26). Otherwise, the fundamental user preference guiding metric selection is whether errors should be penalized by existence or distance (FP2.5.6). In the case of existence-based penalization, Boundary IoU (Fig. SN 3.23) should be preferred over NSD if even slight contour errors can be seen as crucial inconsistencies that should be assessed. In the case of distance-based penalization, Mean Average Surface Distance (MASD) (Fig. SN 3.25) is our default recommendation, as it has mathematical advantages over Average Symmetric Surface Distance (ASSD) (Fig. SN 3.22; see decision guide 7.2 in Suppl. Note 2.7.6) and is not as sensitive to annotation outliers as Hausdorff Distance (HD) and its variants (Fig. SN 3.24)."
    },
    {
      "markdown": "While overlap- and distance-based metrics are the standard metrics used by the general computer vision community, biomedical applications sometimes have special domain-specific requirements. To accommodate specific domain needs, the standard metrics can therefore be complemented by further 'application-specific' metrics as shown in Fig. 2. In medical imaging, for example, the actual volume of an object, for example a tumor, may be of particular interest (FP2.2). In this case, volume metrics such as the Absolute or Relative Volume Error and the Symmetric Relative Volume Difference can be computed [112]. Also, the clDice can be complemented by application-specific connectivity metrics, for instance in the case of tubular structures [40, 114]. Similarly, the explicit agreement of object centers (e.g., in cells) or shapes may be of interest. Note that the latter can often be addressed by choosing a boundary metric with a high tolerance. In other cases, shape agreement may be measured by comparing specific object properties, such as curvatures or principal components. Finally, compliance with prior knowledge, such as hierarchical label structure (FP3.4), can be measured with additional application-specific metrics.\n\nOnce a set of metrics has been selected, an appropriate aggregation strategy should be chosen. We recommend handling of 'Not a Number's (NaNs) by setting the corresponding metric value to the worst possible value (see Fig. SN 2.5). In the case of distance-based metrics such as the HD, the image diagonal can be chosen, for example (see Fig. SN 2.6). In a benchmarking setting, an alternative lies in using a \"rank-then-aggregate\" strategy [168]. A test case with a NaN value can then be assigned the worst rank for the given image.\n![img-52.jpeg](img-52.jpeg)\n\nExtended Data Fig. SN 2.5. Effect of missing values when aggregating metric values. In this example, ignoring missing values leads to a substantially higher Dice Similarity Coefficient (DSC) compared to setting missing values to the worst possible value (here: 0 )."
    },
    {
      "markdown": "![img-53.jpeg](img-53.jpeg)\n\nExtended Data Fig. SN 2.6. Effect of missing values when aggregating metric values for metrics without fixed boundaries (here: Hausdorff Distance (HD)). In this example, ignoring or treating missing values in different ways leads to substantially different HD values.\n\nIn multi-class settings, the metric values for the individual classes can be combined in a single score. This can be done via macro averaging over class-specific scores, indicating equal importance for each class (FP2.5.1 = FALSE) and an interest to compensate for potential class imbalance (FP2.5.5 $=$ TRUE). Alternatively, weighted averaging, which takes the unequal interest across classes and/or different class prevalences into account, may be performed (Fig. SN 2.7).\n![img-54.jpeg](img-54.jpeg)\n\nExtended Data Fig. SN 2.7. Effect of unequal handling of classes. Simple averaging (macro-averaging) of the Accuracy ignores the unequal importance of classes, given by pre-defined weights of classes. Incorrect predictions are indicated by a red square."
    },
    {
      "markdown": "# 2.4 Recommendations for Object detection \n\n## Essentials\n\nFPX.Y refers to a fingerprint item detailed in Figs. SN 1.12-SN 1.14.\nSX refers to a subprocess in Extended Data Figs. 3-4 and Extended Data Figs. 8-9.\nDGX.Y refers to a decision guide in Suppl. Note 2.7.2 - 2.7.4, 2.7.7-2.7.8.\n\nThis section provides recommendations for selecting common reference-based metrics for object detection problems. As depicted in Fig. 2, these common metrics can then be complemented by application-specific metrics as well as non-reference-based metrics (assessing run time or carbon footprint, for example).\n\nObject detection refers to the detection and localization of structures of one or multiple categories. A key feature of object detection algorithms is their ability to distinguish different instances of the same class, which may be of crucial domain interest (see 1). The confusion matrix is generated by comparing reference objects to predicted objects. Based on their matching (see below), True Positives (TPs) (prediction matched to reference object), False Positives (FPs) (prediction without assigned reference object) and False Negatives (FNs) (references without assigned predictions) can be computed. While the design choices in image-level classification are primarily related to the selection of discrimination and calibration metrics (see Suppl. Note 2.2), additional design decisions must be made in object detection due to the object-centric validation. Specifically, a localization criterion must be chosen to determine whether a predicted object spatially corresponds to one of the reference objects and vice versa. To this end, an appropriate representation of objects must be chosen. Typical choices are bounding boxes or other approximating shapes. As the object localization step might lead to ambiguous matchings, such as two predictions being assigned to the same reference object, an assignment strategy needs to be picked as well. Overall, predictions in object detection have to fulfill the following requirements to be labeled TP: Firstly, the localization criterion must be fulfilled (spatial correspondence). Secondly, the predicted class must match the class of the reference object (always given in the binary case). Finally, the assignment strategy must yield a matching reference object for the given prediction. The recommended localization criteria are provided in Suppl. Note 3.1.3."
    },
    {
      "markdown": "![img-55.jpeg](img-55.jpeg)\n\nExtended Data Fig. SN 2.8. Metrics Reloaded recommendation framework for object detection at a glance.\n\nBased on the choice of localization criterion and assignment strategy, standard classification metrics can be computed on object level. Importantly from a mathematical perspective in this context, the absence of True Negatives (TNs) in object detection problems renders many popular classification metrics (e.g., Accuracy, Specificity, Area under the Receiver Operating Characteristic Curve (AUROC)) invalid. Based on these considerations and taking into account all the complementary strengths and weaknesses of existing metrics [127], we propose the following steps for object detection problems (green path in Fig. 2 and SN 2.8):\n\n1: Select localization criterion: The selection of the localization criterion should be performed according to Subprocess S8 (Extended Data Fig. 8). If a rough outline of objects is desired, rather than just obtaining the object position (FP2.4 Desired granularity of localization, see Suppl. Note 1.3 for details), our recommendation is the Box/Approximation Intersection over Union (IoU) (Fig. SN 3.38). If only the position of objects is relevant from a domain interest (e.g. for determining the location of cells), the Center Distance (Fig. SN 2.28) is often an attractive option, although Mask IoU $>0$ (Fig. SN 3.39) or Point inside Mask/Box/Approximation (Fig. SN 3.40) are viable alternatives in case of fine-granular reference annotations (FP4.4 Granularity of provided references).\n2: Select assignment strategy: The recommendations for the assignment strategy are provided in Subprocess S9 (Extended Data Fig. 9). In case of the availability of predicted class scores (FP5.1 = TRUE) Greedy (by Score) Matching (Fig. SN 3.41) is our default recommendation."
    },
    {
      "markdown": "Otherwise, Greedy (by Localization criterion) Matching (Fig. SN 3.42), Optimal (Hungarian) Matching (Fig. SN 3.43) or Matching via Overlap $>0.5$ (Fig. SN 3.44) are viable options, as detailed in decision guide 9.1 in Suppl. Note 2.7.8. The user must also decide whether double assignments should be punished (FP2.5.8).\n3: Select classification metric(s) (if any): Once objects have been located and assigned to reference objects, generation of a confusion matrix (without TN) is possible. The final step therefore simply comprises choosing suitable classification metrics.\na: Select counting metric (if any): The selection of a per-class counting metric according to Subprocess S3 (Extended Data Fig. 3) is governed by the decision rule (FP2.6). If a target value for a specific target metric is provided (e.g. Sensitivity $=0.95$; Fig. SN 3.16), complementary metrics such as Positive Predictive Value (PPV) (Fig. SN 3.15) can be assessed at the provided point on the decision curve (Fig. SN 2.9). Otherwise, we recommend the $\\mathrm{F}_{\\beta}$ Score (Fig. SN 3.7) as a counting metric.\nb: Select multi-threshold metric: Several subfields of biomedical image analysis have converged to choosing solely a counting metric as the primary metric. This choice seems to be a historical artifact from when algorithms did not provide predicted class scores. We generally recommend not discarding the scores typicalls provided by current algorithms and disagree with the practice of basing performance assessment solely on a single, potentially suboptimal, decision rule applied to the predicted class scores. Instead, we primarily propose selecting a multi-threshold metric (Subprocess S4, Extended Data Fig. 4) to present a more holistic picture of performance. As multi-threshold metric, we recommend Average Precision (AP) (Fig. SN 3.20) or Free-Response Receiver Operating Characteristic (FROC) Score (Fig. SN 3.21), depending on whether an easy interpretation (FROC Score) or a standardized metric (AP) is preferred (see decision guide 4.2 in Suppl. Note 2.7.3).\n\nNote that the previous description implicitly assumed single-class problems, but generalization to multi-class problems is straightforward by applying the validation per class."
    },
    {
      "markdown": "![img-56.jpeg](img-56.jpeg)\n\nExtended Data Fig. SN 2.9. Principle of multi-threshold metrics (top) and per-class counting metrics with application-driven thresholds (bottom). Rather than being based on a static threshold (e.g., for generating the confusion matrix), multi-threshold-based metrics integrate over a range of thresholds. Prominent examples are the Area under the Receiver Operating Characteristic Curve (AUROC) (also known as Area under the curve (AUC) or AUC Receiver Operating Characteristic (ROC)) and the Area under the Precision-Recall (PR) curve (AUC PR). Cardinalities, i.e., the true (T)/false (F) positives (P)/negatives (N), are computed based on a threshold (e.g., 0.5) of predicted class probabilities (left). Based on those values, Sensitivity (also known as Recall) and 1 - Specificity/Positive Predictive Value (PPV) are calculated and plotted against each other (right). The procedure is repeated for several thresholds, resulting in the ROC/PR curve. The area under the ROC/PR curve is referred to as AUROC/AUC PR. The latter is often interpolated by the Average Precision (AP) metric. The dashed gray lines refer to a classifier with no skill level (random guessing). In the case of an applicationdriven threshold (e.g., required Sensitivity of 0.95 ), the metrics Sensitivity@Specificity, Specificity@Sensitivity, PPV@Sensitivity and Sensitivity@PPV can be calculated on the basis of the ROC/PR curves. Please note that we use the synonyms Precision instead of PPV and Recall instead of Sensitivity for the PR curve, given their common use."
    },
    {
      "markdown": "![img-57.jpeg](img-57.jpeg)\n\nExtended Data Fig. SN 2.10. Validation on object level can be performed per data set (left) or per image (right). For the per-data set validation of objects, the cardinalities are calculated over the whole data set. For the per-image validation of objects, metric scores are computed per image and aggregated afterward. $\\varnothing$ refers to the average $F_{1}$ Score."
    },
    {
      "markdown": "It is further worth mentioning that metric application is not straightforward in object detection problems. One example is the fact that the number of objects in an image may be extremely small (even zero) compared to the number of pixels in an image. Special considerations with respect to aggregation strategy must therefore be made (Fig. SN 2.10). In fact, in the machine learning community, object detection tasks are typically validated by pooling all matched objects (i.e, TP, FP, and FN) over the entire data set and computing global metrics on the entire pool ('per-data set aggregation'). An alternative strategy is the 'per-image aggregation', where matched objects are aggregated per individual image to compute corresponding metrics (e.g., $\\mathrm{F}_{\\beta}$ Score). The per-image metrics are subsequently averaged over the data set. This alternative aggregation may be desirable for two reasons. Firstly, due to the hierarchical data structure (potentially multiple objects per image and/or multiple images per patient), a hierarchical aggregation of metric values, which compensates for the non-independence of images, is generally recommended. Secondly, from a domain interest, the expected metric value per image (rather than per entire data set) may be desirable. Importantly, the per-image aggregation strategy also changes the way multi-threshold metrics such as AP are computed: While the thresholds are still scanned over the scale of predicted class scores simultaneously for the entire data set, the precision and recall used to generate the PR-curve are now per-image scores averaged over the data set rather than the global per-data set scores. It should further be noted that validating an object-level problem per image rather than per data set comes with the problem that images containing no reference or prediction objects lead to division by zero for some metrics and thus to 'Not a Number' (NaN) as metric output. We therefore propose strategies for NaN handling in Fig. SN 2.11a. In summary, we recommend to exclude NaN cases from metric computation except when an empty prediction corresponds to an empty reference, in which case PPV, and in extension $\\mathrm{F}_{\\beta}$ Score, should be set to 1 .\n\nA further critical consideration for metric application in object detection is the fact that structure sizes may have a large effect on performance metrics [152]. We therefore recommend size stratification, i.e., the separate validation for different size ranges, if size variability is high (FP3.2 = TRUE)."
    },
    {
      "markdown": "![img-58.jpeg](img-58.jpeg)\n\nExtended Data Fig. SN 2.11. Effect of handling a 'Not a Number' (NaN) occurring during metric computation, when object detection/instance segmentation tasks are validated per image. Specifically, NaN cases occur when an image features no target structures and/or no object predictions by the model, which causes division by zero errors in prevalent metrics. (a) Demonstration of how and when NaN can occur. Each column represents a potential scenario for per-image validation of objects, categorized by whether TPs, FNs, and FPs are present $(\\mathrm{n}>0)$ or not present $(\\mathrm{n}=0)$ after matching/assignment. The sketches on the top showcase each scenario when setting \" $n>0$ \" to \" $n=1$ \". For each scenario, Sensitivity, Positive Predictive Value (PPV), and $F_{1}$ Score are calculated. (b) Effect of different NaN handling strategies based on different conventions for the aggregation across multiple images. Four examples are shown for the NaN scenarios from (a) (NaN 1-4). NaN 1 and 4: The intuitive penalization for FPs in \"empty\" images is already established by means of PPV scores (see NaN 4) and further penalization by means of Sensitivity is neither required nor appropriate. Instead, images without reference objects should be ignored when averaging Sensitivity scores over images. NaN 2: The intuitive penalization for FP in \"empty\" images is established when assigning a PPV (and $\\mathrm{F}_{1}$ Score) of 1. NaN 3: The intuitive penalization for FP is established when removing images with FN and no FP from the aggregation of PPV (and $\\mathrm{F}_{1}$ ) scores."
    },
    {
      "markdown": "# 2.5 Recommendations for Instance segmentation \n\n## Essentials\n\nFPX.Y refers to a fingerprint item detailed in Figs. SN 1.15-SN 1.17.\nSX refers to a subprocess in Extended Data Figs. 3-4 and Extended Data Figs. 6-9.\nDGX.Y refers to a decision guide in Suppl. Notes 2.7.2-2.7.8.\n\nSegmentation metrics are to be applied per instance.\n\nThis section provides recommendations for selecting common reference-based metrics for instance segmentation problems. As depicted in Fig. 2, these common metrics can then be complemented by application-specific metrics as well as non-reference-based metrics (assessing run time or carbon footprint, for example).\n\nInstance segmentation can be regarded as delivering the tasks of object detection and semantic segmentation at the same time. In contrast to object detection, instance segmentation also involves the accurate marking of the object boundary. In contrast to semantic segmentation, it distinguishes different instances of the same class. The pitfalls and recommendations for instance segmentation problems are closely related to those for segmentation and object detection [127] and we recommend reading Suppl. Note 2.4 and Suppl. Note 2.3 as a foundation for this section.\n\nOur recommendations for assessing instance segmentation quality can be summarized as follows (purple path in Fig. 2 and Fig. SN 2.12):"
    },
    {
      "markdown": "![img-59.jpeg](img-59.jpeg)\n\nExtended Data Fig. SN 2.12. Metrics Reloaded recommendation framework for instance segmentation at a glance.\n\n1: Select object detection metric(s): From a semantic segmentation perspective, overcoming problems related to instance unawareness (1a (top left)) requires the selection of a set of detection metrics to explicitly measure detection performance. To this end, we follow the same general process as in the object detection recommendation by selecting a localization criterion,"
    },
    {
      "markdown": "an assignment strategy, and suitable classification metrics. The specific recommendations for instance segmentation are:\na: Select localization criterion: Although not common in practice, we argue that for consistency it might be appropriate to base the localization criterion on the corresponding target segmentation metric (see step 2: \"Select segmentation metric(s) (if any)\" below). For example, if the target metric is Normalized Surface Distance (NSD), the localization criterion could be defined accordingly. This may not always be possible, for instance because the target metric has no fixed upper bound (e.g., Hausdorff Distance (HD)), rendering the setting of adequate cutoffs challenging. As an alternative strategy, we therefore recommend choosing the localization criterion according to common practice (see Subprocess S8, Extended Data Fig. 8). For this strategy, given the fine granularity of both the output and the reference annotation, we recommend selecting between Boundary Intersection over Union (IoU) (Fig. SN 3.35), Mask IoU (Fig. SN 3.38), and Intersection over Reference (IoR), (Fig. SN 3.37) using decision guide 8.1 in Suppl. Note 2.7.7.\nb: Select assignment strategy: The recommendations for the assignment strategy are identical to those for object detection (Extended Data Fig. 9). In case of the availability of predicted class scores (FP5.1 = TRUE) Greedy (by Score) Matching is our default recommendation. Otherwise, Greedy (by Localization criterion) Matching, Optimal (Hungarian) Matching. or Matching via Overlap $>0.5$ are viable options, as detailed in decision guide 9.1 in Suppl. Note 2.7.8. The user must also decide whether double assignments should be punished (FP2.5.8).\nc: Select classification metrics: Our recommendations with respect to classification metrics are identical to those for object detection (Suppl. Note 2.4) with a single exception. As depicted in S3, Extended Data Fig. 3, we recommend the Panoptic Quality (PQ) (Fig. SN 3.13) [77] as an alternative to the $\\mathrm{F}_{\\beta}$ Score (Fig. SN 3.7). As illustrated in Fig. SN 2.13, this metric is especially suited for instance segmentation, as it combines the assessment of overall detection performance and segmentation quality of successfully matched (True Positive (TP)) instances in a single score.\n2: Select segmentation metric(s) (if any): In a second step, metrics for explicit assessment of the segmentation quality for the TP instances, i.e., successfully matched instances, may be selected. Here, we follow the exact same process as in semantic segmentation (Subprocesses S6, Extended Data Fig. 6 and S7, Extended Data Fig. 7). The primary difference is that the segmentation metrics are computed per-instance and subsequently averaged resulting, for example, in a 'Dice Similarity Coefficient (DSC) per instance' score.\n\nWhile we have found our recommendations for instance segmentation to match the majority of biomedical problems, standard reference-based metrics are not well-suited for some applications. Specifically, standard metrics struggle in images with structures of extreme density and complex shapes, because overlap often fails as a criterion to establish unique correspondences between predicted and reference instances. In such cases, specialized metrics not relying on one-to-one correspondences may be required, such as pair-counting metrics or information theoretic-based metrics [146]. Another example that calls for application-specific metrics is cell nucleus segmentation, where splitting a reference object by two separate predictions is assessed by a dedicated 'split error', and the converse by a dedicated \"merge error' [25]. These application-specific errors can either be used as stand-alone metrics or integrated into compound metrics such as $\\mathrm{F}_{1}$ Score.\n\nRecommendations for aggregating object detection and instance segmentation metrics are provided in the respective appendices Suppl. Note 2.4 and Suppl. Note 2.5, respectively."
    },
    {
      "markdown": "Reference (Ref) instances\n![img-60.jpeg](img-60.jpeg)\n\nExtended Data Fig. SN 2.13. The Panoptic Quality (PQ) measures the segmentation and detection quality of a prediction in one score. The metric simply averages the IoU scores for all True Positive (TP) instances and multiplies the result with the $F_{1}$ score. For perfect segmentation results, i.e., an average IoU of 1 , the $P Q$ would equal the $F_{1}$ Score."
    },
    {
      "markdown": "# 2.6 Recommendations for Calibration of Predicted Class Scores \n\nWhile most research in biomedical image analysis focuses on the discrimination capabilities of classifiers, a complementary property of relevance is the calibration of predicted class scores. Importantly, a large portion of the research in this field is comparatively young, and a variety of new calibration metrics are proposed every year. As it might be premature to call for rigid standardization in such a vibrant environment, the following recommendations are to be seen as general guidance through the current landscape of calibration metrics, which might be subject to updates in the following years.\n\nIntuitively speaking, a system is well-calibrated if the predicted class scores (i.e., the output of the model) reflect the true probabilities of the outcome. In practice, this means that calibrated scores match the empirical success rate of associated predictions. For example, in a binary classification task, calibration implies that of all the data samples assigned a predicted score of 0.8 for the positive class, empirically, $80 \\%$ belong to this class.\n\nOne common but critically important misconception about calibration is that the predicted class scores of a well-calibrated model express the true posterior probability $\\mathbb{P}_{Y \\mid X}$ of an input belonging to a certain class [122], e.g., that they express a patient's risk for a certain condition based on an image. While this probability is commonly of interest in classification problems, common calibration metrics instead typically consider $\\mathbb{P}_{Y \\mid f(X)}$, i.e., the probability of a model's output score belonging to a certain class. Conditioning on the output entirely ignores the mapping $f: X \\rightarrow \\mathcal{P}$. Thus, while calibration allows making statements about the empirical class membership of predicted scores, such as in the example above, these statements are conditioned on the discrimination power of a model. This means that different models may predict different probabilities for the same input even though all of them are perfectly calibrated [122]. Going back to the clinical example, this implies that a classifier that always predicts the score 0.5 is considered perfectly calibrated on a balanced binary task, although another perfectly calibrated model with better discrimination ability could output completely different, practically more meaningful scores. Again, this discrepancy occurs because calibrated scores reflect the empirical success rate of predictions and not a patient-specific (model-agnostic) inherent risk. The clinical prediction modelling community therefore traditionally distinguishes different levels of calibration [157], where level 4 strong calibration implies correct posteriors $\\left(\\mathbb{P}_{Y \\mid X}\\right)$. As level 4 is practically unfeasible to measure (the true individual posteriors are unknown), common research focuses on level 3 moderate calibration, which implies that the predicted scores match the empirical success rate.\n\nFor a more formal definition of (level 3) calibration, let the random variables $X$ and $Y$ correspond to the feature (e.g., an image) and target variables (encoding the outcome), respectively, with feature and target spaces $\\mathcal{X}$ and $\\mathcal{Y}$. Let $f: X \\rightarrow \\mathcal{P}$ denote a classifier with predicted class scores $f(X)$ and $\\mathcal{P}$ a set of distributions on $\\mathcal{Y}$. We further use the notation $\\mathbb{P}_{Y}, \\mathbb{P}_{Y \\mid f(X)} \\in \\mathcal{P}$, where $\\mathbb{P}_{Y}$ refers to the distribution of $Y$, and $\\mathbb{P}_{Y \\mid f(X)}$ to the conditional distribution of $Y$ given $f(X)$.\n\nIn practice, three different variations of calibration conditions can be distinguished [156]:\n\n- Canonical calibration: $f(X)=\\mathbb{P}_{Y \\mid f(X)}$. This condition implies pairwise matching of all entries across the two distributions (see also the top panel in Fig. SN 2.14). Although not the most commonly applied condition in practice, a common perception is that this condition is the appropriate perspective on calibration in many application scenarios as the weaker conditions (see below) are prone to underestimating miscalibration [51, 56, 123].\n- Class-wise calibration: $f_{k}(X)=\\mathbb{P}\\left(Y=k \\mid f_{k}(X)\\right)$ for all classes $k \\in \\mathcal{Y}$. This is a weaker condition, where not the joint, but the marginal distributions for each class are required"
    },
    {
      "markdown": "to match (see also the middle panel in Fig. SN 2.14). Assessing the calibration quality for individual classes provides crucial information, for example in scenarios where there is a mismatch between class prevalences and class importance (FP2.5.3=TRUE).\n\n- Top-label calibration: $f_{K}(X)=\\mathbb{P}\\left(Y=K \\mid f_{K}(X)\\right)$, where $K=\\arg \\max _{k} f_{k}(X)$ of a model $f: \\mathcal{X} \\rightarrow \\mathcal{P}$. This is the weakest of the three conditions, where only the maximum entry (top label) of each predicted class vector is considered (see also the bottom panel in Fig. SN 2.14). This condition assesses only the highest class score, which is often used to determine the predicted class, and thus implies a strong focus on validating the reliability of a model's decisions.\n\nWhile these three conditions are equivalent for binary classification problems, they may differ substantially in the broader multi-class setting, as illustrated in Fig. SN 2.14.\n\nIn practice, no model is perfectly calibrated. Calibration quality is captured by the Calibration Error (CE), which can be computed via a divergence, i.e., a distance function, between predictions $f(X)$ and either of the three conditions (canonical, class-wise, top-label). For instance, typical choices for quantifying the canonical CE are expected $L_{1}$ or $L_{2}$ errors [56, 84, 110]. These can be further generalized to the $L_{p}$ CE: For $1 \\leq p \\in \\mathbb{R}$, the canonical $\\ell_{p} \\mathbf{C E}\\left(\\mathrm{CE}_{p}\\right)$ of model $f: \\mathcal{X} \\rightarrow \\mathcal{P}$ is defined as:\n\n$$\n\\mathrm{CE}_{p}(f)=\\left(\\mathbb{E}\\left[\\left\\|f(X)-\\mathbb{P}_{Y \\mid f(X)}\\right\\|_{p}^{p}\\right]\\right)^{\\frac{1}{p}}\n$$\n\nThe relations of CE variants associated with the three conditions above intuitively translate to $C E_{\\text {canonical }} \\geq C E_{\\text {class-wise }} \\geq C E_{\\text {top-label }}$. In the example provided in Fig. SN 2.14, the weaker conditions of top-label calibration and class-wise calibration are fulfilled (associated errors are zero), while the broader canonical condition for calibration is not met. The fact that the calibration quality of a classifier varies when assessed through the lens of different conditions causes common calibration measures to be characterized as inconsistent in multi-class settings [56].\n\nThe canonical $\\ell_{p}$ CE can be generalized by replacing the $\\ell_{p}$ norm as a distance measure between $f(X)$ and $\\mathbb{P}_{Y \\mid f(X)}$ with alternative distance functions. For example, [167] introduced a canonical CE based on matrix-valued kernels."
    },
    {
      "markdown": "Top-label calibration\n\n| $g(X)$ | $\\mathrm{P}[\\mathrm{Y} \\in \\cdot \\mid \\mathrm{g}(\\mathrm{X})]$ |\n| :--: | :--: |\n| $(0.1,0.3,0.6)$ | $(0.2,0.2,0.6)$ |\n| $(0.1,0.6,0.3)$ | $(0.0,0.7,0.3)$ |\n| $(0.3,0.1,0.6)$ | $(0.2,0.2,0.6)$ |\n| $(0.3,0.6,0.1)$ | $(0.4,0.5,0.1)$ |\n| $(0.6,0.1,0.3)$ | $(0.7,0.0,0.3)$ |\n| $(0.6,0.3,0.1)$ | $(0.5,0.4,0.1)$ |\n\n## Top-label ECE $=0$\n\nClass-wise calibration\n\n| $g(X)$ | $\\mathrm{P}[\\mathrm{Y} \\in \\cdot \\mid \\mathrm{g}(\\mathrm{X})]$ |\n| :--: | :--: |\n| $(0.1,0.3,0.6)$ | $(0.2,0.2,0.6)$ |\n| $(0.1,0.6,0.3)$ | $(0.0,0.7,0.3)$ |\n| $(0.3,0.1,0.6)$ | $(0.2,0.2,0.6)$ |\n| $(0.3,0.6,0.1)$ | $(0.4,0.5,0.1)$ |\n| $(0.6,0.1,0.3)$ | $(0.7,0.0,0.3)$ |\n| $(0.6,0.3,0.1)$ | $(0.5,0.4,0.1)$ |\n\n## Canonical calibration\n\n| $g(X)$ | $\\mathrm{P}[\\mathrm{Y} \\in \\cdot \\mid \\mathrm{g}(\\mathrm{X})]$ |\n| :--: | :--: |\n| $(0.1,0.3,0.6)$ | $(0.2,0.2,0.6)$ |\n| $(0.1,0.6,0.3)$ | $(0.0,0.7,0.3)$ |\n| $(0.3,0.1,0.6)$ | $(0.2,0.2,0.6)$ |\n| $(0.3,0.6,0.1)$ | $(0.4,0.5,0.1)$ |\n| $(0.6,0.1,0.3)$ | $(0.7,0.0,0.3)$ |\n| $(0.6,0.3,0.1)$ | $(0.5,0.4,0.1)$ |\n\n## $\\square$ <br> $\\square$ <br> canonical ECE $>0$\n\nExtended Data Fig. SN 2.14. Estimating the Calibration Error (CE) according to the three different conditions in multi-class settings yields inconsistent results. For the top-label calibration, only the maximum values of the predicted class scores $f(X)$ are considered, while all other values are neglected. For the computation of the CE, for each distinct output value of $f(X)$ (only 0.6 in this case), $\\mathbb{P}_{Y \\mid f(X)}$ is determined as the average over the empirical rates of this output $(0.6,0.7,0.6,0.5,0.7,0.5$ in this case). The top-label calibration condition (i.e., matching the two scores) results in a perfect $\\mathrm{CE}=0$ in this scenario. Similarly, for the class-wise calibration, the predicted class scores are compared per class, a requirement that is also fulfilled by the depicted system. Only the canonical calibration, which comes with the strict requirement that the model output must match the full probability distribution (implying the comparison of entire vectors rather than single values) indicates a miscalibrated system $(C E>0)$. This figure is inspired by [156].\n\nGenerally, measuring the CE is challenging, because $\\mathbb{P}_{Y \\mid f(X)}$ is unknown and needs to be estimated as the expected value on the available data. Returning to the simple example above, as we only have access to a small subset of all potential cases for which the model would predict a score of 0.8 , we do not know whether the corresponding success rate of these cases is $80 \\%$ in general; instead, our assessment relies on the estimated success rate based on the available samples. The fact that classifier outputs are generally continuous often reduces the number of available samples per prediction to one. Strategies for alleviating the sparse sampling problem include binning the continuous scale of $f(X)$ and estimating the CE per bin (such as done for Expected Calibration Error (ECE) (Fig. SN 3.30) and Class-wise Calibration Error (CWCE) (Fig. SN 3.29), as illustrated in Fig. SN 2.16), or using kernel density estimation methods (such as done for Expected Calibration Error Kernel Density Estimate (ECE ${ }^{\\text {KDE }}$ ), see Fig. SN 3.31). Despite these efforts, the most popular"
    },
    {
      "markdown": "calibration measures are generally biased estimators of the true error, which means their estimates depend on the number of samples (i.e., size of the validation data set). Gruber et al. [56] recently described this bias and how it leads to substantial under- and over-estimations of the true error. Popordanoska et al. [123] showed that straightforward estimators of $\\ell_{p}$ calibration based on density estimation (such as done for $\\mathrm{ECE}^{\\mathrm{KDE}}$ ) have a generally lower bias compared to statistical estimators (such as binning) and presented means to additionally de-bias estimators. There are also ongoing efforts investigating canonical CEs that are not based on the $\\ell_{p}$ norm, such as the Kernel Calibration Error (KCE), where 'maximum mean discrepancy' is used as a distance function instead (see Fig. SN 3.31). These efforts have resulted in fully unbiased estimators, which, however, do not allow for interpretable calibration assessment and further require nontrivial configuration of the kernels and associated hyperparameters.\n\nAn attractive alternative to estimate CEs are so-called Proper Scoring Rules (PSRs) (also referred to as overall performance measures [143]), which measure discrimination and calibration in a single score (e.g. Negative Log Likelihood (NLL), Brier Score (BS); Figs. SN 3.28, SN 3.33). An intuitive example for this metric category is the BS: For a model $f: \\mathcal{X} \\rightarrow \\mathcal{P}$ the BS is defined as the expected value of the squared error between predictions and reference values as determined on the validation data:\n\n$$\n\\operatorname{BS}(f)=\\mathbb{E}\\left[\\left\\|f(X)-Y^{\\prime}\\right\\|_{2}^{2}\\right]\n$$\n\nwhere $Y^{\\prime}$ is the one-hot-encoded version of the reference vector $Y$ for each individual data sample. This equation illustrates the difference between overall performance measures and calibration metrics measuring the CE in Equation 1. While the CE measures whether the predicted class scores match the empirical success rate (see also SN 2.14), BS is defined as an expected value over every single prediction, thus posing a stronger requirement on the scores which can be interpreted as assessing the true posterior probabilities or individual risks. In theory, BS can be decomposed into explicit terms for discrimination and calibration assessment [44]. In practice, however, although overall performance measures do not suffer from the sampling problem, they conflate the true CE with the discrimination error and can thus not make calibration quality explicit. However, proper scores are still useful for comparative studies. Furthermore, it has been shown that the square root of the BS, referred to as the Root Brier Score (RBS) (Fig. SN 3.34), represents a robust estimator and upper bound of the canonical CE [56]. Such a guarantee can be particularly relevant in safety-critical scenarios where the error must not be underestimated.\n\nThe choice of which calibration condition to validate as well as which metric to use depends on the task interest. Methods subject to validation in this context are either classification models whose inherent calibration quality shall be assessed, or so-called 're-calibration methods', i.e., transformations on the classifier outputs aiming to improve the calibration quality. In the most common scenarios, the driving interest may either be a comparative performance assessment, in which methods are ranked according to calibration quality, or an absolute performance assessment, in which an interpretable and communicable measure of calibration quality is desired. We identified four main use cases (U1-U4) which our framework addresses (Fig. SN 2.15)."
    },
    {
      "markdown": "![img-61.jpeg](img-61.jpeg)\n\nExtended Data Fig. SN 2.15. Underlying interest related to the assessment of calibration quality. The user is interested in the comparative calibration assessment (U1-U3) and/or obtaining a reliable estimate of the Calibration Error (CE) for interpreting and communicating the algorithm output (U4). The use cases are detailed in Suppl. Note 2.6. The brackets around re-calibration methods denote that their application is optional in the corresponding use case."
    },
    {
      "markdown": "(1) Ranking methods to determine calibration quality: The following use cases focus on the comparative assessment of the calibration quality of one or multiple classifiers.\n(a) Use case $\\mathbf{1}(\\mathbf{U 1})$ : comparing the effect of one or more re-calibration methods on the same (fixed) classifier. The desired validation output is a ranking of re-calibration methods (possibly including the performance of 'no re-calibration') from which the best method can be selected.\n(b) Use case 2 (U2): comparing the calibration quality across multiple classifiers on the same task. The desired validation output is a ranking of classifiers according to calibration quality. In practice, such a ranking should be accompanied by a ranking according to discrimination performance, as it is not recommended to base model selection purely on calibration performance.\n(c) Use case 3 (U3): comparing the 'overall performance' of classifiers (optionally including potential re-calibration methods), i.e., a joint assessment of discrimination performance and calibration quality. The desired validation output is a single ranking naturally weighing both aspects.\n(2) Interpreting model outputs: Complementary interest may lie in the analysis of the CE to the end of assessing the reliability of the predicted class scores of one or multiple classifiers.\n(a) Use case 4 (U4): interest in understanding the reliability of predicted class scores for a given model as a basis for interpreting and communicating results. The desired validation output is a single score which provides insight into how well the model is calibrated. The reliability of model outputs is often considered crucial upon application, such as for clinical prediction models [46, 158, 171]. Importantly, U4 can be used in addition to U1, U2 or U3 as it is based on an orthogonal interest.\n\nBecause some decision rules assume calibrated model outputs, a further potential interest in calibration validation may lie in determining the quality of a decision rule applied to predicted class scores (see FP2.6), i.e., answering the question: \"How much better could the classifier's decisions have been under this rule if predicted class scores were calibrated?\". While such ablations of classifier design decisions are generally out of the scope of our framework, decision rule-related pitfalls and countermeasures are discussed in Sec. 1.1.\n\nBased on all of the above considerations, we recommend selecting calibration metrics using Subprocess S5 (Extended Data Fig. 5) in case the assessment of calibration quality is desired (FP2.7.1 = TRUE):\n\n1: Select metric for comparative calibration assessment (if any): This step selects an adequate metric in case a comparative assessment of calibration methods is desired (FP2.7.2). The fingerprint FP2.7.2 covers the presented use cases U1-U3 (Fig. SN 2.15). For U1 \"Comparison of re-calibration methods for the same fixed classifier\", one option is to select a metric that assesses the canonical CE, such as KCE as an unbiased estimator of a canonical CE based on an alternative distance function, or $\\mathrm{ECE}^{\\mathrm{KDE}}$ as a well-interpretable estimator of canonical $\\ell_{p}$ calibration. Alternatively, an overall performance measure such as the BS can be used (see DG5.2), because the classifier is fixed in this scenario, the conflation of the CE with discrimination errors is no disturbing factor, and the true CE is exposed for relative comparison of scores. For U2 \"Comparison of calibration quality across classifiers on the same task\", we recommend reporting the CE per class by using an estimator of marginal CE, such as CWCE, if there is an unequal interest across classes (FP2.5.1). Otherwise the canonical CE should be assessed, e.g. using KCE or $\\mathrm{ECE}^{\\mathrm{KDE}}$ (see DG5.1). For U3 \"Comparison of overall performance across classifiers\", we recommend reporting a PSR (i.e., BS or NLL, see"
    },
    {
      "markdown": "DG5.3) as the joint assessment of calibration and discrimination is exactly what this category of metrics is designed for.\n2: Select metric for assessing output interpretability (if any): This step selects an adequate metric for assessing the interpretability of the model output (FP2.7.3), which corresponds to U4. The first decision to be made in FP2.7.3 is whether to assess the calibration quality in isolation, as measured by CE estimates, or jointly with discrimination as measured by overall performance measures. When deciding for calibration-only assessment, the core decision to be made is whether to measure top-label, marginal or canonical CE, as detailed in DG 5.4. If there is an unequal interest across classes (FP2.5.3), a well-interpretable estimator of the marginal CE, such as CWCE, is recommended. Otherwise, the default option is to select a well-interpretable estimator of the canonical CE (e.g., $\\mathrm{ECE}^{\\mathrm{KDE}}$ ) and a corresponding guaranteed upper bound (e.g., RBS), together with the a per-class estimator of marginal CE (e.g., CWCE). Top-label calibration (as measured by ECE) is only recommended in rare cases, as detailed in DG5.4.\n\nNote that the selection of the same metric (e.g., CWCE) in both steps is a potential outcome of the mapping. Crucially, metrics involving calibration assessment are generally prevalence-dependent. Thus, comparative studies as described in U2 and U3 are generally restricted to one data set and, if the prevalence of the data set does not represent the population of interest (see FP4.2), the calibration quality of a classifier needs to be re-validated on each new study cohort (see Fig. SN 1.6).\n\nCalibration is most commonly assessed for image-level classification tasks. Due to the comparatively sparse research basis in the other problem categories, no specific recommendations are provided in our framework at this time. There are however, a few recent studies employing calibration metrics in object detection [85, 115] and slightly more studies in semantic segmentation, especially in the biomedical domain [73, 92, 106, 135].\n\nNevertheless, in theory, Subprocess S5 may also be traversed for object detection, instance segmentation, and semantic segmentation. When traversing S5 for object-level tasks, the following considerations should be noted:\n\n- Calibration recommendations only apply to the classification part of object detectors: As described in Suppl. Note 5.2, object detection and instance segmentation methods commonly provide outputs beyond the predicted class score vector such as bounding box coordinates or, depending on the method, 'intermediate objectness scores' [129]. Thus, it is important to note that when utilizing calibration recommendations in this framework for object-level methods, the recommendations only apply to the classification output.\n- Why considerations in image-level classification translate to object detection: When validating discrimination performance, a fundamental conceptual difference between image level and object level is the fact that True Negatives (TNs) are not defined in the latter case. This difference does not translate to calibration, where only predictions of the model $f(X)$ are validated. As the background class is discarded from validation (see below), this means that only True Positive (TP) and False Positive (FP) predictions are relevant for calibration, i.e., nonmatched predictions are considered while non-matched reference objects (False Negatives (FNs)) are not. A further conceptual difference between object-level classification and imagelevel classification is the former's additional requirement of localization to distinguish TPs and FPs. This aspect is inherently covered by calibration validation because non-matched predictions are simply considered as additional FP errors (mistaking the 'true' background"
    },
    {
      "markdown": "![img-62.jpeg](img-62.jpeg)\n\nExtended Data Fig. SN 2.16. Computation of the Expected Calibration Error (ECE) based on the binning of predicted class scores. The error is based on the discrepancy between the Accuracy per bin $\\operatorname{Accuracy}\\left(B_{m}\\right)=$ $1 /\\left|B_{m}\\right| \\sum_{i \\in B_{m}} 1\\left(\\hat{y}_{i}=y_{i}\\right)$ and the average over predicted class scores per bin $\\operatorname{Confidence}\\left(B_{m}\\right)=$ $1 /\\left|B_{m}\\right| \\sum_{i \\in B_{m}} \\hat{p}_{i}$. The final ECE score is obtained as the average over bin discrepancies weighted by the number of samples $\\left|B_{m}\\right|$ per bin. Here, $n$ denotes the total number of samples, $\\hat{y}_{i}$ denotes the predicted class labels and $y_{i}$ the true class labels, $\\mathbf{1}$ is the indicator function ( 1 if $\\hat{y}_{i}=y_{i}, 0$ otherwise), and $\\hat{p}_{i}$ refers to the predicted class scores. The dashed diagonal line acts as a visual reference for a perfectly calibrated system, where discrepancies between per-bin confidences and accuracies are zero.\nclass for one of the foreground classes), equivalently to the standard FP error case (mistaking two classes).\n\n- Dealing with the background class: When validating classification performance in objectlevel tasks, the model output predicting class scores for the background class is commonly discarded (see Suppl. Note 5.2) [50], because rewarding correct background predictions contradicts the task interest (there are no 'background objects') and would be easily exploitable (predicting high numbers of background objects). Further, penalization of background predictions is already ensured implicitly by considering them as FPs with respect to the true foreground class. Discarding the background class leads to class prediction vectors that do not sum to one, which is of no concern for validation as metrics do not rely on the probabilistic interpretation of scores. These considerations translate directly to calibration validation, which is equivalently exploitable by predicting high numbers of background objects. Here, the background class is discarded from outputs, and calibration of outputs only refers to the foreground class predictions (the actual 'object detection' outputs). Moreover, the calibration conditions introduced above describe the matching of single entries across two distributions and do not rely on their scores summing to one.\n- Non-applicability of the NLL on object level: There is one exception for object-level tasks where metric recommendations differ when traversing SS: The NLL is not applicable. This is because the NLL considers the predicted class score for the correct reference class ('true class probability'). For non-matched predictions, this 'true class' is the background class, which is discarded from validation as described above. In contrast, BS remains applicable and a meaningful measure of CE under the recommended protocol (i.e., when only considering foreground classes)."
    },
    {
      "markdown": "# 2.7 Decision Guides \n\nWhile the problem fingerprint helps exclude common metrics that are not suitable for the driving problem, the final choice in each subprocess may not be unambiguous. In these cases, decision guides support the users in making an educated decision that best matches their preferences.\n\n### 2.7.1 Decision guide S2.\n\nDG2.1: Weighted Cohen's Kappa (WCK) versus Expected Cost (EC)\n\n## Summary of DG2.1: WCK versus EC\n\n## WCK\n\nDesigned for symmetric situations (guesses of two raters)\n(3) Limited interpretability\n(3) Widely used\n(3) Lack of framework to identify and validate the decision rule applied to class scores\n(2) Possibility of paradoxical results\n\nExtended Data Tab. SN 2.3. Comparison of Weighted Cohen's Kappa (WCK) to Expected Cost (EC) in the context of the decision guide DG2.1 for Subprocess S2. Context: unequal severity of class confusions (FP2.5.2 = TRUE), costs for class confusions available (FP2.5.3 = TRUE), and provided class prevalences reflect the population of interest (FP4.2 = TRUE).\n\nBoth WCK (Fig. SN 3.18) and EC (Fig. SN 3.6) are metrics that allow for incorporating taskspecific penalties for confusions between individual pairs of classes. Common use cases for this property are tasks with ordinal classes or diagnostic decisions with errors of varying clinical severity. Importantly, however, Kappa statistics in general and WCK in particular were originally proposed to compare annotations/guesses of two raters, which is a symmetric problem by nature. Validation studies, on the other hand, involve the comparison of a prediction to a reference that approximates the truth (asymmetric setting). Hence, unlike EC, WCK does not conceptually match the intended comparison. For this reason and due to further favorable properties, we generally recommend the usage of EC rather than WCK. When deciding between the two metrics, the following further properties are of relevance:\n\n- Interpretability: While both metrics can be interpreted as 'measures of (dis)agreement', the main difference is the fact that WCK provides this measure in reference to 'agreement by chance'. The equivalent concept for EC is its normalized variant normalized EC (ECN), where the disagreement measure is divided by a 'random performance' measure. Due to the conceptual similarity, it is more sensible to compare WCK to ECN. Both metrics are prevalence-dependent due to relating model performance to a random performance reference. Their main difference is the definition of the 'random reference': In ECN this reference is straightforward to interpret as the 'best possible naive classification system' which always predicts the most dominant class. The definition in WCK stems from its symmetric concept"
    },
    {
      "markdown": "to compare the predictions of two raters. The random reference in this case is the probability of both raters agreeing by chance. Using this definition in classification tasks results in random reference systems that can be weaker than the naive system of ECN. Thus, the random reference in WCK is less intuitive and arguably not useful in classification tasks (i.e., asymmetric settings).\n\n- Undesired behaviour in practice: Using WCK with quadratic weights, often done for ordinal tasks, has been found to lead to 'paradoxical results', as detailed in [165].\n- Popularity: WCK is widely used in the biomedical domain, whenever customized penalties for class confusion are required. EC, on the other hand, is currently mostly found either in statistical textbooks or in non-related domains such as speech recognition.\n- Theoretical foundation: EC comes with a comprehensive theoretical foundation based on Bayesian decision theory [51]. As a consequence, it is possible to analytically derive the optimal decision rule applied to the predicted class scores (more generally: decision region for more than two classes) and empirically validate the quality of this decision rule by means of calibration assessment. This is an important property in this context because the standard argmax-based decision rule (i.e., predicting the class with the highest class score) is by definition not optimal in scenarios with unequal costs of misclassifications.\n\nDG2.2: Balanced Accuracy (BA) versus Expected Cost (EC)\n\n# Summary of DG2.2: BA versus EC \n\nBA\n(c) Prevalence independence\n\n## EC\n\n(7) Possibility of reflecting expected prevalences in the target population\n(8) Not commonly known in biomedical image analysis\n\nExtended Data Tab. SN 2.4. Comparison of Balanced Accuracy (BA) to Expected Cost (EC) in the context of the decision guide DG2.2 for Subprocess S2. Context: Equal severity of class confusions (FP2.5.2 = FALSE), either (1) unequal interest across classes (FP2.5.1 = TRUE) and no mismatch between class prevalences and class importance (FP2.5.3 = FALSE), or (2) equal interest across classes (FP2.5.1 = FALSE) and provided class prevalences do not reflect the population of interest (FP4.2 = FALSE).\n\nWhen deciding between BA and EC in the provided context, two primary scenarios should be distinguished:\n\nClasses should contribute according to prevalence in the target application: Although the user may have an inherently equal interest in all classes (FP2.5.1 = FALSE), reporting a metric score to which all classes contribute equally may not necessarily be desired. Instead, the user may simply be interested in the overall performance on a given data set and thus want classes to contribute according to their prevalence in the target application. This is not straightforward in the provided scenario because the data set at hand does not match the prevalences of the target population (FP4.2 = FALSE). In this case, we recommend EC, because it offers a mechanism to explicit set (expected) class prevalences directly in the formula. This strategy allows getting a glimpse of a model's performance on the target application while validating on the data at hand. Application of EC in this way, however, is only possible if the prevalences can be specified upfront."
    },
    {
      "markdown": "Each class should contribute equally to the metric: In this case, compensation for potential class imbalance is required in order to ensure equal contribution from each class. Here, we recommend BA as metric because it was designed for exactly this purpose. Although EC can be configured to be identical to BA (Suppl. Note 2.1), we favor BA due to its widespread use.\n\nEC also offers a complementary way to address class imbalance as it allows for the exchange of the class priors directly in the formula: When the class priors upon application on a new data set are known, they can be incorporated in EC. This can be useful for identifying the optimal decision rule applied to predicted class scores on a new data set, as described in [51], essentially rendering the re-calibration of class scores unnecessary. However, one might argue that class priors being known upfront is quite a strong assumption for a new application."
    },
    {
      "markdown": "# DG2.3: Balanced Accuracy (BA) versus Matthews Correlation Coefficient (MCC) versus normalized EC (ECN) \n\n## Summary of DG2.3: BA versus MCC versus ECN\n\nBA\n(1) Inherent interpretability with respect to naive classifier\n(2) Implication of equal class contribution\n(3) Insensitive to predictive values (Positive Predictive Value (PPV) and Negative Predictive Value (NPV))\n(4) Availability of framework to identify and validate decision rule applied to class scores\n(5) Good interpretability\n(6) Widely used\n\n## MCC\n\n(1) Inherent interpretability with respect to naive classifier\n(2) Implication of equal class contribution\n(3) High scores ensure high predictive values (PPV and NPV)\n(4) Lack of framework to identify and validate the decision rule applied to class scores\n(5) Limited interpretability\n(6) Fairly well-known but not much used\n\n## ECN\n\n(1) Inherent interpretability with respect to naive classifier\n(2) No establishment of equal class contribution\n(3) Limited sensitivity to predictive values (PPV and NPV)\n(4) Availability of framework to identify and validate the decision rule applied to class scores\n(5) Good interpretability\n(6) Not known or used in the biomedical imaging domain although based on well-studied statistical concepts\nExtended Data Tab. SN 2.5. Comparison of Balanced Accuracy (BA) to Matthews Correlation Coefficient (MCC) to normalized EC (ECN) in the context of the decision guide DG2.3 for Subprocess S2. Context: Equal severity of class confusions (FP2.5.2 = FALSE), either (1) unequal interest across classes (FP2.5.1 = TRUE) and no mismatch between class prevalences and class importance (FP2.5.3 = FALSE) or (2) equal interest across classes (FP2.5.1 = FALSE), provided class prevalences reflect the population of interest (FP4.2 = TRUE), presence of class imbalance (FP4.1 = TRUE) and compensation for class imbalances requested (FP2.5.5 = TRUE).\n\nThree metrics are particularly attractive when class prevalences reflect the population of interest but compensation for class imbalance is desired (FP4.1 = TRUE and FP2.5.5 = TRUE). These are MCC, BA, and the normalized variant of EC, ECN. As described in Suppl. Note 1.3 (FP2.5.5 Compensation for class imbalance requested), there are three effects of class imbalance that can be compensated for."
    },
    {
      "markdown": "- Effect 1: Misleading metric values due to missing reference value for naive classifier\n- Effect 2: Misleading metric values due to unequal contribution of classes\n- Effect 3: Misleading metric values due to missing consideration of predictive values\n\nWhile the most common multi-class metric, Accuracy, is subject to all three pitfalls when used in imbalanced settings, this decision guide discusses the three aforementioned alternatives (BA, MCC, and ECN) that compensate for one or more of these effects. The following aspects are relevant when deciding between the three:\n\nCompensating for Effect 1: All three metrics establish a fixed score for the performance of a naive classifier, i.e., one that always predicts the most frequent class - which is a more realistic baseline in class imbalanced scenarios - compared to an entirely random system. The corresponding scores are 0 for MCC, 1 for ECN, and $1 / \\mathrm{C}$ for BA, where C is the number of classes. However, the nature of the different compensation methods is fundamentally different. Consider the following confusion matrix of a binary classification system, as shown in Tab. SN 2.6:\n\nExtended Data Tab. SN 2.6. Confusion matrix illustrating Effect 1.\n\n|  |  | Prediction |  |\n| :--: | :--: | :--: | :--: |\n|  |  | Positive | Negative |\n| Actual | Positive | $\\mathrm{TP}=100$ | $\\mathrm{FN}=1$ |\n|  | Negative | $\\mathrm{FP}=100$ | $\\mathrm{TN}=10,000$ |\n\nRespective metric values are BA: 0.99, MCC: 0.7, ECN: 1. Although all metrics feature fixed values for a random classifier, the same system can be assessed differently, as it is being considered 'near-perfect' by BA ( 0.99 ), 'fairly good' by MCC ( 0.7 ), and 'random'/'naive' by ECN (1). Intuitively, the BA assessment seems overly optimistic, which can be attributed to the fact that BA does not compensate for Effect 3, as described in more detail below. On the other hand, the ECN assessment appears overly strict, which can be attributed to the fact that ECN does not compensate for Effect 2 as described in more detail below.\n\nCompensating for Effect 2: In balanced scenarios, all classes are weighted equally by common discrimination metrics. In contrast, in imbalanced scenarios, common metrics such as Accuracy are dominated by the frequent classes. Equal contribution of classes in this context would imply that each class can contribute equally to the final metric score, irrespective of prevalence. This is exactly what BA does by computing the average of individual class Sensitivities. An alternative way of thinking about this compensation is tweaking the costs of misclassification errors by assigning higher costs for errors in rare classes and vice versa. Hence, BA can be thought of as a cost instantiation of EC if the costs are set proportional to the inverse of class prevalences (see Suppl. Note 2.1). Importantly, the normalized variant of EC, ECN, does not generally compensate for Effect 2, but merely rescales metric scores in a way that the value of 1 corresponds to a naive classifier always predicting the most frequent class (see Effect 1). In other words, the rankings obtained for a set of test cases would be the same for EC and ECN. Analogously to EC, it is also possible to tweak the costs to compensate for Effect 2 in ECN, but the resulting metric would yield no advantages over BA. Importantly, the fact that ECN does not compensate for Effect 2 implies that if there is an unequal interest across classes (FP2.5.1 = TRUE), then ECN is the only correct choice. Analogously to BA, MCC establishes equal contribution of classes by assessing individual class sensitivities."
    },
    {
      "markdown": "Compensating for Effect 3: The predictive values (PPV and NPV) are an important aspect of assessing the quality of a classification system. To showcase this importance, consider the following confusion matrix of a binary classification task, as shown in Tab. SN 2.7:\n\nExtended Data Tab. SN 2.7. Confusion matrix illustrating Effect 3.\n\n|  |  | Prediction |  |\n| :-- | :-- | :-- | :-- |\n| Actual | Positive | Negative |  |\n|  | Positive | $\\mathrm{TP}=10$ | $\\mathrm{FN}=1$ |\n|  | Negative | $\\mathrm{FP}=100$ | $\\mathrm{TN}=10,000$ |\n\nThis system is assessed as 'near-perfect' by BA (0.95), 'better than random, but not really useful' by MCC (0.29), and 'much worse than random' by ECN (9.2).\n\nThis example shows that BA does not consider predictive values, thus yielding a near-perfect score despite a low PPV of 0.09 . This assessment could be considered a pitfall in many scenarios, where the classification system would be fairly useless. Consider, for instance, a breast cancer screening program where, based on the provided system, $>90 \\%$ of all biopsies (True Positives (TPs) + False Positives (FPs)) would be unnecessary (FPs).\n\nIn contrast, the MCC score could be considered intuitive for many scenarios such as the screening example. This is due to MCC explicitly considering all four basic rates True Positive Rate (TPR), True Negative Rate (TNR), PPV, and NPV. Thus, MCC poses further requirements compared to BA, which focuses only on Sensitivities. ECN also ensures high predictive values by design. In practice, however, it is not always a good indicator for predictive values because of the sometimes overly strict penalization of errors, as seen in the above example. In theory the weights in ECN could be adjusted to simulate the behavior of predictive value-sensitive metrics like MCC, but this implies a trial-and-error tuning process on each new task.\n\nIdentifying the optimal decision rule applied to predicted class scores: The different strategies for identifying a decision rule applied to predicted class scores are described in FP2.6 (see Suppl. Note 1.3). In the multi-class setting, argmax-based decision rules (i.e., predicting the class with the highest class score) are very common, but make arguably strong assumptions such as calibrated scores and equal penalization of all misclassifications.\n\nIt should be noted here that metrics that can be viewed as instantiations of EC (in this case BA and ECN) come with a theoretical framework on how to validate the decision rule, i.e., answering the question \"how much better could the classification performance have been with an optimal decision rule?\" [51]. MCC, on the other hand, lacks such a framework.\n\nInterpretability: Arguably, BA features the most straightforward interpretation as the average over individual class Sensitivities, with bounded scores $[0,1]$ and a fixed random reference at 1/C. ECN scores are also fairly interpretable (\"the EC of the system in relation to the EC of a naive system\"), but scores are not bounded $[0, \\infty)$. Furthermore, the random reference could be interpreted as 'too strict' for many scenarios such as the provided example. As for MCC, a random reference value is provided at 0 and the scores are bounded $[-1,1]$, but all intermediate scores are arguably less intuitive. The general interpretation of MCC would be that it is a metric that depends on individual class Sensitivities and predictive values, i.e., a high MCC score guarantees all of these being high and a low MCC score indicates that at least one of them is low."
    },
    {
      "markdown": "Popularity: BA is a widely used metric. MCC is fairly well-known but arguably not used as much. ECN is used prominently in the field of speaker verification but has not been introduced to the biomedical imaging or clinical community yet, although the statistical concepts it is based upon are long-standing in Bayesian decision theory.\n\n# 2.7.2 Decision guide S3. <br> DG3.1: Metric@(TargetMetric = TargetValue) \n\nIf a target value for a specific metric (typically Sensitivity; Fig. SN 3.16) is provided, the decision rule applied to the predicted class scores is optimized such that the specific target value is reached on a validation data set (Suppl. Note 5.4). Other metrics, depending on the target application, can then be reported for that specific threshold. Example Specificity@(Sensitivity $=0.95$ ): As illustrated in Fig. SN 2.9, the decision rule is set such that a Sensitivity of 0.95 is achieved. Other metric values (here Specificity; Fig. SN 3.17) can then be obtained from the corresponding fixed confusion matrix. In the example, this yields the Specificity at the predefined Sensitivity level. Possible candidates include Sensitivity (Fig. SN 3.16), Specificity (Fig. SN 3.17), PPV (Fig. SN 3.15), NPV, (Fig. SN 3.12) and False Positives per Image (FPPI) (Fig. SN 3.8).\n\n## DG3.2: Net Benefit (NB) versus Expected Cost (EC)\n\n## Summary of DG3.2: NB versus EC\n\n## NB\n\n(1) Decisions can be defined directly based on predicted class scores, interpreted as risks\n(2) Weighting of True Positive (TP) against False Positive (FP) in risk perspective\n(3) Lack of framework to validate the decision rule applied to class scores\n(4) Focus on reflectance of the (e.g., clinical) interest in the scores\n(5) Popular metric in clinical studies but not common in image analysis\n\nExtended Data Tab. SN 2.8. Comparison of Net Benefit (NB) and Expected Cost (EC) in the context of the decision guide DG3.2 for Subprocess S3. Context: FP2.6 = cost-benefit-based decision rule applied to predicted class scores requested.\n\n## EC\n\n(6) Decisions based on explicit definition of misclassification costs\n(7) Weighting of False Positive (FP) against False Negative (FN) in cost perspective\n(8) Availability of framework to validate the decision rule applied to class scores\n(9) Inherent interpretability with respect to naive classifier\n(1) Not known or used in the biomedical imaging domain although based on well-studied statistical concepts\n\nThis decision guide is embedded in the framework in Subprocess S3, which guides the selection of metrics that are reported separately for each class. In multi-class tasks (i.e., more than two classes present) this reporting amounts to a one-versus-rest validation scheme. However, this scheme is not intuitively applicable to a cost-benefit analysis (what are the costs and benefits of the 'rest' class?), which is the concept behind decision rules of both metrics in this decision guide. Thus, for"
    },
    {
      "markdown": "multi-class tasks we recommend to only proceed with the metrics selected in Subprocess S2 (e.g., EC or WCK) and not select any further metrics here to be reported in a one-versus-rest fashion, i.e.. we recommend to skip the guide.\n\nBoth NB (Fig. SN 3.11) and EC (Fig. SN 3.6) are linked to cost-benefit analysis [121] and are wellsuited when a cost-benefit-based approach for determining an appropriate decision rule applied to the predicted class scores is desired (FP2.6 = cost-benefit-based). To this end, both require the knowledge of task-dependent tradeoffs between benefits and costs, as detailed below. The following aspects are relevant when deciding between EC and NB (note that cost-based decision rule applied to predicted class scores is only considered for binary classification tasks in the scope of this work, thus referred to as a cutoff in this context):\n\nCost versus risk perspective: Cost perspective: For EC, explicit costs for both basic misclassifications (FP, FN) need to be defined or estimated. The optimal decision rule (i.e., cutoff on predicted class scores) that minimizes these costs can be analytically determined without data-based optimization. Risk perspective: In contrast, NB does not require the costs to be defined explicitly. Instead, predicted class scores are interpreted as probabilities or 'risks' of certain model output scores belonging to the positive class and the cutoff on the scores is defined directly on this scale based on task interest (e.g., \"only treat patients with cancer risk $>10 \\%$ \"). This can be interpreted as an implicit cost-benefit analysis resulting in a single intuitive risk score. However, it is also common for NB to make this cost-benefit analysis more explicit and define the risk as a relation of the benefit of TPs to the harms caused by FPs. A diagnostic test, for example, may lead to early identification and treatment of a disease, but typically the process will also cause some patients without disease being subjected to unnecessary further interventions. NB allows to consider such tradeoffs by putting the benefits and harms of the test on the same scale so that they can be directly compared. A physician may, for example, state that 10 FPs, resulting in unnecessary biopsies, are acceptable to find one more cancer case (TP).\n\nDecision curves: In most scenarios it is not possible to precisely define the costs or risks associated with the task. For example, it is not straightforward to make an exact decision on how many FPs would be acceptable to obtain one more TP. To compensate for this uncertainty, it is common practice to plot NB over a \"reasonable range of risk thresholds\" resulting in so-called decision curves [163]. This analysis allows assessing and comparing methods according to their NB scores without relying on a single cutoff. Although not common practice, one could also generate such curves for EC when expressing cost ratios as a risk score (i.e., switching from the cost to the risk perspective).\n\nCutoff on predicted class scores: In NB, the cost-benefit-based cutoff, which is determined directly from provided knowledge about the task and does not require data-based optimization, is an explicit part of the metric computation. In contrast, EC allows to alternatively determine a data-based cutoff by taking into account the provided costs in the metric calculation and minimizing EC on a dedicated data split, if available. A further difference between the two metrics is the way prevalence dependency is handled: EC isolates the class priors from the predicted class scores and defines them as a parameter of the cutoff itself, such that all application dependent parameters (costs and class priors) are part of the cutoff [51]. Upon deployment of a model on a new data set, the threshold can simply be updated analytically. Note that this process only works under the arguably strong assumption that the class priors of the new data set are known. In contrast, NB considers risk scores that incorporate the class priors, implying that the threshold depends solely on the cost-benefit tradeoff. As a consequence, when the class priors shift on a new data set, the risk-cutoff in NB requires class scores to be re-calibrated. The latter might be a harder requirement"
    },
    {
      "markdown": "because it requires a labeled validation set for re-calibration as opposed to requiring merely the class priors of the new data set for a threshold update.\n\nInterpretability: The following tradeoff exists between the two metrics regarding interpretability: EC allows reporting a normalized version (ECN), which makes the metric scores interpretable with regard to the performance of a random classifier. In contrast, in NB, the reference to a random classifier is typically done manually (by comparing the two scores), because NB itself allows for an interpretation as the 'proportion of net-TP', which would get lost by normalization.\n\nCalibration: Both metrics rely on the fact that predicted class scores are well-calibrated with regard to a chosen cutoff. EC allows assessing this requirement by calculating the extra cost entailed by miscalibration (or the potential for reducing cost by calibrating scores) [51]. The calibration error here is measured as the increase of EC with the analytical, i.e., task interest-based, cutoff compared to an empirical cutoff optimized on the data. Compared to related calibration errors (see Suppl. Note 2.6), this technique assesses a weaker calibration condition, which is directly targeted to the decision process at hand. For instance, even when assessing the relatively weak top-label calibration condition by means of Expected Calibration Error (ECE) with two bins and the border at the determined cutoff value, the distribution inside the bins would be considered, while EC only focuses on how many more cases would have been on the 'correct side of the cutoff' if scores were calibrated, without considering score distributions on either side of the cutoff.\n\nPopularity: Neither NB nor EC are widely used in the biomedical image analysis community. NB is a popular metric in clinical studies, while EC is currently not used but is part of a coherent framework of intuitive metric formulations (linked to Accuracy, BA, and extends to multi-class scenarios).\n\n# DG3.3: Positive Likelihood Ratio (LR+) versus Sensitivity \n\n## Summary of DG3.3: LR+ versus Sensitivity\n\n## LR+\n\n(1) Straightforward application in the case of an optimization-based decision rule (FP2.6)\n(2) Interpretation often reflecting interest in binary tasks\n\n## Sensitivity\n\n(1) Challenging application in the case of an optimization-based decision rule (FP2.6)\n(2) Good interpretability\n\nExtended Data Tab. SN 2.9. Comparison of LR+ and Sensitivity in the context of the decision guide DG3.3 for Subprocess S3. Context: FP2.6 = optimization- or argmax-based decision rule applied to predicted class scores requested and provided class prevalences do not reflect the population of interest (FP4.2 = FALSE).\n\nThis decision guide helps deciding between LR+ and Sensitivity in the context of per-class validation (Subprocess S3) with an optimization- or argmax-based decision rule applied to predicted class scores (FP2.6).\n\nLR+ (Fig. SN 3.14) is the likelihood ratio of the positive class. In a clinical example where the quality of a diagnostic test is to be assessed, this could be interpreted as the ratio of the likelihood of a diseased patient receiving a positive test result versus a healthy patient receiving a positive"
    },
    {
      "markdown": "test result $(P(t+|d+) / P(t+|d-)$, where $\\mathrm{t} / \\mathrm{d}$ denotes a positive(+)/negative(-) test/disease status). In other words: How much more likely is the occurrence of a positive test result for a diseased person compared to a healthy person? The formal calculation for this metric boils down to the following formula: LR+ = TPR / (1-TNR), where TPR/TNR are the Sensitivities of the positive/negative class.\n\nIn the provided context of this decision guide, where metrics are reported individually per class, Sensitivity (Fig. SN 3.16) and LR+ convey similar information and there is no 'incorrect' choice. Thus, the choice between the two can generally be made as the metric that is easier to interpret in the given task: In binary classification tasks (e.g., the provided example), LR+ conveys Sensitivities of both classes in a single score. Due to its intuitive and meaningful interpretation, it is often reported in clinical studies. In multi-class settings (which, in this context, amount to a one-versus-rest validation scheme), Sensitivities are generally easier to interpret, while the direct interpretation of $\\mathrm{LR}+$ as a property of a (clinical) test does not apply.\n\nIn case the decision rule applied to predicted class scores is to be determined on the basis of optimization on the target class, one additional consideration is of importance (FP2.6 = optimizationbased decision rule). When reporting Sensitivity per class, the decision rule can not be optimized based solely on the single Sensitivity at hand because this would always yield a cutoff value of 1 . LR+ naturally overcomes this problem. Other possible workarounds include choosing a different decision rule (FP2.6) or optimizing a weighted average over Sensitivity for all classes instead. The latter option should only be considered if meaningful weights across classes can be defined (e.g., based on class importance).\n\n# DG3.4: Positive Likelihood Ratio (LR+) versus Sensitivity versus $\\mathbf{F}_{\\beta}$ Score \n\nSummary of DG3.4: LR+ versus Sensitivity versus $\\mathbf{F}_{\\beta}$ Score\n\n| LR+ <br> (1) Meaningful interpretation in binary tasks | Sensitivity <br> (1) Generally good interpretability | $\\mathbf{F}_{\\beta}$ Score <br> (1) Limited interpretability |\n| :--: | :--: | :--: |\n| (2) Inherent interpretability with respect to naive classifier | (3) Inherent interpretability with respect to naive classifier only when averaging over classes | (3) No interpretability with respect to naive classifier |\n| (4) Insensitive to PPV | (5) Insensitive to PPV | (4) High scores ensures high PPV |\n\nExtended Data Tab. SN 2.10. Comparison of LR+, Sensitivity and $\\mathrm{F}_{\\beta}$ Score in the context of the decision guide DG3.4 for Subprocess S3. Context: FP2.6 = optimization- or argmax-based decision rule applied to predicted class scores requested and provided class prevalences reflecting the population of interest (FP4.2 = TRUE).\n\nIn the context of this decision guide, prevalence dependency is not an exclusion criterion (see FP4.2) and thus $\\mathrm{F}_{\\beta}$ Score (Fig. SN 3.7) can be considered as an alternative to Sensitivity-based"
    },
    {
      "markdown": "metrics (Sensitivity and LR+, Figs. SN 3.16 and SN 3.14). Details for the decision between the latter are provided in DG3.3; the present guide focuses on the pros and cons of opting for $\\mathrm{F}_{\\beta}$ Score.\n\nPer-class validation is commonly performed in a one-versus-rest fashion, naturally introducing class imbalance into the validation. Exceptions are binary scenarios with two balanced classes. For this exception, no compensation for class imbalance is needed (FP2.5.5 = FALSE) and the choice between $\\mathrm{F}_{\\beta}$ Score and Sensitivity-based metrics becomes less relevant, i.e., there are no obvious incorrect choices. Thus, the decision can be made on the basis of which metric is easier to interpret in a given task. For all other cases, the decision should be based on whether compensation for class imbalance is required (FP2.5.5 = TRUE).\n\nCompensation for class imbalance: As described in FP2.5.5 (and explained in more detail in Suppl. Note 1.3, \"Compensation for class imbalance\"), there are three aspects of compensation for class imbalance:\n(1) Establishing a reference value for random performance: LR+ provides a fixed random reference value at $\\mathrm{LR}+=1$, while for Sensitivity the scores of individual classes can vary and only their average is fixed at \"1/number of classes\" (equivalent to BA). $\\mathrm{F}_{\\beta}$ Score does not provide a reference value for random performance.\n(2) Establishing equal class contribution: In the provided context (S3), the validation is performed per class, such that this aspect is irrelevant.\n(3) Establishing consideration of predictive values: This aspect is the main reason to opt for $\\mathrm{F}_{\\beta}$ Score in this decision guide, because it is the only metric of the three where high scores ensure a high PPV. In contrast, LR+ and Sensitivity are insensitive to PPV, which, depending on the task interest, can substantially diminish their utility. An exemplary pitfall related to this choice is the confusion matrix of a binary classification task, as shown in Tab. SN 2.7. This classification system yields (for the two individual classes) Sensitivities of ( $90 \\%, 99 \\%$ ) and LR+ of $(90,90)$, respectively. Resulting $\\mathrm{F}_{1}$ Scores are $(0.165,0.995)$, indicating a low PPV and thus unsatisfying performance for the rare positive class. This pitfall may be of practical relevance in class-imbalanced tasks where FPs shall not be neglected. For example, in breast cancer screening, the provided classifier would not be useful, because $>90 \\%$ of all biopsies (TP+FP) would be unnecessary (FP).\n\nInterpretability: Out of the three, Sensitivity is arguably the easiest-to-interpret metric (exceptions are binary tasks, where LR+ might be preferable as detailed in DG3.3, see Tab. SN 2.9). $\\mathrm{F}_{\\beta}$ Score can be interpreted as the harmonic mean of Sensitivity and PPV, which adds a layer of complexity to the interpretation compared to Sensitivity. Thus, if the aspects discussed in \"compensation for class imbalance\" are not relevant, $\\mathrm{F}_{\\beta}$ Score might not be the metric of choice."
    },
    {
      "markdown": "# DG3.5: How to determine $\\beta$ in $\\mathrm{F}_{\\beta}$ Score \n\nSummary of DG3.5: $\\beta$ in $\\mathrm{F}_{\\beta}$ Score\n$\\beta<1$\n( Higher weighting\nof False Positive (FP)\npenalties (Positive Predictive\nValue (PPV))\n$\\beta=1$\n(0) Harmonic mean of PPV and Sensitivity\n\n$\\beta>1$\n(0) Higher weighting of False Negative (FN) penalties (Sensitivity)\n\nExtended Data Tab. SN 2.11. Determining the hyperparameter of the $\\mathrm{F}_{\\beta}$ Score in the context of the decision guide DG3.5 for Subprocess S3. Context: [Image-level classification (ImLC)]: FP2.6 = optimization- or argmax-based decision rule applied to predicted class scores requested and provided class prevalences reflecting the population of interest (FP4.2 = TRUE). [Object detection (ObD) or instance segmentation (InS)]: Either no predicted class scores available (FP5.1 = FALSE) or FP2.6 = optimization- or argmax-based decision rule applied to predicted class scores requested.\n\nThe $\\mathrm{F}_{\\beta}$ Score (Fig. SN 3.7) is defined as:\n\n$$\nF_{\\beta}=\\left(1+\\beta^{2}\\right) \\cdot \\frac{\\mathrm{PPV} \\cdot \\text { Sensitivity }}{\\left(\\beta^{2} \\cdot \\mathrm{PPV}\\right)+\\text { Sensitivity }}=\\frac{\\left(1+\\beta^{2}\\right) \\cdot \\mathrm{TP}}{\\left(1+\\beta^{2}\\right) \\cdot \\mathrm{TP}+\\beta^{2} \\cdot \\mathrm{FN}+\\mathrm{FP}}\n$$\n\nThe most common choice is to set $\\beta$ to 1 , resulting in equal weighting of FP and FN penalties. If unequal penalization of class confusions is desired (see FP2.5.2), higher values of $\\beta$ result in higher weights on FN penalties compared to FP penalties and thus imply a focus on Sensitivity compared to PPV.\n\n## DG3.6: $\\mathrm{F}_{\\beta}$ Score versus Panoptic Quality (PQ)\n\n## Summary of DG3.6: $\\mathrm{F}_{\\beta}$ Score versus PQ\n\n## $\\mathrm{F}_{\\beta}$ Score <br> (0) Pure detection metric <br> $\\mathrm{PQ}$ <br> (0) Hybrid metric for assessing detection and segmentation quality\n\nExtended Data Tab. SN 2.12. Comparison of $\\mathrm{F}_{\\beta}$ Score and Panoptic Quality (PQ) in the context of the decision guide DG3.6 for Subprocess S3. Context: FP2.6 = optimization- or argmax-based decision rule applied to predicted class scores and FP1.1 = instance segmentation (InS).\n\nThe $\\mathrm{F}_{\\beta}$ Score (Fig. SN 3.7) is a pure detection metric counting TP, FP, and FN detections on instance level (specifically, it represents the harmonic mean of PPV (Fig. SN 3.15) and Sensitivity (Fig. SN 3.16; see also [127]). The \"segmentation aspect\" of instance segmentation is here only incorporated via a prior cutoff on the localization criterion operating on pixel level (e.g., \"Intersection over Union (IoU) $>0.5$ \"). In case shifting the focus of validation more towards the segmentation quality of successfully matched (TP) instances is desired, there are two options:"
    },
    {
      "markdown": "(1) Complementary segmentation metric: One option is to select separate segmentation metrics in addition to object detection metrics such as $\\mathrm{F}_{\\beta}$ Score on a per-instance basis (e.g. \"Dice Similarity Coefficient (DSC) per TP-instance\"). This selection is naturally incorporated in the instance segmentation recommendation (Fig. 2) by the subroutines S6 (Extended Data Fig. 6) and S7 (Extended Data Fig. 7).\n(2) Hybrid metric: An alternative is to select PQ (Fig. SN 3.13) instead of F $\\beta$ Score, which allows expressing both interests (detection performance and segmentation quality) in a single score. Essentially, PQ is a modified $\\mathrm{F}_{1}$ Score, where TP instances do not count as \" 1 \" in the calculation, but the \" 1 \" is replaced with the associated DSC score (range [0,1]) of the instance. While combining the two aspects in a single score might be desirable, e.g., for method benchmarking or ranking, on the downside, such combined metrics make it harder to trace back performance to individual aspects (in this case: object detection versus segmentation; see Fig. SN 2.17).\n![img-63.jpeg](img-63.jpeg)\n\nExtended Data Fig. SN 2.17. Effect of assessing segmentation and detection quality in a single score. Prediction 1 achieves a high segmentation but low detection quality (with several False Positive (FP) predictions); vice versa for Prediction 2 (only predicting True Positive (TP) instances; no FP but low segmentation quality). However, both yield the same Panoptic Quality (PQ) score."
    },
    {
      "markdown": "# 2.7.3 Decision guide S4. <br> DG4.1: Area under the Receiver Operating Characteristic Curve (AUROC) versus Average Precision (AP) \n\n## Summary of DG4.1: AUROC versus AP\n\n## AUROC\n\n(1) Insensitive to Positive Predictive Value (PPV) under class imbalance\n(2) Inherent interpretability with respect to naive classifier\n(3) Straightforward interpretability\n\n## AP\n\n(4) High scores ensure high PPV including under class imbalance\n(5) Prevalence-dependent reference value for naive classifier\n(6) Limited interpretability\n\nExtended Data Tab. SN 2.13. Comparison of Area under the Receiver Operating Characteristic Curve (AUROC) and Average Precision (AP) in the context of the decision guide DG4.1 for Subprocess S4. Context: availability of predicted class scores (FP5.1 = TRUE), FP1.1 = image-level classification (ImLC) and provided class prevalences reflecting the population of interest.\n\nThe comparison between the two concepts behind AUROC and AP, i.e., the comparison between Receiver Operating Characteristic (ROC) curves and Precision-Recall (PR) curves has been extensively studied [42]. In practice, the choice between the two metrics boils down to the following aspects (if no clear choice can be made, we recommend reporting both metrics):\n\nCompensation for class imbalance effects: Of relevance in the context of this decision guide is pitfall 3 from FP2.5.5: \"Missing consideration of predictive values\" (more on this topic can be found in Suppl. Note 1.3, \"Compensation for class imbalance\"). AUROC is based on the Sensitivities of the two classes and does not consider predictive values. In class-imbalanced scenarios, this may lead to pitfalls such as depicted in Fig. SN 2.18, where near-perfect AUROC scores hide the fact that a system might have limited to no predictive utility. AP assesses the predictive value of the positive class (PPV) and thus compensates for the undesired effects caused by class imbalance: In the provided example, AP yields an intuitive score of 0.32 , reflecting the low PPV and thus low utility of the system in the context of the task. A technical explanation is given by the fact that the high number of True Negatives (TNs) dominate and suppress the FPs in the calculation of the TNR, thus yielding high scores for AUROC. A practical example for this pitfall might be a breast screening program, where a high PPV is of great importance to prevent unnecessary biopsies (FPs). The focus of AP on the positive class further has the effect that the resulting scores differ depending on which of the two classes is defined as positive and negative. This is in contrast to AUROC, which yields the same scores irrespective of this definition. The general approach for AP-based assessment in class-imbalanced scenarios is to define the rare class as the positive class. The fact that AP focuses on the positive class reflects the task interest of not letting rare (important) events be dominated by frequent events in the metric score.\n\nInterpretability: AUROC is easy to interpret as it simply represents the probability of a randomly sampled positive case having a higher predicted class score than a randomly sampled negative case. It further comes with a fixed reference value for the performance of a random classifier at 0.5 . AP, on the other hand, is harder to interpret and features no fixed random reference value. Instead, the AP score of a random classifier is the prevalence of the positive class which varies on each data set."
    },
    {
      "markdown": "Implementations: For reasons described in [42], the PR curve is more complex to interpolate compared to the ROC curve. This results in the existence of various implementations of AP, whereas no such heterogeneities exist for AUROC.\n\nPopularity: Although AUROC is the common choice for multi-threshold metrics, AP is also widely known and used.\n![img-64.jpeg](img-64.jpeg)\n\nExtended Data Fig. SN 2.18. Area under the Receiver Operating Characteristic Curve (AUROC) scores neglect the Positive Predictive Value (PPV) in class-imbalanced settings and might lead to misinterpretation of a model's discrimination quality. The figure shows the simulation outcome for a binary classification problem with a low prevalence for the positive class. A clinical example of this scenario are cancer screening programs, where most of the subjects are healthy. While AUROC is agnostic to the class prevalence and thus implies near-perfect discrimination with a score of 0.95 , the prevalence-dependent Average Precision (AP) allows focusing on discrimination of the rare positive class by explicitly considering the PPV and yields an intuitive score of 0.32 ."
    },
    {
      "markdown": "# DG4.2: Average Precision (AP) versus Free-Response Receiver Operating Characteristic (FROC) Score \n\n## Summary of DG4.2: AP versus FROC Score\n\nAP\n(6) Standard metric in computer vision community\n(6) Unawareness of data set sizes\n(6) For filtering low confidence predictions, a cutoff on confidence scores is required\n(6) Relatively good standardization of hyperparameters\nExtended Data Tab. SN 2.14. Comparison of Average Precision (AP) and Free-Response Receiver Operating Characteristic (FROC) Score in the context of the decision guide DG4.2 for Subprocess S4. Context: availability of predicted class scores (FP5.1 = TRUE) and FP1.1 = object detection (ObD) or instance segmentation (InS).\n\n## FROC Score\n\n(6) Preference in clinical context due to its domain-centered approach\n(6) Consideration of data set sizes\n(6) No consideration of lowconfidence predictions\n\nLack of standardization\n\nThe following aspects should be taken into account when deciding between AP (Fig. SN 3.20) and FROC Score (Fig. SN 3.21):\n\n- Community preferences: While AP constitutes the undisputed standard metric for object detection and instance segmentation in the computer vision community, the FROC Score is often favoured in the clinical context due to its easier interpretability despite its lack of standardization (employed FPPI Scores vary across studies [12, 72, 138]). Thus, the decision between the two metrics often boils down to a decision between a standardized and technical validation versus an interpretable and application-focused validation.\n- Data set size awareness: In its default configuration, where AP is computed globally over the entire data set, the metric is insensitive to performance per individual images. In contrast, the FROC Score takes into account the total number of images in the data set (see also Fig. SN 2.19). For example, given a data set and an AP as well as FROC score computed for this data set, adding more 'empty' images (i.e., images with no reference objects and no model predictions) would lead to an improved FROC score, because FROC rewards the model for correctly not predicting structures on these images. In contrast, the AP score would not be affected, because globally no new TPs, FPs or FNs are added that would alter the metric score. This property does not affect relative method comparison and can be related to the underlying question \"at which scale are matched objects (cardinalities) aggregated/counted?\". As described in Suppl. Note 2.4, AP can alternatively be configured to aggregate scores per individual image, in which case the total number of images in the data set is considered equally to FROC. [127] demonstrates how to apply AP and other object detection metrics to, for example, clinical scenarios requiring per-image aggregation. FROC score is a hybrid metric in this context, where Sensitivity is computed per data set while FPs are averaged over single images (FPPI)."
    },
    {
      "markdown": "- Dealing with low-confidence predictions: It is often desired to filter low confidence predictions (e.g., objects with high confidence of being background) prior to metric computation. For AP computation, this requires a cutoff on the confidence score or upper limits of considered predictions per image or per data set. For FROC, however, with typical values of FPPI, such low-confidence predictions naturally go unconsidered, thus allowing to avoid additional filtering measures.\n- FPPI: Different FPPI values are used in the field for computing the FROC Score, yielding non-standardized results (see Fig. SN 2.20). A potential default are the values $1 / 8,1 / 4,1 / 2,1$, $2,4,8$, as used for multiple popular benchmarks [138, 159]. Here, lower FPPI values (smaller than one) are weighted equally to higher FPPI values (greater than 1 ; four values each). Deviation from this weighting might be appropriate depending on the application, but should be explained. In the biostatistics community, areas under the curve are sometimes computed constraining the FPPI range to $[0,1][133]$.\n![img-65.jpeg](img-65.jpeg)\n\nExtended Data Fig. SN 2.19. Effect of the number of images per data set on the metric scores. The Average Precision (AP) metric does not take into account the total number of images, yielding the same score for data sets D1 and D2. The Free-Response Receiver Operating Characteristic (FROC) curve plots the average number of False Positives per Image (FPPI) against the Sensitivity, therefore accounting for the number of images. The FPPI is lower for D2, yielding a higher FROC score."
    },
    {
      "markdown": "![img-66.jpeg](img-66.jpeg)\n\nExtended Data Fig. SN 2.20. Effect of defining different ranges for the False Positives per Image (FPPI) used to draw the Free-Response Receiver Operating Characteristic (FROC) curve for the same prediction (top). The resulting FROC Scores change for different boundaries of the $x$-axis."
    },
    {
      "markdown": "# 2.7.4 Decision guide S5. <br> DG5.1: Kernel Calibration Error (KCE) versus Expected Calibration Error Kernel Density Estimate (ECE ${ }^{\\text {KDE }}$ ) \n\n## Summary of DG5.1: KCE versus ECE ${ }^{\\text {KDE }}$\n\n## KCE\n\n(1) Capture of isolated calibration quality\n(2) Unbiased estimator of canonical calibration error based on an alternative distance function\n\nBad interpretability, also due to negative output values\n(3) Recent proposition, not widely used\n\nDepends on nontrivial configuration choices of kernels and associated hyperparameters\nExtended Data Tab. SN 2.15. Comparison of Kernel Calibration Error (KCE) and Expected Calibration Error Kernel Density Estimate (ECE ${ }^{\\text {KDE }}$ ) in the context of the decision guide DG5.1 for Subprocess S5. Context: FP2.7.2 = U2 - comparison of calibration performance across classifiers on the same task requested and no mismatch between class prevalences and class importance (F2.5.3 = FALSE).\n\nThe context for this decision guide between KCE (Fig. SN 3.32) and ECE ${ }^{\\text {KDE }}$ (Fig. SN 3.31) is use case 2 (U2) in Fig. SN 2.15: \"comparing the calibration quality across multiple classifiers on the same task.\"\n\nGeneral differences: Both KCE and ECE ${ }^{\\text {KDE }}$ are estimators of a canonical calibration error, but measure this error based on different divergences, i.e., distance functions: $\\mathrm{ECE}^{\\mathrm{KDE}}$ is based on the $\\ell_{p}$ norm and thus straightforward to interpret and configure. In contrast, KCE is based on the \"maximum mean discrepancy\" and thus not interpretable (it may even take on negative values) and requires nontrivial configuration of kernels as well as associated hyperparameters. On the other hand, $\\ell_{p}$ norm estimators such as $\\mathrm{ECE}^{\\mathrm{KDE}}$ are inherently biased while KCE is an unbiased estimator. Arguably, in the context of this decision guide (U2), interpretability of the calibration error estimate is not required, since only a comparative, or relative assessment is requested rendering the unbiased KCE the intuitive choice. However, recent research on $\\ell_{p}$ norm estimators presents effective debiasing schemes [123], which might render the resulting bias neglectable in the near future and thus make $\\ell_{p}$ estimators such as $\\mathrm{ECE}^{\\mathrm{KDE}}$ a viable alternative for comparative calibration assessment.\n\nPopularity: Calibration error estimates KCE and $\\mathrm{ECE}^{\\mathrm{KDE}}$ are both recently proposed measures that are not widely known in the biomedical community."
    },
    {
      "markdown": "DG5.2: Brier Score (BS) versus Kernel Calibration Error (KCE) versus Expected Calibration Error Kernel Density Estimate (ECE ${ }^{\\text {KDE }}$ )\n\n# Summary of DG5.2: BS versus KCE versus ECE ${ }^{\\text {KDE }}$ \n\nBS\n( Capture of effects of (re-) calibration methods on discrimination performance in addition to calibration quality\n(1) Unbiased measure of an $\\ell_{2}$ norm canonical calibration error\n(3) Straightforward interpretability of relative improvement\n(4) Established statistical concept with long history of applications in many fields of research\n\nKCE\n( Capture of isolated calibration quality\n( Unbiased estimator of canonical calibration error based on an alternative distance function\n\nBad interpretability, also due to negative output values Recent proposition, not widely used\n\n## ECE ${ }^{\\text {KDE }}$\n\nCapture of isolated calibration quality\n\nPotentially biased estimator of an $f_{p}$ canonical calibration error (bias might be rendered neglectable by future de-biasing schemes)\n( Straightforward interpretability of relative improvement Recent proposition, not widely used\n\nThe context for this decision guide between BS (Fig. SN 3.28), ECE ${ }^{\\text {KDE }}$ (Fig. SN 3.31), and KCE (Fig. SN 3.32) is use case 1 (U1) in Fig. SN 2.15: \"comparing the effect of one or more re-calibration methods on the same (fixed) classifier.\"\n\nGeneral differences: BS can be decomposed into discrimination and calibration terms, where the calibration term exactly resembles the canonical calibration error (as defined in Suppl. Note 2.6). As the purpose of the metric in the provided context is to assess the performance of different re-calibration methods for the same classifier, a higher BS score also implies a better calibration in the case of accuracy-preserving calibration methods. As a major difference to BS, KCE estimates the"
    },
    {
      "markdown": "canonical calibration error directly. While this estimation is not biased (i.e., it is not dependent on the data set size), the resulting estimates are not interpretable, that is, they only allow for relative comparison on the same task (equivalently to BS). Further, KCE requires nontrivial configuration of kernels as well as associated hyperparameters. In contrast to KCE, current estimators of $\\ell_{p}$ calibration error are biased, but are highly interpretable and straightforward to configure. Moreover, recent developments in this line of research present effective de-biasing schemes [123], which might render the resulting bias neglectable in the near future and thus make $\\ell_{p}$ estimators such as $\\mathrm{ECE}^{\\mathrm{KDE}}$ a viable alternative also for comparative calibration assessment.\n\nApplicability: Generally, BS is attractive for ranking re-calibration methods that are guaranteed to be accuracy-preserving (such as the common temperature scaling [57]). Otherwise, the metric must be applied with care, because altered discrimination performance will dilute the focus on calibration quality in the ranking. Note that it may also be desirable to capture the effect of (non-accuracy-preserving) re-calibration methods on the discrimination performance. In such cases of comprehensive assessment of re-calibration methods, it is also appropriate to apply BS. In contrast to BS, calibration error estimators such as KCE and $\\mathrm{ECE}^{\\mathrm{KDE}}$ are capable of comparing the calibration error of re-calibration while being agnostic to potential changes of discrimination performance caused by the transformations. For the provided use case, this property allows the ranking of non-accuracy-preserving transformations, such as recently proposed techniques employing spline interpolations [58] or Gaussian processes [166], purely according to their calibration error while ignoring their effects on the discrimination performance.\n\nInterpretability: Defined as the root mean square error between predictions and references, BS is bounded between $[0,1]$ and therefore straightforward to interpret as an overall measure. However, as the calibration error is not isolated and scores are still conflated with the (same fixed) discrimination performance, only a relative comparison of calibration errors is possible. KCE is generally hard to interpret, also because it can yield negative values. $\\mathrm{ECE}^{\\mathrm{KDE}}$ as an estimator of $\\ell_{p}$ calibration error is straightforward to interpret.\n\nPopularity: BS is a widely known metric for overall performance measures with a long history of usage. Calibration error estimates KCE and $\\mathrm{ECE}^{\\mathrm{KDE}}$ are both recently proposed measures and not widely known in the biomedical community.\n\nReasons to not recommend Negative Log Likelihood (NLL) in this context: NLL essentially assesses a weighted version of the canonical calibration error as the logarithm leads to heavy penalization of tail probabilities. As the implications of this weighting on calibration assessment (as opposed to the overall performance measure) are not intuitive, we generally do not recommend NLL in this use case."
    },
    {
      "markdown": "# DG5.3: Brier Score (BS) versus Negative Log Likelihood (NLL) \n\n## Summary of DG5.3: BS versus NLL\n\nBS\nBounded penalization of errors leads to preference of naive systems in imbalanced settings\n\nC Straightforward interpretability as the mean squared error\n(4) Established statistical concept with long history of applications in many fields of research\n\n## NLL\n\n(4) Heavy penalization of extreme scores (close to 0 or 1 ), thus ability to capture missing rare events. General preference of conservative models\n(4) Difficult interpretability due to lack of upper bound\n(4) Established statistical concept with long history of applications in many fields of research\nExtended Data Tab. SN 2.17. Comparison of Brier Score (BS) and Negative Log Likelihood (NLL) in the context of the decision guide DG5.3 for Subprocess S5. Context: FP2.7.2 = U3 - comparison of overall performance across classifiers requested.\n\nThe context for this decision guide between BS and NLL is use case 3 (U3) in Fig. SN 2.15: \"overall performance measure requested.\" Both BS (Fig. SN 3.28) and NLL (Fig. SN 3.33) are overall performance measures, which capture discrimination and canonical calibration in a single score.\n\nPenalization of errors: Like Accuracy, BS penalizes errors of all events equivalently irrespective of the class prevalence. This implies that scores may drastically change when the prevalence changes and thus renders BS a highly prevalence-dependent metric. For instance, in imbalanced scenarios, a naive system that simply predicts the dominant class can receive a high BS, similarly to a high Accuracy score. One strategy to cope with this is to divide the BS by the BS achieved with a naive system, resulting in the normalized variant Brier Skill Score (BSS). Equivalently to ECN, this transformation is a rescaling of scores to establish a 'naive baseline' and enhance interpretability, but errors are still penalized equivalently irrespective of class prevalence. In other words, equal importance of classes (FP2.5.1) is not reflected in the metric, and missing a frequent event is still as heavily penalized as missing a rare event although missing a rare event has a greater effect on the respective class sensitivity. This results in a strict interpretation where the total amount of errors has to be lower than the number of events in the rare class in order for a system to be considered 'better than random'.\n\nCompared to squared error penalization in BS, the logarithm introduces a stronger penalization of tail probabilities [124]. In consequence, overconfident predictions (such as a score of 1 , implying scores of 0 in the other classes) lead to higher losses. For example, predicting 0.001 rather than 0.01 (when the true class is ' 1 ') increases BS by $\\approx 2 \\%$ and NLL by $\\approx 230 \\%$ (for this single entry). A practical effect of this penalty is a naturally higher penalization of naive systems in class imbalance scenarios, addressing the pitfall of BS above. NLL is thus of potential interest in scenarios with high class imbalance, where missing rare events would be heavily penalized, compared to BS which is prone to favoring naive systems. Generally, the penalization effect can also be described as NLL favoring more conservative models that avoid predictions of extreme class scores."
    },
    {
      "markdown": "Interpretability: BS is relatively straightforward to interpret as the mean squared error between predictions and the reference. The resulting scores are bounded ( $[0,1]$ ). NLL is arguably harder to interpret featuring logarithmic penalization of errors and thus no upper bound of the resulting score (bounds: $[0, \\infty]$ )\n\nPopularity: Both metrics are common statistical concepts and come with a long history of usage in many fields of research.\n\n# DG5.4: Expected Calibration Error (ECE)/ Root Brier Score (RBS) versus Class-wise Calibration Error (CWCE) versus Expected Calibration Error Kernel Density Estimate (ECE ${ }^{\\text {KDE }}$ )/ Class-wise Calibration Error (CWCE)/ Root Brier Score (RBS) \n\nThe decision between the sets of metrics boils down to determining whether predicted class scores should be tested for top-label calibration (as measured by ECE, Fig. SN 3.30), marginal calibration (as measured by CWCE, Fig. SN 3.29), or canonical calibration (as measured by ECE ${ }^{\\text {KDE }}$, Fig. SN 3.31. If there is an unequal interest across classes (FP2.5.1), CWCE is the natural choice. In this case we recommend both per-class and weighted reporting (by class importance). Note that only aggregated reporting comes with the pitfall of unstable results, specifically in the case of few samples or many classes. In the case of equal interest across classes, the key question is whether the task interest is limited to the predicted scores that lead to the classification decision (top-label) or whether there are reasons to request all predicted scores to be calibrated.\n\nNotably, in binary classification tasks, the two conditions are equivalent [156].\nReasons for and against focusing on top-label calibration (ECE): The task interest focuses on the decisions made by the classifier and only lies in the probabilities of the resulting decisions. In case the underlying biomedical research question has a dedicated focus on the decision process, toplabel error might be the right choice, because it directly reflects this focus. Conflating the calibration of decisions with other probabilities might be interpreted as washing out the task focus in this case. Although it is common practice to assess calibration quality with ECE, this approach comes with various pitfalls. Importantly, it is often ignored that top-label calibration implies an argmax decision rule based on the predicted class scores, which is often not an optimal decision rule as discussed in Suppl. Note 1.1 ('decision rule on predicted class scores'; see Fig. SN 1.1). Caution should also be exercised if there is a mismatch between class prevalences and class importance (FP2.5.3) as the top-label calibration is highly biased towards the high-prevalence classes. Furthermore, ECE commonly relies on binning of class scores, which introduces a dependency of the resulting metric score on the specific binning scheme. The number of bins is a configuration parameter that should by no means be optimized on the final validation data. Note in this context that binning has been shown to result in a more biased estimation compared to density estimation methods [123].\n\nReasons to extend the focus to all predicted scores (ECE ${ }^{\\text {KDE }}$ and CWCE): A common perception is that the canonical calibration condition, which is the strongest condition considering all predicted class scores, is the appropriate one in many application scenarios [51, 56, 123]. One reason lies in the limitations of top-label calibration and associated binning estimators described above. Another reason could be a broad task interest in all predictions beyond the classification decision. In the clinical context, for instance, the risk for all potential outcomes might be relevant for further treatment or shall be communicated to the patient. In such scenarios, calibration of all probabilities might be of interest. Consider, for instance, a multi-way classification of tumor categories, where one category is more aggressive than others. Even though the final prediction of the system is 'benign lesion', it might be of clinical interest to know (and communicate to the"
    },
    {
      "markdown": "patient) whether the probability for this outcome was $5 \\%$ or $20 \\%$. While the primary calibration metric for such scenarios should be $\\mathrm{ECE}^{\\mathrm{KDE}}$ as an estimate of the canonical calibration, it might be of interest to additionally report marginal calibration (as measured by CWCE) separately for each class. Notably, for these scenarios, alternatively splitting the problem into individual domain questions that result in separate traversals for each class of interest should be considered (see Suppl. Note 1.1).\n\nAdditional reporting of RBS (Fig. SN 3.34) as a guaranteed upper bound on the calibration error: In top-label and canonical calibration, we recommend the additional reporting of RBS as a guaranteed upper bound on the calibration error. As popular methods to assess calibration quality such as ECE or $\\mathrm{ECE}^{\\mathrm{KDE}}$ are known to over- or underestimate the error [56], this guarantee provides additional information, especially in safety-critical applications where the calibration error must not be underestimated.\n\n# 2.7.5 Decision guide S6. <br> DG6.1: Dice Similarity Coefficient (DSC) versus Intersection over Union (IoU) \n\n## Summary of DG 6.1: DSC versus IoU\n\n## DSC\n\n(○) Identical to $\\mathrm{F}_{1}$ Score\n(○) Close relation to IoU (see Eq. 5)\n(○) Preference in medical community\n\n## IoU\n\n(○) Identical to Jaccard Index\n(○) Close relation to DSC (see Eq. 4)\n(○) Preference in computer vision community\n\nExtended Data Tab. SN 2.18. Comparison of Dice Similarity Coefficient (DSC) and Intersection over Union (IoU) in the context of the decision guide DG6.1 for Subprocess S6. Context: no exclusive interest in the center line of structures (FP2.3 = FALSE, FP3.3 = FALSE) and equal severity of class confusions (FP2.5.2 = FALSE).\n\nThe DSC (Fig. SN 3.5) is identical to the $\\mathrm{F}_{1}$ Score on pixel level and closely related to the IoU (Fig. SN 3.9), which, in turn, is identical to the Jaccard Index (see equations 4 and 5). The two metrics will yield the same ranking of aggregated metric values in most applications (theoretically, deviations are possible), such that there is no value in combining them. Commonly, the computer vision community prefers the IoU, while the medical image community favors the DSC.\n\n$$\n\\text { IoU }=\\frac{\\text { DSC }}{2-\\text { DSC }} \\quad \\text { DSC }=\\frac{2 \\text { IoU }}{1+\\text { IoU }}\n$$\n\n## DG6.2: How to determine $\\beta$ in $\\mathrm{F}_{\\beta}$ Score\n\nThe $\\mathrm{F}_{\\beta}$ Score (Fig. SN 3.7) is defined as:\n\n$$\nF_{\\beta}=\\left(1+\\beta^{2}\\right) \\cdot \\frac{\\mathrm{PPV} \\cdot \\text { Sensitivity }}{\\left(\\beta^{2} \\cdot \\mathrm{PPV}\\right)+\\text { Sensitivity }}=\\frac{\\left(1+\\beta^{2}\\right) \\cdot \\mathrm{TP}}{\\left(1+\\beta^{2}\\right) \\cdot \\mathrm{TP}+\\beta^{2} \\cdot \\mathrm{FN}+\\mathrm{FP}}\n$$\n\nThe most common choice is to set $\\beta$ to 1 , resulting in equal weighting of Sensitivity (Fig. SN 3.16) and PPV (Fig. SN 3.15). Higher values of $\\beta$ result in higher weights on FN penalties (undersegmentation in segmentation problems) compared to FP penalties (oversegmentation) and thus imply a focus on Sensitivity compared to PPV."
    },
    {
      "markdown": "# 2.7.6 Decision guide S7. <br> DG7.1: Normalized Surface Distance (NSD) versus Boundary Intersection over Union (IoU) \n\n## Summary of DG7.1: NSD versus Boundary IoU\n\nNSD\n(C) Accounting for noisy images, limited resolution or imprecise reference annotations\n(C) Influence of hyperparameter on scores: distances below tolerance threshold are considered TP\n\n## Boundary IoU\n\n(C) Measurement of overlap between predicted and reference contours up to certain width\n(C) Influence of hyperparameter on scores: distance parameter determines thickness of the considered boundary\nExtended Data Tab. SN 2.19. Comparison of Normalized Surface Distance (NSD) and Boundary Intersection over Union (IoU) in the context of the decision guide DG7.1 for Subprocess S7. Context: possibility of spatial outliers in the reference annotation (FP4.3.2 = TRUE) or, if FALSE, FP2.5.6 = existence-based penalization of outliers.\n\nThe following aspects should be considered when deciding between NSD (Fig. SN 3.26) and Boundary IoU (Fig. SN 3.23):\n\n- Different research questions: Both metrics set the focus on the boundary/contour of structures, but fundamentally differ in what they measure: NSD measures the DSC score on the surface voxels (often interpreted as the ratio of correctly predicted contour), where the strictness for what constitutes a correct boundary is controlled by a tolerance parameter. This way, noise in the image, limited resolution, or imprecise reference annotations can be accounted for. Boundary IoU directly measures the overlap between predicted and reference contours (without tolerance) up to a certain width (which is controlled by a width parameter). Thus, NSD is preferable if a tolerance accounting for imprecise annotations is requested. Boundary IoU, on the other hand, is preferable if contour errors are thought of as crucial inconsistencies that should be assessed, or if a wider area around the contour line is of interest (dynamic transition to the classical IoU).\n- Setting the hyperparameter: The NSD and Boundary IoU both require users to manually set a hyperparameter. NSD: Boundary distances below the tolerance threshold will be considered TP (deviations do not count as errors). This parameter can be set according to the inter-rater variability or, if not available, heuristics. Boundary IoU: The distance parameter determines the thickness of the considered boundary and thus also influences the sensitivity to contour errors (the smaller the distance, the higher the sensitivity). This parameter can also be set according to the inter-rater variability (here in order to capture potential inconsistencies, as opposed to disregarding noise as in NSD) or, if not available, heuristics."
    },
    {
      "markdown": "# DG7.2: Mean Average Surface Distance (MASD) versus Average Symmetric Surface Distance (ASSD) \n\n## Summary of DG7.2: MASD versus ASSD\n\n## MASD\n\n(6) Equal contribution of reference and prediction boundaries to the metric score\n(6) Possibility of misleading results in corner cases (e.g., tiny prediction closely located to the reference)\nExtended Data Tab. SN 2.20. Comparison of Mean Average Surface Distance (MASD) and Average Symmetric Surface Distance (ASSD) in the context of the decision guide DG7.2 for Subprocess S7. Context: FP2.5.6 = distance-based penalization of outliers with contour focus.\n\nThe ASSD (Fig. SN 3.22) puts all boundary distances (all distances from boundary A to boundary B and all distances from boundary B to boundary A) in a list, then takes the mean (Fig. SN 2.21). Thus, if one boundary is much larger than the other, this boundary will impact the mean much more. The MASD (Fig. SN 3.25) computes the sum of the mean distances from boundary A to boundary B and the mean distances from boundary B to boundary A. Therefore, the reference and prediction boundaries contribute equally (see Fig. SN 2.21). While there are corner cases in which MASD features disadvantages compared to ASSD as well (see Fig. SN 2.22), we generally recommend MASD because of the aforementioned advantage.\n![img-67.jpeg](img-67.jpeg)\n\nExtended Data Fig. SN 2.21. Most commonly used distance-based segmentation metrics: (a) the Average Symmetric Surface Distance (ASSD) and (b) the Mean Average Surface Distance (MASD). The term $d(a, b)$ denotes the Euclidean distance between boundary pixels $a$ and $b$. Only the True Positives (TPs) are considered."
    },
    {
      "markdown": "![img-68.jpeg](img-68.jpeg)\n\nExtended Data Fig. SN 2.22. Corner case in which Mean Average Surface Distance (MASD) yields an undesired result. If the Prediction is very small (here: one pixel) and located close to the reference boundary, the Mean Average Surface Distance (MASD) will be much lower compared to the Average Symmetric Surface Distance (ASSD).\n\nDG7.3: Hausdorff Distance (HD) versus $\\mathbf{X}^{t h}$ Percentile Hausdorff Distance ( $\\mathbf{X}^{t h}$ Percentile HD)\n![img-69.jpeg](img-69.jpeg)\n\nThe HD (Fig. SN 3.24) calculates the maximum of all shortest distances for all points from one object boundary to the other, which is why it is also known as the Maximum Symmetric Surface Distance [173]. The $X^{t h}$ Percentile HD calculates the $X^{t h}$ percentile (e.g., $95 \\%$ percentile, the Hausdorff Distance 95th Percentile (HD95), Fig. SN 3.27) instead of the maximum, and should therefore be used instead if spatial outliers should be disregarded (FP2.5.6, see Fig. SN 2.23)."
    },
    {
      "markdown": "# Reference \n\n![img-70.jpeg](img-70.jpeg)\n\nExtended Data Fig. SN 2.23. Effect of annotation errors/noise. A single erroneously annotated pixel may lead to a large decrease in performance, especially in the case of the Hausdorff Distance (HD) when applied to small structures. The Hausdorff Distance 95th Percentile (HD95), on the other hand, was designed to deal with spatial outliers. Further abbreviations: Dice Similarity Coefficient (DSC), Intersection over Union (IoU), Average Symmetric Surface Distance (ASSD), Normalized Surface Distance (NSD)."
    },
    {
      "markdown": "# 2.7.7 Decision guide S8. <br> DG8.1: Mask Intersection over Union (IoU) versus Boundary IoU versus Intersection over Reference (IoR) \n\n![img-71.jpeg](img-71.jpeg)\n\nExtended Data Tab. SN 2.22. Comparison of Mask Intersection over Union (IoU), Boundary IoU and Intersection over Reference (IoR) in the context of the decision guide DG8.1 for Subprocess S8. Context: FP1.1 = instance segmentation (InS).\n\nIn instance segmentation problems, it might be appropriate to base the localization criterion on the corresponding target segmentation metric (custom criterion). For example, if the target segmentation metric chosen in Subprocess S6 (Extended Data Fig. 6) is NSD, the localization criterion could be defined accordingly. This may not always be possible, for example because the target metric has no fixed upper bound (e.g., HD), rendering the setting of adequate localization cutoffs challenging. An alternative strategy is to choose one of the common object detection localization criteria.\n\nThe following aspects should be taken into account when deciding between Mask IoU (Fig. SN 3.9), Boundary IoU (Fig. SN 3.23), and IoR (Fig. SN 3.37) in instance segmentation problems. We will"
    },
    {
      "markdown": "first focus on the more subtle distinction between Mask IoU and Boundary IoU, and finally discuss scenarios for potential usage of IoR:\n\n# Boundary versus Mask IoU \n\n- Boundary focus: While Mask IoU measures the overlap of structures in general, Boundary IoU allows to focus on the correctness of boundaries (FP2.1, see Fig. SN 2.24). Note that the focus on boundaries also comes with pitfalls. Boundary IoU can even be deceived to result in a perfect value of 1.0 despite an imperfect prediction (see Fig. SN 2.25).\n- Small structures: Mask IoU over-penalizes small structures in tasks with high variability of structure sizes (FP3.2) because boundary pixels increase linearly (or quadratically) with size, while total pixels increase quadratically (or cubically) with size. Boundary IoU [28] addresses this issue by selecting only pixels with a maximum distance of \"d\" with regard to the boundary for validation (see Fig. SN 2.24).\n- Hyperparameters: For the computation of Boundary IoU, the distance \"d\" constitutes an additional and sensitive hyperparameter to be determined. It can be determined based on inter-rater variability, for example.\n- Popularity: While Mask IoU represents an established concept that is well-known to the community, Boundary IoU is a recently proposed modification [28] that might thus require specific introduction when used in validation.\nIoR In the case of a high ratio of touching reference objects, 'non-split errors' (one prediction overlaps multiple reference objects) might occur frequently. While the IoU criterion can potentially heavily penalize this scenario resulting in FN and multiple FPs, a less severe penalization might be desired, e.g., in the form of the Intersection over Reference (IoR) [104]. IoR essentially considers the ratio of the area of a reference object that is covered by a prediction (see Fig. SN 2.26), allowing for multiple TP matches of the same prediction. Appropriate penalization in these cases is then ensured either by separating such errors as 'merge errors' [25], or by means of additional segmentation metrics. IoR shares the behaviour of Mask IoU regarding the above discussions on boundaries and small structures. As a major disadvantage, it can be deceived by large predictions. Widespread usage of IoR is currently limited to the field of cell segmentation, where images with high density of structures are present [104]."
    },
    {
      "markdown": "![img-72.jpeg](img-72.jpeg)\n\nExtended Data Fig. SN 2.24. Compared to the Mask Intersection over Union (IoU), the Boundary IoU (third and fourth column, representing two different thresholds) (1) specifically penalizes errors in the boundaries and (2) is more invariant to structure sizes (top: large; bottom: small).\n![img-73.jpeg](img-73.jpeg)\n\nExtended Data Fig. SN 2.25. Example of a perfect Boundary Intersection over Union (IoU) score for an imperfect prediction. Overlapping pixels from the reference and prediction are shown in light blue. For a prediction with a hole in the middle, the Boundary IoU may result in a score of 1.00 if the distance to border contains all mask pixels (here: distance $=2$ ). However, the Mask IoU spots the problem and yields a lower score."
    },
    {
      "markdown": "![img-74.jpeg](img-74.jpeg)\n\nExtended Data Fig. SN 2.26. In case of one prediction assigned to multiple reference objects, an assignment strategy needs to be chosen. This may be based, for example, on the Intersection over Union (IoU) $>0.5$ strategy, which may result in a heavy penalty (two False Negatives (FN) and one False Positive (FP)). Another option is to use the Intersection over Reference (loR) $>0.5$ strategy, which examines whether the prediction was successfully assigned to the reference objects. In an additional step, the \"non-split errors\" will be penalized. Used abbreviations: False Negative (FN), False Positive (FP) and True Positive (TP)."
    },
    {
      "markdown": "# D G 8.2: M ask Intersection over U nion (IoU) $>0$ versus C enter D istance versus Point inside M ask/Box/Approx \n\n## Summary of D G 8.2 : $M$ ask IoU $>0$ versus $C$ enter $D$ istance versus $P$ oint inside M ask/Box/Approx\n\n## Mask IoU $>0$\n\n(1) No hyperparameters (standardized)\n(2) Strictness of criterion cannot be varied\n(3) Potential large ambiguity of the predicted location as only few pixels overlap the reference\n\n## Center Distance\n\n(4) Distance threshold must be provided\n(5) Strictness of criterion can be varied\n(6) Good representation of the object center (FP2.3)\n\n## Point inside <br> M ask/Box/Approx\n\n(1) No hyperparameters (standardized)\n(2) Strictness of criterion cannot be varied\n(3) Relatively good representation of tubular or disconnected structures\n\nExtended D ata Tab. SN 2.23. Comparison of M ask Intersection over U nion (IoU) $>0$, Center D istance and Point inside M ask/Box/Approx localization criteria in the context of the decision guide DG8.2 for Subprocess S8. Context: FP1.1 = object detection (ObD) problems in the case of either (1) FP4.4 = reference annotations provided as exact outline and FP2.4 = a desired localization as only position or (2) FP4.4 = reference annotations provided as rough outline and FP2.4 = a desired localization as only position. Note that $M$ ask $\\mathrm{IoU}>0$ is only relevant for case (1).\n\nWhen choosing a localization criterion for tasks where the mere existence of objects is of interest (as opposed to the outlining of objects), the following aspects should be considered:\n\n- Loose criterion: (only recommended if the reference is provided as exact outline (FP2.4)) The intuitive choice of a very loose IoU criterion (e.g., \"IoU $>0$ \" or \"at least one pixel overlap\", Fig. SN 3.9) comes with simplicity but implies the pitfall that the size of the predicted structure is in theory unbounded, i.e., the predicted location can be ambiguous (see Fig. SN 2.27).\n- Point-based criteria: A preferable alternative for the case of pure localization (without interest in outlines) is to constrain the prediction to a single coordinate. A common criterion for this scenario is the distance to the center point of the structure (Fig. SN 3.36; which can also be of explicit interest, see FP2.3, Fig. SN 2.28). The center point ${ }^{4}$, however, might not be a good reference for tubular structures (check FP3.3) or disconnected structures (check FP3.6). In such cases (and if annotations are provided in the form of masks), a binary Point inside M ask/Box/Approx criterion (Fig. SN 3.40) might be the better choice. On the other hand, the Point inside M ask/Box/Approx criterion does not allow for a variation of the criterion's strictness (i.e., threshold). A pplication despite this shortcoming should be well-justified.\n\n[^0]\n[^0]:    ${ }^{4}$ Depending on what kind of information the center point is derived from, different definitions are possible as detailed in Fig. SN 3.36 ."
    },
    {
      "markdown": "![img-75.jpeg](img-75.jpeg)\n\nExtended Data Fig. SN 2.27. Effect of a loose Intersection over Union (IoU) criterion. When defining a True Positive (TP) by an $\\mathrm{IoU}>0$, the resulting localizations may be deceived by very large predictions."
    },
    {
      "markdown": "![img-76.jpeg](img-76.jpeg)\n\nExtended Data Fig. SN 2.28. Pitfalls of the Center Distance. (a) Ignoring overlap between objects. Both predictions have the same distance to their corresponding reference center. The Center Distance, which requires a threshold distance $\\tau$ between center points not be exceeded, does not take into account the overlap between objects. However, the right prediction does not overlap the reference and should thus not be considered a True Positive (TP). (b) Tubular structures. The Center Distance is not an ideal criterion because it implies that the prediction shown would result in a False Positive (FP), although it hits the elongated structure. This could be overcome by a Point inside Mask criterion."
    },
    {
      "markdown": "# DG8.3: Choose localization threshold \n\n## Summary DG8.3: Choose localization threshold\n\n## Lower thresholds\n\n(1) Interest in the existence of objects rather than their precise localization\n(3) Small size of structures (FP3.1)\n(4) High variability of structure sizes (FP3.2)\n(5) 3D input images\n(6) Uncertainties in the reference (FP4.3.1)\nExtended Data Tab. SN 2.24. Choosing the localization threshold in the context of the decision guide DG8.3 for Subprocess S8. This decision guide does not apply for Point inside Mask/Box/Approx criteria.\n\n## Higher thresholds\n\n(7) Interest in precise localization\n(8) Dense distribution of structures in images (FP3.5)\n\nNote that most localization criteria require a threshold to be set (e.g., IoU $>0.5$ counts as detected). However, such cutoff renders the validity of results limited to the specific threshold. To increase robustness of reported metrics, it is common practice in the computer vision community to average metrics over multiple cutoff values (default for IoU criteria: from 0.5 until 0.9 in steps of 0.05 ). On the other hand, certain properties of the underlying problem may limit the relevance of cutoff values to lower or higher values.\n\nThe following properties might warrant validation with lower thresholds: interest in the existence of objects rather than their precise localization, small size of structures (FP3.1), high variability of structure sizes (FP3.2), 3D input images (as volume increases cubically with size, the desired overlap ratio might require adaptation), uncertainties in the reference (FP4.3.1). Conversely, these properties typically warrant validation at higher thresholds: interest in precise localization, dense distribution of structures in images (FP3.5).\n\nIt should be noted that no threshold is needed for the Point inside Mask/Box/Approx and Mask $\\mathrm{IoU}>0$ criteria."
    },
    {
      "markdown": "# 2.7.8 Decision guide S9. <br> DG9.1: Assignment without predicted class probabilities on instance level \n\n## Summary DG9.1\n\n![img-77.jpeg](img-77.jpeg)\n\nThe following aspects should be considered when selecting the assignment strategy:\n\n- Matching via Overlap $>\\mathbf{0 . 5}$ (Fig. SN 3.44): If overlapping predictions are not possible (FP5.4 = FALSE), sophisticated matching strategies are often avoided in the biomedical domain by setting the threshold for the localization criterion (Mask IoU, Boundary IoU, or IoR) to > 0.5 . With this strategy, assignment ambiguities are inherently avoided. However, if either overlapping predictions are possible, a non-overlap based criterion is employed, or a criterion with a threshold above 0.5 is not appropriate, one of the following strategies should be chosen.\n- Greedy Matching (Figs. SN 3.41, SN 3.42): A greedy approach can be taken, in which each reference is assigned to the best matching prediction. If predicted class scores are available (FP5.1 = TRUE) this is typically achieved based on the class score (\"Greedy by Score Matching\", Fig. SN 3.41). In the given scenario with FP5.1 = FALSE, an intuitive alternative is to rank predictions by the localization criterion score (\"Greedy by localization criterion Matching\", Fig. SN 3.42). Assignment is then achieved by stepping through the ranked list, matching the current prediction with the most overlapping reference object, and removing the reference object from the assignment process.\n- Optimal (Hungarian) Matching (Fig. SN 3.43): The Hungarian algorithm optimizes the matching between predictions and reference objects while minimizing a given cost function, such as the average overlap for all matched pairs. Notably, this optimization generally leads to optimistic interpretation/validation of ambiguous model outputs, but might not represent the most realistic approximation of model performance upon application (see Fig. SN 2.29)."
    },
    {
      "markdown": "![img-78.jpeg](img-78.jpeg)\n\nExtended Data Fig. SN 2.29. Comparison of Greedy versus Hungarian matching assignment strategies.\n\n# SUPPL. NOTE 3 STEP 3 - METRIC APPLICATION \n\nOnce a suitable metric pool has been generated, the chosen metrics must be applied to the given data set. We recommend beginning with the setting of the global decision threshold in case metrics based on a fixed cutoff on the predicted class scores (FP2.6; more generally: decision region for more than two classes) have been selected, which is generally the case. In order to avoid overestimation of algorithm performance, this threshold needs to be set globally for all classes and metrics, as detailed in Suppl. Note 1.1. Once raw metric values have been computed for all metrics, metric values are aggregated, potentially combined (for rankings) and reported according to the recommendations in Extended Data Tab. 1. Importantly, we support the user by providing cheat sheets for the entire pool of Metrics Reloaded metrics that help find reference implementations and overcome metric-specific pitfalls (Suppl. Note 3.1)."
    },
    {
      "markdown": "# 3.1 Metrics Cheat Sheets \n\nIn this section, we present cheat sheets for the metrics deemed particularly relevant by the Metrics Reloaded consortium. We provide a description along with the formula as well the respective value range. For every metric, we indicate further important characteristics, such as the recommended problem categories or potential prevalence dependency. Finally, we highlight our recommendations. Many of the presented metrics rely on the confusion matrix, which is illustrated in Fig. SN 3.1.\n\nBINARY CONFUSION MATRIX\n![img-79.jpeg](img-79.jpeg)\n\nBINARY EXAMPLE\n![img-80.jpeg](img-80.jpeg)\n\nMULTI-CLASS CONFUSION MATRIX\n![img-81.jpeg](img-81.jpeg)\n\nMULTI-CLASS WEIGHT MATRIX\n![img-82.jpeg](img-82.jpeg)\n\nExtended Data Fig. SN 3.1. Schematic example of the confusion matrix for two and for $C$ classes. For the latter case, we also present a weight or cost matrix with weights $w_{i j}>0$ without loss of generality. For the binary confusion matrix, we show an example illustrating the cardinalities for a prediction of triangles and circles."
    },
    {
      "markdown": "# 3.1.1 Discrimination metrics. \n\n## Counting metrics\n\n## ACCURACY\n\n![img-83.jpeg](img-83.jpeg)\n\nExtended Data Fig. SN 3.2. Cheat Sheet for the Accuracy. The upward arrow in the value range indicates that higher values are better than lower values. Abbreviations used in the figure: Balanced Accuracy (BA), Bookmaker Informedness (BM), Cohen's Kappa (CK), Expected Cost (EC), Error Rate (ER), False Negative (FN), False Positive (FP), Image-level Classification (ImLC), Instance Segmentation (InS), Object Detection (ObD), Semantic Segmentation (SemS), True Negative (TN), True Positive (TP), Weighted Cohen's Kappa (WCK). Reference used in the figure: Tharwat, 2020: [148]. We recommend Accuracy as a multi-class counting metric in Subprocess 52 (Extended Data Fig. 2)."
    },
    {
      "markdown": "# BALANCED ACCURACY (BA) \n\n$$\nB A=\\frac{1}{2}(\\text { Sensitivity }+ \\text { Specificity })=\\frac{1}{2}\n$$\n\nVALUE RANGE: $[0,1] \\uparrow$\n\n## DESCRIPTION\n\nBA measures the arithmetic mean of Sensitivities for each class, i.e., for each class, it measures the fraction of actual positive samples that were predicted as such.\n\n## DEFINITION\n\n[Tharwat, 2020]\n\n## IMPORTANT RELATIONS\n\n- J = 2BA - 1\n- (W)CK = 2BA - 1, if classes are balanced (and using 0 -1-costs)\n- Accuracy = BA, if classes are balanced\n- $E C=1-B A$, if $E C$ costs are chosen such that $w_{i j}=0$ and $w_{i j}=1 /\\left(C_{m}\\right)$, where $w_{i j}$ are the costs for a sample of actual class $i$ that was predicted as class $j, C$ is number of classes and $P_{i}$ is prevalence of class $i$.\n\nRECOMMENDATIONS\n\n- BA should not be applied if...\n- ... there is an unequal interest across classes.\n- ... predictive values should be assessed.\n- ... class confusions are of unequal severity (examples: ordinal target classes, cost-benefit analysis).\n- Otherwise, it should specifically be considered...\n- ... in the presence of high class imbalance in case there is an equal interest across classes.\n- ... if a comparison of performance across data sets with different prevalences is desired.\n- BA can be used to identify and validate the decision rule applied to predicted class scores.\n\nExtended Data Fig. SN 3.3. Cheat Sheet for the Balanced Accuracy (BA). The upward arrow in the value range indicates that higher values are better than lower values. Abbreviations used in the figure: Image-level Classification (ImLC), Instance Segmentation (InS), Object Detection (ObD), Semantic Segmentation (SemS), Cohen's Kappa (CK), Expected Cost (EC), Youden Index (J), Weighted Cohen's Kappa (WCK). We recommend BA as a multi-class counting metric in Subprocess S2 (Extended Data Fig. 2)."
    },
    {
      "markdown": "# CENTERLINE DICE SIMILARITY COEFFICIENT (clDICE) \n\n![img-84.jpeg](img-84.jpeg)\n\n## DESCRIPTION\n\nclDice measures the overlap between two structures, ideally tubular-shaped. The formula is similar to the DSC, but relies on the topology precision and topology sensitivity which are defined based on the skeletons of the structures.\n\n## DEFINITION\n\n[Shit et al., 2021]\n\n## RECOMMENDATIONS\n\n- An overlap-based metric, such as clDice, should be used in most cases of segmentation assessment. An exception is the case of consistently tiny structures along with a noisy reference.\n- clDice should be preferred over the standard DSC ...\n- ... in the presence of tubular structures.\n- ... if structure center lines are of particular interest.\n- clDice should generally be used in combination with a boundary-based metric if boundaries are of interest.\n\nExtended Data Fig. SN 3.4. Cheat Sheet for the centerline Dice Similarity Coefficient (clDice). The upward arrow in the value range indicates that higher values are better than lower values. Abbreviations used in the figure: Dice Similarity Coefficient (DSC), Image-level Classification (ImLC), Instance Segmentation (InS), Object Detection (ObD), Semantic Segmentation (SemS). We recommend clDice as an overlap-based metric in Subprocess S6 (Extended Data Fig. 6)."
    },
    {
      "markdown": "# DICE SIMILARITY COEFFICIENT (DSC) \n\nSynonyms: Dice, Dice Coefficient, Sørensen-Dice Coefficient, F ${ }_{1}$ Score, Balanced F Score\n![img-85.jpeg](img-85.jpeg)\n\n$$\n\\begin{aligned}\n\\operatorname{DSC}(A, B) & =\\frac{2}{1+1}=\\frac{2|A \\cap B|}{|A|+|B|} \\\\\n& =\\frac{2 \\text { PPV } \\cdot \\text { Sensitivity }}{\\text { PPV }+ \\text { Sensitivity }}\n\\end{aligned}\n$$\n\nVALUE RANGE: $[0,1] \\uparrow$\n\n## DESCRIPTION\n\nDSC measures the overlap between two structures.\n\n## DEFINITION\n\n[Dice, 1945]\n\n|  | RECOMMENDED FOR |  |  |\n| :--: | :--: | :--: | :--: |\n| ImLC | SemS | ObD | InS |\n|  |  |  |  |\n\n## RECOMMENDATIONS\n\n- An overlap-based metric (by default the DSC or loU) should be used in most cases of segmentation assessment. An exception is the case of consistently tiny structures along with a noisy reference.\n- DSC should generally be used in combination with a boundary-based metric if boundaries are of interest.\n- DSC should generally not be considered if...\n\" ... there is a high variability of structure sizes within an image or across images.\n\"... inter-rater variability is requested to be compensated.\n\"... over- and undersegmentation should be treated similarly.\n- DSC should be considered as a metric in the medical community rather than in the computer vision and biology communities (where the almost identical loU is preferred).\n\nExtended Data Fig. SN 3.5. Cheat Sheet for the Dice Similarity Coefficient (DSC). The upward arrow in the value range indicates that higher values are better than lower values. Abbreviations used in the figure: False Negative (FN), False Positive (FP), Image-level Classification (ImLC), Instance Segmentation (InS), Intersection over Union (IoU), Object Detection (ObD), Positive Predictive Value (PPV), Semantic Segmentation (SemS), True Negative (TN), True Positive (TP). Reference used in the figure: Dice, 1945: [45]. We recommend DSC as an overlap-based metric in Subprocess S6 (Extended Data Fig. 6)."
    },
    {
      "markdown": "# EXPECTED COST (EC)/NORMALIZED EC (ECN) Synonyms: Expected prediction error, Expected loss \n\n![img-86.jpeg](img-86.jpeg)\n\n## DESCRIPTION\n\nEC is a generalization of the probability of error (which is, in turn, 1 - Accuracy) for cases in which errors cannot all be considered to have equally severe consequences. It is defined as the expectation of the cost, where the cost incurred on a certain sample depends on the sample's class and the decision made for that sample. In practice, the expectation can be estimated as a simple average of the costs over the evaluation samples.\nEC describes the weighted sum of error rates. It can be used to measure discrimination and calibration in one score.\n\n## VARIANT\n\nNormalized EC (ECN): normalizes EC by the EC of a naive system.\n\n## DEFINITION\n\n[Bishop and Nasrabadi, 2006; Hastie et al., 2009; Ferrer, 2022]\n\n## IMPORTANT RELATIONS\n\n- $\\mathrm{BA}=1$ - EC, if costs are chosen such that $w_{i j}=0$ for all $i$ and $w_{i j}=$ 1/(C-P $)$, where $w_{i j}$ are the costs for a sample of actual class $i$ that was predicted as class $j, C$ is number of classes and $P_{j}$ is prevalence of class i.\n- Accuracy $=1-\\mathrm{EC}$, if using 0-1-costs\n- Sensitivity, = EC, if costs are set $w_{i j}=1$ for that single $i$ and 0 otherwise\n- Specificity, = EC, if costs are set $w_{i j}=1$ all $j \\neq i$ and 0 otherwise\n\n\n## MULTI-CLASS DEFINITION\n\nFor $C$ classes, $E C$ is defined as:\n\n$$\nE C=\\sum_{i=1}^{c} \\sum_{j=1}^{c} P \\cdot w_{i j} \\cdot \\frac{n_{j j}}{\\bar{n}_{i}}\n$$\n\n## RECOMMENDED FOR\n\n| ImiC | SemS | ObD | InS |\n| :--: | :--: | :--: | :--: |\n| (1) | (2) | (3) | (4) |\n\n## PREVALENCE DEPENDENCY?\n\nBoth options are possible depending on how the priors are set in the definition of the metric.\n\n## RECOMMENDATIONS\n\n- EC is generally recommended as multi-class counting metric due to its flexibility in handling costs and prevalences. It is specifically recommended if problem-specific error costs are available, because it naturally integrates these in the metric score.\n- If class confusions are of unequal severity (examples: ordinal target classes), it is our default recommendation as mul-ti-class counting metric.\n- In binary settings, it is well-suited as a per-class counting metric in case a cost-benefit-based decision rule is desired for converting predicted class scores to decisions.\n- The prevalences $P_{j}$ can be set according to the expected prevalences in the target population (if known) in case the class prevalences of a data set do not match the prevalences of the target population.\n- EC can be used as the basis for decomposing the system performance into discrimination and calibration components. - EC can be used to identify and validate the decision rule applied to predicted class scores.\n\nExtended Data Fig. SN 3.6. Cheat Sheet for the Expected Cost (EC)/normalized EC (ECN). The downward arrow in the value range indicates that lower values are better than higher values. Abbreviations used in the figure: Balanced Accuracy (BA), Image-level Classification (ImLC), Instance Segmentation (InS), Object Detection (ObD), Semantic Segmentation (SemS). References used in the figure: Bishop and Nasrabadi, 2006: [18], Ferrer 2022: [51], Hastie et al., 2009: [61]. We recommend EC as a per-class counting metric in Subprocess S2 (Extended Data Fig. 2)."
    },
    {
      "markdown": "# $F_{\\beta}$ SCORE \n\n$$\n\\begin{aligned}\nF_{\\beta} \\text { Score } & =\\left(1+\\beta^{2}\\right) \\frac{\\text { PPV } \\cdot \\text { Sensitivity }}{\\beta^{2} \\cdot \\text { PPV }+ \\text { Sensitivity }} \\\\\n& =\\frac{\\left(1+\\beta^{2}\\right) \\cdot \\text { TP }}{\\left(1+\\beta^{2}\\right) \\cdot \\text { TP }+\\beta^{2} \\cdot \\text { FN }+ \\text { FP }}=\\frac{\\left(1+\\beta^{2}\\right) \\cdot}{\\left(1+\\beta^{2}\\right) \\cdot+\\beta^{2}+\\beta^{2}+}\n\\end{aligned}\n$$\n\n## DESCRIPTION\n\nThe $F_{\\beta}$ Score weights PPV (FP) and Sensitivity (FN) with the parameter $\\beta$.\n\nThe special case of $\\beta=1$ is the harmonic mean of PPV and Sensitivity and is a common metric in segmentation problems (here usually referred to as DSC). In segmentation problems, $\\mathrm{F}_{\\beta}$ Score weights the penalization of oversegmentation (FP) and undersegmentation (FN) with the parameter $\\beta$.\n\n## DEFINITION\n\n[Van Rijsbergen, 1979;\nChinchor 1992]\n\n## IMPORTANT RELATIONS\n\n$\\mathrm{DSC}=\\mathrm{F}_{\\beta}$, if $\\beta=1, \\operatorname{loU}=\\mathrm{F}_{\\beta} /\\left(2-\\mathrm{F}_{\\beta}\\right)$, if $\\beta=1$\n\n## RECOMMENDATIONS\n\n- $\\mathrm{F}_{\\beta}$ Score should generally not be considered ...\n\" ... in the presence of class imbalance unless class prevalences reflect the interest across classes (ImLC).\n\" ...if comparison of performance across data sets with different prevalences is desired (ImLC).\n\" ... in the case of a high variability of structure sizes within an image or across images (SemS, InS).\n\"... if inter-rater variability is requested to be compensated (SemS, InS).\n\"... if the comparability relative to a naive classifier should be provided.\n- Otherwise, the $\\mathrm{F}_{\\beta}$ Score is specifically recommended as per-class counting metric if ...\n\"... the task is object detection because unlike most popular per-class counting metrics, $\\mathrm{F}_{\\beta}$ Score does not require TN.\n\"... an optimization or argmax-based decision rule should be applied.\n\"... no predicted class scores are available.\n\"... high metric values should imply a high PPV.\n- In segmentation tasks, $\\mathrm{F}_{\\beta}$ Score with $\\beta \\neq 1$ should be considered if over- and undersegmentation are of unequal severity (SemS, InS).\n- In InS tasks, the object-level $\\mathrm{F}_{\\beta}$ Score should be considered if the detection quality should be assessed independently from the segmentation quality.\n\nExtended Data Fig. SN 3.7. Cheat Sheet for the $\\mathrm{F}_{\\beta}$ Score. The upward arrow in the value range indicates that higher values are better than lower values. Abbreviations used in the figure: Dice Similarity Coefficient (DSC), False Negative (FN), False Positive (FP), Image-level Classification (ImLC), Instance Segmentation (InS), Intersection over Union (IoU), Object Detection (ObD), Positive Predictive Value (PPV), Semantic Segmentation (SemS), True Negative (TN), True Positive (TP). References used in the figure: Chinchor 1992: [30], Van Rijsbergen, 1979: [161]. We recommend $\\mathrm{F}_{\\beta}$ Score as a per-class counting metric in Subprocess S3 (Extended Data Fig. 3)."
    },
    {
      "markdown": "# FALSE POSITIVES PER IMAGE (FPPI) \n\n![img-87.jpeg](img-87.jpeg)\n\nExtended Data Fig. SN 3.8. Cheat Sheet for the False Positives per Image (FPPI) [10, 159]. The upward arrow in the value range indicates that higher values are better than lower values. Abbreviations used in the figure: False Positive (FP), Free-Response Receiver Operating Characteristic (FROC), Image-level Classification (ImLC), Instance Segmentation (InS), Object Detection (ObD), Semantic Segmentation (SemS). References used in the figure: Van Ginneken et al., 2010: [159], Bandos et al., 2009: [10]. We recommend FPPI as a per-class counting metric in Subprocess S3 (Extended Data Fig. 3) for target value-based optimization using the concept Metric@(TargetMetric = TargetValue) (e.g., FPPI@Sensitivity = 0.95; see glossary in Suppl. Note 5.4)."
    },
    {
      "markdown": "# INTERSECTION OVER UNION (IoU) \n\nSynonyms: Jaccard Index, Tanimoto Coefficient\n![img-88.jpeg](img-88.jpeg)\n\n$$\n\\begin{aligned}\n\\operatorname{loU}(A, B) & =\\frac{\\square}{\\square+\\square-\\square} \\\\\n& =\\frac{|A \\cap B|}{|A|+|B|-|A \\cap B|}=\\frac{|A \\cap B|}{|A \\cup B|} \\\\\n& =\\frac{\\text { PPV } \\cdot \\text { Sensitivity }}{\\text { PPV }+ \\text { Sensitvity }-\\text { PPV } \\cdot \\text { Sensitivity }}\n\\end{aligned}\n$$\n\n## DESCRIPTION\n\nIoU measures the overlap between two structures. It is often referred to as Box IoU when comparing bounding boxes, Mask IoU when comparing segmentation masks, or Approx IoU when comparing approximations of objects beyond bounding boxes.\n\n## DEFINITION\n\n[Jaccard, 1912]\n\n## RECOMMENDATIONS\n\n- An overlap-based metric (by default DSC or loU) should be used in most cases for segmentation assessment. An exception is the case of consistently tiny structures along with a noisy reference.\n- IoU should generally be used in combination with a boundary-based metric if boundaries are of interest.\n- IoU should generally not be considered if...\n- ... there is a high variability of structure sizes within an image or across images.\n- ... inter-rater variability is requested to be compensated.\n- ... over- and undersegmentation should be treated similarly.\n- IoU should be considered as a metric in the computer vision and biology communities rather than in the medical community (which prefers the almost identical DSC).\n\nExtended Data Fig. SN 3.9. Cheat Sheet for the Intersection over Union (IoU). The upward arrow in the value range indicates that higher values are better than lower values. Abbreviations used in the figure: Dice Similarity Coefficient (DSC), Image-level Classification (ImLC), Instance Segmentation (InS), Object Detection (ObD), Positive Predictive Value (PPV), Semantic Segmentation (SemS). Reference used in the figure: Jaccard, 1912: [67]. We recommend IoU as an overlap-based metric in Subprocess S6 (Extended Data Fig. 6)."
    },
    {
      "markdown": "# MATTHEWS CORRELATION COEFFICIENT (MCC) \n\nSynonyms: Phi Coefficient\n![img-89.jpeg](img-89.jpeg)\n\n## VALUE RANGE: $[-1,1] \\uparrow$\n\nA value of 0 refers to a prediction which is not better than random guessing.\n\n## DESCRIPTION\n\nMCC measures the correlation between the actual and the predicted class.\n\n## DEFINITION\n\n[Matthews, 1975]\nRECOMMENDED FOR\nImLC SemS ObD InS\n![img-90.jpeg](img-90.jpeg)\n\nPREVALENCE DEPENDENCY\n\n## IMPORTANT RELATIONS\n\nMCC can be rewritten as:\n$\\mathrm{MCC}=\\sqrt{\\mathrm{PPV} \\cdot \\text { Sensitivity } \\cdot \\text { Specificity } \\cdot \\text { NPV }}-\\sqrt{(1-\\text { PPV }) \\cdot(1-\\text { Sensitivity }) \\cdot(1-\\text { Specificity }) \\cdot(1-\\text { NPV) }}$\nMCC is equivalent to the geometric mean of Markedness and Informedness.\n\n## MULTI-CLASS DEFINITION\n\nFor C classes, MCC can be defined as:\n$n_{i j}$ : entry of the confusion matrix for row $i$ and column $j$, i.e., samples of actual class $i$ that were predicted as class $j$\n\n## RECOMMENDATIONS\n\n- MCC should not be used/used with care if...\n- ... class confusions are of unequal severity (example: ordinal target classes).\n- ... the provided class prevalences do not reflect the population of interest.\n- ... there is a mismatch between class prevalences and class importance.\n- ... compensation for class imbalance is not requested.\n- Otherwise, MCC should be used as a multi-class metric specifically if all basic error rates (Sensitivity, Specificity, PPV, NPV) should be captured in one score.\n- MCC scores should be carefully interpreted in the presence of class imbalance as the distribution becomes skewed [Zhu 2020].\n\nExtended Data Fig. SN 3.10. Cheat Sheet for the Matthews Correlation Coefficient (MCC). The upward arrow in the value range indicates that higher values are better than lower values. Abbreviations used in the figure: Image-level Classification (ImLC), Instance Segmentation (InS), Negative Predictive Value (NPV), Positive Predictive Value (PPV), Object Detection (ObD), Semantic Segmentation (SemS). References used in the figure: Matthews, 1975: [103], Zhu, 2020: [175]. We recommend MCC as a multi-class counting metric in Subprocess S2 (Extended Data Fig. 2)."
    },
    {
      "markdown": "# NET BENEFIT (NB) \n\n![img-91.jpeg](img-91.jpeg)\n\n## DESCRIPTION\n\nNB validates the quality of a model intended to support a specific clinical decision. NB gives the 'net' proportion of TPs that results from a prediction. This is equivalent to the proportion of TPs in the absence of FPs. For its calculation, NB considers a task-related risk threshold (= exchange rate between the benefit of TPs and harm of FPs).\n\nWhen varying the risk threshold over a 'reasonable range' of possible thresholds, plotting NB by risk threshold yields a decision curve. It is a strictly proper performance measure.\n\n## DEFINITION\n\n[Vickers and Elkin, 2006]\n\n## RELATIONS\n\nNB can be reformulated as:\nNB = Sensitivity $\\cdot$ Prevalence -\n(1 - Specificity) $\\cdot$ (1 - Prevalence ) $\\cdot$\n\"odds at the threshold\"\n\n## RECOMMENDATIONS\n\n- NB should be considered if (1) the predicted class scores indicate the risk associated with a case belonging to a particular class and (2) there is a (range of reasonable) task-related risk threshold(s) or problem-specific penalties.\n- If problem-specific penalties can be provided, these can be used to calculate the decision threshold, see [Pauker \\& Kassirer, 1975].\n\nExtended Data Fig. SN 3.11. Cheat Sheet for the Net Benefit (NB). The upward arrow in the value range indicates that higher values are better than lower values. Abbreviations used in the figure: False Negative (FN), False Positive (FP), Image-level Classification (ImLC), Instance Segmentation (InS), Object Detection (ObD), Semantic Segmentation (SemS), True Negative (TN). References used in the figure: Pauker and Kassirer, 1975: [121], Vickers and Elkin, 2006: [162], Vickers et al., 2016: [163]. We recommend NB as a per-class counting metric in Subprocess S3 (Extended Data Fig. 3)."
    },
    {
      "markdown": "# NEGATIVE PREDICTIVE VALUE (NPV) \n\n![img-92.jpeg](img-92.jpeg)\n\n$$\n\\text { NPV }_{\\text {corrected }}=\\frac{\\text { Specificity } \\cdot(1-\\text { Prevalence })}{\\text { Specificity } \\cdot(1-\\text { Prevalence })+(1-\\text { Sensitivity }) \\cdot \\text { Prevalence }}\n$$\n\nVALUE RANGE: $[0,1] \\uparrow$\n\n## DESCRIPTION\n\nNPV represents the probability of a negative prediction corresponding to an actual negative sample.\n\n## IMPORTANT RELATIONS\n\nNPV is the complement of FOR (False omission rate):\nNPV $=1$ - FOR\nNPV is used as a component of MCC.\n\n## RECOMMENDATIONS\n\n- NPV should not be used....\n- ... as a standalone metric but should always be reported together with Specificity or PPV.\n- ... for comparisons across data sets with different prevalences or in imbalanced settings, where the prevalence-corrected NPV is more appropriate.\n- Otherwise, NPV should especially be considered ...\n- ... in combination with complementary metrics using the concept Metric@(TargetMetric = TargetValue) (e.g., NPV@Specificity $=0.95$; see glossary)\n- ... if ease of interpretation or popularity are of particular relevance.\n\nExtended Data Fig. SN 3.12. Cheat Sheet for the Negative Predictive Value (NPV). The upward arrow in the value range indicates that higher values are better than lower values. Abbreviations used in the figure: False Negative (FN), False Omission Rate (FOR), False Positive (FP), Image-level Classification (ImLC), Instance Segmentation (InS), Object Detection (ObD), Positive Predictive Value (PPV), Semantic Segmentation (SemS), True Negative (TN). Reference used in the figure: Tharwat, 2020: [148]. We recommend NPV as a per-class counting metric in Subprocess S3 (Extended Data Fig. 3) for target value-based optimization using the concept Metric@(TargetMetric = TargetValue) (e.g., NPV@Sensitivity = 0.95; see glossary in Suppl. Note 5.4)."
    },
    {
      "markdown": "# PANOPTIC QUALITY \n\n![img-93.jpeg](img-93.jpeg)\n\n## VALUE RANGE: $[0,1] \\uparrow$\n\n![img-94.jpeg](img-94.jpeg)\n\n## DESCRIPTION\n\nPQ assesses segmentation and detection quality in one metric. The segmentation quality is measured by averaging the IoU scores of all TP instances. The detection quality is measured by the $F_{1}$ Score. While in the $F_{1}$ Score, each TP counts as \" 1 \", PQ replaces this \" 1 \" score in the numerator with the IoU score of each TP.\nThe $F_{1}$ Score as a detection metric implies two cutoffs:\n\n1. a prior cutoff on a localization criterion for matching and\n2. a prior cutoff on object class scores to generate a confusion matrix.\n\nIn this context, PQ can be interpreted as making the localization quality in $F_{1}$ Score explicit (1) and thus only relying on the cutoff on class scores (2).\n\n## RECOMMENDED FOR\n\n|  |  |  |  |  |\n| :-- | :-- | :-- | :-- | :-- |\n| [Kirillov et al., 2019] |  |  |  |  |\n|  | ImLC | SemS | ObD | InS |\n|  | $\\odot$ | $\\odot$ | $\\odot$ |  |\n\n## RECOMMENDATIONS\n\nPQ should be considered if the detection and segmentation quality should be assessed in a single score, e.g., when the overall best model needs to be selected, which can naturally not be done based on two separate metrics. For comprehensive validation, individual detection and segmentation performance should also be reported.\n\nExtended Data Fig. SN 3.13. Cheat Sheet for the Panoptic Quality (PQ). The upward arrow in the value range indicates that higher values are better than lower values. Abbreviations used in the figure: Average Precision (AP), False Negative (FN), False Positive (FP), Free-Response Receiver Operating Characteristic (FROC), Image-level Classification (ImLC), Instance Segmentation (InS), Intersection over Union (IoU), Object Detection (ObD), Semantic Segmentation (SemS), True Positive (TP). Reference used in the figure: Kirillov et al., 2019: [77]. We recommend PQ as a per-class counting metric in Subprocess S3 (Extended Data Fig. 3)."
    },
    {
      "markdown": "# POSITIVE LIKELIHOOD RATIO (LR+) \n\nSynonyms: Likelihood ratio positive, Likelihood ratio for positive results\n\n$$\nL R+=\\frac{\\text { Sensitivity }}{1-\\text { Specificity }}=\\frac{1}{\\sqrt{1-\\frac{1}{\\mathrm{~L}_{\\infty}}}}\n$$\n\n## VALUE RANGE: $[0, \\infty) \\uparrow$\n\n![img-95.jpeg](img-95.jpeg)\n\nRECOMMENDED FOR\n![img-96.jpeg](img-96.jpeg)\n\nPREVALENCE DEPENDENCY\nDEFINITION\n[Attia, 2003]\n\n## RECOMMENDATIONS\n\n- LR+ should not be considered if...\n\" ... predictive values (PPV, NPV) are of interest.\n\"... a cost-benefit-based analysis is desired.\n- Otherwise, we recommend it as a per-class counting metric ...\n\"... for a comparison across data sets given its prevalence independence.\n\"... in the presence of class imbalances.\n\"... if the comparison with respect to a naive classifier is desired.\n\nExtended Data Fig. SN 3.14. Cheat Sheet for the Positive Likelihood Ratio (LR+). The upward arrow in the value range indicates that higher values are better than lower values. Abbreviations used in the figure: Image-level Classification (ImLC), Instance Segmentation (InS), Object Detection (ObD), Positive Predictive Value (PPV), Semantic Segmentation (SemS). Reference used in the figure: Attia, 2003: [7]. We recommend LR+ as a per-class counting metric in Subprocess S3 (Extended Data Fig. 3)."
    },
    {
      "markdown": "# POSITIVE PREDICTIVE VALUE (PPV) \n\nSynonym: Precision\n![img-97.jpeg](img-97.jpeg)\n\n## VALUE RANGE: $[0,1] \\uparrow$\n\n## DESCRIPTION\n\nPPV represents the probability of a positive prediction corresponding to an actual positive sample.\n\n## IMPORTANT RELATIONS\n\nPPV is the complement of FDR (False discovery rate): PPV $=1-$ FDR\nPPV is used as part of many other metrics such as $F_{0}$ Score and MCC.\n\n## RECOMMENDATIONS\n\n- PPV should not be used....\n\" ... as a standalone metric but should always be reported together with Sensitivity or NPV.\n\"... for comparisons across data sets with different prevalences or in imbalanced settings, where the prevalence-corrected PPV or alternatively the positive Likelihood ratio (LR+) are more appropriate.\n\"... at image level in case of many images with empty predictions (ObD, InS).\n- Otherwise, PPV should especially be considered ...\n\"... in combination with complementary metrics using the concept Metric@(TargetMetric = TargetValue) (e.g., PPV@Sensitivity = 0.95; see glossary)\n\"... if ease of interpretation or popularity are of particular relevance.\n\nExtended Data Fig. SN 3.15. Cheat Sheet for the Positive Predictive Value (PPV). The upward arrow in the value range indicates that higher values are better than lower values. Abbreviations used in the figure: False Discovery Rate (FDR), False Positive (FP), Image-level Classification (ImLC), Instance Segmentation (InS), Positive Likelihood Ratio (LR+), Matthews Correlation Coefficient (MCC), Negative Predictive Value (NPV), Object Detection (ObD), Semantic Segmentation (SemS), True Positive (TP). Reference used in the figure: Tharwat, 2020: [148]. We recommend PPV as a per-class counting metric in Subprocess S3 (Extended Data Fig. 3) for target value-based optimization using the concept Metric@(TargetMetric = TargetValue) (e.g., PPV@Sensitivity = 0.95; see glossary in Suppl. Note 5.4)."
    },
    {
      "markdown": "# SENSITIVITY \n\nSynonyms: Recall, True Positive Rate (TPR), Hit Rate\n\n$$\n\\text { Sensitivity }=\\frac{\\text { TP }}{\\text { TP + FN }}=\n$$\n\n![img-98.jpeg](img-98.jpeg)\n\n## DESCRIPTION\n\nSensitivity measures how good a method is in classifying truly positive samples as positive.\n\n## IMPORTANT RELATIONS\n\nSensitivity $=1$ - False Negative Rate (FNR)\nSensitivity is used as part of many other metrics, e.g., BA, BM, $\\mathrm{F}_{\\mathrm{p}}$ Score, LR+, MCC and more.\n\nRECOMMENDATIONS\n\n- Sensitivity should not be considered ...\n\" ... as a standalone metric but should always be reported together with Specificity or PPV.\n\" ... at image level in case of many images with empty reference (ObD, InS).\n- Otherwise, Sensitivity should especially be considered ...\n\" ... in combination with complementary metrics using the concept Metric@(TargetMetric = TargetValue) (e.g., Specificity@Sensitivity = 0.95; see glossary).\n\" ... for comparisons across data sets with different prevalences given its prevalence independence.\n\" ... in the presence of class imbalances.\n\" ... if ease of interpretation or popularity are of particular relevance.\n\nExtended Data Fig. SN 3.16. Cheat Sheet for the Sensitivity. The upward arrow in the value range indicates that higher values are better than lower values. Abbreviations used in the figure: Bookmaker Informedness (BM), False Negative (FN), Image-level Classification (ImLC), Instance Segmentation (InS), Positive Likelihood Ratio (LR+), Matthews Correlation Coefficient (MCC), Object Detection (ObD), Semantic Segmentation (SemS), True Positive (TP). Reference used in the figure: Tharwat, 2020: [148]. We recommend Sensitivity as a per-class counting metric in Subprocess S3 (Extended Data Fig. 3) for target value-based optimization using the concept Metric@(TargetMetric = TargetValue) (e.g., Specificity@Sensitivity = 0.95; see glossary in Suppl. Note 5.4)."
    },
    {
      "markdown": "# SPECIFICITY \n\nSynonyms: Selectivity, True Negative Rate (TNR)\n![img-99.jpeg](img-99.jpeg)\n\nExtended Data Fig. SN 3.17. Cheat Sheet for the Specificity. The upward arrow in the value range indicates that higher values are better than lower values. Abbreviations used in the figure: Bookmaker Informedness (BM), False Positive (FP), Image-level Classification (ImLC), Instance Segmentation (InS), Positive Likelihood Ratio (LR+), Matthews Correlation Coefficient (MCC), Object Detection (ObD), Semantic Segmentation (SemS), True Negative (TN). Reference used in the figure: Tharwat, 2020: [148]. We recommend Specificity as a per-class counting metric in Subprocess S3 (Extended Data Fig. 3) for target value-based optimization using the concept Metric@(TargetMetric = TargetValue) (e.g., Specificity@Sensitivity = 0.95; see glossary in Suppl. Note 5.4)."
    },
    {
      "markdown": "WEIGHTED COHEN'S KAPPA (WCK)\nSynonyms:Weighted Cohen's Kappa Coefficient, Weighted Kappa Statistic, Weighted Kappa Score\n![img-100.jpeg](img-100.jpeg)\n\n# VALUE RANGE: $[-1,1] \\uparrow$ \n\nA value of 0 refers to a prediction which is not better than random guessing.\n$w_{t o} / w_{t o j} / w_{t o r} / w_{t m}$ : (estimation of) costs of the respective cardinalities; can be adjusted as a weighting of them.\n\n## DESCRIPTION\n\nWCK calculates the degree of agreement between the reference and prediction while incorporating the agreement resulting from chance. WCK is a generalization of CK with 0-1 weights.\n\n## DEFINITION\n\n[Cohen, 1960]\n\n## RECOMMENDED FOR\n\n| ImLC | SemS | ObD | InS |\n| :--: | :--: | :--: | :--: |\n|  |  |  |  |\n\nPREVALENCE DEPENDENCY\n\n## IMPORTANT RELATIONS\n\n- WCK is a generalization of CK $=\\left[2 \\cdot\\right.$ Prevalence $\\cdot$ (1-Prevalence) $\\cdot$ (Sensitivity + Specificity- 1)] / [Prevalence ${ }^{2}+$ (1-Prevalence) ${ }^{2}+$ (1-2 $\\cdot$ Prevalence) $\\cdot$ (Prevalence $\\cdot$ Sensitivity - (1-Prevalence) $\\cdot$ Specificity)], and equal for 0-1-weights\n- For a Prevalence of $50 \\%$ and weights of $1, \\mathrm{WCK}=\\mathrm{J}=2 \\mathrm{BA}-1$\n- Accuracy $=(W C K+1) / 2$, for 0-1-weights and balanced classes\n- $\\mathrm{BA}=(\\mathrm{WCK}+1) / 2$, for 0-1-weights and balanced classes\n\n\n## MULTI-CLASS DEFINITION\n\nFor C classes, WCK can be defined as: WCK $=1-\\left(\\sum_{i=1}^{c} \\sum_{j=1}^{c} w_{(j} \\cdot n_{j}\\right) /\\left(\\sum_{i=1}^{c} \\sum_{j=1}^{c} w_{(j} \\cdot \\frac{n_{j} \\cdot n_{j j}}{N^{2}}\\right)$\n$n_{j j}$ : entry of the confusion matrix for row $i$ and column $j$, i.e. samples of actual class $i$ that were predicted as class $j$\n$N$ : total number of samples\n$n_{j j}$ : sum of entries of row $i$ of the confusion matrix\n$n_{j j}$ : sum of entries of column j of the confusion matrix\n$w_{i j}$ : costs for the entry of the confusion matrix for row $i$ and column $j$, i.e., the cost for samples of actual class $i$ that were predicted as class $j$\n\n## RECOMMENDATIONS\n\n- WCK has the rare advantage of being able to incorporate unequal costs for class confusions. However, we generally recommend EC over WCK for this use case due to the former's strong theoretical foundation and straightforward behaviour.\n- WCK should generally not be considered in the presence of class imbalance unless class prevalences reflect the interest across classes.\n\nExtended Data Fig. SN 3.18. Cheat Sheet for the Weighted Cohen's Kappa (WCK). The upward arrow in the value range indicates that higher values are better than lower values. Abbreviations used in the figure: Balanced Accuracy (BA), Cohen's Kappa (CK), False Negative (FN), False Positive (FP), Image-level Classification (ImLC), Instance Segmentation (InS), Object Detection (ObD), Semantic Segmentation (SemS), True Negative (TN), True Positive (TP). Reference used in the figure: Cohen, 1960: [34]. We recommend WCK as a multi-class counting metric in Subprocess S2 (Extended Data Fig. 2)."
    },
    {
      "markdown": "# Multi-threshold metrics \n\n## AREA UNDER THE RECEIVER OPERATING CHARACTERISTIC CURVE (AUROC) Synonyms: AUC (Area under Curve), AUC - ROC (Area Under The Curve - Receiver Operating Characteristics), C-Index, C-Statistics\n\n![img-101.jpeg](img-101.jpeg)\n\nFIG U 0.1\n![img-102.jpeg](img-102.jpeg)\n\nVALUE RANGE: $[0,1] \\uparrow$\nAn AUROC value of 0.5 refers to a prediction which is not better than random guessing.\n\n## DESCRIPTION\n\nAUROC measures the area under the ROC curve and indicates how well the probabilities of the positive class are separated from those of the negative class. In other words, AUROC represents the probability of a randomly sampled positive case having a higher predicted class score than a randomly sampled negative case.\n\nRECOMMENDED FOR\n![img-103.jpeg](img-103.jpeg)\n\n## DEFINITION\n\n[Hanley and McNeil, 1982]\n\n## RECOMMENDATIONS\n\n- AUROC is our recommended default multi-threshold metric for ImLC due to its ease of interpretation (also with respect to a naive classifier).\n- AUROC should be applied with care ...\n- ... in the presence of class imbalance, unless class prevalences reflect the interest across classes.\n- ... if predictive values (PPV/NPV) are of interest.\n- Otherwise, it should specifically be considered in the case of relatively balanced data due to ease of interpretation and popularity.\n\nExtended Data Fig. SN 3.19. Cheat Sheet for the Area under the Receiver Operating Characteristic Curve (AUROC). The upward arrow in the value range indicates that higher values are better than lower values. Abbreviations used in the figure: False Negative (FN), False Positive (FP), Image-level Classification (ImLC), Instance Segmentation (InS), Object Detection (ObD), Receiver Operating Characteristic (ROC), Semantic Segmentation (SemS), True Negative (TN), True Positive (TP). Reference used in the figure: Hanley and McNeil, 1982: [60]. We recommend AUROC as a multi-threshold metric in Subprocess S4 (Extended Data Fig. 4)."
    },
    {
      "markdown": "# AVERAGE PRECISION (AP) \n\n![img-104.jpeg](img-104.jpeg)\n\n## DESCRIPTION\n\nAP measures the interpolated area under the PR curve. It differs from the well known ROC curve and the associated AUROC metric by measuring Sensitivity in combination with PPV instead of in combination with Specificity. This replacement has the effect that TNs are not explicitly considered by the PR curve and AP. Ignoring TN can be desirable in settings with a dominating negative class and thus large amounts of TN suppressing a focus on the rare positive class. A prominent example are tasks with imbalanced classes such as those with retrieval character, where AUROC is typically not applied, because the large amount of TNs leads to an insensitivity to subtle performance changes for the rare positive class.\n\n## IMPORTANT RELATIONS\n\nThe mean Average Precision (mAP) is a commonly used extension which measures the average of the AP over multiple classes.\n\n|  | RECOMMENDED FOR |  |  |  |\n| :--: | :--: | :--: | :--: | :--: |\n| ImLC | SemS | ObD | InS | DEFINITION <br> [Lin et al., 2014; Everingham et al. 2015] |\n| (1) |  |  |  |  |\n\n## RECOMMENDATIONS\n\n- AP is our default recommended multi-threshold metric for ObD and InS.\n- AP should generally not be considered...\n- ... for a comparison across data sets given its dependency on the prevalence-dependent PPV.\n- ... if the performance on single images is important given that AP is computed over the full data set.\n- Otherwise, it should specifically be considered...\n- ... in the case of class imbalances (but should be used with care given its prevalence dependency).\n- ... if a standardized metric (difference to FROC) is required.\n\nExtended Data Fig. SN 3.20. Cheat Sheet for the Average Precision (AP). The upward arrow in the value range indicates that higher values are better than lower values. Abbreviations used in the figure: False Negative (FN), False Positive (FP), Image-level Classification (ImLC), Instance Segmentation (InS), Mean Average Precision (mAP), Object Detection (ObD), Positive Predictive Value (PPV), Precision-Recall (PR), Semantic Segmentation (SemS), True Negative (TN), True Positive (TP). References used in the figure: Lin et al., 2014: [94], Everingham et al., 2015: [49]. We recommend AP as a multi-threshold metric in Subprocess S4 (Extended Data Fig. 4)."
    },
    {
      "markdown": "# FREE RESPONSE RECEIVER OPERATING CHARACTERISTIC (FROC) SCORE \n\n![img-105.jpeg](img-105.jpeg)\n\n## DESCRIPTION\n\nFROC Score approximates the area under the FROC curve, which plots the Sensitivity as a function of the average number of FPPI. It thus indicates how well the probabilities of the positive class are separated from those of the negative class while considering object-level information.\n\n## RECOMMENDATIONS\n\n- FROC Score should not be considered...\n- ... if a highly standardized metric is desired.\n- Otherwise, FROC Score should specifically be considered...\n- ... in a clinical context given its user-centric interpretation (explicit tradeoff between FPPI and Sensitivity).\n- ... if low confidence FP predictions should be rejected.\n- It should be noted that FROC@FPPI suffers from non-standardization but allows for flexible weighting of clinically interesting operating points.\n\n\n## DEFINITION\n\n[Van Ginneken et al., 2010]\n\nExtended Data Fig. SN 3.21. Cheat Sheet for the Free-Response Receiver Operating Characteristic (FROC). The upward arrow in the value range indicates that higher values are better than lower values. Abbreviations used in the figure: False Negative (FN), False Positive (FP), False Positives per Image (FPPI), Image-level Classification (ImLC), Instance Segmentation (InS), Object Detection (ObD), Semantic Segmentation (SemS), True Negative (TN), True Positive (TP). Reference used in the figure: Van Ginneken et al., 2010: [159]. We recommend FROC as a multi-threshold metric in Subprocess S4 (Extended Data Fig. 4)."
    },
    {
      "markdown": "# Boundary-based metrics \n\n## AVERAGE SYMMETRIC SURFACE DISTANCE (ASSD)\n\nSynonym: Weighted bilateral mean contour distance\n![img-106.jpeg](img-106.jpeg)\n\n## DESCRIPTION\n\nASSD measures the average of all shortest boundary distances between contour A to any point on contour B and vice versa, symmetrically.\n\n## DEFINITION\n\n[Yeghiazaryan,\nVarduhi and Voiculescu, 2015]\n\n## RECOMMENDATIONS\n\n- Boundary-based metrics such as ASSD should generally be reported together with an over-lap-based metric.\n- We generally recommend MASD over ASSD for boundary-based penalization of spatial outliers with contour focus.\n- ASSD should not be used ...\n\" ... if the sizes of reference and predictions potentially vary a lot.\n\" ... if inter-rater variability should be compensated.\n\"... to compare relationships between boundaries of many dense objects.\n- For missing value handling, an advanced strategy should be defined, for example by setting the penalty equal to the largest distance within an image.\n\nExtended Data Fig. SN 3.22. Cheat Sheet for the Average Symmetric Surface Distance (ASSD). The downward arrow in the value range indicates that lower values are better than higher values. Abbreviations used in the figure: Image-level Classification (ImLC), Instance Segmentation (InS), Object Detection (ObD), Semantic Segmentation (SemS). Reference used in the figure: Yeghiazaryan, Varduhi and Voiculescu, 2015: [172]. We recommend Average Symmetric Surface Distance (ASSD) as a boundary-based metric in Subprocess S7 (Extended Data Fig. 7)."
    },
    {
      "markdown": "# BOUNDARY INTERSECTION OVER UNION (BOUNDARY IOU) \n\n![img-107.jpeg](img-107.jpeg)\n\nExtended Data Fig. SN 3.23. Cheat Sheet for the Boundary Intersection over Union (IoU). The upward arrow in the value range indicates that higher values are better than lower values. Abbreviations used in the figure: Image-level Classification (ImLC), Instance Segmentation (InS), Mean Average Surface Distance (MASD), Object Detection (ObD), Semantic Segmentation (SemS). Reference used in the figure: Cheng et al., 2021: [28]. We recommend Boundary IoU as a boundary-based metric in Subprocess S7 (Extended Data Fig. 7)."
    },
    {
      "markdown": "# HAUSDORFF DISTANCE (HD) \n\nSynonyms: Maximum Symmetric Surface Distance, Hausdorff Metric, Pompeiu-Hausdorff Distance\n![img-108.jpeg](img-108.jpeg)\n\n## DESCRIPTION\n\nHD is the largest of all the distances from a point on one boundary to the closest point on the other boundary.\n\n| DEFINITION | RECOMMENDED FOR |  |  |  |\n| :--: | :--: | :--: | :--: | :--: |\n|  | ImLC | SemS | ObD | InS |\n|  |  |  |  |  |\n\n## RECOMMENDATIONS\n\n- Boundary-based metrics such as HD should generally be reported together with an overlap-based metric.\n- We generally recommend the xth percentile of the HD over HD for boundary-based penalization of spatial outliers with outlier focus.\n- However, HD should be considered if spatial outliers should be heavily penalized.\n- HD should not be used if inter-rater variability should be compensated.\n- For missing value handling, an advanced strategy should be defined, for example by setting the penalty equal to the largest distance within an image.\n\nExtended Data Fig. SN 3.24. Cheat Sheet for the Hausdorff Distance (HD). The downward arrow in the value range indicates that lower values are better than higher values. Abbreviations used in the figure: Image-level Classification (ImLC), Instance Segmentation (InS), Object Detection (ObD), Semantic Segmentation (SemS). Reference used in the figure: Huttenlocher, 1993: [64]. We recommend HD as a boundary-based metric in Subprocess S7 (Extended Data Fig. 7)."
    },
    {
      "markdown": "# MEAN AVERAGE SURFACE DISTANCE (MASD) \n\n## Synonym: Mean Surface Distance\n\n![img-109.jpeg](img-109.jpeg)\n\n## DESCRIPTION\n\nMASD measures the mean of the averages over all shortest distances from all sampled points on one boundary to any other point on another boundary.\n\n## RECOMMENDATIONS\n\n- Boundary-based metrics such as MASD should generally be reported together with an over-lap-based metric.\n- We generally recommend MASD as a boundary-based penalization metric with contour focus over ASSD.\n- MASD should not be used if inter-rater variability should be compensated.\n- For missing value handling, an advanced strategy should be defined, for example by setting the penalty equal to the largest distance within an image.\n\nExtended Data Fig. SN 3.25. Cheat Sheet for the Mean Average Surface Distance (MASD). The downward arrow in the value range indicates that lower values are better than higher values. Abbreviations used in the figure: Average Symmetric Surface Distance (ASSD), Image-level Classification (ImLC), Instance Segmentation (InS), Object Detection (ObD), Semantic Segmentation (SemS). Reference used in the figure: Beneš and Zitová, 2015: [13]. We recommend MASD as a boundary-based metric in Subprocess S7 (Extended Data Fig. 7)."
    },
    {
      "markdown": "# NORMALIZED SURFACE DISTANCE (NSD) \n\nSynonyms: Normalized Surface Dice, Surface Distance, Surface Dice, Surface DSC\n![img-110.jpeg](img-110.jpeg)\n\n## DESCRIPTION\n\nNSD measures the DSC on boundary pixels with an uncertainty margin. The degree of strictness for what constitutes a correct boundary is represented by the tolerance parameter $\\tau$. Only boundary parts within the border regions defined by $\\tau$ are counted as TP. NSD therefore captures known uncertainties in the reference and allows acceptable deviations from the reference for the predicted boundary.\n\n| DEFINITION |  | RECOMMENDED FOR |  |  |\n| :-- | :-- | :-- | :-- | :-- |\n|  | ImLC | SemS | ObD | InS |\n| [Nikolov et al., 2021] | $\\Theta$ | $\\Theta$ | $\\Theta$ | $\\Theta$ |\n\n## RECOMMENDATIONS\n\n- Boundary-based metrics such as NSD should generally be reported together with an over-lap-based metric.\n- NSD is our default boundary-based metric in case annotation imprecisions should be compensated for.\n- NSD should be considered if ...\n- ... structure boundaries are of particular interest.\n- ... annotation imprecisions should be compensated.\n- ... spatial outliers should be penalized by their existence rather than their distance.\n- The hyperparameter $\\tau$ denotes the tolerated distance between reference and prediction below of which pixels are considered TP. It should be chosen according to inter-rater variability, for example.\n\nExtended Data Fig. SN 3.26. Cheat Sheet for the Normalized Surface Distance (NSD). The upward arrow in the value range indicates that higher values are better than lower values. Abbreviations used in the figure: Dice Similarity Coefficient (DSC), Image-level Classification (ImLC), Instance Segmentation (InS), Object Detection (ObD), Semantic Segmentation (SemS), True Positive (TP). Reference used in the figure: Nikolov et al., 2021: [116]. We recommend NSD as a boundary-based metric in Subprocess S7 (Extended Data Fig. 7)."
    },
    {
      "markdown": "# $X^{\\text {th }}$ PERCENTILE OF HAUSDORFF DISTANCE \n\n![img-111.jpeg](img-111.jpeg)\n\n## DESCRIPTION\n\nThe $x^{\\text {th }}$ percentile of the Hausdorff Distance (HD) measures the $x^{\\text {th }}$ percentile of all the distances from a point on one boundary to the closest point on the other boundary. A common value is $x=95$ (HD95).\n\n|  |  |  |  |  |\n| :-- | :-- | :-- | :-- | :-- |\n| DEFINITION |  | RECOMMENDED FOR |  |  |\n|  | ImLC |  | SemS | ObD | InS |\n|  |  |  |  |  |  |\n\n## RECOMMENDATIONS\n\n- Boundary-based metrics such as the $x^{\\text {th }}$ Percentile of HD should generally be reported together with an overlap-based metric.\n- We generally recommend the $x^{\\text {th }}$ Percentile of HD as a boundary-based penalization metric with outlier focus.\n- The $x^{\\text {th }}$ Percentile of HD should not be used ...\n- ... if inter-rater variability should be compensated.\n- ... in SemS cases where multiple entities are represented within an image.\n- For missing value handling, an advanced strategy should be defined, for example by setting the penalty equal to the largest distance within an image.\n\nExtended Data Fig. SN 3.27. Cheat Sheet for the $X^{t h}$ Percentile Hausdorff Distance ( $X^{t h}$ Percentile HD). The downward arrow in the value range indicates that lower values are better than higher values. Abbreviations used in the figure: Hausdorff Distance (HD), Image-level Classification (ImLC), Instance Segmentation (InS), Object Detection (ObD), Semantic Segmentation (SemS). Reference used in the figure: Huttenlocher, 1993: [64]. We recommend $X^{t h}$ Percentile HD as a boundary-based metric in Subprocess S7 (Extended Data Fig. 7)."
    },
    {
      "markdown": "# 3.1.2 Calibration metrics. \n\n## BRIER SCORE (BS)/BRIER SKILL SCORE (BSS)\n\n$$\nB S=\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{k=1}^{C}\\left(p_{i k}-y_{i k}\\right)^{2}\n$$\n\nVALUE RANGE: $[0,2] \\downarrow$\n$N$ : number of samples\n$p_{i k}$ : predicted probability for sample $x_{i}$ and class $k$\n$C$ : number of classes\n$y_{i k}$ : outcome; $y_{i k}=1$ if $y_{i}$ is equal to $k$ and 0 otherwise\n\n## DESCRIPTION\n\nBS ist the mean squared error of a predicted class score and the actual outcome, thus assessing discrimination and calibration in one joint score. It is a proper scoring rule.\n\n## VARIANT\n\nBrier Skill Score (BSS): normalizes BS by the BS of a naive system.\n\n## DEFINITION\n\n[Gneiting and Raftery, 2007]\n\n## RECOMMENDATIONS\n\n- BS should be considered if ...\n\" ... discrimination and calibration performance should be simultaneously assessed in one score.\n\"... the true posterior probabilities, i.e., the \"risks\" for individual cases are of interest.\n\"... a straightforward interpretation of relative improvement is desired.\n- BS should carefully be used ...\n\"... in imbalanced settings as the bounded penalization of errors leads to preference of naive systems.\n\"... for classes with unequal severity of confusions (e.g. ordinal classes).\n\nExtended Data Fig. SN 3.28. Cheat Sheet for the Brier Score (BS)/Brier Skill Score (BSS). The downward arrow in the value range indicates that lower values are better than higher values. Abbreviations used in the figure: Brier Skill Score (BSS), Image-level Classification (ImLC), Instance Segmentation (InS), Object Detection (ObD), Semantic Segmentation (SemS). An introduction to calibration and corresponding terminology can be found in Suppl. Note 2.6. Reference used in the figure: Gneiting and Raftery, 2007: [53]. We recommend BS/Brier Skill Score (BSS) as a calibration metric in Subprocess S5 (Extended Data Fig. 5)."
    },
    {
      "markdown": "# CLASS-WISE CALIBRATION ERROR (CWCE) \n\n![img-112.jpeg](img-112.jpeg)\n\nCWCE $=\\frac{1}{C} \\sum_{c=1}^{c} \\sum_{m=1}^{M}\\left|B_{c, m}\\right| \\mid$ Accuracy $_{c}\\left(B_{c, m}\\right)$ - Confidence $_{c}\\left(B_{c, m} \\mid\\right\\|_{p}^{p}$\n\nN : number of samples; $C$ : number of classes\n$B_{c, m}$ : bin $m$ for class $c$\n$p$ : determines which $L_{p}$ calibration\nerror is desired; typically $p=1$\nVALUE RANGE: $[0,1] \\downarrow$\n\n## DESCRIPTION\n\nCWCE is an estimator of the marginal calibration error applying binning to estimate the observed probabilities corresponding to a confidence range. It can be reported per class or in an aggregated fashion with class-specific weights reflecting prevalence or importance of classes, for example.\n\n## DEFINITION\n\n[Kull et al., 2019; Kumar et al., 2019]\n\n## RECOMMENDATIONS\n\n- CWCE should generally not be considered...\n\" ... for small sample sizes (CWCE is dependent on the sample size).\n\"... if the canonical calibration error should be assessed.\n- Otherwise, CWCE should be considered...\n\"... for a class-wise calibration assessment.\n\"... in the case of an unequal interest across classes.\n- It is generally advisable to report the CWCE both per class and in an aggregated fashion.\n- In the case of small sample sizes, the number of bins should be adjusted and additional metrics capturing canonical calibration (KCE or RBS) should be reported as well.\n\nExtended Data Fig. SN 3.29. Cheat Sheet for the Class-wise Calibration Error (CWCE). The downward arrow in the value range indicates that lower values are better than higher values. Abbreviations used in the figure: Image-level Classification (ImLC), Instance Segmentation (InS), Kernel Calibration Error (KCE), Object Detection (ObD), Root Brier Score (RBS), Semantic Segmentation (SemS). An introduction to calibration and corresponding terminology can be found in Suppl. Note 2.6. References used in the figure: Kumar et al., 2019: [84], Kull et al., 2019: [83]. We recommend CWCE as a calibration metric in Subprocess S5 (Extended Data Fig. 5)."
    },
    {
      "markdown": "# EXPECTED CALIBRATION ERROR (ECE) \n\n![img-113.jpeg](img-113.jpeg)\n\n## DESCRIPTION\n\nECE is an estimator for the $L_{p}$ top-label calibration error. For a binned estimation, it is the weighted average of the absolute difference between the average predicted class score (Confidence) of the top label per bin $B_{m}$ and the corresponding fraction of correct predictions (Accuracy).\n\nVARIANT\nThe marginal variant of ECE is CWCE.\n\n## DEFINITION\n\n[Naeini et al., 2015]\n\n## RECOMMENDED FOR\n\n![img-114.jpeg](img-114.jpeg)\n\nTYPE OF CALIBRATION\nTop-label Marginal Canonical\n\n## RECOMMENDATIONS\n\n- ECE should generally not be considered...\n\" ... for small sample sizes (ECE is dependent on the sample size).\n\"... if the canonical or marginal calibration error should be assessed.\n- Otherwise, ECE should be considered...\n\"... if the top-label calibration error should be assessed (especially in binary settings, where it equals the marginal and canonical calibration error).\n- ECE should be reported together with RBS, which gives an unbiased upper bound of the canonical calibration error.\n\nExtended Data Fig. SN 3.30. Cheat Sheet for the Expected Calibration Error (ECE). The downward arrow in the value range indicates that lower values are better than higher values. Abbreviations used in the figure: Image-level Classification (ImLC), Instance Segmentation (InS), Object Detection (ObD), Semantic Segmentation (SemS). An introduction to calibration and corresponding terminology can be found in Suppl. Note 2.6. Reference used in the figure: Naeini et al., 2015: [110]. We recommend ECE as a calibration metric in Subprocess S5 (Extended Data Fig. 5)."
    },
    {
      "markdown": "# EXPECTED CALIBRATION ERROR KERNEL DENSITY ESTIMATE (ECE ${ }^{\\text {KDE }}$ ) \n\n$E C E^{\\text {KDE }}=\\frac{1}{N} \\sum_{j=1}^{N}\\left\\|\\frac{\\sum_{i \\neq j} k\\left(f\\left(x_{j}\\right), f\\left(x_{j}\\right)\\right) e_{y_{j}}}{\\sum_{i \\neq j} k\\left(f\\left(x_{j}\\right), f\\left(x_{j}\\right)\\right)}-f\\left(x_{j}\\right)\\right\\|_{p}^{p}$\n$N$ : number of samples\nk: kernel, e.g. Dirichlet kernel [Popordanoska et al., 2022]\n$f(x)$ : predicted probability vector, $y_{j}$ : outcome (one-hot encoded)\n$e_{y}$ : C-dimensional vector with $y_{j}$-th entry being 1 , else 0\n$p$ : determines which $L_{p}$ calibration error is desired; typically $p \\in\\{1,2\\}$ VALUE RANGE: $[0,2] \\downarrow$\n\n## DESCRIPTION\n\nECE ${ }^{\\text {KDE }}$ is an estimator for the canonical calibration error. It uses a kernel density estimate in contrast to the binning strategy applied by the standard ECE.\n\n## DEFINITION\n\n[Popordanoska et al., 2022]\n\n## RECOMMENDATIONS\n\n- ECE ${ }^{\\text {KDE }}$ should generally not be considered...\n\" ... for small sample sizes (ECE ${ }^{\\text {KDE }}$ is dependent on sample size).\n\"... for very large numbers of classes or unequal interest across classes. In such cases, CWCE should be considered instead.\n- Otherwise, ECE ${ }^{\\text {KDE }}$ should be considered...\n\"... for quantifying the canonical calibration error.\n- ECE ${ }^{\\text {KDE }}$ should be reported together with RBS, which gives an unbiased upper bound on the canonical calibration error.\n\nExtended Data Fig. SN 3.31. Cheat Sheet for the Expected Calibration Error Kernel Density Estimate (ECE ${ }^{\\text {KDE }}$ ). The downward arrow in the value range indicates that lower values are better than higher values. Abbreviations used in the figure: Class-wise Calibration Error (CWCE), Image-level Classification (ImLC), Instance Segmentation (InS), Object Detection (ObD), Root Brier Score (RBS), Semantic Segmentation (SemS). An introduction to calibration and corresponding terminology can be found in Suppl. Note 2.6. Reference used in the figure: Popordanoska et al., 2022: [123]. We recommend ECE ${ }^{\\text {KDE }}$ as a calibration metric in Subprocess S5 (Extended Data Fig. 5)."
    },
    {
      "markdown": "# KERNEL CALIBRATION ERROR (KCE) \n\n$\\mathrm{KCE}=\\left(\\mathrm{E}\\left(\\left(\\mathrm{e}_{\\mathrm{y}}-\\mathrm{f}(\\mathrm{x})\\right)^{\\mathrm{T}} \\mathrm{k}\\left(\\mathrm{f}(\\mathrm{x}), \\mathrm{f}\\left(\\mathrm{x}^{\\prime}\\right)\\right)\\left(\\mathrm{e}_{\\mathrm{y}^{\\prime}}-\\mathrm{f}\\left(\\mathrm{x}^{\\prime}\\right)\\right)\\right)\\right)^{1 / 2}$\nExample estimator: $\\widehat{\\mathrm{KCE}}=\\left(\\left(\\frac{\\mathrm{N}}{2}\\right)^{-1} \\sum_{i=1}^{N} \\sum_{j=i+1}^{N}\\left(e_{y i}-f\\left(x_{i}\\right)\\right)^{\\mathrm{T}} \\mathrm{k}\\left(f\\left(x_{i}\\right), f\\left(x_{j}\\right)\\right)\\left(e_{y j}-f\\left(x_{j}\\right)\\right)\\right)^{1 / 2}$\n$N$ : number of samples; $k$ : matrix-valued kernel; $f(x)$ : predicted probability vector; $y_{i}$ : outcome; $e_{y i}$ : C-dimensional vector with $y_{i}$-th entry being 1 , else 0\nVALUE RANGE: Kernel dependent; in expectation $>0$ but estimator can be arbitrarily negative\n\n## DESCRIPTION\n\nKCE measures a canonical calibration error based on an alternative distance function, the \"maximum mean discrepancy\" (MMD). It is based on a mat-rix-valued kernel $k$.\nKCE is an unbiased estimator of the calibration error measured by MMD.\n\n## DEFINITION\n\n[Widmann et al., 2019; Gruber and Buettner, 2022]\n\n## RECOMMENDED FOR\n\nImLC\nSemS\nObD\nInS\n\nTYPE OF CALIBRATION\nTop-label Marginal Canonical\n\n## RECOMMENDATIONS\n\n- KCE should generally not be considered...\n\" ... for unequal interest across classes. In such a case, CWCE should be considered instead.\n\" ... for interpreting calibration performance (absolute values) as it is relatively hard to interpret (for example compared to BS), also due to negative output values.\n- KCE should be considered...\n\" ... for a canonical calibration assessment if interpretability of absolute values is not of key interest. This holds especially true for comparative calibration assessment as KCE is an unbiased estimator of the calibration error measured by MMD.\n\" ... in the presence of small sample sizes and a large number of classes as it is an unbiased estimator and therefore also well-suited.\n- KCE should be configured carefully as it depends on nontrivial configuration choices of kernels and associated hyperparameters.\n\nExtended Data Fig. SN 3.32. Cheat Sheet for the Kernel Calibration Error (KCE). The downward arrow in the value range indicates that lower values are better than higher values. Abbreviations used in the figure: Brier Score (BS), Image-level Classification (ImLC), Instance Segmentation (InS), Object Detection (ObD), Semantic Segmentation (SemS). An introduction to calibration and corresponding terminology can be found in Suppl. Note 2.6. References used in the figure: Gruber and Buettner, 2022: [56], Widmann et al., 2019: [167]. We recommend KCE as a calibration metric in Subprocess S5 (Extended Data Fig. 5)."
    },
    {
      "markdown": "# NEGATIVE LOG LIKELIHOOD (NLL) <br> Synonym: Cross Entropy Loss \n\n$$\n\\text { NLL }=-\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{k=1}^{c} y_{i k} \\cdot \\log \\left(p_{i k}\\right)\n$$\n\nVALUE RANGE: $[0, \\infty) \\downarrow$\nN: number of samples\n$p_{i k}$ : predicted probability for sample $x_{i}$ and class $k$\nC: number of classes\n$y_{i k}$ : outcome; $y_{i k}=1$ if $y_{i}$ is equal to $k$ and 0 otherwise\n\n## DESCRIPTION\n\nNLL is the negative logarithm of a predicted class score and the actual outcome. It is a proper scoring rule that can be used to measure the discrimination and calibration quality in one joint score.\n\n## DEFINITION\n\n[Cybenko et al., 1998]\n\n## RECOMMENDATIONS\n\n- NLL should be considered if ...\n- ... calibration and discrimination should be assessed in a single score for comparative calibration assessment.\n- ... extreme scores should be heavily penalized.\n- NLL should not be used ...\n- ... for classes with unequal severity of confusions (e.g. ordinal classes).\n- ... if an easy interpretation is desired (it has no upper bound).\n\nExtended Data Fig. SN 3.33. Cheat Sheet for the Negative Log Likelihood (NLL). The downward arrow in the value range indicates that lower values are better than higher values. Abbreviations used in the figure: Image-level Classification (ImLC), Instance Segmentation (InS), Object Detection (ObD), Semantic Segmentation (SemS). An introduction to calibration and corresponding terminology can be found in Suppl. Note 2.6. Reference used in the figure: Cybenko et al., 1998: [39]. We recommend NLL as a calibration metric in Subprocess S5 (Extended Data Fig. 5)."
    },
    {
      "markdown": "# ROOT BRIER SCORE (RBS) \n\n$$\nR B S=\\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{k=1}^{C}\\left(p_{i k}-y_{i k}\\right)^{2}} \\quad \\text { VALUE RANGE: }[0, \\sqrt{2}] \\downarrow\n$$\n\nN : number of samples\n$p_{i k}$ : predicted probability for sample $x_{i}$ and class $k$\n$C$ : number of classes\n$y_{i k}$ : outcome; $y_{i k}=1$ if $y_{i}$ is equal to $k$ and 0 otherwise\n\n## DESCRIPTION\n\nRBS is the square root of the mean squared error of a predicted class score and the actual outcome.\n\nIt represents a robust upper bound of the canonical calibration error.\n\n## DEFINITION\n\n[Gruber and Buettner, 2022]\n\n## TYPE OF CALIBRATION\n\nTop-label Marginal Canonical\n\n## RECOMMENDATIONS\n\nRBS should be considered as a guaranteed upper bound of the canonical calibration error and should be reported together with ECE/ECE ${ }^{\\text {KDE }}$.\n\nExtended Data Fig. SN 3.34. Cheat Sheet for the Root Brier Score (RBS). The downward arrow in the value range indicates that lower values are better than higher values. Abbreviations used in the figure: Expected Calibration Error (ECE), Expected Calibration Error Kernel Density Estimate (ECE ${ }^{\\text {KDE }}$ ), Image-level Classification (ImLC), Instance Segmentation (InS), Object Detection (ObD), Semantic Segmentation (SemS). An introduction to calibration and corresponding terminology can be found in Suppl. Note 2.6. Reference used in the figure: Gruber and Buettner, 2022: [56]. We recommend RBS as a calibration metric in Subprocess S5 (Extended Data Fig. 5)."
    },
    {
      "markdown": "# 3.1.3 Localization criteria. \n\n## BOUNDARY INTERSECTION OVER UNION (BOUNDARY IOU)\n\n![img-115.jpeg](img-115.jpeg)\n\n## DESCRIPTION\n\nBoundary loU measures the overlap between the predicted and reference boundaries up to a predefined width d. Combined with a localization threshold $\\tau$ it can be used as a localization criterion.\n\n## RECOMMENDATIONS\n\n- Boundary loU should be considered as localization criterion if structure boundaries are of particular interest.\n- The hyperparameter d influences the Boundary loU score and denotes the thickness of the considered boundary. It should be chosen according to inter-rater variability, for example. For sufficiently large d, Boundary loU is equal to Mask loU.\n\nExtended Data Fig. SN 3.35. Metric profile of the Boundary loU (localization criterion). The upward arrow in the value range indicates that higher values are better than lower values. Abbreviations used in the figure: Image-level Classification (ImLC), Instance Segmentation (InS), Object Detection (ObD), Semantic Segmentation (SemS). Reference used in the figure: Cheng et al., 2021: [28]. We recommend Boundary Intersection over Union (loU) as a localization criterion in Subprocess S8 (Extended Data Fig. 8)."
    },
    {
      "markdown": "# CENTER DISTANCE \n\n![img-116.jpeg](img-116.jpeg)\n\nReference\n![img-117.jpeg](img-117.jpeg)\n\n## DESCRIPTION\n\nThe Center Distance measures the (typically Euclidean) distance between the reference and predicted center point of an object. The prediction is considered as a hit if the distance is smaller than a predefined threshold $\\tau$. Depending on what kind of information the center point is derived from, different definitions are possible, for instance:\n\n- Geometric center of the box/approximation shape,\n- Geometric center of a binary mask, i.e., average of positions of all pixels/voxels,\n- Center of mass of a binary mask overlaid with the original image, i.e., weighted average of positions of all pixels/voxels with weight equal to (or derived from) the intensity of a particular pixel/voxel.\n\n\n## DEFINITION\n\n[Gurcan et al., 2010]\n\n## RECOMMENDATIONS\n\n- The Center Distance is our default recommendation if the reference object is merely represented by its position.\n- Center Distance should not be considered as a localization criterion...\n\" ... in the presence of tubular-shaped objects.\n\" ... if the overlap between objects or their boundaries are of interest.\n- Otherwise, we specifically recommend it as a localization criterion if the desired granularity of localization is only the position.\n- The localization threshold should be chosen carefully in an application-specific manner.\n\nExtended Data Fig. SN 3.36. Cheat Sheet for the Center Distance. The downward arrow in the value range indicates that lower values are better than higher values. Abbreviations used in the figure: False Negative (FN), False Positive (FP), Image-level Classification (ImLC), Intersection over Union (IoU), Instance Segmentation (InS), Object Detection (ObD), Semantic Segmentation (SemS). Reference used in the figure: Gurcan et al., 2010: [59]. We recommend Center Distance as a localization criterion in Subprocess S8 (Extended Data Fig. 8)."
    },
    {
      "markdown": "# INTERSECTION OVER REFERENCE (loR) \n\nSynonyms: Pixel-level Sensitivity\n![img-118.jpeg](img-118.jpeg)\n$\\operatorname{loR}(A, B)=\\frac{\\square}{\\square}=\\frac{|A \\cap B|}{|A|}$\nVALUE RANGE: $[0,1] \\uparrow$\n\nLOCALIZATION CRITERION\n$\\square \\geq \\tau:$ TP\n$\\square<\\tau:$ FP\n\n## DESCRIPTION\n\nloR measures the overlap between two structures. It is defined as the pixel-level Sensitivity and only considers the FN pixels (not the FPs). The metric is rather uncommon for segmentation assessment, but combined with a localization $\\tau$ threshold it can be used as a localization criterion.\n\n|  | RECOMMENDED FOR |  |  |  |\n| :-- | :--: | :--: | :--: | :--: |\n| DEFINITION | ImLC | SemS | ObD | InS |\n|  | $\\bigcirc$ | $\\bigcirc$ | $\\bigcirc$ |  |\n\n## RECOMMENDATIONS\n\n- loR is a rather uncommon metric/localization criterion, which we do not generally recommend as it can yield extremely misleading results in the case of large predictions.\n- loR should be preferred over loU as localization criterion in InS in the case of touching structures, in which one prediction may overlap multiple reference objects. In this case, assignment of one prediction to multiple reference objects must be enabled and a penalty for this type of algorithm error (\"non-split error\") introduced (see Fig. SN 2.26).\n\nExtended Data Fig. SN 3.37. Cheat Sheet for the Intersection over Reference (loR). The upward arrow in the value range indicates that higher values are better than lower values. Abbreviations used in the figure: Image-level Classification (ImLC), Instance Segmentation (InS), Object Detection (ObD), Semantic Segmentation (SemS). Reference used in the figure: Maška et al., 2014: [102]. We recommend loR as a localization criterion in Subprocess S8 (Extended Data Fig. 8)."
    },
    {
      "markdown": "# MASK/BOX/APPROX INTERSECTION OVER UNION (MASK/BOX/APPROX loU) \n\nSynonyms: Jaccard Index, Tanimoto Coefficient\n![img-119.jpeg](img-119.jpeg)\n\n$$\n\\begin{aligned}\n\\operatorname{loU}(A, B) & =\\frac{\\square}{\\square+1-\\square} \\\\\n& =\\frac{|A \\cap B|}{|A|+|B|-|A \\cap B|}=\\frac{|A \\cap B|}{|A \\cup B|} \\\\\n& =\\frac{\\text { PPV } \\cdot \\text { Sensitivity }}{\\text { PPV }+ \\text { Sensitvity }- \\text { PPV } \\cdot \\text { Sensitivity }}\n\\end{aligned}\n$$\n\n## A B $\\square$ A $\\cap$ B\n\n![img-120.jpeg](img-120.jpeg)\n\n## DESCRIPTION\n\nloU measures the overlap between two structures (see above). Combined with a localization threshold, it is a common localization criterion. It is often referred to as Box loU when comparing bounding boxes. Mask loU when comparing segmentation masks, or Approx loU when comparing approximations of objects beyond bounding boxes.\n\n## DEFINITION\n\n[Jaccard, 1912]\n\n## RECOMMENDED FOR\n\n| ImLC | SemS | ObD | InS |\n| :--: | :--: | :--: | :--: |\n|  |  |  |  |\n\n## IMPORTANT RELATIONS\n\n$$\n\\text { loU }=\\frac{\\text { DSC }}{2 \\text { - DSC }} \\quad \\begin{aligned}\n& \\text { loU }=\\frac{F_{\\beta}}{2-F_{\\beta}} \\\\\n& \\text { for } \\beta=1\n\\end{aligned}\n$$\n\n## RECOMMENDATIONS\n\n- loU is our default recommended localization criterion if a rough outline of the target structures is desired (in contrast to scenarios in which only the position is of importance).\n- loU should not be used ...\n- ... if contour agreement is important for deciding on a match between predicted and reference object.\n- ... to approximate disconnected or tubular structures as boxes (Box loU).\n- Otherwise, it is specifically well-suited if overlap is a meaningful measure of how well an object has been located.\n- The localization threshold should be chosen carefully in an application-specific manner.\n\nExtended Data Fig. SN 3.38. Metric profile of the Mask/Box/Approx Intersection over Union (IoU) (localization criterion). Abbreviations used in the figure: Image-level Classification (ImLC), Instance Segmentation (InS), Object Detection (ObD), Positive Predictive Value (PPV), Semantic Segmentation (SemS). Reference used in the figure: Jaccard, 1912: [67]. We recommend Mask/Box/Approx IoU as a localization criterion in Subprocess 58 (Extended Data Fig. 8)."
    },
    {
      "markdown": "# MASK INTERSECTION OVER UNION (MASK IoU) > 0 \n\nSynonyms: Jaccard Index $>0$, Tanimoto Coefficient $>0$\n![img-121.jpeg](img-121.jpeg)\n\n## DESCRIPTION\n\nMask IoU generally measures the overlap between two segmentation masks and is a common localization criterion. Mask $\\operatorname{loU}>0$ is a special case of a very loose localization criterion, in which only one pixel overlaps.\n\n## DEFINITION\n\n[Wack et al., 2012; Jaccard, 1912]\n\n## RECOMMENDATIONS\n\n- Mask $\\operatorname{loU}>0$ should not be used if ...\n- ... the predicted location is of importance.\n- ... a high amount of overlap with the reference structure is required.\n- ... very large predictions are likely.\n- Otherwise, it is specifically well-suited as an intuitive loose localization criterion if the predicted object should only be characterized by its rough position (e.g., center point), i.e., if outlining structures or massive overlap with the reference is not important.\n\nExtended Data Fig. SN 3.39. Metric profile of the Mask Intersection over Union (IoU) $>0$. Abbreviations used in the figure: False Positive (FP), Image-level Classification (ImLC), Instance Segmentation (InS), Object Detection (ObD), Positive Predictive Value (PPV), Semantic Segmentation (SemS). References used in the figure: Jaccard, 1912: [67], Wack et al., 2012: [164]. We recommend Mask IoU $>0$ as a localization criterion in Subprocess S8 (Extended Data Fig. 8)."
    },
    {
      "markdown": "# POINT INSIDE MASK/BOX/APPROXIMATION \n\n![img-122.jpeg](img-122.jpeg)\n\nReference\n\n## $\\%$ Predicted point\n\nVALUE RANGE: (True, False)\n\n## DESCRIPTION\n\nThe Point inside Mask/Box/Approximation is a localization criterion that defines a point-based prediction as a hit as long as it is inside the reference object, which may be a mask, bounding box, or other approximation of a structure.\n\n## DEFINITION\n\nhttps://cada.grand-challenge.org/Assessment/\n\n## RECOMMENDATIONS\n\n- The Point inside Mask/Box/Approximation criterion should be considered as a localization criterion...\n\" ... if only a rough estimate of the object location is required.\n\" ... for complex shapes, e.g., tubular objects.\n- It should be noted that the Point inside Mask/Box/Approximation criterion does not allow to adjust the localization strictness. If such a property is desired, a different localization criterion should be chosen, such as Box/Approx IoU.\n\nExtended Data Fig. SN 3.40. Cheat Sheet for the Point inside Mask/Box/Approximation. Abbreviations used in the figure: Dice Similarity Coefficient (DSC), Image-level Classification (ImLC), Instance Segmentation (InS), Object Detection (ObD), Positive Predictive Value (PPV), Semantic Segmentation (SemS). Reference used in the figure: https://cada.grand-challenge.org/Assessment/. We recommend Point inside Mask/Box/Approximation as a localization criterion in Subprocess S8 (Extended Data Fig. 8)."
    },
    {
      "markdown": "# 3.1.4 Assignment strategies. \n\n## GREEDY (BY SCORE) MATCHING\n\n![img-123.jpeg](img-123.jpeg)\n\nExtended Data Fig. 5N 3.41. Cheat Sheet for the Greedy (by Score) Matching. Reference used in the figure: Everingham et al., 2015: [50]. We recommend Greedy (by Score) Matching as an assignment strategy in Subprocess S9 (Extended Data Fig. 9)."
    },
    {
      "markdown": "# GREEDY (BY LOCALIZATION CRITERION) MATCHING \n\n![img-124.jpeg](img-124.jpeg)\n\n1) Compute localization criterion between all predicted and reference objects\n![img-125.jpeg](img-125.jpeg)\n\n2) Assign reference to prediction with highest localization criterion.\n![img-126.jpeg](img-126.jpeg)\n\nREPEAT\n\n## DESCRIPTION\n\nIf no predicted class scores are available, the Greedy (by Score) Matching can be replaced with the Greedy (by Localization Criterion) Matching. For this strategy, the reference with the highest localization criterion for a predicted object is matched.\n\nNEED FOR PREDICTED CLASS SCORES?\n\n## DEFINITION\n\n[Maier-Hein et al., 2022]\n\n## RECOMMENDATIONS\n\nWhen no confidence scores (i.e., predicted class scores) are available, selecting the prediction that overlaps most with the reference is the natural choice. Of note, this assignment strategy is currently not used in the field.\n\nExtended Data Fig. SN 3.42. Cheat Sheet for the Greedy (by Localization Criterion) Matching. Reference used in the figure: Maier-Hein et al., 2022: [99]. We recommend Greedy (by Localization Criterion) Matching as an assignment strategy in Subprocess S9 (Extended Data Fig. 9)."
    },
    {
      "markdown": "# OPTIMAL (HUNGARIAN) MATCHING \n\n![img-127.jpeg](img-127.jpeg)\n\n1) Compute localization criterion between all predicted and reference objects\n![img-128.jpeg](img-128.jpeg)\n2) Use cost function to find the optimal assignment of predictions and references based on the localization criterion.\n\n## DESCRIPTION\n\nThe Optimal (Hungarian) Matching is associated with a cost function, usually depending on the localization criterion, which is minimized to find the optimal assignment of predictions and reference.\n\nNEED FOR PREDICTED CLASS SCORES?\n\n## DEFINITION\n\n[Kuhn, 1955]\n\n## RECOMMENDATIONS\n\nIf the application at hand provides a dedicated cost function as to which assignments should be penalized, this strategy is the natural choice. In most cases, however, an adequate cost function would need to be defined by the user which can often not be justified given the available, more intuitive assignment strategies.\n\nExtended Data Fig. SN 3.43. Cheat Sheet for the Optimal (Hungarian) Matching. Reference used in the figure: Kuhn et al., 1955: [82]. We recommend the Optimal (Hungarian) Matching as an assignment strategy in Subprocess S9 (Extended Data Fig. 9)."
    },
    {
      "markdown": "# MATCHING VIA OVERLAP > 0.5 \n\nPREREQUISITE: Overlapping predictions are not possible.\n![img-129.jpeg](img-129.jpeg)\n\n1) Compute overlap-based localization criterion between all predicted and reference objects\n![img-130.jpeg](img-130.jpeg)\n\n2) If the overlap is greater than 0.5 , assign prediction to the reference.\n![img-131.jpeg](img-131.jpeg)\n\n3) Remove assigned reference object\n\n## DESCRIPTION\n\nIf there are no overlapping predictions, complex assignment strategies can be avoided by simply setting the localization criterion to $\\operatorname{loU}>0.5$. This strategy inherently avoids matching conflicts, because any secondary prediction would by definition have an overlap $<0.5$ of the same reference object.\n\nNEED FOR PREDICTED CLASS SCORES?\n\n## DEFINITION\n\n[Everingham et al., 2006]\n\n## RECOMMENDATIONS\n\n- This assignment strategy should not be applied if ...\n\" ... overlapping predictions are possible.\n\"... a non-overlap based criterion is employed.\n\"... an loU threshold lower than 0.5 is requested for localization.\n- Otherwise, it represents a simple and intuitive localization criterion that inherently avoids matching conflicts and thus the need for a dedicated assignment strategy. This criterion is often used in the cell segmentation domain.\n\nExtended Data Fig. SN 3.44. Cheat Sheet for the Matching via Overlap > 0.5. Reference used in the figure: Everingham et al., 2006: [48]. We recommend Matching via Overlap $>0.5$ as an assignment strategy in Subprocess S9 (Extended Data Fig. 9)."
    },
    {
      "markdown": "# SUPPL. NOTE 4 RECOMMENDATIONS FOR SELECTED USE CASES \n\nWe instantiated the framework for several biological and medical image analysis use cases. The list of use cases with a link to the figures representing the recommendations is provided below:\n\n### 4.1 Image-level classification\n\nThe following use cases have been instantiated for image-level classification problems. The resulting metric recommendations can be found in Fig. SN 4.1, while Figs. SN 4.5-SN 4.7 provide a detailed overview of the recommendations for the use cases in the metric selection Subprocesses S2-S5.\n\nImLC-1 Frame-based sperm motility classification from microscopy time-lapse video of human spermatozoa [62]\nImLC-2 Disease classification in dermoscopic images [33, 154]\nImLC-3 Classification of the overall autophagy stage for a collection of cells [111, 174]\nImLC-4 Diagnostic standard plane classification in ultrasound images [11]\nImLC-5 Identification of new lesions in brain multi-modal magnetic resonance imaging (MRI) images of patients with multiple sclerosis (MS) [36, 79]\nImLC-6 Breast cancer classification in mammography images [89]\nImLC-7 Multi-class cardiac disease classification in MRI images [15]"
    },
    {
      "markdown": "![img-132.jpeg](img-132.jpeg)\n\nExtended Data Fig. SN 4.1. Instantiation of the framework with recommendations for concrete biomedical image-level classification problems. (ImLC-1) Frame-based sperm motility classification from microscopy time-lapse video of human spermatozoa [62]. (ImLC-2) Disease classification in dermoscopic images [33, 154]. (ImLC-3) Classification of the overall autophagy stage for a collection of cells [111, 174]. (ImLC-4) Diagnostic standard plane classification in ultrasound images [11]. (ImLC-5) Identification of new lesions in brain multi-modal magnetic resonance imaging (MRI) images of MS patients [36, 79]. (ImLC-6) Breast cancer classification in mammography images [89]. (ImLC-7) Multi-class cardiac disease classification in MRI images [15]."
    },
    {
      "markdown": "# 4.2 Semantic segmentation \n\nThe following use cases have been instantiated for semantic segmentation problems. The resulting metric recommendations can be found in Fig. SN 4.2, while Figs. SN 4.10-SN 4.11 provide a detailed overview of the recommendations for the use cases in the metric selection Subprocesses S6 and S7.\n\nSemS-1 Embryo segmentation from microscopy images [147]\nSemS-2 Liver segmentation in computed tomography (CT) images [2, 141]\nSemS-3 Labeling of invasive/non-invasive/benign lesions in breast whole slide imaging (WSI) [3]\nSemS-4 Cortical structure segmentation from 3D MRI images [23]\nSemS-5 Aneurysm segmentation in time-of-flight magnetic resonance angiography (TOF-MRA) images $[150]$"
    },
    {
      "markdown": "![img-133.jpeg](img-133.jpeg)\n\nExtended Data Fig. SN 4.2. Instantiation of the framework with recommendations for concrete biomedical semantic segmentation problems. (SemS-1) Embryo segmentation from microscopy images [147]. (SemS-2) Liver segmentation in computed tomography (CT) images [2, 141]. (SemS-3) Labeling of invasive/non-invasive/benign lesions in breast whole slide imaging (WSI) [3]. (SemS-4) Cortical structure segmentation from 3D magnetic resonance imaging (MRI) images[23]. (SemS-5) Aneurysm segmentation in time-of-flight magnetic resonance angiography (TOF-MRA) images [150]."
    },
    {
      "markdown": "# 4.3 Object detection \n\nThe following use cases have been instantiated for object detection problems. The resulting metric recommendations can be found in Fig. SN 4.3, while Figs. SN 4.6-SN 4.9 provide a detailed overview of the recommendations for the use cases in the metric selection Subprocesses S3 - S4, S8 - S9.\n\nObD-1 Cell detection and tracking during the autophagy process in time-lapse microscopy $[111,174]$\nObD-2 MS lesion detection in multi-modal brain MRI images [36, 79]\nObD-3 Polyp detection in colonoscopy videos with predefined sensitivity of $0.95[14,134]$\nObD-4 Mitosis detection in histopathology images [8]\nObD-5 Lung nodule detection in CT images [4, 5, 32]"
    },
    {
      "markdown": "![img-134.jpeg](img-134.jpeg)\n\nExtended Data Fig. SN 4.3. Instantiation of the framework with recommendations for concrete biomedical object detection problems. (ObD-1) Cell detection and tracking during the autophagy process in time-lapse microscopy [111, 174]. (ObD-2) Multiple sclerosis (MS) lesion detection in multi-modal brain magnetic resonance imaging (MRI) images [36, 79]. (ObD-3) Polyp detection in colonoscopy videos with predefined sensitivity of 0.95 [14, 134]. (ObD-4) Mitosis detection in histopathology images [8]. (ObD-5) Lung nodule detection in computed tomography (CT) images [4, 5, 32]."
    },
    {
      "markdown": "# 4.4 Instance segmentation \n\nThe following use cases have been instantiated for instance segmentation problems. The resulting metric recommendations can be found in Fig. SN 4.4, while Figs. SN 4.6-SN 4.11 provide a detailed overview of the recommendations for the use cases in the metric selection Subprocesses S3 - S4, S6 - S9.\n\nInS-1 Instance segmentation of neurons from the fruit fly in 3D multi-color light microscopy images $[100,108,151]$\nInS-2 Surgical instrument instance segmentation in colonoscopy videos [98]\nInS-3 Cell nuclei instance segmentation in time-lapse light microscopy for cell tracking [153]\nInS-4 MS lesion segmentation in multi-modal brain MRI images [36, 79]"
    },
    {
      "markdown": "![img-135.jpeg](img-135.jpeg)\n\nExtended Data Fig. SN 4.4. Instantiation of the framework with recommendations for concrete biomedical instance segmentation problems. (InS-1) Instance segmentation of neurons from the fruit fly in 3D multi-color light microscopy images [100, 108, 151]. (InS-2) Surgical instrument instance segmentation in colonoscopy videos [98]. (InS-3) Cell nuclei instance segmentation in time-lapse light microscopy for cell tracking [153]. (InS-4) Multiple sclerosis (MS) lesion segmentation in multi-modal brain magnetic resonance imaging (MRI) images [36, 79]."
    },
    {
      "markdown": "# Recommendations shown within the metric selection Subprocesses S2-S9 \n\n![img-136.jpeg](img-136.jpeg)\n\nExtended Data Fig. SN 4.5. Instantiation of Subprocess S2 for the selection of multi-class counting metrics with recommendations for concrete biomedical problems. Included use cases: (ImLC-1) Framebased sperm motility classification based on microscopy time-lapse video containing human spermatozoa [62]. (ImLC-2) Disease classification in dermoscopic images [33, 154]. (ImLC-3) Classification of the overall autophagy stage for a collection of cells [111, 174]. (ImLC-5) Identification of new lesions in brain multi-modal magnetic resonance imaging (MRI) images of multiple sclerosis (MS) patients [36, 79]. (ImLC-6) Breast cancer classification in mammography images [89]. (ImLC-7) Multi-class cardiac disease classification in MRI images [15]."
    },
    {
      "markdown": "![img-137.jpeg](img-137.jpeg)\n\nExtended Data Fig. SN 4.6. Instantiation of Subprocess S3 for the selection of per-class counting metrics with recommendations for concrete biomedical problems. Included use cases: (ImLC-1) Frame-based sperm motility classification from microscopy time-lapse video of human spermatozoa [62]. (ImLC-2) Disease classification in dermoscopic images [33, 154]. (ImLC-3) Classification of the overall autophagy stage for a collection of cells [111, 174]. (ImLC-4) Diagnostic standard plane classification in ultrasound images [11]. (ImLC-5) Identification of new lesions in brain multi-modal magnetic resonance imaging (MRI) images of multiple sclerosis (MS) patients [36, 79]. (ImLC-6) Breast cancer classification in mammography images [89]. (ImLC-7) Multi-class cardiac disease classification in MRI images [15]. (ObD-1) Cell detection and tracking during the autophagy process in time-lapse microscopy [111, 174]. (ObD-2) MS lesion detection in multi-modal brain MRI images [36, 79]. (ObD-3) Polyp detection in colonoscopy videos with predefined sensitivity of $0.95[14,134]$. (ObD-4) Mitosis detection in histopathology images [8]. (ObD5) Lung nodule detection in computed tomography (CT) images [4, 5, 32]. (InS-1) Instance segmentation of neurons from the fruit fly in 3D multi-color light microscopy images [100, 108, 151]. (InS-2) Surgical instrument instance segmentation in colonoscopy videos [98]. (InS-3) Cell nuclei instance segmentation in time-lapse light microscopy with a subsequent goal of cell tracking [153]. (InS-4) MS Lesion segmentation in multi-modal brain MRI images [36, 79]."
    },
    {
      "markdown": "![img-138.jpeg](img-138.jpeg)\n\nExtended Data Fig. SN 4.7. Instantiation of Subprocess S4 for the selection of multi-threshold metrics with recommendations for concrete biomedical problems. Included use cases: (ImLC-1) Frame-based sperm motility classification from microscopy time-lapse video of human spermatozoa [62]. (ImLC-2) Disease classification in dermoscopic images [33, 154]. (ImLC-3) Classification of the overall autophagy stage for a collection of cells [111, 174]. (ImLC-4) Diagnostic standard plane classification in ultrasound images [11]. (ImLC-5) Identification of new lesions in brain multi-modal magnetic resonance imaging (MRI) images of multiple sclerosis (MS) patients [36, 79]. (ImLC-6) Breast cancer classification in mammography images [89]. (ImLC-7) Multi-class cardiac disease classification in MRI images [15]. (ObD-1) Cell detection and tracking during the autophagy process in time-lapse microscopy [111, 174]. (ObD-2) MS lesion detection in multimodal brain magnetic resonance imaging (MRI) images [36, 79]. (ObD-3) Polyp detection in colonoscopy videos with predefined sensitivity of $0.95[14,134]$. (ObD-5) Lung nodule detection in computed tomography (CT) images [4, 5, 32]. (InS-1) Instance segmentation of neurons from the fruit fly in 3D multi-color light microscopy images [100, 108, 151]. (InS-2) Surgical instrument instance segmentation in colonoscopy videos [98]. (InS-4) MS lesion segmentation in multi-modal brain MRI images [36, 79]."
    },
    {
      "markdown": "![img-139.jpeg](img-139.jpeg)\n\nExtended Data Fig. SN 4.8. Instantiation of Subprocess S5 for the selection of calibration metrics with recommendations for concrete biomedical problems. Included use cases: (ImLC-1) Frame-based sperm motility classification from microscopy time-lapse video of human spermatozoa [62]. (ImLC-2) Disease classification in dermoscopic images [33, 154]. (ImLC-4) Diagnostic standard plane classification in ultrasound images [11]. (ImLC-5) Identification of new lesions in brain multi-modal magnetic resonance imaging (MRI) images of multiple sclerosis (MS) patients [36, 79]. (ImLC-6) Breast cancer classification in mammography images [89]."
    },
    {
      "markdown": "![img-140.jpeg](img-140.jpeg)\n\nExtended Data Fig. SN 4.9. Instantiation of Subprocess S6 for the selection of overlap-based metrics with recommendations for concrete biomedical problems. (SemS-1) Embryo cell segmentation from microscopy images [147]. (SemS-2) Liver segmentation in computed tomography (CT) images [2, 141]. (SemS-3) Labeling of invasive/ non-invasive/ benign lesions in breast whole slide imaging (WSI) [3]. (SemS-4) Cortical structure segmentation from 3D magnetic resonance imaging (MRI) images[23]. (SemS-5) Aneurysm segmentation in time-of-flight magnetic resonance angiography (TOF-MRA) images [150]. (InS-1) Instance segmentation of neurons from the fruit fly in 3D multi-color light microscopy images [100, 108, 151]. (InS-2) Surgical instrument instance segmentation in colonoscopy videos [98]. (InS-3) Cell nuclei instance segmentation in time-lapse light microscopy for cell tracking [153]. (InS-4) MS lesion segmentation in multi-modal brain MRI images $[36,79]$."
    },
    {
      "markdown": "![img-141.jpeg](img-141.jpeg)\n\nExtended Data Fig. SN 4.10. Instantiation of Subprocess S7 for the selection of boundary-based metrics with recommendations for concrete biomedical problems. (SemS-1) Embryo segmentation from microscopy images [147]. (SemS-2) Liver segmentation in computed tomography (CT) images [2, 141]. (SemS-3) Labeling of invasive/non-invasive/benign lesions in breast whole slide imaging (WSI) [3]. (SemS-4) Cortical structure segmentation from 3D magnetic resonance imaging (MRI) images[23]. (SemS-5) Aneurysm segmentation in time-of-flight magnetic resonance angiography (TOF-MRA) images [150]. (InS-1) Instance segmentation of neurons from the fruit fly in 3D multi-color light microscopy images [100, 108, 151]. (InS-2) Surgical instrument instance segmentation in colonoscopy videos [98]. (InS-3) Cell nuclei instance segmentation in time-lapse light microscopy for cell tracking [153]. (InS-4) MS lesion segmentation in multimodal brain MRI images [36, 79]."
    },
    {
      "markdown": "![img-142.jpeg](img-142.jpeg)\n\nExtended Data Fig. SN 4.11. Instantiation of Subprocess S8 for the selection of localization criteria with recommendations for concrete biomedical problems. (ObD-1) Cell detection and tracking during the autophagy process in time-lapse microscopy [111, 174]. (ObD-2) multiple sclerosis (MS) lesion detection in multi-modal brain magnetic resonance imaging (MRI) images [36, 79]. (ObD-3) Polyp detection in colonoscopy videos with predefined sensitivity of $0.9[14,134]$. (ObD-4) Mitosis detection in histopathology images [8]. (ObD-5) Lung nodule detection in computed tomography (CT) images [4, 5, 32]. (InS-1) Instance segmentation of neurons from the fruit fly in 3D multi-color light microscopy images [100, 108, 151]. (InS-2) Surgical instrument instance segmentation in colonoscopy videos [98]. (InS-3) Cell nuclei instance segmentation in time-lapse light microscopy for cell tracking [153]. (InS-4) MS lesion segmentation in multi-modal brain MRI images $[36,79]$."
    },
    {
      "markdown": "![img-143.jpeg](img-143.jpeg)\n\nExtended Data Fig. SN 4.12. Instantiation of Subprocess S9 for the selection of assignment strategies with recommendations for concrete biomedical problems. (ObD-1) Cell detection and tracking during the autophagy process in time-lapse microscopy [111, 174]. (ObD-2) multiple sclerosis (MS) Lesion detection in multi-modal brain magnetic resonance imaging (MRI) images [36, 79]. (ObD-3) Polyp detection in colonoscopy videos with predefined sensitivity of $0.95[14,134]$. (ObD-4) Mitosis detection in histopathology images [8]. (ObD-5) Lung nodule detection in computed tomography (CT) images [4, 5, 32]. (InS-1) Instance segmentation of neurons from the fruit fly in 3D multi-color light microscopy images [100, 108, 151]. (InS-2) Surgical instrument instance segmentation in colonoscopy videos [98]. (InS-3) Cell nuclei instance segmentation in time-lapse light microscopy for cell tracking [153]. (InS-4) MS lesion segmentation in multi-modal brain MRI images $[36,79]$."
    },
    {
      "markdown": "# SUPPL. NOTE 5 TERMINOLOGY AND NOTATION \n\n### 5.1 Symbol References\n\n| Symbol | Explanation |\n| :--: | :--: |\n| 0 | Start of a process |\n| 0 | End of a process |\n| Read general instructions (App. R.1) | Task to be performed by the user |\n| Select <br> multi-class <br> counting metric <br> + 5 | Subprocess |\n| Check <br> Problem <br> category! | Task with reference to problem fingerprint |\n| Check <br> 2015 <br> 2016 <br> 2017 <br> 2018 <br> 2019 <br> 2020 <br> 2021 <br> 2022 <br> 2023 <br> Accuracy | Exclusion criterion |\n| ![img-144.jpeg](img-144.jpeg) | Pool of options the user can choose from in the respective step |\n| Select <br> Accuracy | A metric/criterion/strategy is selected |\n| ![img-145.jpeg](img-145.jpeg) | Exclusive gateway: An exclusive gateway (or XOR-gateway) allows the user to make a decision. It can have multiple outgoing sequence flows. It is used when several conditions are mutually exclusive and only one selection is possible. <br> An exclusive gateway is also used to join multiple incoming flows together and improve the readability of the diagram. |\n| ![img-146.jpeg](img-146.jpeg) | Group |\n| ![img-147.jpeg](img-147.jpeg) | Notes |\n| ![img-148.jpeg](img-148.jpeg) | Further information |\n| ![img-149.jpeg](img-149.jpeg) | The respective step needs to be repeated for each class |\n\nExtended Data Fig. SN 5.1. Overview of symbols used in the process diagrams. The notation used in the process diagrams originates from Business Process Model and Notation (BPMN)."
    },
    {
      "markdown": "# 5.2 Expected formats of reference and algorithm output \n\nImage-level Classification: The metric mapping expects the following format for image-level classification with $C$ classes: For each image $I$ there is a reference annotation $y_{I}$ that either indicates the class for the image $\\left(y_{I} \\in\\{1, . ., C\\}\\right)$, or, in the case of multi-label classification, indicates presence for each class $\\left(y_{I} \\in\\{0,1\\}^{C}\\right)$. If the algorithm does not provide predicted class scores (FP5.1 = FALSE), the algorithm output should be provided in an identical format. Otherwise, for each image $I$, the continuous class scores for each of the classes $\\left(\\hat{y}_{I} \\in[0,1]^{C}\\right)$, indicating the predicted class probability, should be provided.\nSemantic Segmentation We assume the reference annotation and the algorithm output to be in the same coordinate system with identical spacing. The metric mapping expects the following format for semantic segmentation with $C$ classes: For each pixel $P$ there is a reference annotation $y_{P}$ that either assigns a single class to $P\\left(y_{P} \\in\\{1, . ., C\\}\\right)$ or, in case of possible multiple labels per pixel, indicates assignment for each class $\\left(y_{P} \\in\\{0,1\\}^{C}\\right)$. As for the algorithm output, for each pixel $P$ there is expected to be either a single prediction $\\left(\\hat{y}_{P} \\in\\{1, . ., C\\}\\right)$ or, in case of multiple possible labels per pixel, a prediction for each class $\\left(\\hat{y}_{P} \\in\\{0,1\\}^{C}\\right)$. Some segmentation metrics require structure boundaries. For each class, boundaries are expected to be provided as a list of boundary pixels for both the reference and the prediction.\nObject detection: The metric mapping expects the following format for object detection with $C$ classes: For each object $O$ the reference consists of a tuple $\\left(y_{O}, l_{O}\\right)$, where $y_{O} \\in\\{1, . ., C\\}$ indicates the class of the object and $l_{O}$ is some location information (box, center point, radius, etc.). The algorithm output for an object prediction $O$ is expected to comprise a tuple $\\left(\\hat{y}_{O}, \\hat{l}_{O}\\right)$ as well, where $\\hat{y}_{O}$ indicates a single predicted class $\\left(\\hat{y}_{O} \\in\\{1, . ., C\\}\\right)$ optionally accompanied by an associated predicted class score $\\left(\\hat{c}_{O} \\in[0,1]\\right)$. Note that methods usually provide a predicted class score for the background class as well, but this score is typically discarded in validation as there are no \"background objects\" [50]. See FP5.1 in case no predicted class score is provided. $\\hat{l}_{O}$ is expected to provide location information about the prediction in a similar format as the reference (box, center point, radius, etc.). In case reference objects are represented by rough outlines (FP4.3) we assume that the chosen shapes (e.g., bounding box or ellipsoid) represent the underlying object adequately. Particular attention needs to be given to this aspect if objects feature a tubular shape (FP3.3) or can potentially appear disconnected (FP3.6).\nInstance Segmentation The metric mapping expects the following format for instance segmentation with $C$ classes: For each object $O$ the reference consists of a tuple $\\left(y_{O}, m_{O}\\right)$, where $y_{O} \\in\\{1, . ., C\\}$ indicates the class of the object and $m_{O} \\in\\{0,1\\}^{H \\times W}$ is a binary pixel map per instance matching the size of the image (height $H$ and width $W$ ) and indicating pixelwise location. The algorithm output for an object prediction $O$ is expected to comprise a tuple $\\left(\\hat{y}_{O}, \\hat{m}_{O}\\right)$, where, similarly to object detection, $\\hat{y}_{O}$ indicates a single predicted class $\\left(\\hat{y}_{O} \\in\\{1, . ., C\\}\\right)$ optionally accompanied by an associated predicted class score $\\left(\\hat{c}_{O} \\in[0,1]\\right)$. $\\hat{m}_{O}$ denotes a binary pixel map per instance analogously to $m_{O}$. For both the reference and the predictions, structure boundaries should be provided as a list of boundary pixels separately for each instance. Note that annotations from semantic segmentation (not distinguishing instances of the same class) can be transformed to the instance segmentation format via connected component analysis (in case of purely non-touching and connected instances)."
    },
    {
      "markdown": "In case the provided reference annotations deviate from the expected format, matching can be achieved via various measures (e.g., aggregation of pixel-level reference to required image-level reference).\n\n# 5.3 Acronyms \n\nAI artificial intelligence\nAP Average Precision\nASSD Average Symmetric Surface Distance\nAUC Area under the curve\nAUROC Area under the Receiver Operating Characteristic Curve\nBA Balanced Accuracy\nBM Bookmaker Informedness\nBPMN Business Process Model and Notation\nBS Brier Score\nBSS Brier Skill Score\nCE Calibration Error\nCK Cohen's Kappa\nclDice centerline Dice Similarity Coefficient\nCT computed tomography\nCWCE Class-wise Calibration Error\nDSC Dice Similarity Coefficient\nEC Expected Cost\nECE Expected Calibration Error\n$\\mathbf{E C E}^{\\text {KDE }}$ Expected Calibration Error Kernel Density Estimate\nECI Expected Calibration Index\nECN normalized EC\nEQUATOR Enhancing the QUAlity and Transparency Of health Research\nER Error Rate\nFN False Negative\nFP False Positive\nFPPI False Positives per Image\nFDR False Discovery Rate\nFOR False Omission Rate\nFROC Free-Response Receiver Operating Characteristic\nHD Hausdorff Distance\nHD95 Hausdorff Distance 95th Percentile\nImLC Image-level Classification\nInS Instance Segmentation\nIoU Intersection over Union\nIoR Intersection over Reference\nJ Youden Index\nKCE Kernel Calibration Error\nLR+ Positive Likelihood Ratio\nmAP Mean Average Precision\nMASD Mean Average Surface Distance\nMCC Matthews Correlation Coefficient\nMICCAI Medical Image Computing and Computer Assisted Interventions"
    },
    {
      "markdown": "MK Markedness\nML machine learning\nMONAI Medical Open Network for Artificial Intelligence\nMRI magnetic resonance imaging\nMS multiple sclerosis\nNaN 'Not a Number'\nNB Net Benefit\nNLL Negative Log Likelihood\nNPV Negative Predictive Value\nNSD Normalized Surface Distance\nObD Object Detection\nO:E ratio Observed:Expected ratio\nPPV Positive Predictive Value\nPQ Panoptic Quality\nPHI Protected Health Information\nPR Precision-Recall\nPSR Proper Scoring Rule\nRBS Root Brier Score\nRI Rand Index\nROC Receiver Operating Characteristic\nROI Region of Interest\nSemS Semantic Segmentation\nTN True Negative\nTNR True Negative Rate\nTOF-MRA time-of-flight magnetic resonance angiography\nTP True Positive\nTPR True Positive Rate\nVoI Variation of Information\nWCK Weighted Cohen's Kappa\nWSI whole slide imaging\n$\\mathbf{X}^{t h}$ Percentile HD $\\mathbf{X}^{t h}$ Percentile Hausdorff Distance"
    },
    {
      "markdown": "# 5.4 Glossary \n\n- Bounding box: A bounding box is a rectangle, typically the smallest possible, drawn around and completely surrounding an object to be detected.\n- Calibration plot: A calibration plot, also referred to as reliability diagram, is a visualization of the calibration ability of a model's outputs (see e.g., [57]). Specifically, the diagram allows to diagnose a model's general bias towards \"overconfident\" or \"underconfident\" predictions by visualizing the deviation from perfect calibration (diagonal line in the plot) for different output scores. The diagram also acts as the basis for further diagnostic measurements such as the calibration slope.\n- Challenge: A challenge is an international competition, commonly hosted by individual researchers, an institute, or a professional society, that aims to comparatively assess the performance of competing algorithms on an identical data set, and thus serves to validate them. This validation is a crucial step towards the translation of an algorithm into practice.\n- Classification task: A classification task is the task of giving categorical labels to an image or parts thereof. We distinguish classification at different scales, e.g., at image level, pixel level or object level.\n- Confidence: See Predicted class scores.\n- Continuous class scores: See Predicted class scores.\n- Decision rule: A rule transforming continuous predicted class scores into discrete classification decisions. This rule amounts to setting a simple cutoff value in binary classification problems but is more complex to define in multi-class problems (for more information see Suppl. Note 1.1).\n- Evaluation: See Validation.\n- Hierarchical structure of classes/data: A hierarchical structure of classes/data is present when classes or data are dependent on each other or paired, e.g., when data have been derived from the same patient, or from the same center. It requires interpretation and statistical efforts different from those suitable for independent data.\n- Hyperparameter: A hyperparameter is a parameter whose value is optimized to control the training of an algorithm. In contrast to other parameters, it is not derived through the training process itself, but rather set before the training procedure.\n- Inference: In the context of ML, inference denotes the processing of data by an algorithm to produce the desired output.\n- Instance: An instance refers to a dedicated object, structure or entity in an image, such as an individual cell, tumor or medical instrument.\n- Image-level classification: Image-level classification is the assignment of one or multiple category labels to an entire image, as detailed in Suppl. Note 1.1.\n- Instance segmentation: Detection and delineation of each distinct object of a particular class in an image, as detailed in Suppl. Note 1.1.\n- Instantiation: Instantiation here refers to the act of creating a specific application case of a general principle/framework.\n- Macro/micro averaging: Macro averaging is the process of computing a metric (e.g., Sensitivity) for each class and subsequently averaging the metric scores. Micro averaging is the process of aggregating an average metric score over all classes.\n- Meta-information: Meta-information refers to data about an image that is not explicitly contained within the image, e.g., Protected Health Information (PHI) data about the patient in radiology images."
    },
    {
      "markdown": "- Metric: Metrics are the measures according to which performance of algorithms is quantified and validated. Depending on the domain-specific validation goal and property of interest, we distinguish between different types of metrics, e.g., reference-based vs. non-reference based (see Reference/Reference-based metrics). Metrics can further be subdivided into different families based on their mathematical properties.\n- Metric@(TargetMetric = TargetValue): (e.g., Specificity@(Sensitivity = 0.95)): Once a cutoff value for the predicted class probabilities has been set in such a way that the target metric value is achieved (here: target metric Sensitivity with a target value of 0.95 ), other metric values (here: Specificity) are obtained from the corresponding fixed confusion matrix. In the example, this yields the Specificity at the predefined Sensitivity level.\n- Object detection: Detection and localization of structures of one or multiple categories in an image, as detailed in Suppl. Note 1.1.\n- (Output) Calibration: In application scenarios that involve interpreting the raw algorithm output (specifically the predicted class scores), output calibration can be used to obtain a reliable measure of confidence associated with the decision (see description of FP2.7 in Suppl. Note. 1.2).\n- Image-level classification: Assignment of one or multiple category labels to the entire image or fixed regions/predefined locations within an image.\n- Instance Segmentation: Localization and delineation of each distinct structure of a particular class. It can be regarded as delivering the tasks of object detection and semantic segmentation at the same time. In contrast to object detection, instance segmentation also involves the accurate marking of the structure boundary. In contrast to semantic segmentation, it distinguishes different structures of the same class.\n- Object detection: Localization and categorization of an unknown number of structures, as detailed in Suppl. Note 1.1\n- Precision: Precision is a term used differently in different scientific communities. In the medical community, for example, it commonly refers to the confidence of an output. Here, we use the term to denote the Positive Predictive Value (PPV).\n- Predicted class scores: Modern neural network-based approaches usually output predicted class scores (also referred to as continuous class scores, confidence scores or pseudoprobabilities) between 0 and 1 for every image/object/pixel and class, indicating the probability of the image/object/pixel belonging to a specific class.\n- Prediction: Prediction refers to the output of an algorithm. It is not used in the temporal sense in this paper.\n- Problem category: Biomedical image analysis problems can be subdivided into problem categories according to the procedures performed. The category a problem falls into informs the appropriate choice of metrics. In this paper, we focus on four problem categories: Imagelevel classification, Semantic Segmentation, Object Detection, and Instance Segmentation.\n- Pseudo-probabilities: See Predicted class scores.\n- Reference/Reference-based metrics: We assume that the validation process is based on the comparison of the algorithm output and a reference (sometimes called gold standard), which is assumed to be close or equal to the correct result - the (often forever unknown) ground truth. In terms of metrics, we distinguish between reference-based metrics [71], which use the image-based reference, and non-reference-based metrics that assess complementary properties, such as runtime, memory consumption, or carbon footprint.\n- Reliability diagram: See calibration plot.\n- Semantic segmentation: Assignment of one or multiple category labels to each pixel in an image, as detailed in Suppl. Note 1.1."
    },
    {
      "markdown": "- Structure instance: See Instance.\n- Training/Test case: The data sets used in the process of algorithm development and validation comprise training/test cases. A case refers to the data (typically an n-dimensional image, possibly enhanced with clinical context information) that is required for an algorithm to produce one result (e.g., a segmentation or classification). A training case refers to a data set that includes reference annotations and is thus used for training an algorithm. A test case refers to a data set that is used for performance assessment\n- Type 1 and Type 2 error: A type 1 error is a False Positive (FP) result, e.g., a false detection of something that is not present. A type 2 error is a False Negative (FN) result, e.g., a nondetection of something that is present.\n- Validation: Validation is the process of assessing that the validated algorithm is effectively doing what it is expected to do and what it was developed for, for example that a segmentation method is actually segmenting. Evaluation is the process of assessing that the algorithm is valuable, i.e., that it brings quantifiable added value for the clinical user in a dedicated clinical context [66]."
    }
  ],
  "usage_info": {
    "pages_processed": 209,
    "doc_size_bytes": 36884489
  },
  "_metadata": {
    "model_version": "0.0.0",
    "llm_model": "mistral-ocr-2505",
    "query_data": {
      "pdf": "/Users/satyaortiz-gagne/CODE/paperext/data/cache/fulltext/c67800fbbf25fa27687cd11db8bb4d9d/fulltext.pdf"
    },
    "model_id": "parsepdf"
  }
}