{
  "pages": [
    {
      "markdown": "# ClimateGAN: Raising Climate Change Awareness by Generating Images of Floods \n\nVictor Schmidt* ${ }^{1,2}$, Alexandra Luccioni* ${ }^{1,2}$, Mélisande Teng ${ }^{1,2}$, Tianyu Zhang ${ }^{1,2}$, Alexia Reynaud ${ }^{1,2}$, Sunand Raghupathi ${ }^{1,3}$, Gautier Cosne ${ }^{1,2}$, Adrien Juraver ${ }^{1,2}$, Vahe Vardanyan ${ }^{4}$, Alex Hernández-García ${ }^{1,2}$, and Yoshua Bengio ${ }^{1,2}$<br>${ }^{1}$ Mila Québec AI Institute, Montréal, Canada<br>${ }^{2}$ Université de Montréal, Montréal, Canada<br>${ }^{3}$ Columbia University, New York City, USA<br>${ }^{4}$ CDRIN, Matane, Canada\n\n\n#### Abstract\n\nClimate change is a major threat to humanity, and the actions required to prevent its catastrophic consequences include changes in both policy-making and individual behaviour. However, taking action requires understanding the effects of climate change, even though they may seem abstract and distant. Projecting the potential consequences of extreme climate events such as flooding in familiar places can help make the abstract impacts of climate change more concrete and encourage action. As part of a larger initiative to build a website that projects extreme climate events onto user-chosen photos, we present our solution to simulate photo-realistic floods on authentic images. To address this complex task in the absence of suitable training data, we propose ClimateGAN, a model that leverages both simulated and real data for unsupervised domain adaptation and conditional image generation. In this paper, we describe the details of our framework, thoroughly evaluate components of our architecture and demonstrate that our model is capable of robustly generating photo-realistic flooding.\n\n\n## 1 Introduction\n\nClimate change is an increasingly serious danger to our planet, with warming temperatures causing extreme weather events such as droughts, storms and heatwaves. These phenomena affect the livelihood of millions of people globally, with that number rising each year (Hauer, 2017; Neumann et al., 2015; Hoegh-Guldberg et al., 2018; Watts et al., 2019). Climate change is also making flooding worse, both due to rising sea levels as well as increasing precipitation and faster snow melt in some areas, presenting a major risk to both coastal and in-land populations worldwide (Dottori et al., 2016; Vousdoukas et al., 2018).\n\nVisualizing the effects of climate change has been found to help overcome distancing, a psychological phenomenon resulting in climate change being perceived as temporally and spatially distant and uncertain (Sheppard, 2012; Spence et al., 2012), and thus less likely to trigger action. In fact, images of extreme weather events (Leviston et al., 2014) and their impacts (Chapman et al., 2016) have been found to be especially likely to trigger behavioral changes.\n\nPrevious research has shown that simulating first-person perspectives of climate change can contribute to reducing distancing (Berenguer, 2007; Sevillano et al., 2007). Digital technologies are increasingly used for this purpose, ranging from interactive data dashboards (Herring et al., 2017) to web-based geographic visualizations (Neset et al., 2016; Glaas et al., 2017) to immersive digital environments of ecosystems (Ahn et al., 2014). However, existing technologies have targeted specific environments or regions, manually ren-\n\n[^0]Preprint. Under review.\n\n\n[^0]:    *Corresponding authors - [schmidtv, luccionis]@mila.quebec"
    },
    {
      "markdown": "![img-0.jpeg](img-0.jpeg)\n\nFigure 1: We present ClimateGAN, a model that simulates extreme floods (right) on real scene images (left).\ndering climate change effects; there is, to our knowledge, no tool that can render impacts of climate change in arbitrary locations to that end.\n\nIn this context, we endeavor to develop a system enabling users to enter an address of their choice and visualize the associated first-person image from Google Street View, transformed as if it had been impacted by extreme flooding. To do so, we propose our model ClimateGAN, which decomposes the task of flooding into two parts: a Masker model to predict which pixel locations in an image would be under water if a flood occurred, and a Painter model to generate contextualized water textures conditionally on both the input image and the Masker's prediction. Our contributions are: proposing and motivating the new task of generating floods, a novel architecture for the Masker, a data set of images of floods from a virtual world (Section 3), and a procedure to evaluate such a model in the absence of ground-truth data (Section 4). We also compare our model to existing generative modeling frameworks and provide a thorough ablation study of the components of our model (Section 5). We conclude with future work and ways of improving and extending our tool, as well as our plans to deploy it on a website to raise climate change awareness (Sections 6 and 7).\n\n# 2 Related Work \n\nWhile the task of generating extreme street-level flooding in images retrieved from Google StreetView is novel, related work has been carried out in applying Deep Learning for flood segmentation (Sazara et al., 2019) and flood depth estimation (Kharazi \\& Behzadan, 2021). Generative modeling has also been used for transferring weather conditions on street scenes using both imagery (Li et al., 2021) and semantic maps (Wenzel et al., 2018). For the purposes of our task and given its unique constraints, we frame our approach in the context of image-to-image translation, involving conditional image synthesis and domain adaptation. We present relevant related work from these three areas in the sections below.\n\nImage-to-image translation (IIT) is a computer vision task whose goal is to map a given image from one domain to another (Liu et al., 2017; Huang et al., 2018; Yi et al., 2017). We can divide IIT approaches into two categories: those that carry out the translation on the entire input image, and those that utilize masks to guide the translation task. In the first category, several notable architectures have enabled the field to progress in new directions. While IIT initially relied on the existence of two aligned domains such as photographs and sketches of the same objects, CycleGAN (Zhu et al., 2017) overcame this constraint, learning two models and regularizing them via a cycle-consistency loss that allowed for the preservation of the symmetry of the transformation while allowing the domains themselves to remain unaligned. Further progress in IIT was made by architectures such as MUNIT (Huang et al., 2018), which enabled the disentanglement of content and style, allowing multimodal translations, FUNIT (Liu et al., 2019a), which enabled few-shot IIT, and, more recently, the CUT model (Park et al., 2020), which enabled single image unpaired translation with the help of contrastive learning.\nA second category of IIT focuses the translation process on particular input image areas, typically by leveraging attention or segmentation masks. This can be useful in more targeted translation, for instance when only certain objects in the foreground of an input image are meant to be translated. This more closely resembles our case, as we aim to transform only one part of the image with flooding water. Examples in this category include Attention-Guided GANs (Tang et al., 2019), which learn attention masks during training and utilize"
    },
    {
      "markdown": "them at inference time to render more precise transformations, and InstaGAN (Mo et al., 2018) which uses instance-level semantic masks to guide the translation process.\n\nConditional image synthesis differs from IIT in that the input can be a label, text or a segmentation map, instead of another image (Mirza \\& Osindero, 2014; Frolov et al., 2021; Hong et al., 2018; Zhu et al., 2020). One approach from this category that is particularly relevant to our work is SPADE (SPatially-Adaptive (DE)normalization) (Park et al., 2019), a module that enables the transformation of a semantic layout—such as that of a street scene or landscape-into an image that semantically matches this layout. This approach also introduced the GauGAN generator, which leverages SPADE blocks to learn a spatially-adaptive transformation, enabling the synthesis of realistic images based on the input maps.\n\nDomain adaptation is a branch of transfer learning that aims at transferring knowledge from one domain to another using different data sources (Pan \\& Yang, 2010; Ganin \\& Lempitsky, 2015). This can be particularly useful in tasks where more (labeled) data is available from a simulated world than in the real world, like in our application. Domain adaptation techniques can then be used to bridge the distributional gap between real and simulated scenes, learning useful tasks such as semantic segmentation and depth prediction, which function both in real and simulated scenarios. Examples of domain adaptation approaches that adopt these techniques include: CYCADA (Hoffman et al., 2017), which leverages cycle-consistency constraints similar to those proposed by CycleGAN (Zhu et al., 2017) to improve domain adaptation, ADVENT (Vu et al., 2018), which uses Adversarial Entropy Minimization to achieve state-of-the-art performance in unsupervised domain adaptation for semantic segmentation, and Depth-aware Domain Adaptation (DADA) (Vu et al., 2019), which improves on ADVENT by leveraging dense depth maps.\n\n# 3 Creating Images of Floods \n\nOur task of generating flooding resembles that of unsupervised image-to-image translation. However, this framework poses two major problems: first, the translation needs to be restricted to the portion of the image that would contain water, so we cannot consider approaches which alter the image globally. Second, we are only concerned with adding water and not the reverse, which eliminates approaches that rely on the symmetry of the translation task. In order to undertake this task, we therefore developed a novel conditional image synthesis approach that consists of two models: a Masker that produces a binary mask of where water would plausibly go in the case of a flood, and a Painter that renders realistic water given a mask and an image. We provide an overview of this procedure in Fig. 2, and detail the individual components in this section.\n\n### 3.1 Data\n\nTo overcome the lack of available data, we combined real images and simulated data from a virtual world, binding the two sources using unsupervised domain adaptation.\n\n### 3.1.1 Real Data\n\nFirst-person images of floods are scarce; moreover, even when such pictures exist, it is exceedingly rare to find the corresponding image of the same location before the flooding. We therefore pursued several alternate approaches for gathering real flooded images, spanning from web-scraping to creating a website and a mobile app to collect crowd-sourced images. We complemented flooded images with images of 'before' floods, i.e. of normal streets and houses. We aimed to cover a broad scope of geographical regions and types of scenery: urban, suburban and rural, with an emphasis on images from the Cityscapes (Cordts et al., 2016) and Mapillary (Neuhold et al., 2017) data sets, which closely resemble the viewpoint of Google Street View. We collected a total of 6740 images: 5540 non-flooded scenes to train the Masker, and 1200 flooded images to train the Painter.\n\n### 3.1.2 Simulated Data\n\nBesides the lack of paired data, another limitation of real-world images is that they do not contain scene geometry annotations and semantic segmentation labels, which we want to leverage during model training. To solve both of these problems, we created a $1.5 \\mathrm{~km}^{2}$ virtual world using the Unity3D game engine. To be as realistic as possible, it contains urban, suburban and rural areas, which we flooded with approximately 1 m of water to gather 'before' and 'after' pairs (see Fig. 10 in the supplementary materials, for example images)."
    },
    {
      "markdown": "![img-1.jpeg](img-1.jpeg)\n\nFigure 2: The ClimateGAN generation process: first, the input $\\boldsymbol{x}$ goes through the shared encoder $E$. Three decoders use the resulting representation $\\boldsymbol{z}: D$ predicts a depth map $\\boldsymbol{d}, S$ produces a depth-informed segmentation map $\\boldsymbol{s}$, and lastly $M$ outputs a binary flood mask, taking $\\boldsymbol{z}$ as input and sequentially denormalizing it with SPADE blocks (Park et al., 2019), conditionally on $\\boldsymbol{d}, \\boldsymbol{s}$ and $\\boldsymbol{x}$. Finally, the SPADE-based Painter $P$ generates an image $\\hat{\\boldsymbol{y}}$ of a flood, conditioned on the input image $\\boldsymbol{x}$ and the binary predicted mask $\\boldsymbol{m}$. Note that the Masker and the Painter are trained independently and only combined at test-time.\n\nFor each pair of images obtained, we also captured the corresponding depth map and semantic segmentation layout of the scene. In the end, we gathered approximately 20000 images from 2000 different viewpoints in the simulated world, which we used to train the Masker. We make this data set publicly available ${ }^{2}$ to enable further research.\n\n# 3.2 Masker \n\nWhile we have access to ground-truth supervision in the simulated domain, we need to transfer the Masker's knowledge to real images to perform our task. We therefore adapted the ADVENT methodology (Vu et al., 2018) to train the Masker, providing additional depth information to improve its predictions as proposed by the authors of DADA (Vu et al., 2019). Moreover, we introduced a segmentation decoder to inform the Masker of the semantics of the input image, since we hypothesized that adding this information to the flooding process would help the Masker produce more sensible flood masks. To that end, we structured the Masker as a multi-headed network with a shared encoder and three decoders performing depth prediction, semantic segmentation and masking. We trained these decoders simultaneously in a multi-task way by minimizing the sum of their losses. We provide further details on the losses used by the Masker in subsequent sections, as well as in Appendix B.\n\nIn the following section, subscripts $s$ and $r$ respectively identify the simulated and real domains; we use $i \\in\\{r, s\\}$ to refer to an arbitrary domain. $E$ is an encoder network while $D, S$ and $M$, are the depth, segmentation and flood mask decoders, respectively, as per Fig. 2.\n\nTo guide decoders in the early training stages, we used pseudo labels as supervision targets. In other words, we considered predictions of ad hoc pretrained models as ground truth. However, because those labels are noisy, we limited that procedure to the beginning of training.\n\nDepth decoder We consider depth in the disparity space, and predict the normalized inverse depth $\\boldsymbol{d}_{i}=D\\left(E\\left(\\boldsymbol{x}_{i}\\right)\\right)$ from an input image $\\boldsymbol{x}_{i}$. We used the scale-invariant loss from MiDaS (Lasinger et al., 2019), which is composed of a scale and shift-invariant MSE loss term $\\mathcal{L}_{\\text {SSIMSE }}$ and a gradient matching term $\\mathcal{L}_{G M}$. We used the following targets to compute this loss: for simulated input images, we leveraged available ground-truth depth maps, and for real input images, we used pseudo labels inferred from the MiDaS v2.1 model (Lasinger et al., 2019). The complete depth loss is:\n\n$$\n\\mathcal{L}_{D e p t h}=\\lambda_{1} \\mathcal{L}_{S S I M S E}+\\lambda_{2} \\mathcal{L}_{G M}\n$$\n\nSegmentation decoder The segmentation decoder $S$ is implemented such that $S \\circ E$ corresponds to the DeepLabv3+ architecture (Chen et al., 2018). It is trained as described in (Vu et al., 2019), leveraging depth\n\n[^0]\n[^0]:    ${ }^{2}$ https://github.com/cc-ai/mila-simulated-floods"
    },
    {
      "markdown": "information available in the simulated world to improve segmentation predictions by giving more attention to closer objects, producing $\\boldsymbol{s}_{i}=S\\left(E\\left(\\boldsymbol{x}_{i}\\right), \\boldsymbol{d}_{i}\\right)$. Two fusion mechanisms encourage this: feature fusion, which multiplies element-wise the latent vector $\\boldsymbol{z}_{i}=E\\left(\\boldsymbol{x}_{i}\\right)$ by a depth vector obtained from the depth decoder, and $D A D A$ fusion, which multiplies the self-information map $I\\left(\\boldsymbol{s}_{i}\\right)$ element-wise with the depth predictions $\\boldsymbol{d}_{i}$ to obtain the depth-aware self-information map $\\hat{I}\\left(\\boldsymbol{s}_{i}\\right)=I\\left(\\boldsymbol{s}_{i}\\right) \\odot \\boldsymbol{d}_{i}$. In this context, the self-information map of a probability distribution $\\boldsymbol{q}_{i}$ is defined (Vu et al., 2018) as $I\\left(\\boldsymbol{q}_{i}\\right)^{(h, w)}=-\\boldsymbol{q}_{i}^{(h, w)} \\cdot \\log \\boldsymbol{q}_{i}^{(h, w)}$, where $\\boldsymbol{q}_{i}^{(h, w)}$ is a probability distribution over channels (i.e. semantic classes for $\\boldsymbol{s}_{i}$ ) at a given location $(h, w)$.\nThe training procedure of the segmentation decoder starts with the cross-entropy loss $\\mathcal{L}_{C E}\\left(\\boldsymbol{y}_{i}, \\boldsymbol{s}_{i}\\right)$ between predictions $\\boldsymbol{s}_{i}$ and labels $\\boldsymbol{y}_{i}$. In the simulated domain, these labels correspond to the ground truth. In contrast, in the real domain, we used pseudo labels inferred from HRNet (Tao et al., 2020), a model that has achieved state-of-the art results in semantic segmentation on the Cityscapes data set (Cordts et al., 2016). A key difference between the real and simulated domains is the model's confidence in its predictions. Therefore, to encourage real domain predictions to be confident and reduce the gap with simulated predictions, an entropy minimization (EM) term $\\mathcal{L}_{E M}\\left(\\boldsymbol{s}_{r}\\right)$ similar to Eq. (6) is added.\nFinally, we shrank the domain gap between the distributions of real and simulated self-information maps using adversarial training. A discriminator $Q^{S}$ is introduced to distinguish between the two depth-aware maps and is trained using a WGAN objective (Arjovsky et al., 2017): $\\mathcal{L}_{W G A N}\\left(Q^{S}, \\hat{I}\\left(\\boldsymbol{s}_{r}\\right), \\hat{I}\\left(\\boldsymbol{s}_{s}\\right)\\right)$.\nThe overall loss of the segmentation decoder is therefore:\n\n$$\n\\mathcal{L}_{S e g}=\\lambda_{3} \\mathcal{L}_{C E}+\\lambda_{4} \\mathcal{L}_{E M}+\\lambda_{5} \\mathcal{L}_{W G A N}\n$$\n\nFlood mask decoder This decoder is structured to be conditioned not only on the input image, but also on predictions $\\boldsymbol{d}_{i}$ and $\\boldsymbol{s}_{i}$ from other decoders. To implement this dependence, we used SPADE (Park et al., 2019) conditional blocks. The idea behind SPADE is to create residual blocks where the input is first normalized and then denormalized in a spatially relevant way by small convolutional networks $\\gamma$ and $\\beta$, functions of the conditioning variables $\\boldsymbol{U}$. Given an activation $\\boldsymbol{a}_{n, c, h, w}$ at a given layer, its mean per channel $\\mu_{c}$ and standard deviation per channel $\\sigma_{c}$, the output of the SPADE denormalization layer is:\n\n$$\n\\gamma_{c, h, w}(\\boldsymbol{U}) \\frac{\\boldsymbol{a}_{n, c, h, w}-\\mu_{c}}{\\sigma_{c}}+\\beta_{c, h, w}(\\boldsymbol{U})\n$$\n\nIn our case, for an input $\\boldsymbol{x}_{i}$, the conditioning variable is therefore $\\boldsymbol{U}_{i}=\\left[\\boldsymbol{x}_{i}, \\boldsymbol{d}_{i}, \\boldsymbol{s}_{i}\\right]$, where we concatenate the tensors along the channel axis. The mask $\\boldsymbol{m}_{i}=M\\left(\\boldsymbol{z}_{i}, \\boldsymbol{U}_{i}\\right)$ and its self-information map $I\\left(\\boldsymbol{m}_{i}\\right)$ are computed from the latent representation $\\boldsymbol{z}_{i}=E\\left(\\boldsymbol{x}_{i}\\right)$. We also included a total variation (TV) loss on the mask $\\boldsymbol{m}_{i}$ for both domains in order to encourage the predictions to be smooth, ensuring that neighboring pixels have similar values (Johnson et al., 2016)-note that $\\Delta$ here means spatial difference of the image mesh.\n\n$$\n\\mathcal{L}_{T V}\\left(\\boldsymbol{m}_{i}\\right)=\\mathbb{E}_{n, h, w}\\left[\\left(\\Delta_{h} \\boldsymbol{m}_{i}\\right)^{2}+\\left(\\Delta_{w} \\boldsymbol{m}_{i}\\right)^{2}\\right]\n$$\n\nIn the simulated domain, we used a standard binary cross-entropy loss $\\mathcal{L}_{B C E}\\left(\\boldsymbol{y}_{m_{s}}, \\boldsymbol{m}_{s}\\right)$ with the groundtruth mask $\\boldsymbol{y}_{m_{s}}$. In the real domain, where we have no ground truth, we encouraged the predicted flood mask $\\boldsymbol{m}_{r}$ to at least encompass the ground by introducing a ground intersection (GI) loss, penalizing masks that assign a low probability to locations where a pre-trained model detected ground. The formulation of this loss leverages pseudo labels $\\boldsymbol{g}_{r}$ from HRNet (Tao et al., 2020) for the ground mask in $\\boldsymbol{x}_{r}$ :\n\n$$\n\\mathcal{L}_{G I}\\left(\\boldsymbol{g}_{r}, \\boldsymbol{m}_{r}\\right)=\\mathbb{E}_{n, h, w}\\left[\\mathbb{1}_{\\left(\\boldsymbol{g}_{r}-\\boldsymbol{m}_{r}\\right)>0.5}\\right]\n$$\n\nAs per the ADVENT approach, we also added an entropy minimization loss to increase the mask decoder's confidence in its real domain predictions:\n\n$$\n\\mathcal{L}_{E M}\\left(\\boldsymbol{m}_{r}\\right)=\\mathbb{E}_{n, c, h, w}\\left[-\\boldsymbol{m}_{r} \\log \\boldsymbol{m}_{r}\\right]\n$$\n\nLastly, similarly to the segmentation decoder, we adversarially trained the flood mask decoder with the loss $\\mathcal{L}_{W G A N}\\left(Q^{M}, I\\left(\\boldsymbol{m}_{r}\\right), I\\left(\\boldsymbol{m}_{s}\\right)\\right)$ to attempt to produce self-information maps $I\\left(\\boldsymbol{m}_{i}\\right)$ indistinguishable by a discriminator $Q^{M}$. All in all, $M$ 's total loss is a weighted sum of all of the above losses:\n\n$$\n\\mathcal{L}_{M a s k}=\\lambda_{6} \\mathcal{L}_{T V}+\\lambda_{7} \\mathcal{L}_{G I}+\\lambda_{8} \\mathcal{L}_{B C E}+\\lambda_{9} \\mathcal{L}_{E M}+\\lambda_{10} \\mathcal{L}_{W G A N}\n$$\n\nThe Masker's final loss sums the losses of the three decoders: $\\mathcal{L}_{\\text {Masker }}=\\mathcal{L}_{\\text {Depth }}+\\mathcal{L}_{\\text {Seg }}+\\mathcal{L}_{\\text {Mask }}$."
    },
    {
      "markdown": "![img-2.jpeg](img-2.jpeg)\n\nFigure 3: Example inferences of ClimateGAN, along with intermediate outputs. The first row shows how the Masker is able to capture complex perspectives and how the Painter is able to realistically contextualize the water with the appropriate sky and building reflections. On the second row, we can see that close-ups with distorted objects do not prevent the Masker from appropriately contouring objects. Finally, the last row illustrates how, in unusual scenes, the Masker may imperfectly capture the exact geometry of the scene but the final rendering by the Painter produces an acceptable image of a flood. More inferences including failure cases are shown in Fig. 18.\n\n# 3.3 Painter \n\nGiven an input image and a binary mask, the goal of the Painter is to generate water in the masked area while accounting for context in the input image. This is important for the realism of a flooded image because water typically reflects the sky and surrounding objects, and is colored conditionally on the environment. We built our Painter model based on the GauGAN (Park et al., 2019) architecture, using SPADE conditioning blocks. As we explained in the previous section, these blocks learn transformations that vary both spatially and conditionally on the input, allowing the model to better propagate spatial semantic information through the generation pipeline than if it were only available at the input of the generator. We adapted GauGAN to fit our task: rather than conditioning on a semantic map, we conditioned on a masked image. We trained the Painter on the 1200 real flooded images and pseudo labels of water segmented by a trained DeepLabv3+ model (Chen et al., 2017), whereas at inference time, the mask $\\boldsymbol{m}$ for an input image $\\boldsymbol{x}$ is generated by the Masker. Finally, we copied the context (non-masked area) back onto the generated image, to ensure that the other objects in the image (e.g. buildings and sky) remain intact. Thus, the output $\\hat{\\boldsymbol{y}}$ of the Painter $P$ is:\n\n$$\n\\hat{\\boldsymbol{y}}=P(\\epsilon,(1-\\boldsymbol{m}) \\odot \\boldsymbol{x}) \\odot \\boldsymbol{m}+\\boldsymbol{x} \\odot(1-\\boldsymbol{m})\n$$\n\nwhere $\\epsilon \\sim N(\\mathbf{0}, \\mathbf{I})$. The Painter generator is a sequence of 7 SPADE-denormalized residual blocks followed by bilinear upsampling operations with a scale factor of 2 . At each level, an interpolation of the masked input image with the appropriate size is given as conditioning variables $\\boldsymbol{U}$ to the corresponding SPADE block. We used a multi-resolution PatchGAN discriminator, as was done in the original GauGAN implementation (Isola et al., 2017). According to this paper, a perceptual VGG loss (Ledig et al., 2016) and a discriminator featurematching loss (Salimans et al., 2016) are essential for good performance. Since these last two losses rely on paired examples, we trained the Painter separately from the Masker using images of floods.\n\n## 4 Evaluation Method\n\nThe quality and photo-realism of our model's output depend on both the accuracy of the Masker in determining a realistic area for the flood and the ability of the Painter to infill the input mask with water with a realistic"
    },
    {
      "markdown": "texture matching the surrounding scene. In order to best understand the contribution of each component, we evaluated the Masker and the final flooded images separately.\n\n# 4.1 Masker Evaluation \n\nThe lack of ground-truth data is not only an obstacle for training, but also for obtaining reliable evaluation metrics to assess the performance of our model. Therefore, in order to obtain metrics for evaluating the quality of the Masker and for comparing the individual contribution of the proposed components to its architecture, we manually labeled a test set of 180 images retrieved from Google Street View. We collected images of diverse geographical provenance, level of urban development, and composition (crowds, vehicles, vegetation types, etc.). We manually annotated every pixel of each image with one of three classes: (1) cannot-be-flooded-pixels higher than 1.5 m above the ground level; (2) must-be-flooded-anything with height less than 0.5 m , and (3) may-be-flooded-all remaining pixels. We provide further details in Appendix C.1.\n\n### 4.1.1 Metrics\n\nWe propose the following metrics to evaluate the quality of masks (more details in Appendix C.2):\nError rate The perceptual quality of the final images is highly impacted by the area of the image with errors in the predicted mask. We found that both large gaps in must-be-flooded areas (i.e. false negatives, $F N$ ), and large sections of predicted masks in cannot-be-flooded areas (i.e. false positives, $F P$ ) account for low perceptual quality. Thus, we propose the error rate to account for the amount of prediction errors in relation to the image size:\n\n$$\n\\text { error }=(F N+F P) /(H \\times W)\n$$\n\nF05 Score So as to consider the precision and recall of the mask predictions, we also evaluated the $F_{\\beta=0.5}$ (F05) score, which lends more weight to precision than to recall (van Rijsbergen, 1979).\n\nEdge coherence It is also important to take into account the shape similarity between the predicted mask and the ground-truth label, particularly in the uncertainty region defined by the may-be-flooded class. We capture this property by computing the standard deviation $\\sigma$ of the minimum Euclidean distance $d(\\cdot, \\cdot)$ between every pixel in the boundary of the predicted mask $\\hat{B}$ and the boundary of the must-be-flooded area $B$, which are computed by filtering the masks with the Sobel edge detector:\n\n$$\n\\text { edge coherence }=1-\\sigma\\left(\\min _{j}\\left[d\\left(\\hat{B}_{i}, B_{j}\\right) / H\\right]\\right)\n$$\n\n### 4.1.2 Ablation Study\n\nIn order to assess the contribution of each of the components described in Section 3.2 to the overall Masker performance, we performed an ablation study by training 18 models, each with a different combination of techniques. As illustrated in Table 1, we analyzed the effect of including the following components in the Masker architecture: training with pseudo labels, $D, S$, DADA for $S$ (Vu et al., 2019), DADA for $M$, and the SPADE (Park et al., 2019) architecture for $M$. When not using SPADE, $M$ is based on residual blocks followed by convolutional and upsampling layers, taking $z$ as input, and we can also inform it with depth according to the DADA procedure just like we do for $S$.\n\nWe compared these variants of the Masker module with two baselines: ground segmentation from HRNet (Tao et al., 2020) instead of the flood mask (G) and InstaGAN (I) (Mo et al., 2018). For each model variant in the ablation study (columns) we computed mask predictions for each of the 180 test set images and the values of the three metrics: error, F05 and edge coherence. In order to determine whether each technique (row) improves the performance, we computed the difference between the metrics on each image for every pair of models where the only difference is the inclusion or exclusion of this technique. Finally, we carried out statistical inference through the percentile bootstrap method (Efron, 1992; Rousselet et al., 2019) to obtain robust estimates of the performance differences and confidence intervals. In particular, we obtained one million bootstrap samples (sampling with replacement) for each metric to get a distribution of the bootstrapped $20 \\%$ trimmed mean, which is a robust measure of location (Wilcox, 2011). We then compared the distribution against the null hypothesis, which indicates no difference in performance (see Fig. 5). We considered a technique to be beneficial for the Masker if its inclusion reduced the error rate. In case of inconclusive error rate results, we considered an increase in the F05 score and finally in the edge coherence."
    },
    {
      "markdown": "![img-3.jpeg](img-3.jpeg)\n\nFigure 4: Example inferences of ClimateGAN and comparable approaches on three diverse street scenes from the test set. We can see that ClimateGAN is able to generate both realistic water texture and color, as well as a complex mask that surrounds objects such as cars and buildings. Comparable approaches are often too destructive, producing artifacts in the buildings and sky (e.g. $1^{\\text {st }}$ row of MUNIT) or not destructive enough, resembling more rain on the ground than high floods (e.g. $3^{\\text {rd }}$ row of CycleGAN and $2^{\\text {nd }}$ row of InstaGAN).\n\n# 4.2 Comparables \n\nWhile the nature of our task is specific to our project, we can nonetheless benchmark ClimateGAN against IIT models. In fact, in earlier iterations of our project, we leveraged the CycleGAN (Zhu et al., 2017) architecture in order to achieve initial results (Schmidt et al., 2019), before adopting a more structured approach. Therefore, to be as comprehensive in our benchmarking as possible, we trained the following five models on the same data as the ClimateGAN Painter and used the same test set for the comparison: CycleGAN, MUNIT (Huang et al., 2018), InstaGAN (Mo et al., 2018), InstaGAN using the mask to constrain the transformation to only the masked area (similarly to Eq. (8)), and the ClimateGAN Painter applied to ground segmentation masks predicted by HRNet (Tao et al., 2020). These models were trained using the original papers' configurations, with hyper-parameter tuning to be fair in our comparison. Some samples from the models can be seen in Fig. 4, and further results are provided in Appendix D, Fig. 16.\n\n## 5 Results\n\nThis section presents the model evaluation results, including the Masker ablation study and the comparison of the overall proposed model-Masker and Painter—against comparable approaches via human evaluation. Visual examples of the inferences of our model can be seen in Fig. 3.\n\n|  | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | G | I |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| Pseudo labels | ・ | ・ | ・ | ・ | ・ | ・ | ・ | ・ | ・ |  |  |  |  |  |  |  |  |  |  |  |\n| Depth $(D)$ |  | ・ |  | ・ | ・ | ・ | ・ | ・ | ・ |  | ・ |  | ・ | ・ | ・ | ・ | ・ | ・ | ・ |  |\n| Segmentation $(S)$ |  |  | ・ | ・ | ・ | ・ | ・ | ・ | ・ |  |  | ・ | ・ | ・ | ・ | ・ | ・ | ・ | ・ |  |\n| SPADE |  |  |  |  | ・ |  | ・ |  |  |  |  |  |  | ・ | ・ | ・ |  |  |  |  |\n| DADA $(S)$ |  |  |  |  |  | ・ | ・ |  | ・ |  |  |  |  |  | ・ | ・ |  | ・ |  |  |\n| DADA $(M)$ |  |  |  |  |  |  |  | ・ | ・ |  |  |  |  |  |  |  | ・ | ・ | ・ |  |\n\nTable 1: Summary of the ablation study of the Masker. Each numbered column corresponds to a trained model and black dots indicate which techniques (rows) were included in the model. The last two columns correspond to the baseline models: ground (G) segmentation from HRNet as flood mask and InstaGAN (I)."
    },
    {
      "markdown": "![img-4.jpeg](img-4.jpeg)\n\nFigure 5: Summary of the statistical inference tests of the ablation study. The shaded area indicates metric improvement. All techniques but DADA $(M)$ significantly improved the error rate, and some further improved the F05 score and edge coherence.\n\n# 5.1 Masker Evaluation \n\nThe main conclusion of our ablation study is that five of the six techniques proposed to improve the quality of the Masker positively contribute to the performance. In Fig. 5, we show the median differences and confidence intervals obtained through the bootstrap. The error rate improved-with $99 \\%$ confidence-in the models that included pseudo labels (for the first 10 training epochs), a depth and a segmentation head, SPADE and DADA for the segmentation head-recall that the contribution of each technique was tested separately. For some but not all techniques, the F05 score and edge coherence also improved significantly. In contrast, we found that both the error and the F05 score were worse when DADA for the Masker was included.\n![img-5.jpeg](img-5.jpeg)\n\nFigure 6: Evaluation metrics for a subset of the models studied in the ablation study and presented in Table 1-models trained without pseudo labels or with DADA for the Masker are excluded-as well as the two baselines for comparison. The solid symbols indicate the median of the distribution and the error lines the bootstrapped $99 \\%$ confidence intervals. The shaded area highlights the best model-7.\n\nIn Fig. 6, we show the Masker evaluation metrics for a subset of the models, 1-7, selected upon the conclusions of the ablation study, as well as the two baseline models. The main conclusion of the evaluation is that our proposed Masker largely outperforms the baselines for the three metrics, especially in terms of the error and the F05 score. Also, the metrics of the individual models support the findings of the ablation study, as the best performance-5, 6 and 7-is achieved by the models that include all or most techniques: pseudo labels, depth and segmentation heads, DADA for $S$ and SPADE. Surprisingly, model 2, which only includes pseudo labels and the depth head, also achieves high performance. In contrast, models 3 and 4, which include a segmentation head without DADA or SPADE, obtain worse performance. Therefore, it seems that the contribution of the segmentation head is clearly complemented by using DADA and SPADE. We include a complete analysis of the ablation study and Masker evaluation metrics in Appendix C. The final architecture we selected for the Masker, 7, includes: pseudo labels, $D, S$, DADA for $S$ and SPADE for $M$.\n\n### 5.2 Human Evaluation\n\nThe difficulty of evaluating the performance of generative models is a well-known challenge in the field (Theis et al., 2015; Zhou et al., 2019). Commonly used metrics for evaluating the quality of generated images, such as the Fréchet Inception Distance (FID) (Heusel et al., 2017), need a large number of samples to be statistically significant (Seitzer, 2020), which is not feasible in our low data regime. Therefore, in order to compare"
    },
    {
      "markdown": "ClimateGAN to related approaches, we carried out a survey with human evaluators using the Amazon Mechanical Turk platform. We presented evaluators with pairs of images generated based on the same input image-one created by ClimateGAN, the other by one of the five related works described in Fig. 4. We asked them to pick which image looks more like an actual flood, collecting 3 evaluations per pair. We present the results of this procedure in Fig. 7, where we can observe that ClimateGAN is consistently preferred by a significant margin to all other models. Interestingly, the single fact of constraining transformations to the masked area improves the performance of InstaGAN, and we hypothesize that this operation contributes to the performance of ClimateGAN. In addition, we can see that the second-best model is the Painted Ground, meaning that even with an off-the-shelf ground segmentation model to produce masks, the Painter is good enough on its own to produce realistic images, but that it performs best when paired with the Masker.\n![img-6.jpeg](img-6.jpeg)\n\nFigure 7: Results of the human evaluation: the blue bars indicate the rate of selection of ClimateGAN over each alternative. The error lines indicate $99 \\%$ confidence intervals.\n\n# 6 Limitations and Improvements \n\nAn intuitive extension of our model would be to render floods at any chosen height. Despite the appeal of this approach, its development is compromised by data challenges. To our knowledge, there is no data set of metric height maps of street scenes, which would be necessary for converting relative depth and height maps into absolute ones. Moreover, simulated worlds-including our own-that have metric height maps do not cover a large enough range of scenes to train models that would generalize well on worldwide Google Street View images. Another promising direction for improvement would be to achieve multi-level flooding, controlling water level represented by a mask, which faces the same challenges.\nWe also further explored the integration of multi-task learning in our approach, attempting to apply several of the methods mentioned in (Crawshaw, 2020) to weigh the various Masker losses, including dynamic weight average (Liu et al., 2019b) and weighting losses by uncertainty (Kendall et al., 2018). We found that our manual tuning of constant weights performs better than the aforementioned methods. In future work, we aim to try more advanced techniques like gradient modulation methods (Yu et al., 2020; Maninis et al., 2019) to explore the further integration of multi-task learning to our approach.\n\n## 7 Conclusion\n\nIn the present article, we have proposed to leverage advances in modern generative modeling techniques to create visualizations of floods, in order to raise awareness about climate change and its dire consequences. In fact, the approach described is part of a wider initiative aiming to create an interactive web-based tool that allows users to input the address of their choice and visualize the consequences of extreme climate events like floods, while learning about the causes and impacts of climate change.\nOur contributions comprise a data set of images and labels from a 3D virtual world, an architecture we call ClimateGAN, and a thorough evaluation method to measure the performance of our system. We found our proposed Masker-Painter dichotomy to be superior to existing comparable techniques of conditional generations in two ways: first, we empirically showed that we are able to produce more realistic flood masks by informing the flood mask decoder with geometrical information from depth predictions and semantic information from segmentation maps, and by training the three decoders together. Second, we established, using human evaluations, that our Painter alone is able to create realistic water textures when provided ground masks, and that its performance increases even further when provided with the Masker's predictions. Overall, this allows ClimateGAN to produce compelling and robust visualizations of floods on a diverse set of firstperson views of urban, suburban and rural scenes."
    },
    {
      "markdown": "# References \n\nAhn, S. J. G., Bailenson, J. N., and Park, D. Short-and long-term effects of embodied experiences in immersive virtual environments on environmental locus of control and behavior. Computers in Human Behavior, 2014.\n\nArjovsky, M., Chintala, S., and Bottou, L. Wasserstein GAN. In ICLR, 2017.\nBerenguer, J. The effect of empathy in proenvironmental attitudes and behaviors. Environment and behavior, 2007.\n\nChapman, D. A., Corner, A., Webster, R., and Markowitz, E. M. Climate visuals: A mixed methods investigation of public perceptions of climate images in three countries. Global Environmental Change, 2016.\n\nChen, L.-C., Papandreou, G., Kokkinos, I., Murphy, K., and Yuille, A. L. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017.\n\nChen, L.-C., Zhu, Y., Papandreou, G., Schroff, F., and Adam, H. Encoder-decoder with atrous separable convolution for semantic image segmentation. In ECCV, 2018.\n\nCordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Roth, S., and Schiele, B. The Cityscapes Dataset for Semantic Urban Scene Understanding. In CVPR, 2016.\n\nCrawshaw, M. Multi-task learning with deep neural networks: A survey, 2020.\nDottori, F., Salamon, P., Bianchi, A., Alfieri, L., Hirpa, F. A., and Feyen, L. Development and evaluation of a framework for global flood hazard mapping. Advances in Water Resources, 2016.\n\nEfron, B. Bootstrap methods: another look at the jackknife. In Breakthroughs in Statistics, pp. 569-593. Springer, 1992.\n\nFrolov, S., Hinz, T., Raue, F., Hees, J., and Dengel, A. Adversarial text-to-image synthesis: A review. arXiv preprint arXiv:2101.09983, 2021.\n\nGanin, Y. and Lempitsky, V. Unsupervised Domain Adaptation by Backpropagation. ICML, 2015.\nGlaas, E., Ballantyne, A. G., Neset, T.-S., and Linnér, B.-O. Visualization for supporting individual climate change adaptation planning: Assessment of a web-based tool. Landscape and Urban Planning, 2017.\n\nHauer, M. E. Migration induced by sea-level rise could reshape the us population landscape. Nature Climate Change, 2017.\n\nHerring, J., VanDyke, M. S., Cummins, R. G., and Melton, F. Communicating local climate risks online through an interactive data visualization. Environmental Communication, 2017.\n\nHeusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Guyon, I., von Luxburg, U., Bengio, S., Wallach, H. M., Fergus, R., Vishwanathan, S. V. N., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, 2017.\n\nHoegh-Guldberg, O. et al. Impacts of 1.5 c global warming on natural and human systems. Global warming of 1.5 C. An IPCC Special Report, 2018.\n\nHoffman, J., Tzeng, E., Park, T., Zhu, J.-Y., Isola, P., Saenko, K., Efros, A. A., and Darrell, T. CyCADA: Cycle-Consistent Adversarial Domain Adaptation. arXiv:1711.03213 [cs], December 2017. arXiv: 1711.03213 .\n\nHong, S., Yang, D., Choi, J., and Lee, H. Inferring semantic layout for hierarchical text-to-image synthesis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.\n\nHuang, X., Liu, M.-Y., Belongie, S., and Kautz, J. Multimodal Unsupervised Image-to-Image Translation. arXiv:1804.04732 [cs, stat], August 2018. arXiv: 1804.04732."
    },
    {
      "markdown": "Isola, P., Zhu, J.-Y., Zhou, T., and Efros, A. A. Image-to-image translation with conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2017.\n\nJohnson, J., Alahi, A., and Fei-Fei, L. Perceptual losses for real-time style transfer and super-resolution. arXiv preprint arXiv:1603.08155, 2016.\n\nKendall, A., Gal, Y., and Cipolla, R. Multi-task learning using uncertainty to weigh losses for scene geometry and semantics. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018. 2018.\n\nKharazi, B. A. and Behzadan, A. H. Flood depth mapping in street photos with image processing and deep neural networks. Computers, Environment and Urban Systems, 2021.\n\nLacoste, A., Luccioni, A., Schmidt, V., and Dandres, T. Quantifying the Carbon Emissions of Machine Learning. arXiv:1910.09700 [cs], November 2019. arXiv: 1910.09700.\n\nLasinger, K., Ranftl, R., Schindler, K., and Koltun, V. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. CoRR, 2019.\n\nLedig, C. et al. Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network. arXiv:1609.04802 [cs, stat], September 2016. arXiv: 1609.04802 version: 5.\n\nLeviston, Z., Price, J., and Bishop, B. Imagining climate change: The role of implicit associations and affective psychological distancing in climate change responses. European Journal of Social Psychology, 2014.\n\nLi, X., Kou, K., and Zhao, B. Weather gan: Multi-domain weather translation using generative adversarial networks. arXiv preprint arXiv:2103.05422, 2021.\n\nLiu, M., Breuel, T., and Kautz, J. Unsupervised image-to-image translation networks. In Guyon, I., von Luxburg, U., Bengio, S., Wallach, H. M., Fergus, R., Vishwanathan, S. V. N., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, 2017.\n\nLiu, M., Huang, X., Mallya, A., Karras, T., Aila, T., Lehtinen, J., and Kautz, J. Few-shot unsupervised image-to-image translation. In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019. 2019a.\n\nLiu, S., Johns, E., and Davison, A. J. End-to-end multi-task learning with attention. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019. 2019b.\n\nManinis, K., Radosavovic, I., and Kokkinos, I. Attentive single-tasking of multiple tasks. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019. 2019.\n\nMirza, M. and Osindero, S. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784, 2014.\nMo, S., Cho, M., and Shin, J. InstaGAN: Instance-aware Image-to-Image Translation. arXiv:1812.10889 [cs, stat], December 2018. arXiv: 1812.10889.\n\nNeset, T.-S., Glaas, E., Ballantyne, A. G., Linnér, B.-O., Opach, T., Navarra, C., Johansson, J., Bohman, A., Rød, J. K., and Goodsite, M. Climate change effects at your doorstep: Geographic visualization to support nordic homeowners in adapting to climate change. Applied Geography, 2016.\n\nNeuhold, G., Ollmann, T., Bulò, S. R., and Kontschieder, P. The mapillary vistas dataset for semantic understanding of street scenes. In IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017. 2017.\n\nNeumann, B., Vafeidis, A. T., Zimmermann, J., and Nicholls, R. J. Future coastal population growth and exposure to sea-level rise and coastal flooding-a global assessment. PloS one, 2015.\n\nPan, S. J. and Yang, Q. A Survey on Transfer Learning. IEEE Transactions on Knowledge and Data Engineering, 2010.\n\nPark, T., Liu, M.-Y., Wang, T.-C., and Zhu, J.-Y. Semantic Image Synthesis with Spatially-Adaptive Normalization. arXiv:1903.07291 [cs], November 2019. arXiv: 1903.07291."
    },
    {
      "markdown": "Park, T., Efros, A. A., Zhang, R., and Zhu, J.-Y. Contrastive learning for conditional image synthesis. In ECCV, 2020.\n\nRousselet, G., Pernet, C., and Wilcox, R. R. A practical introduction to the bootstrap: a versatile method to make inferences by using data-driven simulations. PsyArXiv preprint PsyArXiv:h8ft7, 2019.\n\nSalimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., and Chen, X. Improved techniques for training gans. arXiv preprint arXiv:1606.03498, 2016.\n\nSazara, C., Cetin, M., and Iftekharuddin, K. M. Detecting floodwater on roadways from image data with handcrafted features and deep transfer learning. In 2019 IEEE Intelligent Transportation Systems Conference (ITSC). 2019.\n\nSchmidt, V., Luccioni, A., Mukkavilli, S. K., Balasooriya, N., Sankaran, K., Chayes, J., and Bengio, Y. Visualizing the consequences of climate change using cycle-consistent adversarial networks. arXiv preprint arXiv:1905.03709, 2019.\n\nSchwartz, R., Dodge, J., Smith, N. A., and Etzioni, O. Green AI. arXiv preprint arXiv:1907.10597, 2019.\nSeitzer, M. pytorch-fid: FID Score for PyTorch. https://github.com/mseitzer/pytorch-fid, 2020. Version 0.1.1.\n\nSevillano, V., Aragonés, J. I., and Schultz, P. W. Perspective taking, environmental concern, and the moderating role of dispositional empathy. Environment and behavior, 2007.\n\nSheppard, S. R. Visualizing climate change: a guide to visual communication of climate change and developing local solutions. Routledge, 2012.\n\nSpence, A., Poortinga, W., and Pidgeon, N. The psychological distance of climate change. Risk Analysis: An International Journal, 2012.\n\nStrubell, E., Ganesh, A., and McCallum, A. Energy and policy considerations for deep learning in nlp. arXiv preprint arXiv:1906.02243, 2019.\n\nTang, H., Xu, D., Sebe, N., and Yan, Y. Attention-guided generative adversarial networks for unsupervised image-to-image translation. In 2019 International Joint Conference on Neural Networks (IJCNN). 2019.\n\nTao, A., Sapra, K., and Catanzaro, B. Hierarchical multi-scale attention for semantic segmentation. arXiv preprint arXiv:2005.10821, 2020.\n\nTheis, L., Oord, A. v. d., and Bethge, M. A note on the evaluation of generative models. arXiv preprint arXiv:1511.01844, 2015.\nvan Rijsbergen, C. Information retrieval, 2nd edbutterworths, 1979.\nVousdoukas, M. I., Mentaschi, L., Voukouvalas, E., Verlaan, M., Jevrejeva, S., Jackson, L. P., and Feyen, L. Global probabilistic projections of extreme sea levels show intensification of coastal flood hazard. Nature Communications, December 2018.\n\nVu, T.-H., Jain, H., Bucher, M., Cord, M., and Pérez, P. ADVENT: Adversarial Entropy Minimization for Domain Adaptation in Semantic Segmentation. arXiv:1811.12833 [cs], November 2018. arXiv: 1811.12833.\n\nVu, T.-H., Jain, H., Bucher, M., Cord, M., and Pérez, P. DADA: Depth-aware Domain Adaptation in Semantic Segmentation. arXiv:1904.01886 [cs], August 2019. arXiv: 1904.01886.\n\nWatts, N. et al. The 2019 report of the lancet countdown on health and climate change: ensuring that the health of a child born today is not defined by a changing climate. The Lancet, 2019.\n\nWenzel, P., Khan, Q., Cremers, D., and Leal-Taixé, L. Modular vehicle control for transferring semantic information between weather conditions using gans. In Conference on Robot Learning. 2018.\n\nWilcox, R. R. Introduction to robust estimation and hypothesis testing. Academic press, 2011.\nYi, Z., Zhang, H. R., Tan, P., and Gong, M. Dualgan: Unsupervised dual learning for image-to-image translation. In IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017. 2017."
    },
    {
      "markdown": "Yu, T., Kumar, S., Gupta, A., Levine, S., Hausman, K., and Finn, C. Gradient surgery for multi-task learning. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M. F., and Lin, H. (eds.), Advances in Neural Information Processing Systems. 2020.\n\nZhou, S., Gordon, M. L., Krishna, R., Narcomey, A., Fei-Fei, L., and Bernstein, M. S. Hype: A benchmark for human eye perceptual evaluation of generative models. arXiv preprint arXiv:1904.01121, 2019.\n\nZhu, J.-Y., Park, T., Isola, P., and Efros, A. A. Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks. arXiv:1703.10593 [cs], March 2017. arXiv: 1703.10593.\n\nZhu, P., Abdal, R., Qin, Y., and Wonka, P. Sean: Image synthesis with semantic region-adaptive normalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020."
    },
    {
      "markdown": "# A Simulated data \n\nIn this section, we expand on Section 3.1.2 and provide details about our simulated world and the images and labels we can obtain from it.\nWe created a $1.5 \\mathrm{~km}^{2}$ virtual world using the Unity3D game engine containing urban, suburban and rural areas.\n\nThe urban environment contains skyscrapers, large buildings, and roads, as well as objects such as traffic items and vehicles. Fig. 8 shows a bird's eye view of the urban area of our virtual environment. The rural environment consists of a landscape of grassy hills, forests, and mountains, with sparse houses and other buildings such as a church, and no roads. The rural and urban areas make up for $1 \\mathrm{~km}^{2}$ of our virtual world.\n![img-7.jpeg](img-7.jpeg)\n\nFigure 8: Bird's eye view of the urban area (city) and rural area (outskirts of the city) of our simulated world\n\nThe suburban environment (Figure 9) is a residential area of $0.5 \\mathrm{~km}^{2}$ with many individual houses with front yards.\n![img-8.jpeg](img-8.jpeg)\n\nFigure 9: Bird's eye views of the suburban area of our simulated world\n\nTo gather the simulated dataset, we captured 'before' and 'after' flood pairs from 2000 viewpoints with the following modalities:\n\n- 'before' : non-flooded RGB image, depth map, segmentation map\n- 'after' : flooded RGB image, binary mask of the flooded area, segmentation map\n\nThe camera was placed about 1.5 m above ground, and has a field of view of $120^{\\circ}$, and the resolution of the images is $1200 \\times 900$. At each viewpoint, we took 10 pictures, by varying slightly the position of the camera in order to augment the dataset."
    },
    {
      "markdown": "Fig. 10 shows the different modalities captured at each viewpoint, and Fig. 11 shows samples of our simulated dataset in urban, suburban and rural areas.\n\nDepth The depth maps are provided as RGB images for the 'before' case, and the depth is recorded up to 1000 m away from the camera, with precision of 4 mm .\n\nSegmentation There are nine different classes of objects in the simulated world:\n\n- sky\n- ground: road, sidewalks, road markings, anything that is asphalt\n- building\n- traffic item: lampposts, traffic signs, poles\n- vegetation: small bushes, trees, hedges excludes grass, lawns\n- terrain: rocks, soil, lawns\n- car: cars and trucks\n- other: miscellaneous objects such as postboxes, trashcans, garbage bags, etc.\n- water: only present in the 'after' flooded images\n\nWhile people are not included in the simulated world, the segmentation model is able to learn this class from the real world due to the supervision signal given by the HRNet pseudo-labels.\n\nMask We also include binary masks of the flood (water segmentation) for the 'after' images. The masks are used to train the Masker with ground truth target flood masks in the simulated domain.\n![img-9.jpeg](img-9.jpeg)\n\nFigure 10: Sample data obtained at one spot for one camera position in our virtual world. The top row shows the modalities of the 'before' flood image: RGB image of the scene, depth map and segmentation map; and the bottom row shows those obtained in the 'after' configuration: RGB image, segmentation map and binary flood mask.\n\n# B Losses \n\nFor clarity, in Section 3.2 we did not detail the exact formulation of some of the losses since they are either straight-forward (as is the Cross Entropy) or direct applications of their definitions as per their original paper (e.g. the SSIMSE loss). We expand on those here.\n\n## B. 1 Depth Decoder\n\nIn this section, we detail the depth decoder loss presented in Section 3.2 and adapted from (Lasinger et al., 2019)."
    },
    {
      "markdown": "![img-10.jpeg](img-10.jpeg)\n\nFigure 11: Samples from our simulated dataset in urban (top row), rural (middle row), and suburban (bottom row) areas.\n\nLet $d$ a predicted disparity map and $d^{*}$ the corresponding ground truth. The aligned disparity maps with zero translation and unit scale are:\n\n$$\n\\hat{d}=\\frac{d-\\mathbf{t}(d)}{\\mathbf{s}(d)}, \\hat{d}^{*}=\\frac{d^{*}-\\mathbf{t}\\left(d^{*}\\right)}{\\mathbf{s}\\left(d^{*}\\right)}\n$$\n\nwhere $\\mathbf{t}$ and $\\mathbf{s}$ are defined as $\\mathbf{t}(d)=\\operatorname{median}(d)$ and $\\mathbf{s}(d)=\\frac{1}{N} \\sum_{n}|d-\\mathbf{t}(d)|$.\nThe depth decoder loss is composed of a scale-and-shift invariant MSE loss term :\n\n$$\n\\mathcal{L}_{S S I M S E}=\\frac{1}{2} \\mathbb{E}_{n, h, w}\\left[\\left(\\hat{d}^{(n, h, w)}-\\hat{d}^{*(n, h, w)}\\right)^{2}\\right]\n$$\n\nan a multi-scale gradient matching term to enforce smooth gradients and sharp discontinuities:\n\n$$\n\\mathcal{L}_{G M}=\\mathbb{E}_{n}\\left[\\sum_{k} \\sum_{h, w}\\left|\\nabla_{x} R_{k}^{(n, h, w)}\\right|+\\left|\\nabla_{y} R_{k}^{(n, h, w)}\\right|\\right]\n$$\n\nwhere $R^{(n, h, w)}=\\hat{d}^{(n, h, w)}-\\hat{d}^{*(n, h, w)}$ and $R_{k}$ corresponds to the difference of disparity (inverse depth) maps at scale $k$.\nFollowing (Lasinger et al., 2019), we consider 4 scale levels, downsampling by a factor two the image at each scale.\n\n# B. 2 Segmentation Decoder \n\nThis decoder computes the segmentation map $\\boldsymbol{s}_{i}$ given an input image $\\boldsymbol{x}_{i}, \\boldsymbol{s}_{i}$ being a 4D tensor of shape $N \\times C \\times H \\times W$. The number of channels $C$ corresponds to the number of classes, nine in our case: ground, building, traffic item, vegetation, terrain, car, sky, person and other.\nFirst, we detail the two fusion mechanisms used in the DADA approach (Vu et al., 2019):\n\n- Feature fusion: It is the element-wise multiplication between the latent vector and a depth vector of the same size. This depth vector is obtained by a $1 \\times 1$ convolutional layer applied to the depth head before the average pooling. The fused features are given as input to the segmentation decoder."
    },
    {
      "markdown": "- DADA fusion: Instead of giving the self-information map $I_{i}^{S}$ to the AdvEnt (Vu et al., 2018) discriminator $Q^{S}$, we give it the element-wise multiplication between $I_{i}^{S}$ and the depth predictions $\\boldsymbol{d}_{i}$. The obtained matrix is called the depth-aware map $\\hat{I}_{i}^{S}$.\n\nWe also detail all segmentation losses that are not defined in the main paper. First, the cross-entropy loss is defined as:\n\n$$\n\\mathcal{L}_{C E}\\left(\\boldsymbol{y}_{i}, \\boldsymbol{s}_{i}\\right)=-\\sum_{c} \\mathbb{E}_{n, h, w}\\left[\\boldsymbol{y}_{i}^{(n, c, h, w)} \\log \\boldsymbol{s}_{i}^{(n, c, h, w)}\\right]\n$$\n\nFurthermore, the entropy minimization loss, which is used as suggested by the authors of ADVENT(Vu et al., 2018), is computed according to the following equation:\n\n$$\n\\mathcal{L}_{E M}\\left(\\boldsymbol{s}_{r}\\right)=\\mathbb{E}_{n, c, h, w}\\left[-\\boldsymbol{s}_{r} \\log \\boldsymbol{s}_{r}\\right]\n$$\n\nFinally, we detail the WGAN loss that is computed using a discriminator $Q_{s}$ which outputs the probability of the depth-aware map coming from the real domain. If updating the decoder $S$, the loss tries to fool the discriminator:\n\n$$\n\\mathcal{L}_{G-G A N}\\left(Q^{S}, \\hat{I}\\left(\\boldsymbol{s}_{r}\\right), \\hat{I}\\left(\\boldsymbol{s}_{s}\\right)\\right)=-\\mathbb{E}_{n}\\left[Q^{S}\\left(\\hat{I}\\left(\\boldsymbol{s}_{r}\\right)\\right)\\right]\n$$\n\nIf updating D , the loss tries to improve the discriminator's prediction:\n\n$$\n\\mathcal{L}_{D-G A N}\\left(Q^{S}, \\hat{I}\\left(\\boldsymbol{s}_{r}\\right), \\hat{I}\\left(\\boldsymbol{s}_{s}\\right)\\right)=-\\mathbb{E}_{n}\\left[Q^{S}\\left(\\hat{I}\\left(\\boldsymbol{s}_{s}\\right)\\right)-Q^{S}\\left(\\hat{I}\\left(\\boldsymbol{s}_{r}\\right)\\right)\\right]\n$$\n\n# B. 3 Flood Mask Decoder \n\nSimilarly to the segmentation decoder, we detail in this section the WGAN loss that is computed using a discriminator $Q^{M}$ in order to train M . This discriminator outputs the probability of the self-information map $I\\left(\\boldsymbol{m}_{i}\\right)$ coming from the real domain. When updating the decoder $M$, the loss tries to fool the discriminator:\n\n$$\n\\mathcal{L}_{G-G A N}\\left(Q^{M}, I\\left(\\boldsymbol{m}_{r}\\right), I\\left(\\boldsymbol{m}_{s}\\right)\\right)=-\\mathbb{E}_{n}\\left[Q^{M}\\left(I\\left(\\boldsymbol{m}_{r}\\right)\\right)\\right]\n$$\n\nWhen updating $Q^{M}$, the loss tries to improve the discriminator's predictions:\n\n$$\n\\begin{aligned}\n\\mathcal{L}_{D-G A N} & \\left(Q^{M}, I\\left(\\boldsymbol{m}_{r}\\right), I\\left(\\boldsymbol{m}_{s}\\right)\\right)= \\\\\n& -\\mathbb{E}_{n}\\left[Q^{M}\\left(I\\left(\\boldsymbol{m}_{s}\\right)\\right)-Q^{M}\\left(I\\left(\\boldsymbol{m}_{r}\\right)\\right)\\right]\n\\end{aligned}\n$$\n\n## C Masker Evaluation\n\nIn Section 4.1, we introduced the method for evaluating the quality of Masker's output. Here, we provide details of the annotation procedure for generating a Masker test set (Appendix C.1) and the metrics proposed to assess the performance (Appendix C.2).\n\n## C. 1 Test Set Collection and Annotation\n\nIn this section, we provide more details about our test set data collection procedure, and the annotation guidelines we followed.\nWe collected the 180 images of our test set from Google Street View with the following selection guidelines:\n\n- Geographical diversity: we selected images in a variety of cities on all continents, with different architectural styles and general landscapes.\n- Varied levels of urban development: both urban and rural areas were included."
    },
    {
      "markdown": "- Variety of scenes content: we endeavored to cover a wide range of objects in our test set, including images containing bike racks, dense crowds, and various vehicles in urban areas, and different vegetation types in rural areas.\n\nWe also carefully collected challenging images to determine the limitations of our model, including images facing slopes (going up or down), or stairs, with various ground textures, around areas under construction, and in areas near canals among others. The images were manually annotated, and the pixels of each image were categorized in one of three classes according to the following instructions:\n\n- Must be flooded: This class contains the minimal region that should be flooded. We want to represent floods of height at least 0.5 m . Typically, this corresponds to flooding up to the knees of adult pedestrians or up to the top of cars' wheels. In cases when no such reference objects were available, the annotator would make the 0.5 m estimate based on other cues, such as doorsteps, traffic signs and vegetation.\n- Cannot be flooded: This label indicates regions we absolutely do not want to be flooded. We generally put any pixel corresponding to an object with a height greater than 1.5 m above ground in this category. This includes car roofs and adult pedestrians' heads, which we used as reference to determine the lower limit of this region.\n- May be flooded: This category contains any pixel not assigned to the other classes. It reflects the fact that we do not enforce flooding at a specific height yet value plausible flood location.\n\nWe show examples of the labeled test images in Fig. 12.\n![img-11.jpeg](img-11.jpeg)\n\nFigure 12: Examples of labeled images from our test set."
    },
    {
      "markdown": "# C. 2 Metrics \n\nIn Section 4.1.1 we proposed three metrics to compare the masks predicted by the Masker and the labeled test images: error (Eq. (9)), F05 score and edge coherence (Eq. (10)). Here, we delve into the reasons why we proposed more than one metric and the contribution of each metric.\n\nWe proposed the error-number of erroneously predicted pixels divided by the size of the image-as the main evaluation metric, since it characterizes the size of the errors in the image, which directly impacts the perceived mask quality.\nHowever, we argue that the error alone does not capture all aspects of the performance of a Masker model in the test set. For example, the error, as defined in Eq. (9) does not take into account the size of the labeled areas (see image \"D\" in Fig. 19). While the size of the labels may have a smaller perceptual impact, in order to characterize the precision and sensitivity of the model, additional metrics are useful. We proposed the F05 score:\n\n$$\nF 05=\\frac{1.25 \\times \\text { precision } \\times \\text { recall }}{0.25 \\times \\text { precision }+ \\text { recall }}=\\frac{1.25 \\times \\frac{T P}{T P+F P} \\times \\frac{T P}{T P+F N}}{0.25 \\times \\frac{T P}{T P+F P}+\\frac{T P}{T P+F N}}\n$$\n\nwhich computes the weighted harmonic mean of precision and recall-also known as sensitivity or true positive rate. We used $\\beta=0.5$ in order to weigh precision more than recall, that is to penalize more false positives than false negatives. In our context, this translates into setting a higher penalty for flooding cannot-be-flooded pixels-for instance heads of pedestrians, automobile roofs and high areas of the image, in general-than for missing areas that should be flooded. While both types of errors should be penalized, the former has a higher perceptual impact.\n![img-12.jpeg](img-12.jpeg)\n\nFigure 13: Distribution of the three metrics for the selected best Masker. The annotations correspond to the images in Fig. 19.\n\nFinally, we proposed an edge coherence metric (Eq. (10)) in order to take into consideration the shape of the predicted mask, with respect to the shape of the must-be-flooded label. Note that neither the error or the F05 score account for the shape, as only the amount of correct or incorrect pixels matter, regardless of the position. As explained in the previous section, we defined a may-be-flooded class in order to allow for different levels of flooding in the mask prediction. That is, higher levels of flooding in a prediction should not be penalized necessarily, as long as the mask is consistent with the semantics in the image. Our proposed metric of edge coherence is based on the assumption that predictions whose border is roughly parallel to the border of the must-be-flooded label should be less penalized than highly dissimilar shapes.\nFig. 13 shows the value of the three metrics for all the images in the test set. While there is certain correlation between the metrics, especially because the bulk of the distributions is around images for which the predictions are very accurate and hence all the metrics are near perfect, the plot shows that various metrics"
    },
    {
      "markdown": "|  | Error | p |\n| :-- | :-- | :-- |\n| Pseudo labels | $-6.9 \\times 10^{-4}\\left[-9.5 \\times 10^{-4},-4.5 \\times 10^{-4}\\right]$ | $=0.0$ |\n| Depth | $-3.7 \\times 10^{-4}\\left[-7.3 \\times 10^{-4},-1.4 \\times 10^{-5}\\right]$ | $<0.01$ |\n| Seg. (S) | $-3.6 \\times 10^{-4}\\left[-6.0 \\times 10^{-4},-1.2 \\times 10^{-4}\\right]$ | $<0.0001$ |\n| SPADE | $-2.6 \\times 10^{-4}\\left[-4.7 \\times 10^{-4},-5.1 \\times 10^{-5}\\right]$ | $<0.01$ |\n| DADA (S) | $-3.5 \\times 10^{-4}\\left[-5.5 \\times 10^{-4},-1.7 \\times 10^{-4}\\right]$ | $=0.0$ |\n| DADA (M) | $+2.1 \\times 10^{-3}\\left[+1.5 \\times 10^{-3},+2.9 \\times 10^{-3}\\right]$ | $=0.0$ |\n\nTable 2: Details of the results of the ablation study. The values in each cell are the $20 \\%$ trimmed mean error difference between models with and without a given technique, in brackets the $99 \\%$ confidence intervals, and the $p$ value.\n\n|  | F05 score | p |\n| :-- | :-- | :-- |\n| Pseudo labels | $+7.4 \\times 10^{-4}\\left[+4.3 \\times 10^{-4},+1.1 \\times 10^{-3}\\right]$ | $=0.0$ |\n| Depth | $-3.9 \\times 10^{-4}\\left[-9.1 \\times 10^{-4},+5.9 \\times 10^{-5}\\right]$ | $<0.1$ |\n| Seg. (S) | $+4.1 \\times 10^{-4}\\left[+1.2 \\times 10^{-4},+7.3 \\times 10^{-4}\\right]$ | $<0.001$ |\n| SPADE | $+2.9 \\times 10^{-5}\\left[-2.6 \\times 10^{-4},+2.9 \\times 10^{-4}\\right]$ | $>0.01$ |\n| DADA (S) | $+3.8 \\times 10^{-4}\\left[+1.3 \\times 10^{-4},+6.5 \\times 10^{-4}\\right]$ | $<0.0001$ |\n| DADA (M) | $-2.6 \\times 10^{-3}\\left[-3.5 \\times 10^{-3},-1.8 \\times 10^{-3}\\right]$ | $=0.0$ |\n\nTable 3: The analogue to Table 2 for the F05 score\nare useful for identifying a few images for which not all metrics are low. We further illustrate the meaning of our proposed metrics in Fig. 19, where we show images that obtained the lowest-2nd quantile-and highest- 98 th quantile-values of each metric.\n\n# C. 3 Ablation Study \n\nHere, we detail the methodology used for the ablation study of the Masker, presented in Section 4.1.2, as well extend the set of results provided in Section 5.1.\n\nIn the ablation study, we studied the contribution to the Masker evaluation metrics of each technique: training with pseudo labels, a depth head $(D)$, a segmentation head $(S)$, SPADE, DADA for the segmentation head and DADA for the masker head. For every technique $t$, we considered all models $m_{j}^{t}$ which included such technique, and the paired models $m_{j}^{t o}$ which differed only by the absence of $t$. Then, for every metric $r$, we constructed datasets of metric differences for every image $i$ :\n\n$$\nd_{i j}^{r}=r\\left(m_{j}^{t}\\right)_{i}-r\\left(m_{j}^{t o}\\right)_{i}\n$$\n\nand obtained 1 million bootstrapped samples. On each bootstrap sample we computed the $20 \\%$ trimmed mean, which forms a bootstrap distribution from which we derived the confidence intervals we reported graphically in Fig. 5. In particular, we computed the $99 \\%$ confidence intervals, that is the lower bound is the 0.5 th quantile and the upper bound is the 99.5 th quantile of the bootstrap distribution. In Table 2, we\n\n|  | Edge coherence | p |\n| :-- | :-- | :-- |\n| Pseudo labels | $+3.7 \\times 10^{-4}\\left[+1.2 \\times 10^{-5},+7.4 \\times 10^{-4}\\right]$ | $<0.01$ |\n| Depth | $-1.4 \\times 10^{-3}\\left[-2.1 \\times 10^{-3},-7.4 \\times 10^{-4}\\right]$ | $=0.0$ |\n| Seg. (S) | $+2.8 \\times 10^{-4}\\left[-1.7 \\times 10^{-4},+7.3 \\times 10^{-4}\\right]$ | $>0.01$ |\n| SPADE | $-9.3 \\times 10^{-4}\\left[-1.4 \\times 10^{-3},-5.0 \\times 10^{-4}\\right]$ | $=0.0$ |\n| DADA (S) | $-1.0 \\times 10^{-4}\\left[-4.2 \\times 10^{-4},+2.2 \\times 10^{-4}\\right]$ | $>0.01$ |\n| DADA (M) | $+4.6 \\times 10^{-4}\\left[-2.8 \\times 10^{-4},+1.2 \\times 10^{-3}\\right]$ | $>0.01$ |\n\nTable 4: The analogue to Table 2 for the edge coherence"
    },
    {
      "markdown": "![img-13.jpeg](img-13.jpeg)\n\nFigure 14: Bootstrapped distribution of the $20 \\%$ trimmed means of the difference in edge coherence between models that included pseudo labels and their counterparts. Equivalent distributions were obtained for all other techniques and metrics in the ablation study.\n![img-14.jpeg](img-14.jpeg)\n\nFigure 15: Distribution of the metrics—error, F05 score and edge coherence—of the complete set of models tested in the ablation study, 1-18, excluding the two baselines, whose distributions are shown in Fig. 6. The solid symbols indicate the median of the distribution and the error lines the bootstrapped $99 \\%$ confidence intervals. The shaded area highlights the best model—7.\nprovide the numerical details of all the tests in the ablation study. One advantage of the bootstrap over other statistical inference methods is that the outcome is a data-driven distribution rather than a binary test. In order to illustrate this, we provide the distribution for one of the techniques-pseudo labels-and one of the metrics—edge coherence-in Fig. 14.\n\nIn Fig. 15 we extend Fig. 6, where we show the distribution of the metrics for the complete set of models in the ablation study. We do not include the baseline models, which obtained significantly worse metrics, in order to better visualize the differences within the models of the ablation study. From the figure, it becomes apparent that models trained without pseudo labels (9-18), and models trained with DADA for the masker head $(8,9,17,18)$ achieved worse performance, in general. This is consistent with the results of the ablation study obtained through the bootstrap.\n\n# D Human Evaluation Results \n\nDuring the human evaluation of our results, we found that overall, evaluators primarily preferred ClimateGAN to other comparable approaches to varying degrees (a complete presentation of results can be found in Section 5.2). However, there were a number of cases when evaluators preferred other approaches to Cli-"
    },
    {
      "markdown": "![img-15.jpeg](img-15.jpeg)\n\nFigure 16: Example inferences of ClimateGAN and comparable approaches on images from the test set. Images outlined in red are those that were systematically (i.e. 3 out of 3 times) preferred over ClimateGAN.\nmateGAN. We present such images in Fig. 16, where the images outlined in red are those that are those for which all 3 evaluators preferred a comparable approach to ClimateGAN.\n\nIt can be observed that it is hard to consider these approaches as being better than ClimateGAN - i.e., they do not appear to be more realistic neither in terms of the quality of generated water nor in the portion of the image that is flooded. However, the concept of realism and quality of generated imagery is, indeed, hard to define and to quantify, so it is unsurprising that some images were systematically preferred by evaluators. In order to carry out a more thorough human evaluation of our results, it would be necessary to have more evaluators per pair of images and, ideally, different experimental setups - for instance, different captions and different designs of the comparison interface. Overall, it is hard to draw a conclusion for the motivation behind the preferences of individual evaluators. It can nonetheless be observed that in the case of some models, such as InstaGAN, images of light flooding (i.e. that after moderate rain) were preferred to that of more severe flooding generated by ClimateGAN. In other cases, such as that of InstaGAN+Mask, images with more pronounced reflections were chosen over those of the murkier, rippled water produced by ClimateGAN.\n\nFinally, we feel that the best evaluation of the impact of images generated by ClimateGAN is to compare them with other mediums of climate communication. For this purpose, we are working with a group of researchers in psychology to test the effect that our images have on climate risk perception. The experimental setup involves providing subjects with a text regarding the risks of climate change-induced flooding. One group of subjects will only see the text, a second will see the text accompanied by an image of a generic flooded house, whereas the third will see an image of their address of residence flooded using ClimateGAN. Preliminary results have indicated that individuals who saw their own place of residence flooded are more likely to perceive climate change as a real threat to their livelihood, and more likely to take action to fight it."
    },
    {
      "markdown": "# E Carbon Impact \n\nThe environmental impact of machine learning is becoming an increasingly major issue for our field, given the extensive experimentation and hyper-parameter tuning required to successfully train large neural networks (Strubell et al., 2019; Schwartz et al., 2019; Lacoste et al., 2019). In order to estimate our own carbon footprint, we counted all of the GPU hours (weighted by GPU usage) that our entire team used during the whole course of our project, from initial stages to model selection to the ablation study and hyper-parameter tuning on our final ClimateGAN model. We used the MLCO ${ }_{2}$ Emissions Calculator ${ }^{3}$ (Lacoste et al., 2019) to obtain a final estimate of $\\mathbf{3 6 2 . 7 2}$ kilograms of $\\mathbf{C O}_{2}$ eq., which is comparable to 900 miles driven by an average passenger vehicle, $42 \\%$ of a US household's yearly energy consumption ${ }^{4}$ or round trips between Paris, France and Saint-Petersburg, Russia ( $369 \\mathrm{~kg} \\mathrm{CO}_{2}$ eq.) or between Boston, MA and Miami, FL (345kg $\\mathrm{CO}_{2}$ eq) ${ }^{5}$. This does not include the rest of the computational infrastructure's energy consumption (CPUs, data transfers and storage etc.) nor the full Life Cycle Analysis of any of the hardware used.\nWe are incredibly lucky that our power grid is powered predominantly by renewable energy. If this were not the case and we were using coal-powered energy, the figure stated above would be 50-80 times larger, and thereby more problematic. We hope that our colleagues will also start tallying and sharing the carbon footprint of their research using tools such as CodeCarbon and the Experiment Impact Tracker and that our community will start being more mindful regarding the trade-off between scientific progress and environmental impact.\n\n## F Full ClimateGAN Architecture\n\nIn order to better illustrate the overall training procedure of ClimateGAN, we provide a more detailed overview in Fig. 17.\n\nThe Masker can be seen in the top part of the Figure, encompassing the three decoders described in Section 3.2: Depth (D) , Segmentation (S) and Flood-Mask (M). The Painter is shown in the lower part of the image, with its SPADE-based encoder P. The losses of each component are indicated in the rounded white boxes within each decoder. Tensors are represented using squares, with different colors for input, label, intermediate and output tensors, and pseudo-labels with dotted outlines. To emphasize the SPADE-based conditional architectures of M and P , they are colored in green. As per Fig. 2, the output of ClimateGAN, i.e. the flooded image $\\bar{y}$, is in blue. Note that the dotted and dashed line from $\\boldsymbol{m}_{r}$ to P is not leveraged during training and only used for test-time inferences. Discriminators are conceptually included in the WGAN losses.\n\n## G Supplementary Images\n\nIn order to further illustrate the performance and capabilities of ClimateGAN, we provide additional inferences in Fig. 18, complementing those presented in Fig. 3. The first two rows were chosen to be successful inferences, middle two rows were selected at random and bottom two rows are failure cases.\n\n[^0]\n[^0]:    ${ }^{3}$ mlco2.github.io/impact\n    ${ }^{4}$ Source: EPA\n    ${ }^{5}$ Source: ICAO"
    },
    {
      "markdown": "![img-16.jpeg](img-16.jpeg)\n\nFigure 17: Detailed diagram of the training procedure of ClimateGAN. We highlight how the simulated and real data paths in the model are similar yet different. We use dashed arrows and boxes to emphasize that pseudo labels are only used as noisy signal in the beginning of training. All the losses represented in the white rounded boxes are detailed in Section 3.2 and Appendix B."
    },
    {
      "markdown": "![img-17.jpeg](img-17.jpeg)\n\nFigure 18: More samples of the full ClimateGAN forward pass: input image, inferences from the depth, segmentation and flood mask decoders of the Masker, followed by the masked input image fed to the Painter and finally the flooded image output by the Painter. Notably, the Painter is able to produce consistently contextualized water, with color and reflections relevant to the surrounding objects and to the sky. While generally able to appropriately understand a scene's perspective and circumvent objects, the Masker is however sometimes unable to predict plausible flood masks (as illustrated in the bottom two rows). It is difficult to exactly understand the source of this because both the depth and segmentation maps look generally appropriate."
    },
    {
      "markdown": "![img-18.jpeg](img-18.jpeg)\n\nFigure 19: Examples of images that obtained good and bad-within the 2nd and 98th quantiles, respectively-Masker's predictions metrics. From top two rows to bottom two: error, F05 score and edge coherence. The first row of each metric corresponds to an image with good values of the metric. The white segments in the images illustrating the edge coherence indicate the shortest distance between the predicted and the ground truth mask. The legend of the column \"Labels\" is the same as in Fig. 12, and the images can be identified in Fig. 13 by the letters on the left."
    }
  ],
  "usage_info": {
    "pages_processed": 27,
    "doc_size_bytes": 15037699
  },
  "_metadata": {
    "model_version": "0.0.0",
    "llm_model": "mistral-ocr-2505",
    "query_data": {
      "pdf": "/Users/satyaortiz-gagne/CODE/paperext/data/cache/fulltext/88a57585a6ec9472e02196470df005e1/fulltext.pdf"
    },
    "model_id": "parsepdf"
  }
}