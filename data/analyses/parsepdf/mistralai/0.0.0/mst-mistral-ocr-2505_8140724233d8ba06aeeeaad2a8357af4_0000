{
  "pages": [
    {
      "markdown": "# Stochastic Neural Network with Kronecker Flow \n\nChin-Wei Huang<br>Mila<br>Ahmed Touati<br>Mila<br>Pascal Vincent<br>Mila, Facebook Research<br>Gintare Karolina Dziugaite<br>Element AI<br>Aaron Courville<br>Mila, CIFAR Fellow\n\n\n#### Abstract\n\nRecent advances in variational inference enable the modelling of highly structured joint distributions, but are limited in their capacity to scale to the high-dimensional setting of stochastic neural networks. This limitation motivates a need for scalable parameterizations of the noise generation process, in a manner that adequately captures the dependencies among the various parameters. In this work, we address this need and present the Kronecker Flow, a generalization of the Kronecker product to invertible mappings designed for stochastic neural networks. We apply our method to variational Bayesian neural networks on predictive tasks, PAC-Bayes generalization bound estimation, and approximate Thompson sampling in contextual bandits. In all setups, our methods prove to be competitive with existing methods and better than the baselines.\n\n\n## 1 Introduction\n\nStochastic neural networks (SNN) are a central tool in many subfields of machine learning, including (1) Bayesian deep learning (MacKay, 1992; Blundell et al., 2015; HernÃ¡ndez-Lobato and Adams, 2015; Gal and Ghahramani, 2016), (2) exploration in reinforcement learning (Ian et al., 2013; Osband et al., 2016; Riquelme et al., 2018), and (3) statistical learning theory such\n\n[^0]as PAC-Bayesian learning (McAllester, 1999; Langford and Seeger, 2001; Dziugaite and Roy, 2017a). Perturbations of the network parameters induce a distribution over the model, and this intrinsic uncertainty is the subject of great interest to machine learning practitioners and theoreticians alike. For example, deep Bayesian models are often used to adequately measure uncertainty, and determine whether the model itself is inherently familiar with the unseen data. This is especially important in the context of autonomous vehicles, where decisions must be made to meet specific safety standards (McAllister et al., 2017). Conversely, the lack of confidence can be leveraged to efficiently guide exploration in reinforcement learning, via randomizing the approximate value function (Azizzadenesheli et al., 2018; Touati et al., 2018) or maximizing intrinsic rewards (Houthooft et al., 2016).\n\nFurthermore, a considerable proportion of statistical learning theory is devoted to understanding what implies generalization, or what constitutes an appropriate measure of complexity (Bartlett et al., 2017; Arora et al., 2018; Neyshabur et al., 2017). PAC-Bayesian learning theory (McAllester, 1999) specifically explores the generalization property of a randomized prediction rule, and has been recently studied in the context of stochastic neural networks (Dziugaite and Roy, 2017a). In this particular study, the working hypothesis is that good generalization can be guaranteed on the premise that stochastic gradient descent (Robbins and Monro, 1951) finds a solution that obtains certain structural properties, such as flatness.\n\nFor computational reasons, considerable effort has been devoted to modelling uncertainty through the injection of independent noise to the network parameters (Graves, 2011; Blundell et al., 2015; Kingma et al., 2015). However, noise independence largely restricts the expressivity of the noise distribution and thus the resulting uncertainty measures are ill-calibrated (Minka et al., 2005; Turner and Sahani, 2011). Attempts have been made to correlate parameters of a neural network,\n\n\n[^0]:    Work done while Chin-Wei was an intern at Element AI and Ahmed at Facebook Research."
    },
    {
      "markdown": "including Louizos and Welling (2017); Krueger et al. (2017); Pawlowski et al. (2017), for example, by adapting expressive non-linear invertible transformations developed in the variational inference literature (Rezende and Mohamed, 2015; Kingma et al., 2016; Huang et al., 2018), or via implicit methods (Goodfellow et al., 2014). However, these methods are limited due to their inability to scale well. Louizos and Welling (2017), for instance, resort to a specific multiplicative noise sampled from a lower dimensional space and have to use an auxiliary method to bound the entropy. Krueger et al. (2017), on the other hand, give up on injecting noise on the entire set of parameters and model the distribution of the scale and shift parameter of the pre-activations.\n\nIn attempts to address some of the challenges articulated above and efficiently model the joint distribution of a network's parameters, we take inspiration from the Kronecker product, which we notice can be thought of as left-transforming a matrix via a linear map, and then right-transforming it using another linear map, thus providing us an efficient way to correlate the weight parameters. We propose the Kronecker Flow, an invertible transformation-based method that generalizes the Kronecker product to its nonlinear counterparts. Our contributions are as follows.\n\n1. We extend the idea of Kronecker product to more general invertible mappings to induce non-linear dependencies, and apply this trick to parameterizing deep stochastic neural networks.\n2. We apply our method to predictive tasks and show that our methods work better on larger architectures compared to existing methods.\n3. We are the first to apply flow-based methods to tighten the PAC-Bayes bound. We show that the KL divergence in the PAC-Bayes bound can be estimated with high probability, and demonstrate the generalization gap can be further reduced and explained by leveraging the structure in the parameter space.\n4. Our methods prove to be competitive over other methods in approximate Thompson sampling in contextual bandit problems.\n\n## 2 Background\n\nStochastic neural networks with parameter perturbation normally follow the stochastic process: $\\Theta \\sim q_{\\phi}(\\Theta)$, $y \\mid x \\sim p(y \\mid x, \\Theta)=f_{\\Theta}(x)$, where $\\Theta$ is the parameters of the neural network $f$, which outputs the prediction probability vector for classification or the predicted values for regression. We let $D=\\left\\{\\left(x_{i}, y_{i}\\right): i \\in[m]\\right\\}$\nbe the training set of size $m^{1}, H$ be the differential entropy $H[q]=-\\mathbb{E}_{q}[\\log q], \\beta>0$ be the coefficient controlling the amount of noise injected into the model and the degree of regularization, $l(y, \\bar{y})$ be the loss function and $\\bar{R}_{D}(\\Theta)=\\frac{1}{m} \\sum_{i=1}^{m} l\\left(y_{i}, f_{\\Theta}\\left(x_{i}\\right)\\right)$ be the empirical risk.\n\n### 2.1 Variational Bayesian neural networks\n\nVariational Bayesian neural networks are a type of stochastic neural network. Bayesian inference updates our prior belief $p(\\Theta)$ over the model parameters according to the Bayes rule $p(\\Theta \\mid D) \\propto p(D \\mid \\Theta) p(\\Theta)$, by incorporating information from the training set through the likelihood function $p(D \\mid \\Theta)$. Variational inference is a computational realization of Bayesian inference, which casts inference as an optimization problem, where one maximizes the variational lower bound (also known as the evidence lower bound, or the ELBO) on the log marginal likelihood:\n\n$$\n\\log p(D) \\geq \\mathbb{E}_{q_{\\phi}}[\\log p(D \\mid \\Theta)+\\log p(\\Theta)]+H\\left(q_{\\phi}(\\Theta)\\right)\n$$\n\nwhere $q_{\\phi}$ is the variational approximate posterior and $p(D \\mid \\Theta)$ can be decomposed into $\\prod_{i=1}^{m} p\\left(y_{i} \\mid x_{i}, \\Theta\\right)$ due to conditional independence assumption. The optimal $q$ is the true posterior, i.e. $q^{*}(\\Theta)=\\frac{p(D \\mid \\Theta) p(\\Theta)}{p(D)}$. In our case, we use $\\Theta$ to parameterize a neural network. Prediction can be carried out via the (approximate) predictive posterior\n\n$$\n\\begin{aligned}\np(y \\mid x, D) & =\\mathbb{E}_{\\Theta \\sim p(\\Theta \\mid D)}[p(y \\mid x, \\Theta)] \\\\\n& \\approx \\mathbb{E}_{\\Theta \\sim q_{\\phi}(\\Theta)}[p(y \\mid x, \\Theta)] \\\\\n& \\approx \\frac{1}{K} \\sum_{k=1}^{K} p(y \\mid x, \\Theta_{k})\n\\end{aligned}\n$$\n\nfor $\\left\\{\\Theta_{k}\\right\\}_{k \\in[K]}$ drawn i.i.d. from $q_{\\phi}(\\Theta)$, where we use the variational distribution $q$ to approximate $p(\\Theta \\mid D)$ and a Monte Carlo estimate to estimate the integral.\n\nThe prior distribution can be used to encode some form of inductive bias, such as one that is in favor of parameter values closer to some $\\Theta_{0}$ chosen a priori. We choose the prior to be an isotropic Gaussian, centered at the random initialization $\\Theta_{0}$, i.e., $p(\\Theta)=\\mathcal{N}\\left(\\Theta ; \\Theta_{0}, \\lambda \\boldsymbol{I}\\right)$. The entropy term ensures the variational posterior does not collapse to a point estimate. Both of them can be thought of as some form of regularizer, so we attach a coefficient $\\beta$ in front of them as a hyperparameter ${ }^{2}$.\n\n[^0]\n[^0]:    ${ }^{1}$ We use the notation $[n]$ to compactly describe the set of integers $\\{1,2, \\ldots, n\\}$.\n    ${ }^{2}$ Like the $\\lambda$ parameter in Zhang et al. (2017)"
    },
    {
      "markdown": "### 2.2 PAC-Bayes generalization bound\n\nAnother use case of stochastic neural networks is to understand generalization, via PAC-Bayes bounds. The aim is to bound a divergence between the empirical risk, $\\hat{\\mathcal{L}}[q]=\\mathbb{E}_{q}\\left[\\hat{R}_{D}(\\Theta)\\right]$, and the risk measured on the true distribution $\\mathcal{D}, \\mathcal{L}[q]=\\mathbb{E}_{q}\\left[\\mathbb{E}_{\\mathcal{D}}\\left[l\\left(y, f_{\\Theta}(x)\\right)\\right]\\right]$. While this quantity is unbounded in the general case, assuming a bounded loss function $l$ (e.g.: zero-one loss), we can obtain a probabilistic bound that holds with probability $1-\\delta$ over the choice of $D$, for $\\delta>0$. More specifically, with probability $1-\\delta, \\Delta(\\hat{\\mathcal{L}}[q], \\mathcal{L}[q]) \\leq$ $\\Omega\\left(D_{\\mathrm{KL}}(q \\| p), m, \\delta\\right)$, where $\\Omega$ is a measure of complexity that scales proportionally with the Kullback-Leibler (KL) divergence and $\\Delta$ a measure of divergence (e.g.: square distance or convex functions (Germain et al., 2009)).\n\nFor instance, Dziugaite and Roy (2017a) minimize the following bound originally due to McAllester (1999) and then tightened by Langford and Seeger (2001):\nTheorem 1. Let $l$ be the zero-one loss. For any $\\delta>0$ and data distribution $\\mathcal{D}$, and any distribution $p$ on the space of $\\Theta$, with probability at least $1-\\delta$ over the choice of a training set $D \\sim \\mathcal{D}^{m}$, for all distributions $q$ on the space of $\\Theta$,\n\n$$\nD_{\\mathrm{KL}}(\\hat{\\mathcal{L}}[q] \\| \\mathcal{L}[q]) \\leq \\frac{D_{\\mathrm{KL}}(q \\| p)+\\log \\frac{m}{2}}{m-1}\n$$\n\nwhere the KL on the LHS is between two Bernoulli distributions, defined by the probability of performing an error.\n\nWe refer to the above bound as the McAllester bound. The KL divergence on the RHS of the bound, also known as the information gain, tells us to what extent the posterior $q$ is dependent on the training data. The sharper and more confident $q$ is, and the farther away it is from the prior $p$, the larger the KL will be, which in turn is reflected by the larger bound on the generalization gap. This is consistent with traditional notion of bias-variance trade-off.\nAlternatively, we consider the following bound due to Catoni (2007):\nTheorem 2. With the $l, \\delta, \\mathcal{D}$, and $p$ as defined in Theorem 1, and with a fixed $\\beta>1 / 2$, the following bound holds with probability over $1-\\delta$ :\n\n$$\n\\mathcal{L}[q] \\leq \\frac{1}{1-\\frac{1}{2 \\beta}}\\left(\\hat{\\mathcal{L}}[q]+\\frac{\\beta}{m}\\left(D_{\\mathrm{KL}}(q \\| p)+\\ln \\frac{1}{\\delta}\\right)\\right)\n$$\n\nWe refer to this bound as the Catoni bound. We notice the linear relationship (which is also noticed by Germain et al. (2016)) between the empirical risk and the KL divergence. This allows us to make use of the\nlinearity of expectation to perform change of variable (see the next section). We also note that the optimal $\\beta$ in Equation 3 is always larger than 1, so the PAC-Bayes bound is actually more conservative than Bayesian inference in this sense.\n\n### 2.3 Normalizing flows\n\nMinimization of Equation 1 and 3 requires (i) computing the gradient with respect to the parameter of the (PAC-)Bayesian posterior $\\phi$, and (ii) computing the entropy of $q$. One approach to do this is via change of variable under an invertible mapping. Let $\\boldsymbol{\\epsilon} \\sim q_{0}$ be a random variable in $\\mathbb{R}^{d}$, and $\\boldsymbol{g}_{\\phi}: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}^{d}$ be a bijection parameterized by $\\phi$. Let $\\Theta=\\boldsymbol{g}_{\\phi}(\\boldsymbol{\\epsilon})$ and $q_{\\phi}$ be its density. Then we can rewrite the loss function as ${ }^{3}$\n\n$$\n\\begin{aligned}\n\\mathbb{E}_{\\Theta}\\left[\\hat{R}_{D}(\\Theta)+\\log q_{\\phi}(\\Theta)\\right] & =\\mathbb{E}_{\\boldsymbol{\\epsilon}}\\left[\\hat{R}_{D}\\left(\\boldsymbol{g}_{\\phi}(\\boldsymbol{\\epsilon})\\right)+\\log q_{0}(\\boldsymbol{\\epsilon})\\right. \\\\\n& \\left.-\\log \\left\\lvert\\, \\operatorname{det} \\frac{\\partial \\boldsymbol{g}_{\\phi}(\\boldsymbol{\\epsilon})}{\\partial \\boldsymbol{\\epsilon}}\\right.\\right]\n\\end{aligned}\n$$\n\nwhere we apply the change of variable (see Appendix A for the detailed derivation). The log-determinant (logdet) term ensures that we obtain a valid probability density function after $g_{\\phi}$ is applied, which can be a sequence of invertible mappings itself, hence referred to as the normalizing flow (Rezende and Mohamed, 2015). This way, the random variable and the parameters are decoupled, so that we can differentiate the integrand to have an unbiased estimate of the gradient (fixing some $\\boldsymbol{\\epsilon} \\sim q_{0}$ ). We let $q_{0}$ be the standard normal.\n\n## 3 Kronecker Flows\n\nWe consider maximizing the ELBO and minimizing the Catoni bound via normalizing flow-based SNNs. Conventionally, mean-field approximation using factorized distributions (such as multivariate Gaussian with diagonal covariance) has been well explored in the variational inference (VI) literature (Blundell et al., 2015). We are interested in better capturing the structure in the parameter space as restricted VI methods are known to exhibit overconfidence (Minka et al., 2005; Turner and Sahani, 2011). However, the parameters of a neural network are usually very high dimensional (on the order of millions), requiring a novel way to parameterize the joint distribution over the parameters.\nIn its general form, neural networks can be represented by a collection of tensors i.e. $\\Theta=\\left\\{\\boldsymbol{W}_{l}: l \\in[L]\\right\\}$. While our method below can easily be generalized to highdimensional tensors (such as for convolutional kernels), to simplify notation, we describe the matrix form.\n\n[^0]\n[^0]:    ${ }^{3}$ Since the weighting coefficient $\\beta$ can be absorbed into the loss function $l$, we neglect it for simplicity now."
    },
    {
      "markdown": "### 3.1 Linear Kronecker Flow\n\nThe matrix-variate normal $(\\mathcal{M N})$ distribution generalizes the multivariate normal distribution to matrixvalued random variables, such as weight matrices of a neural network (Louizos and Welling, 2016). Matrix normal is a multivariate normal distribution whose covariance matrix is a Kronecker product $(\\otimes)$, which allows us to model the correlation among the parameters to some degree.\n\nMore concretely, assume $\\boldsymbol{E}_{i j} \\xrightarrow{\\text { i.i.d. }} \\mathcal{N}(0,1)$ is an $n \\times p$ random Gaussian matrix, and $\\boldsymbol{A} \\in \\mathbb{R}^{n \\times n}, \\boldsymbol{B} \\in \\mathbb{R}^{p \\times p}$ and $\\boldsymbol{M} \\in \\mathbb{R}^{n \\times p}$ are real-valued matrices. Then $\\boldsymbol{M}+$ $\\boldsymbol{A} \\boldsymbol{E} \\boldsymbol{B}$ has a matrix normal distribution, as\n\n$$\n\\operatorname{vec}(\\boldsymbol{M}+\\boldsymbol{A} \\boldsymbol{E} \\boldsymbol{B}) \\sim \\mathcal{N}\\left(\\operatorname{vec}(\\boldsymbol{M}), \\boldsymbol{B}^{\\top} \\boldsymbol{B} \\otimes \\boldsymbol{A} \\boldsymbol{A}^{\\top}\\right)\n$$\n\nwhere vec is the vectorization of a matrix that concatenates all the columns. This allows us to represent the covariance matrix in a more compact manner $\\left(n^{2} p^{2} / 2\\right.$ parameters versus $n^{2} / 2+p^{2} / 2$ parameters for Kronecker product).\n\nLimitation of the Kronecker product. The Kronecker product covariance matrix is not a strict generalization of diagonal covariance matrix. To observe this, let $\\boldsymbol{U}=\\operatorname{diag}(\\boldsymbol{u}), \\boldsymbol{V}=\\operatorname{diag}(\\boldsymbol{v})$ (this is the case of Louizos and Welling (2016)), and $\\boldsymbol{S}=\\operatorname{diag}(\\boldsymbol{s})$, where $\\boldsymbol{u} \\in \\mathbb{R}_{>0}^{n}, \\boldsymbol{v} \\in \\mathbb{R}_{>0}^{p}$, and $\\boldsymbol{s} \\in \\mathbb{R}_{>0}^{n p}$. Then $\\boldsymbol{U} \\otimes \\boldsymbol{V}$ is also a diagonal matrix of size $n p \\times n p$. Equating $\\boldsymbol{U} \\otimes \\boldsymbol{V}=\\boldsymbol{S}$ to solve for $\\boldsymbol{u}$ and $\\boldsymbol{v}$ will result in $n p$ nonlinear equations with $n+p$ variables, which can be over-determined for $n, p>2$. For example, let $n=2, p=3$, and $\\boldsymbol{s}=[1, \\epsilon, \\epsilon, 1,1,1]$ for some $\\epsilon>0$. Then the nonlinear system below does not have a solution:\n\n$$\n\\begin{aligned}\n& \\boldsymbol{U} \\otimes \\boldsymbol{V}=\\boldsymbol{S} \\Longleftrightarrow \\begin{array}{l}\n\\boldsymbol{u}_{1} \\boldsymbol{v}_{1} \\stackrel{(a)}{=} 1 \\quad \\boldsymbol{u}_{1} \\boldsymbol{v}_{2} \\stackrel{(b)}{=} \\epsilon \\quad \\boldsymbol{u}_{1} \\boldsymbol{v}_{3} \\stackrel{(c)}{=} \\epsilon \\\\\n\\boldsymbol{u}_{2} \\boldsymbol{v}_{1} \\stackrel{(d)}{=} 1 \\quad \\boldsymbol{u}_{2} \\boldsymbol{v}_{2} \\stackrel{(e)}{=} 1 \\quad \\boldsymbol{u}_{2} \\boldsymbol{v}_{3} \\stackrel{(f)}{=} 1\n\\end{array}\n\\end{aligned}\n$$\n\nTo see this, dividing (a) by (b) and dividing (d) by (e) yield $\\boldsymbol{v}_{1}=\\boldsymbol{v}_{2} / \\epsilon$ and $\\boldsymbol{v}_{1}=\\boldsymbol{v}_{2}$, respectively, which doesn't have a solution if $\\epsilon \\neq 1$. This is because the Kronecker product is essentially parameter sharing, which can heavily restrict the matrix it can represent.\n\nTo remedy the above limitation, we can further decouple the reparameterization of the parameter matrix into two parts: (1) one that models the marginal variance and (2) one that models correlations. Assume $\\boldsymbol{S} \\in \\mathbb{R}_{>0}^{n \\times p}$ is a positive-valued matrix, and let $\\boldsymbol{W}:=\\boldsymbol{M}+\\boldsymbol{A}(\\boldsymbol{E} \\circ \\boldsymbol{S}) \\boldsymbol{B}$. Then $\\operatorname{vec}(\\boldsymbol{W})$ is a Gaussian distribution with the following property, which is useful in calculating the KL divergence:\n\nProperty 1. Let $\\boldsymbol{W}$ be given as above, with $\\mu=$ $\\mathbb{E}[\\operatorname{vec}(\\boldsymbol{W})]$ and $\\Sigma=\\operatorname{Var}(\\operatorname{vec}(\\boldsymbol{W}))$. Then\n(P1) $\\mu=\\operatorname{vec}(\\boldsymbol{M})$, and\n\n$$\n\\Sigma=\\left(\\boldsymbol{B}^{\\top} \\otimes \\boldsymbol{A}\\right) \\operatorname{diag}\\left(\\operatorname{vec}\\left(\\boldsymbol{S}^{2}\\right)\\right)\\left(\\boldsymbol{B} \\otimes \\boldsymbol{A}^{\\top}\\right)\n$$\n\n(P2) $\\operatorname{det}(\\Sigma)=\\operatorname{det}(\\boldsymbol{A})^{2 p} \\operatorname{det}(\\boldsymbol{B})^{2 n} \\prod_{i j} \\boldsymbol{S}_{i j}^{2}$\n(P3) $\\operatorname{Tr}(\\Sigma)=\\sum_{i j}\\left(\\boldsymbol{A}^{2} \\boldsymbol{S}^{2} \\boldsymbol{B}^{2}\\right)_{i j}$\n\nSee Appendix B for the derivation and interpretation of the property. Naive implemetations of this can be inefficient and numerically unstable, as the entropy term involves computing the log-determinant of $\\boldsymbol{A}$ and $\\boldsymbol{B}$, requiring the standard automatic differentiation libraries to resort to singular value decomposition when the matrix is near-singular. Thus, we choose to parameterize $\\boldsymbol{A}$ and $\\boldsymbol{B}$ as lower triangular matrices ${ }^{4}$ with ones on the diagonal, leaving the uncertainty to be modeled by $\\boldsymbol{S}$. This means $\\operatorname{det}(\\Sigma)=\\prod_{i j} \\boldsymbol{S}_{i j}^{2}$.\n\nSimulation. To validate the limited expressiveness of kronecker product, we randomly initialize a target density $p$ to be a multivariate Gaussian with mean zero, and covariance being the square of a random standard Gaussian matrix. We choose the dimensionality $d$ of the Gaussian such that it can be decomposed into a product of integers, and parameterize $q$ using independent Gaussian (dubbed Diag), the Kronecker product with diagonal $A$ and $B$ (K-Diag), and the Kronecker product with elementwise scaling (K-Linear). We minimize $D_{\\mathrm{KL}}(q \\| p)$; see Figure 1 for the results. We also conduct the same experiment with 3D tensors (instead of matrices). We see that K-Diag consistently underperforms when compared to Diag, which indicates parameter sharing does restrict the family of distributions it can represent, and K-Linear is consistently better as it captures some correlation.\n\n### 3.2 Nonlinear Kronecker Flow\n\nIn this section, we generalize the Kronecker product to more general non-linear mappings. In Appendix C, we make a connection to non-decreasing triangle maps (Villani, 2008) that are general enough to model any probability distributions.\n\nFirst, notice that left-multiplying $\\boldsymbol{E}$ by $\\boldsymbol{A}$ amounts to introducing linear correlation among the $n$ rows of $\\boldsymbol{E}$, applied to each of the $p$ columns. Likewise, rightmultiplying $\\boldsymbol{E}$ by $\\boldsymbol{B}$ amounts to correlating column entries of each row of $\\boldsymbol{E}$. Inspired by this, we consider applying an invertible mapping to each row of the random weight matrix, and another invertible mapping to each column of the matrix. We call this the Kronecker Flow ${ }^{5}$.\n\n[^0]\n[^0]:    ${ }^{4}$ This is achieved by masking.\n    ${ }^{5}$ To differentiate this from K-Linear from the previous section, we refer to using non-linear $\\boldsymbol{g}$ as K-Nonlinear."
    },
    {
      "markdown": "![img-0.jpeg](img-0.jpeg)\n\nFigure 1: Minimizing KL divergence between $q$ and a randomly initialized distribution $p$. X-axis indicates the shape of the random matrix/tensor, sorted according to the dimensionality. The shaded area is the error bar with 0.1 -standard deviation away from the mean performance, averaged across 25 trials.\n\nSpecifically, let $\\boldsymbol{g}_{A}: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{n}$ and $\\boldsymbol{g}_{B}: \\mathbb{R}^{p} \\rightarrow \\mathbb{R}^{p}$ be invertible mappings. We define the matrix-matrix function $\\boldsymbol{G}: \\mathbb{R}^{n \\times p} \\rightarrow \\mathbb{R}^{n \\times p}$ as $\\boldsymbol{G}_{B}\\left(\\boldsymbol{G}_{A}\\left(\\boldsymbol{E}^{\\top}\\right)^{\\top}\\right)$, with the following batch-operations (for $i \\in[n]$ and $j \\in[p]$ ):\n\n$$\n\\boldsymbol{G}_{A}\\left(\\boldsymbol{E}^{\\top}\\right)_{j}:=\\boldsymbol{g}_{A}\\left(\\boldsymbol{E}_{: j}\\right) \\quad \\boldsymbol{G}_{B}(\\boldsymbol{E})_{i:}:=\\boldsymbol{g}_{B}\\left(\\boldsymbol{E}_{i:}\\right)\n$$\n\nIt is easy to verify that $\\boldsymbol{G}$ is invertible. Due to the partial dependency of $\\boldsymbol{G}_{A}$ and $\\boldsymbol{G}_{B}$, the Jacobians of the vectorized forms (after proper permutation) are block-diagonal, so we have\n\n$$\n\\begin{aligned}\n& \\operatorname{det} \\frac{\\partial \\operatorname{vec}(\\boldsymbol{G}(\\boldsymbol{E}))}{\\partial \\operatorname{vec}(\\boldsymbol{E})} \\\\\n& \\quad=\\prod_{j \\in[p]} \\operatorname{det} \\frac{\\partial \\boldsymbol{g}_{A}\\left(\\boldsymbol{E}_{: j}\\right)}{\\partial \\boldsymbol{E}_{: j}} \\cdot \\prod_{i \\in[n]} \\operatorname{det} \\frac{\\partial \\boldsymbol{g}_{B}\\left(\\boldsymbol{G}_{A}\\left(\\boldsymbol{E}^{\\top}\\right)_{: i}\\right)}{\\partial \\boldsymbol{G}_{A}\\left(\\boldsymbol{E}^{\\top}\\right)_{: i}}\n\\end{aligned}\n$$\n\nIn practice, we use the volume preserving version of RealNVP (Dinh et al., 2016) and inverse autoregressive flow (IAF) (Kingma et al., 2016) to parameterize $\\boldsymbol{g}_{A}$ and $\\boldsymbol{g}_{B}$ for our experiments ${ }^{6}$. The K-Linear from the previous section can be thought of as using a linear map as $\\boldsymbol{g}_{A}$ and $\\boldsymbol{g}_{B}$.\n\n## 4 Concentration of empirical KL with normalizing flows\n\nIn their study, Dziugaite and Roy (2017a) use independent Gaussian for $q$ to minimize the McAllester\n\n[^0]bound, so they can compute the KL between Gaussians analytically. This is no longer feasible when we use more flexible families for $q$, such as normalizing flows. Moreover, a Monte Carlo estimate might result in underestimating the bound after inverting the KL between Bernoullis on the LHS of Equation 2 (which is a concave function; see Appendix A of Reeb et al. (2018) for an illustration). This necessitates a high probability bound on the concentration of the empirical estimate.\n\nIn Section 2.3, we have established $D_{\\mathrm{KL}}(q \\| p)$ can be written in the following form\n$\\mathbb{E}_{\\boldsymbol{\\epsilon}}\\left[\\log \\mathcal{N}(\\boldsymbol{\\epsilon} ; \\mathbf{0}, \\boldsymbol{I})-\\log \\left|\\operatorname{det} \\frac{\\partial \\boldsymbol{g}_{\\boldsymbol{\\epsilon}}(\\boldsymbol{\\epsilon})}{\\partial \\boldsymbol{\\epsilon}}\\right|-\\log \\mathcal{N}\\left(\\boldsymbol{g}_{\\bar{\\psi}}(\\boldsymbol{\\epsilon}) ; \\mathbf{0}, \\boldsymbol{I}\\right)\\right]$,\nwhere both $q_{0}$ and $p$ are standard Gaussian (the mean and variance can be absorbed into the invertible mapping $\\boldsymbol{g}$ if this is not the case).\nThe first term in the KL can be computed analytically. The second term usually can be almost surely bounded (e.g. using Block neural autoregressive flows) so that we can use Hoeffding-type concentration or it can simply be made zero using e.g. volume preserving flows. The challenge now lies in the third term, which has a quadratic form $\\frac{1}{2}\\|\\boldsymbol{g}(\\boldsymbol{\\epsilon})\\|^{2}$, neglecting the normalizing constant.\n\nNow assume $\\boldsymbol{g}$ is a $L_{0}$-Lipschitz ${ }^{7}$. Let $g(\\boldsymbol{\\epsilon})=$ $\\frac{1}{\\sqrt{2}}\\|\\boldsymbol{g}(\\boldsymbol{\\epsilon})\\|$. Then $g$ is $L_{0} / \\sqrt{2}$-Lipschitz:\n\n[^1]\n[^0]:    ${ }^{6}$ We also experimented with Block NAF by De Cao et al. (2019), but did not include it in the manuscript: its performance was similar to IAF, but it is much slower to sample from (we adapted the open implementation from De Cao et al. (2019)).\n\n[^1]:    ${ }^{7}$ The following flows are all Lipschitz (with Lipschitz activation functions): volume preserving version of Dinh et al. (2016); Kingma et al. (2016), Berg et al. (2018), Behrmann et al. (2018), De Cao et al. (2019), etc."
    },
    {
      "markdown": "$$\n\\begin{aligned}\n\\left|g\\left(\\epsilon_{1}\\right)-g\\left(\\epsilon_{2}\\right)\\right|= & \\frac{1}{\\sqrt{2}}\\left|\\left\\|\\boldsymbol{g}\\left(\\epsilon_{1}\\right)\\right\\|-\\left\\|\\boldsymbol{g}\\left(\\epsilon_{2}\\right)\\right\\|\\right| \\\\\n& \\leq \\frac{1}{\\sqrt{2}}\\left\\|\\boldsymbol{g}\\left(\\epsilon_{1}\\right)-\\boldsymbol{g}\\left(\\epsilon_{2}\\right)\\right\\| \\leq \\frac{L_{0}}{\\sqrt{2}}\\left\\|\\boldsymbol{\\epsilon}_{1}-\\boldsymbol{\\epsilon}_{2}\\right\\|\n\\end{aligned}\n$$\n\nThis is key in deriving a tail bound on $g^{2}$, as Lipschitz functions of canonical Gaussian random variables are sub-Gaussians, meaning they have a tail that decays faster than a Gaussian random variable. The following theorem provides a concentration bound for the empirical average of $g^{2}$ similar to that of a Chi-square random variable, as $g^{2}$ (square of a sub-Gaussian) is sub-exponential.\nTheorem 3. Let $g$ be defined as above with a Lipschitz constant $L=L_{0} / \\sqrt{2}$. Let $\\bar{g}^{2}=\\frac{1}{K} \\sum_{k=1}^{K} g_{k}^{2}$. Then the following concentration bound holds\n\n$$\n\\mathbb{P}\\left(\\bar{g}^{2}-\\mathbb{E}\\left[g^{2}\\right]>\\epsilon\\right) \\leq \\exp \\left(-\\frac{K \\epsilon^{2}}{2\\left(4 C^{2}+C \\epsilon\\right)}\\right)\n$$\n\nwhere $C=\\left(6 L^{2}+\\frac{L}{\\sqrt{\\log 2}}\\left(\\sqrt{d}+\\left\\|\\boldsymbol{g}^{-1}(\\mathbf{0})\\right\\|\\right)\\right)^{2}$.\nNote that in practice the empirical KL that we use is inversely scaled by the size of the training set $m$ (see Equation 2), so the Lipschitz constant can be made small in practice to dominate the dimensionality.\n\n## 5 Experiments\n\nWe evaluate our proposed method in the context of two prediction tasks (Section 5.1), PAC-Bayes bound minimization (Section 5.2) and contextual bandit (Section 5.3). For the two prediction tasks, we use the MNIST handwritten digit dataset (Lecun et al., 1998) and CIFAR-10 (Krizhevsky, 2009). See Appendix E for a detailed description.\n\n### 5.1 Classification\n\nOne benefit of Bayesian neural networks compared to the regular ones is that the trade-off between the prior and the likelihood is a form of regularization. In this section, we evaluate the generalization performance of our method applied to Bayesian neural networks. We consider two architectures: LeNet-5 (Lecun et al., 1998) and a modified version VGG-16 (Simonyan and Zisserman, 2014) proposed by Zhang et al. (2017).\nWe first compare to the multiplicative normalizing flow (MNFG) proposed by Louizos and Welling (2017), applying our method to LeNet-5 (see Table 1). Our Diag matches the performance of their FFG (fully factorized Gaussian). K-Diag outperforms Diag in this case, perhaps due to the smaller number of parameters which makes it easier to optimize. K-Nonlinear yields the best generalization error in this case. On the CIFAR-5 experiment (we take the first 5 classes of CIFAR-10), our methods are on par with MNFG.\nSecond, we compare with the noisy K-FAC proposed by Zhang et al. (2017), applying our methods to the larger architecture VGG-16 (see Table 2). Noisy KFAC applies an approximate natural gradient method. Despite this advantage, our methods (K-Linear and K-Nonlinear) have simiar prediction accuracy in the regular setup. We also include the results of data augmentation with horizontal flip and random crop where K-Nonlinear outperforms all the other methods.\n\n### 5.2 PAC Bayes bound minimization\n\nFor the PAC-Bayes bound estimation, we minimize Equation 3. We follow the recipe of Dziugaite and Roy (2017a). We upper bound the zero-one loss by crossentropy divided by $\\log |\\mathcal{Y}|$ (where $|\\mathcal{Y}|$ is the number of classes) to make the upper bound tight. We set the prior to be $\\mathcal{N}\\left(\\Theta_{0}, \\lambda \\boldsymbol{I}\\right)$, where $\\Theta_{0}$ is the initial value of the parameters, and apply a union bound to tune the prior variance $\\lambda$. We also tune the $\\beta$ coefficient as a parameter during training ${ }^{8}$, and report the McAllester bound for comparison (since it is the tightest). For more details, see Dziugaite and Roy (2017a) for reference.\n\nWe test with a multi-layer perceptron with 1 or 2 hidden layers with 600 neurons and LeNet-5, evaluated on the MNIST dataset (see Table 3). For further clarification, we follow the steps of Dziugaite and Roy (2017a) by minimizing the McAllester bound, using Pinsker's inequality to bound the inverse of the Bernoulli KL (which we call the Pinsker bound). Since this bound has a square root in the complexity term, we can only use the Gaussian family with an analytic form of the KL. The result we have is slightly looser than Dziugaite and Roy (2017a) since we have a 10-class problem and they deal with a binary version of MNIST. We see that the bound can indeed be improved by capturing the correlation among the parameters. We then compare to minimizing the Catoni bound, which is slightly looser since the linear relationship between the empirical risk and the KL term penalizes the latter more when the KL is larger. However, by modelling the non-linear dependencies, K-Nonlinear clearly outperforms the other methods (even compared to the ones minimizing the Pinsker bound). This indicates there exists a considerable amount of structure in the parameter space that may explain the gap between the test error and the generalization bound.\n\n[^0]\n[^0]:    ${ }^{8}$ We are allowed to do so since we treat Equation 3 as an optimization objective, rather than report it as a bound. We report the McAllester bound, which holds for any $q$, even if it depends on $\\beta$."
    },
    {
      "markdown": "Table 1: Test error with LeNet (\\%) on MNIST and the first 5 classes of CIFAR-10. First 3 columns are from Louizos and Welling (2017). K-Diag on CIFAR-5 diverged, so we did not include the result.\n\n| Dataset | $\\mathbf{L 2}$ | FFG | MNFG | Diag | K-Diag | K-Linear | K-Nonlinear |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| MNIST | 0.6 | 0.9 | 0.7 | 0.92 | 0.67 | 0.70 | 0.60 |\n| CIFAR-5 | 24 | 22 | 16 | 19.0 | - | 16.8 | 17.4 |\n\nTable 2: Test error with modified version of VGG16 (\\%) on CIFAR10. First 4 columns are from Zhang et al. (2017). $\\mathbf{R}$ means regular training and $\\mathbf{D}$ means training with data augmentation.\n\n| Setup | SGD | KFAC | BBB | Noisy-KFAC | Diag | K-Diag | K-Linear | K-Nonlinear |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| $\\mathbf{R}$ | 18.21 | 17.61 | 17.18 | 14.48 | 17.71 | 16.71 | 14.65 | 14.74 |\n| $\\mathbf{D}$ | 11.65 | 11.11 | 11.69 | 10.65 | 10.69 | 13.65 | 11.35 | 9.88 |\n\nTable 3: PAC-Bayes bound estimation: We minimize the Pinsker bound (an upper bound on the McAllester bound) and the Catani bound using different flows, and estimate the McAllester bound at inference time using Newton's method.\n\n| Bound | Pinsker Bound |  |  |  | Catoni Bound |  |  |  |  | Catoni Bound |  |  |  |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| Flow | Diag |  | K-Linear |  | Diag |  | K-Linear |  | K-Nonlinear |  | D | K-D | K-L | K-N |\n| $L-1$ | 1 | 2 | 1 | 2 | 1 | 2 | 1 | 2 | 1 | 2 |  | LeNet-5 |  |  |\n| $\\mathcal{L}[q]$ | 6.62 | 6.00 | 6.09 | 5.90 | 8.04 | 7.66 | 8.10 | 8.33 | 5.96 | 5.90 | 2.12 | 2.95 | 2.00 | 2.01 |\n| $\\mathcal{L}[q]$ | 6.66 | 6.12 | 5.98 | 5.96 | 7.78 | 7.70 | 7.98 | 8.26 | 5.83 | 5.76 | 2.31 | 2.87 | 1.91 | 2.14 |\n| bound | 23.77 | 25.94 | 21.69 | 25.33 | 24.11 | 26.41 | 22.88 | 26.43 | 20.41 | 22.53 | 10.83 | 12.96 | 10.09 | 10.03 |\n| KL | 5968 | 7829 | 5292 | 7554 | 5001 | 6555 | 4334 | 5996 | 4725 | 5921 | 3177 | 3477 | 2913 | 2873 |\n\nTable 4: Cumulative regret incurred by different algorithms on the bandit benchmarks described in Riquelme et al. (2018). Values reported are the mean over 3 independent trials with standard error of the mean, normalized with respect to the performance of the uniform policy.\n\n| Bandit | SGD | fBNN | Diag | K-Diag | K-Linear | K-Nonlinear |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| Mushroom | $4.06 \\pm 0.71$ | $3.91 \\pm 0.89$ | $2.16 \\pm 0.29$ | $2.41 \\pm 0.73$ | $\\mathbf{1 . 8 5} \\pm \\mathbf{0 . 1 5}$ | $3.47 \\pm 0.47$ |\n| Statlog | $1.29 \\pm 0.20$ | $\\mathbf{0 . 7 3} \\pm \\mathbf{0 . 0 1}$ | $1.01 \\pm 0.01$ | $0.84 \\pm 0.06$ | $0.81 \\pm 0.01$ | $0.79 \\pm 0.04$ |\n| Covertype | $30.01 \\pm 0.21$ | $32.03 \\pm 0.40$ | $28.42 \\pm 0.30$ | $29.19 \\pm 0.16$ | $28.13 \\pm 0.12$ | $\\mathbf{2 8 . 0 6} \\pm \\mathbf{0 . 1 5}$ |\n| Financial | $6.08 \\pm 0.47$ | $7.27 \\pm 1.09$ | $7.43 \\pm 0.57$ | $5.88 \\pm 0.25$ | $5.88 \\pm 0.35$ | $\\mathbf{5 . 7 8} \\pm \\mathbf{0 . 2 8}$ |\n| Jester | $\\mathbf{5 6 . 2 4} \\pm \\mathbf{1 . 9 3}$ | $59.70 \\pm 2.48$ | $59.34 \\pm 2.26$ | $57.17 \\pm 1.81$ | $57.66 \\pm 2.11$ | $57.96 \\pm 2.58$ |\n| Adult | $79.31 \\pm 0.47$ | $84.45 \\pm 0.82$ | $76.32 \\pm 0.09$ | $77.28 \\pm 0.01$ | $\\mathbf{7 5 . 9 4} \\pm \\mathbf{0 . 1 2}$ | $77.30 \\pm 0.26$ |"
    },
    {
      "markdown": "We also notice that, despite the linear relationship, the Catoni bound focuses more on the complexity term than the ELBO. For example, the empirical risks of LeNet-5 in Table 3 are much higher compared to the test loss of Table 1. The reasons are: (1) the optimal $\\beta$ in Equation 3 is larger than 1 (depending on the relative value of the KL), and (2) to properly upper bound the zero-one loss, we scale down the cross-entropy loss by $\\log \\|\\mathcal{Y}\\|$ during optimization. This means a learning algorithm based on a tight PAC-Bayes risk bound cannot overfit by much; see Dziugaite and Roy (2017b) for a recent demonstration. A smaller value of $\\beta$ could bring down the risk and empirical risk, resulting in a looser bound but usually better test performance. This trade-off between test set performance and the tightness of the bound is a general issue for generalization bounds.\n\nOne reason for the interest in PAC-Bayes bounds is that their optimization leads to training algorithms with generalization guarantees. The bounds, however, are considerably looser than held-out estimates. ${ }^{9}$ Our work produces much tighter bounds by building flexible families of distributions on neural network weight matrices.\n\n### 5.3 Contextual bandit\n\nUncertainty modeling lies at the heart of the exploration-exploitation dilemma in sequential decisionmaking. In order to maximize its collected cumulative rewards, an agent should trade off exploring different actions and gaining more knowledge about the reward estimate vs. exploiting the current estimate and allocating resources to the actions that are likely rewarding. Thompson sampling (TS) (Thompson, 1933) is one the popular approaches that deals with the latter tradeoff by maintaining posterior distribution over reward models and randomizing actions on the basis of their probability of being optimal.\n\nIn this section, we investigate the effectiveness of our proposed method for performing an approximate Thompson sampling in the particular setting of contextual bandits. In the latter setting, at each time $t=1 \\ldots T$, the agent sees a $d$-dimensional context $X_{t}$, selects one of the $k$ available actions, $a_{t}$, and earns a reward $r_{t}$ generated by the environment. The agent aims to minimize its cumulative regret defined as $R=\\mathbb{E}\\left[\\sum_{t=1}^{T} r_{t}^{*}-r_{t}\\right]$ where $r_{t}^{*}$ is the highest expected reward given the context $X_{t}$ and the expectation is over the randomness of both environment and the agent's choice of actions.\n\n[^0]We compare different methods on a range of real-world bandit problems introduced by Riquelme et al. (2018). We train the models every 50 time steps for 200 iterations using a batch-size of 512 . We ran each experiment with 3 different random seeds and we report the means and standard errors of cumulative regret normalized with respect to the uniform baseline in the table 4. We include the functional variational Bayesian neural networks (fBNN), recently introduced by Sun et al. (2019) as a baseline, and we use their open sourced implementation of fBNN in the bandit setting. From table 4, we see that across the 6 bandit problems, our proposed method (K-Linear and K-Nonlinear) provides competitive and consistent results. They outperform other baselines in 4 problems out of 6 .\n\n## 6 Conclusion\n\nIn this work, we present the Kronecker Flow, a flowbased method to induce complex distribution inspired by the Kronecker product. Our methods scale to larger architectures such as VGG-16 since it takes advantage of the shape of the parameters. We demonstrate our methods work better than vanilla Kronecker product with diagonal matrices on multiple setups, including classification and approximate Thompson sampling in contexual bandit, and prove to be competitive with existing methods in the Bayesian neural network literature. We are also the first to apply flow-based methods to obtain a tighter numerical generalization bound. Our work shows that the dependencies among network parameters constitute a non-negligible portion of the gap between risk and PAC-Bayes generalization bound.\n\n## Acknowledgement\n\nCWH would like to thank Kris Sankaran for pointing to the TIS inequality for Gaussian concentration, which is a key component in deriving the tail bound on Lipschitz flows.\n\n## References\n\nArora, S., Ge, R., Neyshabur, B., and Zhang, Y. (2018). Stronger generalization bounds for deep nets via a compression approach. arXiv preprint arXiv:1802.05296.\nAzizzadenesheli, K., Brunskill, E., and Anandkumar, A. (2018). Efficient exploration through bayesian deep q-networks. CoRR, abs/1802.04412.\nBartlett, P. L., Foster, D. J., and Telgarsky, M. J. (2017). Spectrally-normalized margin bounds for neural networks. In Advances in Neural Information Processing Systems, pages 6240-6249.\n\n\n[^0]:    ${ }^{9} \\mathrm{~A}$ recent work by Rivasplata et al. (2019) shows PACBayes bound can potentially be made tight; however, we could not reproduce the results and the best bound using Gaussian prior was around $40 \\%$ in their setting."
    },
    {
      "markdown": "Behrmann, J., Duvenaud, D., and Jacobsen, J.-H. (2018). Invertible residual networks. arXiv preprint arXiv:1811.00995.\nBerg, R. v. d., Hasenclever, L., Tomczak, J. M., and Welling, M. (2018). Sylvester normalizing flows for variational inference. arXiv preprint arXiv:1803.05649.\nBlundell, C., Cornebise, J., Kavukcuoglu, K., and Wierstra, D. (2015). Weight uncertainty in neural networks. In Proceedings of The 32nd International Conference on Machine Learning, pages 1613-1622.\nBogachev, V. I., Kolesnikov, A. V., and Medvedev, K. V. (2005). Triangular transformations of measures. Sbornik: Mathematics, 196(3):309-335.\nBoucheron, S., Lugosi, G., and Massart, P. (2013). Concentration inequalities: A nonasymptotic theory of independence. Oxford university press.\nCatoni, O. (2007). Pac-bayesian supervised classification: The thermodynamics of statistical learning. Lecture Notes-Monograph Series, 56:i-163.\nDe Cao, N., Titov, I., and Aziz, W. (2019). Block neural autoregressive flow. arXiv preprint arXiv:1904.04676.\nDinh, L., Sohl-Dickstein, J., and Bengio, S. (2016). Density estimation using real nvp. arXiv preprint arXiv:1605.08803.\nDziugaite, G. K. and Roy, D. M. (2017a). Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. In Proceedings of the 33rd Conference on Uncertainty in Artificial Intelligence.\nDziugaite, G. K. and Roy, D. M. (2017b). Entropy-sgd optimizes the prior of a pac-bayes bound: Generalization properties of entropy-sgd and data-dependent priors. arXiv preprint arXiv:1712.09376.\nGal, Y. and Ghahramani, Z. (2016). Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In international conference on machine learning, pages 1050-1059.\nGermain, P., Bach, F., Lacoste, A., and Lacoste-Julien, S. (2016). Pac-bayesian theory meets bayesian inference. In Advances in Neural Information Processing Systems, pages 1884-1892.\nGermain, P., Lacasse, A., Laviolette, F., and Marchand, M. (2009). Pac-bayesian learning of linear classifiers. In Proceedings of the 26th Annual International Conference on Machine Learning, pages $353-360$. ACM.\nGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. (2014). Generative adversarial nets. In Advances in neural information processing systems.\n\nGraves, A. (2011). Practical variational inference for neural networks. In Shawe-Taylor, J., Zemel, R. S., Bartlett, P. L., Pereira, F., and Weinberger, K. Q., editors, Advances in Neural Information Processing Systems 24, pages 2348-2356. Curran Associates, Inc.\nHernÃ¡ndez-Lobato, J. M. and Adams, R. (2015). Probabilistic backpropagation for scalable learning of bayesian neural networks. In International Conference on Machine Learning, pages 1861-1869.\nHouthooft, R., Chen, X., Chen, X., Duan, Y., Schulman, J., De Turck, F., and Abbeel, P. (2016). Vime: Variational information maximizing exploration. In Advances in Neural Information Processing Systems 29.\n\nHuang, C.-W., Krueger, D., Lacoste, A., and Courville, A. (2018). Neural autoregressive flows. In International Conference on Machine Learning.\nHyvÃ¤rinen, A. and Pajunen, P. (1999). Nonlinear independent component analysis: Existence and uniqueness results. Neural Networks, 12(3).\nIan, O., Benjamin, V. R., and Daniel, R. (2013). (more) efficient reinforcement learning via posterior sampling. In Proceedings of the 26th International Conference on Neural Information Processing Systems, USA. Curran Associates Inc.\nJaini, P., Selby, K. A., and Yu, Y. (2019). Sum-ofsquares polynomial flow. In International Conference on Machine Learning.\nKingma, D. P., Salimans, T., Jozefowicz, R., Chen, X., Sutskever, I., and Welling, M. (2016). Improved variational inference with inverse autoregressive flow. In Advances in Neural Information Processing Systems.\nKingma, D. P., Salimans, T., and Welling, M. (2015). Variational dropout and the local reparameterization trick. In Cortes, C., Lawrence, N. D., Lee, D. D., Sugiyama, M., and Garnett, R., editors, Advances in Neural Information Processing Systems 28, pages 2575-2583. Curran Associates, Inc.\nKrizhevsky, A. (2009). Learning multiple layers of features from tiny images. Technical report.\nKrueger, D., Huang, C.-W., Islam, R., Turner, R., Lacoste, A., and Courville, A. (2017). Bayesian hypernetworks. arXiv preprint arXiv:1710.04759.\nLangford, J. and Seeger, M. (2001). Bounds for averaging classifiers.\nLecun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11).\nLouizos, C. and Welling, M. (2016). Structured and efficient variational deep learning with matrix gaussian"
    },
    {
      "markdown": "posteriors. In International Conference on Machine Learning, pages 1708-1716.\nLouizos, C. and Welling, M. (2017). Multiplicative normalizing flows for variational bayesian neural networks. In International Conference on Machine Learning.\n\nMacKay, D. J. (1992). A practical bayesian framework for backpropagation networks. Neural computation, $4(3): 448-472$.\n\nMcAllester, D. A. (1999). Pac-bayesian model averaging. In COLT, volume 99, pages 164-170. Citeseer.\nMcAllister, R., Gal, Y., Kendall, A., Van Der Wilk, M., Shah, A., Cipolla, R., and Weller, A. (2017). Concrete problems for autonomous vehicle safety: Advantages of bayesian deep learning. In Proceedings of the 26th International Joint Conference on Artificial Intelligence, IJCAI'17, pages 4745-4753. AAAI Press.\n\nMinka, T. et al. (2005). Divergence measures and message passing. Technical report, Technical report, Microsoft Research.\n\nMÃ¼ller, T., McWilliams, B., Rousselle, F., Gross, M., and NovÃ¡k, J. (2018). Neural importance sampling. arXiv preprint arXiv:1808.03856.\n\nNeyshabur, B., Bhojanapalli, S., Mcallester, D., and Srebro, N. (2017). Exploring generalization in deep learning. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R., editors, Advances in Neural Information Processing Systems 30, pages 5947-5956. Curran Associates, Inc.\n\nOsband, I., Van Roy, B., and Wen, Z. (2016). Generalization and exploration via randomized value functions. In Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48, ICML'16, pages 2377-2386. JMLR.org.\n\nPawlowski, N., Brock, A., Lee, M. C. H., Rajchl, M., and Glocker, B. (2017). Implicit Weight Uncertainty in Neural Networks. arXiv e-prints.\n\nReeb, D., Doerr, A., Gerwinn, S., and Rakitsch, B. (2018). Learning gaussian processes by minimizing pac-bayesian generalization bounds. In Advances in Neural Information Processing Systems, pages 33373347.\n\nRezende, D. J. and Mohamed, S. (2015). Variational inference with normalizing flows. In International Conference on Machine Learning.\n\nRiquelme, C., Tucker, G., and Snoek, J. R. (2018). Deep bayesian bandits showdown.\n\nRivasplata, O., Tankasali, V. M., and Szepesvari, C. (2019). Pac-bayes with backprop. arXiv preprint arXiv:1908.07380.\n\nRobbins, H. and Monro, S. (1951). A stochastic approximation method. Ann. Math. Statist., 22(3):400-407.\nSimonyan, K. and Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556.\n\nSun, S., Zhang, G., Shi, J., and Grosse, R. (2019). Functional variational bayesian neural networks. arXiv preprint arXiv:1903.05779.\n\nThompson, W. R. (1933). On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika.\n\nTouati, A., Satija, H., Romoff, J., Pineau, J., and Vincent, P. (2018). Randomized value functions via multiplicative normalizing flows. arXiv preprint arXiv:1806.02315.\n\nTurner, R. E. and Sahani, M. (2011). Two problems with variational expectation maximisation for timeseries models. Bayesian Time series models, 1(3.1):31.\n\nVan den Oord, A., Kalchbrenner, N., Espeholt, L., Vinyals, O., Graves, A., et al. (2016). Conditional image generation with pixelcnn decoders. In Advances in neural information processing systems, pages 47904798.\n\nVillani, C. (2008). Optimal transport: old and new, volume 338. Springer Science \\& Business Media.\n\nZhang, G., Sun, S., Duvenaud, D., and Grosse, R. (2017). Noisy natural gradient as variational inference. arXiv preprint arXiv:1712.02390."
    },
    {
      "markdown": "# A Law of the unconscious statistician \n\nLet $(\\Omega, \\mathcal{F}, \\mathbb{P})$ be our probability space. Let $\\boldsymbol{\\epsilon} \\in \\mathbb{R}^{d}$ be a random variable following the (Lebesgue) density $q_{0}(\\boldsymbol{\\epsilon})=\\frac{\\mathrm{d} \\epsilon_{*} \\mathbb{P}}{\\mathrm{~d} \\mu}$ and $\\boldsymbol{\\epsilon}_{*} \\mathbb{P}$ being its pushforward measure, and write $\\Theta=\\boldsymbol{g}_{\\phi}(\\boldsymbol{\\epsilon}) \\in \\mathbb{R}^{d}$ with $q_{\\phi}=\\frac{\\mathrm{d}\\left(\\boldsymbol{g}_{\\phi} \\circ \\epsilon\\right)_{*} \\mathbb{P}}{\\mathrm{d} \\mu}$ being its density and $\\left(\\boldsymbol{g}_{\\phi} \\circ \\boldsymbol{\\epsilon}\\right)_{*} \\mathbb{P}$ being its pushforward measure, and $A=\\hat{R}_{D}(\\Theta)+\\log q_{\\phi}(\\Theta) \\in \\mathbb{R}$. Then\n\n$$\n\\begin{aligned}\n\\mathbb{E}[A]=\\int_{\\mathbb{R}^{d}} A \\mathrm{~d}\\left(\\boldsymbol{g}_{\\phi} \\circ \\boldsymbol{\\epsilon}\\right)_{*} \\mathbb{P} & =\\int_{\\mathbb{R}^{d}}\\left(\\hat{R}_{D}(\\Theta)+\\log q_{\\phi}(\\Theta)\\right) q_{\\phi}(\\Theta) \\mathrm{d} \\Theta \\\\\n=\\int_{\\mathbb{R}^{d}} A \\circ \\Theta \\mathrm{~d} \\boldsymbol{\\epsilon}_{*} \\mathbb{P} & =\\int_{\\mathbb{R}^{d}}\\left(\\hat{R}_{D}\\left(\\boldsymbol{g}_{\\phi}(\\boldsymbol{\\epsilon})\\right)+\\log q_{\\phi}\\left(\\boldsymbol{g}_{\\phi}(\\boldsymbol{\\epsilon})\\right)\\right) q_{0}(\\boldsymbol{\\epsilon}) \\mathrm{d} \\boldsymbol{\\epsilon} \\\\\n& =\\int_{\\mathbb{R}^{d}}\\left(\\hat{R}_{D}\\left(\\boldsymbol{g}_{\\phi}(\\boldsymbol{\\epsilon})\\right)+\\log q_{0}(\\boldsymbol{\\epsilon})-\\log \\left|\\operatorname{det} \\frac{\\partial \\boldsymbol{g}_{\\phi}(\\boldsymbol{\\epsilon})}{\\partial \\boldsymbol{\\epsilon}}\\right|\\right) q_{0}(\\boldsymbol{\\epsilon}) \\mathrm{d} \\boldsymbol{\\epsilon}\n\\end{aligned}\n$$\n\n## B Derivation and interpretation of Property 1\n\nWe first derive Property 1 algebraically, and give an interpretation that can be genralized to higher dimensional tensor operation. Recall that we have the following givens:\n\n- Assume $\\boldsymbol{E}_{i j} \\stackrel{\\text { i.i.d. }}{\\sim} \\mathcal{N}(0,1)$ is a $n \\times p$ random Gaussian matrix.\n- Assume $\\boldsymbol{A} \\in \\mathbb{R}^{n \\times n}$ and $\\boldsymbol{B} \\in \\mathbb{R}^{p \\times p}$.\n- $\\boldsymbol{S} \\in \\mathbb{R}_{>0}^{n \\times p}$.\n- $\\boldsymbol{M} \\in \\mathbb{R}^{n \\times p}$.\n\nIf we rescale $\\boldsymbol{E}$ elementwise by $\\boldsymbol{S}$ before inducing the column-wise and row-wise correlation, we have: (superscript is Hadamard power)\n\n$$\n\\begin{aligned}\n\\Sigma:=\\operatorname{Var}(\\operatorname{vec}(\\boldsymbol{M}+\\boldsymbol{A}(\\boldsymbol{E} \\circ \\boldsymbol{S}) \\boldsymbol{B})) & =\\operatorname{Var}\\left(\\left(\\boldsymbol{B}^{\\top} \\otimes \\boldsymbol{A}\\right) \\operatorname{vec}(\\boldsymbol{E} \\circ \\boldsymbol{S})\\right) \\\\\n& =\\left(\\boldsymbol{B}^{\\top} \\otimes \\boldsymbol{A}\\right) \\operatorname{diag}\\left(\\operatorname{vec}\\left(\\boldsymbol{S}^{2}\\right)\\right)\\left(\\boldsymbol{B}^{\\top} \\otimes \\boldsymbol{A}\\right)^{\\top} \\\\\n& =\\left(\\boldsymbol{B}^{\\top} \\otimes \\boldsymbol{A}\\right) \\operatorname{diag}\\left(\\operatorname{vec}\\left(\\boldsymbol{S}^{2}\\right)\\right)\\left(\\boldsymbol{B} \\otimes \\boldsymbol{A}^{\\top}\\right)\n\\end{aligned}\n$$\n\nIf $\\boldsymbol{S}$ is a matrix of ones, the RHS equals $\\left(\\boldsymbol{B}^{\\top} \\otimes \\boldsymbol{A}\\right)\\left(\\boldsymbol{B} \\otimes \\boldsymbol{A}^{\\top}\\right)=\\left(\\boldsymbol{B}^{\\top} \\boldsymbol{B}\\right) \\otimes\\left(\\boldsymbol{A} \\boldsymbol{A}^{\\top}\\right)$, which is the covariance of the matrix normal.\n\nGenerally, $\\boldsymbol{S}$ might not be a matrix of ones. But we can still compute the determinant and trace of the covariance matrix (useful in computing KL ):\n\n$$\n\\begin{aligned}\n& \\operatorname{det}(\\Sigma)=\\operatorname{det}(\\boldsymbol{A})^{2 p} \\operatorname{det}(\\boldsymbol{B})^{2 n} \\prod_{i j} \\boldsymbol{S}_{i j}^{2} \\\\\n& \\operatorname{Tr}(\\Sigma)=\\sum_{i} \\Sigma_{i i} \\\\\n& =\\sum_{i} \\sum_{j}\\left(\\boldsymbol{B}^{\\top} \\otimes \\boldsymbol{A}\\right)_{i j} \\operatorname{vec}\\left(\\boldsymbol{S}^{2}\\right)_{j}\\left(\\boldsymbol{B} \\otimes \\boldsymbol{A}^{\\top}\\right)_{j i} \\\\\n& =\\sum_{i} \\sum_{j}\\left(\\boldsymbol{B}^{2 \\top} \\otimes \\boldsymbol{A}^{2}\\right)_{i j} \\operatorname{vec}\\left(\\boldsymbol{S}^{2}\\right)_{j} \\\\\n& =\\sum_{i}\\left(\\left(\\boldsymbol{B}^{2 \\top} \\otimes \\boldsymbol{A}^{2}\\right) \\operatorname{vec}\\left(\\boldsymbol{S}^{2}\\right)\\right)_{i} \\\\\n& =\\sum_{i j}\\left(\\boldsymbol{A}^{2} \\boldsymbol{S}^{2} \\boldsymbol{B}^{2}\\right)_{i j}\n\\end{aligned}\n$$"
    },
    {
      "markdown": "Interpretation of the determinant and trace. The determinant measures the change in volume due to the linear map. Since each operation (elementwise multiplication with $\\boldsymbol{S}$, left-multiplication with $\\boldsymbol{A}$, and rightmultiplication with $\\boldsymbol{B}$ ) is an invertible map, the determinant of the composition is a product of determinants. After elementwise multiplication with $\\boldsymbol{S}$ (hence $\\prod \\boldsymbol{S}_{i j}$ ) ${ }^{10}$, we apply the same linear map $\\boldsymbol{A}$ to the columns of $(\\boldsymbol{E} \\circ \\boldsymbol{S})$; in vector form, this corresponds to left-multiplication with a block diagonal of $p \\boldsymbol{A}$ 's, hence $\\operatorname{det}(\\boldsymbol{A})^{p}$. The same reasoning explains $\\operatorname{det}(\\boldsymbol{B})^{n}$.\nThe trace of the covariance can be written as $\\operatorname{Tr}(\\Sigma)=\\sum_{i j} \\operatorname{Var}\\left(\\boldsymbol{W}_{i j}\\right)$, i.e. the sum of marginal variances. Each of the $\\boldsymbol{W}_{i j}$ is a linear combination of the entries of $\\boldsymbol{E}$, which have unit variance and are uncorrelated, so by the additive property of variance of sum of uncorrelated random variables and the quadratic scaling property of variance, $\\operatorname{Var}\\left(\\boldsymbol{W}_{i j}\\right)=A_{i \\cdot}^{2} \\boldsymbol{S}^{2} \\boldsymbol{B}_{j \\cdot}^{2}$.\n\n# C Connection to triangular maps. \n\nMuch of the recent work on normalizing flows has been dedicated to inverse autoregressive transformations (Kingma et al., 2016; Huang et al., 2018; MÃ¼ller et al., 2018; De Cao et al., 2019; Jaini et al., 2019), as they are general enough to induce any density function (HyvÃ¤rinen and Pajunen, 1999; Bogachev et al., 2005; Villani, 2008). When such transformations are used for $\\boldsymbol{g}_{A}$ and $\\boldsymbol{g}_{B}$, the overall transformation $\\boldsymbol{G}$ is also a triangle map, since $\\boldsymbol{G}(\\boldsymbol{E})_{i j}$ depends on $\\boldsymbol{E}_{i^{\\prime} j^{\\prime}}$ for $i^{\\prime} \\leq i$ and $j^{\\prime} \\leq j$. Such a function has some \"blind spots\" similar to the ones discovered by Van den Oord et al. (2016). One avenue for improvement is to design a transformation that increase the connectivity. Another avenue for improvement is to condition each (row-wise or column-wise) transformation on a learnable embedding of the row/column, such that each row/column is transformed by a slightly different function than another ${ }^{11}$.\n\n## D Tail bound of empirical KL\n\nWe begin with some preliminaries and lemmas in Section D.1, and prove the main result in Section D.2.\n\n## D. 1 Basic tail bounds and Bernstein inequality\n\nThe tools developed in this section is to translate the coefficients (such as variance) of sub-Gaussian random variables and sub-exponential random variables. We start with the definition of sub-Gaussians:\nDefinition 1. We write $X \\sim \\operatorname{sub} \\mathcal{N}\\left(L^{2}\\right)$ if $X$ is a random variable satisfying\n\n$$\n\\mathbb{P}(|X|>t) \\leq 2 \\exp \\left(-\\frac{t^{2}}{2 L^{2}}\\right)\n$$\n\nWe write $\\Gamma(\\cdot)$ as the Gamma function: $\\Gamma(z)=\\int_{0}^{\\infty} e^{-u} u^{z-1} \\mathrm{~d} u$. Note that for positive integers $z, \\Gamma(z)=(z-1)$ !. The following lemma gives an upper-bound on the moments of a sub-Gaussian.\nLemma 4. For $X \\sim \\operatorname{sub} \\mathcal{N}\\left(L^{2}\\right)$, for any integer $p \\geq 1, \\mathbb{E}\\left[|X|^{p}\\right] \\leq\\left(2 L^{2}\\right)^{p / 2} p \\Gamma(p / 2)$.\nProof. Since $|X|^{p}$ is non-negative, similar to Lemma 6, we have\n\n$$\n\\begin{aligned}\n\\mathbb{E}\\left[|X|^{p}\\right] & =\\int_{0}^{\\infty} \\mathbb{P}\\left(|X|^{p} \\geq s\\right) \\mathrm{d} s=\\int_{0}^{\\infty} \\mathbb{P}(|X| \\geq t) p t^{p-1} \\mathrm{~d} t \\\\\n& \\leq 2 p \\int_{0}^{\\infty} e^{-t^{2} / 2 L^{2}} t^{p-1} \\mathrm{~d} t=\\leq p\\left(2 L^{2}\\right)^{p / 2} \\int_{0}^{\\infty} e^{-u} u^{p / 2-1} \\mathrm{~d} u=p\\left(2 L^{2}\\right)^{p / 2} \\Gamma(p / 2)\n\\end{aligned}\n$$\n\nwhere we let $s=t^{p}$ and $u=t^{2} / 2 L^{2}$.\nThe following definition is the main tool for translating the coefficients.\n\n[^0]\n[^0]:    ${ }^{10}$ The power 2 comes from the fact that we are looking at the determinant of the covariance. Direct computation of the likelihood involves $\\frac{1}{2} \\log \\operatorname{det}(\\Sigma)$, which is equivalent to the log-determinant of the invertible map.\n    ${ }^{11}$ We try this idea in the preliminary stage of the project, but find it harder to optimize. This is potentially due to the extra parameters that have to be learned."
    },
    {
      "markdown": "Definition 2. Let $X$ be a random variable. For integer $k \\geq 1$, define the $\\psi_{k}$-Orlicz norm as\n\n$$\n\\|X\\|_{\\psi_{k}}:=\\inf \\left\\{t>0: \\mathbb{E}\\left[\\exp \\left(|X|^{k} / t^{k}\\right)\\right] \\leq 2\\right\\}\n$$\n\ni.e, the smallest constant $t>0$ for which the super-exponential moment of $X^{k} / t^{k}$ is bounded by 2. The Orlicz norm is infinity if there's no finite $t$ for which $\\mathbb{E}\\left[\\exp \\left(|X|^{k} / t^{k}\\right)\\right]$ exists.\n\nIt is easy to verify that the Orlicz norm is indeed a norm. We call $\\|\\cdot\\|_{\\psi_{2}}$ the sub-Gaussian norm, and $\\|\\cdot\\|_{\\psi_{1}}$ the sub-exponential norm. Note that $\\left\\|X^{2}\\right\\|_{\\psi_{1}}=\\|X\\|_{\\psi_{2}}^{2}$.\n\nThe following lemma upper bounds the sub-Gaussian norm by its variance.\nLemma 5. If $X \\sim \\operatorname{sub} \\mathcal{N}\\left(L^{2}\\right),\\|X\\|_{\\psi_{2}} \\leq 6 L^{2}$.\n\nProof. By power series expansion of the exponential function,\n\n$$\n\\mathbb{E}\\left[e^{c X^{2}}\\right]=1+\\sum_{p=1}^{\\infty} \\frac{c^{p} \\mathbb{E}\\left[X^{2 p}\\right]}{p!} \\leq 1+\\sum_{p=1}^{\\infty} \\frac{c^{p}}{p!} 2\\left(2 L^{2}\\right)^{p} p!=1+2 \\sum_{p=1}^{\\infty}\\left(2 c L^{2}\\right)^{p}\n$$\n\nwhere we used Lemma 4 for the inequality. The RHS converges and is equal to 2 if $c=1 / 6 L^{2}$. Thus, $\\|X\\|_{\\psi_{2}} \\leq 6 L^{2}$.\n\nThe following lemma gives an upper bound on the moments of sub-exponential random variables.\nLemma 6. If for some $C>0, \\mathbb{E}[\\exp (|X| / C)] \\leq 2$, then $\\mathbb{E}\\left[|X|^{p}\\right] \\leq 2 C^{p} p!$.\n\nProof. By Markov's inequality,\n\n$$\n\\mathbb{P}(|X|>t) \\leq \\frac{\\mathbb{E}[\\exp (|X| / C)]}{\\exp (t / C)} \\leq 2 e^{-t / C}\n$$\n\nFor $p \\in \\mathbb{Z}^{+}$, since $|X|^{p}$ is non-negative,\n\n$$\n\\begin{aligned}\n\\mathbb{E}[|X|^{p}] & =\\int_{0}^{\\infty} \\mathbb{P}\\left(|X|^{p} \\geq s\\right) d s=\\int_{0}^{\\infty} \\mathbb{P}(|X| \\geq t) p t^{p-1} d t \\\\\n& \\leq 2 p \\int_{0}^{\\infty} e^{-t / C} t^{p-1} d t=2 p C^{p} \\int_{0}^{\\infty} e^{-u} u^{p-1} d u=2 p C^{p} \\Gamma(p)=2 C^{p} p!\n\\end{aligned}\n$$\n\nwhere we let $s=t^{p}$ and $u=t / C$.\n\nFinally, we derive a concentration bound for sub-exponential random variables.\nTheorem 7. (Bernstein's inequality for sub-exponential random variables) Let $\\left(X_{i}\\right)_{i \\in[n]}$ be independent real-valued random variables satisfying $\\mathbb{E}[\\exp (|X| / C)] \\leq 2$ for some $C>0$, with mean $\\mu_{X}=\\mathbb{E}[X]$, and let $\\bar{X}=\\frac{1}{n} \\sum_{i=1}^{n} X_{i}$. Then, for any $\\epsilon>0$, the following concentration bound holds:\n\n$$\n\\mathbb{P}\\left(\\bar{X}-\\mu_{X}>\\epsilon\\right) \\leq \\exp \\left(-\\frac{n \\epsilon^{2}}{2\\left(4 C^{2}+C \\epsilon\\right)}\\right)\n$$\n\nProof. Let $\\nu=4 n C^{2}$ and $c=C$. Then by Lemma $6, \\sum_{i=1}^{n} \\mathbb{E}\\left[X_{i}^{2}\\right] \\leq n \\cdot 4 C^{2}=\\nu$ and for integers $p>2$ : $\\sum_{i=1}^{n} \\mathbb{E}\\left[\\left|X_{i}\\right|^{p}\\right] \\leq 2 n C^{p} p!=\\nu C^{p-2} p!/ 2=\\nu c^{p-2} p!/ 2$. Then by Corollary 2.11 of Boucheron et al. (2013), we have\n\n$$\n\\mathbb{P}\\left(\\bar{X}-\\mu_{X}>\\epsilon\\right)=\\mathbb{P}\\left(\\sum_{i=1}^{n}\\left(X_{i}-\\mu_{X}\\right)>n \\epsilon\\right) \\leq \\exp \\left(-\\frac{n \\epsilon^{2}}{2\\left(4 C^{2}+C \\epsilon\\right)}\\right)\n$$"
    },
    {
      "markdown": "# D. 2 Proof of Theorem 3 \n\nSince $\\bar{g}(\\boldsymbol{\\epsilon}):=g(\\boldsymbol{\\epsilon})-\\mathbb{E}[g(\\boldsymbol{\\epsilon})]$ is $L$-Lipschitz, according to Theorem 5.5 and 5.6 of Boucheron et al. (2013), $\\bar{g} \\sim \\operatorname{sub} \\mathcal{N}\\left(L^{2}\\right)$. And we have that\n\n$$\n\\left\\|g^{2}\\right\\|_{\\psi_{1}}=\\|g\\|_{\\psi_{2}}^{2}=\\left\\|\\bar{g}+\\mathbb{E}[g]\\right\\|_{\\psi_{2}}^{2} \\leq\\left(\\|\\bar{g}\\|_{\\psi_{2}}+\\frac{\\mathbb{E}[g]}{\\sqrt{\\log 2}}\\right)^{2}\n$$\n\ndue to triangle inequality of the norm. Now since $g$ is $L$-Lipschitz, its expectation can be bounded by\n\n$$\n\\mathbb{E}[g]=\\mathbb{E}\\left[\\frac{1}{\\sqrt{2}}\\|\\boldsymbol{g}(\\boldsymbol{\\epsilon})-\\mathbf{0}\\|\\right] \\leq L \\mathbb{E}\\left[\\left\\|\\boldsymbol{\\epsilon}-\\boldsymbol{g}^{-1}(\\mathbf{0})\\right\\|\\right] \\leq L\\left(\\mathbb{E}\\left[\\|\\boldsymbol{\\epsilon}\\|\\right]+\\left\\|\\boldsymbol{g}^{-1}(\\mathbf{0})\\right\\|\\right)\n$$\n\nSince $\\boldsymbol{\\epsilon}$ is standard-normally distributed, $\\|\\boldsymbol{\\epsilon}\\|$ follows the chi distribution with $d$ degrees of freedom, which has an expectation that can be upper-bounded using Gautschi's inequality (using Wendel's version of the upper bound):\n\n$$\n\\mathbb{E}[|\\boldsymbol{\\epsilon}||]=\\sqrt{2} \\frac{\\Gamma((d+1) / 2)}{\\Gamma(d / 2)} \\leq \\sqrt{2}\\left(\\frac{d}{2}\\right)^{1 / 2}=\\sqrt{d}\n$$\n\nCombining the above and using Lemma 5, we have\n\n$$\n\\left\\|g^{2}\\right\\|_{\\psi_{1}} \\leq\\left(6 L^{2}+\\frac{L}{\\sqrt{\\log 2}}\\left(\\sqrt{d}+\\left\\|\\boldsymbol{g}^{-1}(\\mathbf{0})\\right\\|\\right)\\right)^{2}\n$$\n\nSetting $C$ to be the RHS and applying Theorem 7 yield the desired result.\n\n## E Experimental Details\n\nFor the predictive tasks (Section 5.1), we use a cosine annealing schedule for the learning rate, scaling down to 0.01 of the initial learning rate, and pretrain a deterministic network for 10 epochs using the Adam optimizer with a learning rate of 0.001 , to initialize the mean of the Gaussian $q_{0}$, and train $q$ for 200 epochs.\n\nLeNet-5 MNIST. We use a linear annealing schedule of the $\\beta$ coefficient (from 0 back to 1 ) for 50,000 iterations. We use the Adam optimizer with a learning rate of 0.0005 . The result we get for K-Linear uses polyak averaging with exponential decay coefficient 0.995 . We use the volume preserving version of RealNVP for the K-Nonlinear. We use the standard Gaussian prior for $p$.\n\nLeNet-5 CIFAR-5 We use the same architecture as Louizos and Welling (2017) (192 convolutional kernels and 1,000 hidden units for the fully connected layers). We use a linear annealing schedule of the $\\beta$ coefficient (from 0 back to 1) for 20,000 iterations for Diag, and no annealing for K-Linear and K-Nonlinear. We use the Adam optimizer with a learning rate of $0.0003,0.0003,0.0005$ for Diag, K-Linear and K-Nonlinear, respectively. We use the volume preserving version of RealNVP for K-Nonlinear. We use the standard Gaussian prior for $p$.\n\nVGG-16 CIFAR-10 We use the modified version of VGG-16 proposed by Zhang et al. (2017). We use a learning rate of 0.0005 for all experiments but K-Nonlinear in the regular setup (where we use 0.001 ). We use the isotropic Gaussian prior with variance being 0.1 , and set $\\beta$ to be $[0.5,0.1,0.1,0.5]$ in the regular setup and $[0.5$, $0.1,0.1,0.1]$ in the data augmented setup for Diag, K-Diag, K-Linear, and K-Nonlinear, respectively. We use the volume preserving version of RealNVP for the K-Nonlinear.\n\nPAC-Bayes MLP We follow the same steps as Dziugaite and Roy (2017a), except we did not discretize the prior variance after tuning. In practice this does not affect the bound much. We also did not initialize the mean of $q_{0}$ in our setup using SGD for our experiments. We train the stochastic network for 300 epochs, with a learning rate of 0.002 . The bound holds with probability at least 0.965 over the choice of prior and the training set. The $b$ and $c$ coefficients in Dziugaite and Roy (2017a) are set as 100 and 0.1 . We use the volume preserving version of IAF for the K-Nonlinear.\n\nPAC-Bayes LeNet-5 The same setup as PAC-Bayes MLP, except with polyak averaging with coefficient 0.995 . We use the volume preserving version of IAF for the K-Nonlinear."
    },
    {
      "markdown": "Bandit Benchmark All the models share the same architechture: one hidden layer with 50 units. We use the volume preserving version of RealNVP for K-NonLinear. We train models every 50 time steps for 200 training iterations using a batch-size of 512 .\n\nTable 5: Description of bandit problem: number of actions and number of contexts used for experiments. Comparing to Riquelme et al. (2018) benchmark, we restrict ourself to 50000 contexts for Covertype instead of 150000 contexts.\n\n| Bandit problem | number of actions | number of contexts |\n| :--: | :--: | :--: |\n| Mushroom | 2 | 50000 |\n| Statlog | 7 | 43500 |\n| Covertype | 7 | 50000 |\n| Financial | 8 | 3713 |\n| Jester | 8 | 19181 |\n| Adult | 14 | 45222 |\n\nTable 6: Additional results: Cumulative regret incurred by different algorithms on the bandit benchmarks described in Riquelme et al. (2018). Values reported are the mean over 5 independent trials with standard error of the mean, normalized with respect to the performance of the uniform policy. We use the same hyperparameters for different algorithms without any finetuning: learning rate $=0.0001$ and 100 training epochs.\n\n| Bandit | SGD | Diag | K-Diag | K-Linear | K-Nonlinear |\n| :--: | :--: | :--: | :--: | :--: | :--: |\n| Mushroom | $\\mathbf{1 . 8 2} \\pm \\mathbf{0 . 5 3}$ | $2.12 \\pm 0.13$ | $2.12 \\pm 0.47$ | $2.20 \\pm 0.18$ | $3.55 \\pm 0.74$ |\n| Statlog | $3.31 \\pm 1.27$ | $4.12 \\pm 0.16$ | $\\mathbf{1 . 2 3} \\pm \\mathbf{0 . 0 6}$ | $3.49 \\pm 0.16$ | $1.33 \\pm 0.02$ |\n| Covertype | $31.70 \\pm 0.17$ | $34.64 \\pm 0.16$ | $30.82 \\pm 0.21$ | $32.84 \\pm 0.16$ | $\\mathbf{2 9 . 2 4} \\pm \\mathbf{0 . 1 0}$ |\n| Financial | $20.31 \\pm 2.24$ | $27.60 \\pm 1.37$ | $\\mathbf{1 1 . 8 3} \\pm \\mathbf{0 . 6 7}$ | $25.10 \\pm 1.10$ | $13.10 \\pm 0.36$ |\n| Jester | $56.90 \\pm 1.20$ | $59.26 \\pm 1.38$ | $57.22 \\pm 1.35$ | $58.16 \\pm 1.20$ | $\\mathbf{5 6 . 8 8} \\pm \\mathbf{1 . 9 1}$ |\n| Adult | $78.70 \\pm 0.46$ | $79.16 \\pm 0.19$ | $77.30 \\pm 0.18$ | $\\mathbf{7 6 . 9 8} \\pm \\mathbf{0 . 0 4}$ | $77.83 \\pm 0.19$ |"
    },
    {
      "markdown": "![img-1.jpeg](img-1.jpeg)\n\nFigure 2: Posterior predictive (with 20 samples) on rotated MNIST digit 3 and 5. Top left: Diag; top right: K-Diag; bottom left: K-Linear; bottom right: K-Nonlinear."
    }
  ],
  "usage_info": {
    "pages_processed": 16,
    "doc_size_bytes": 1500588
  },
  "_metadata": {
    "model_version": "0.0.0",
    "llm_model": "mistral-ocr-2505",
    "query_data": {
      "pdf": "/Users/satyaortiz-gagne/CODE/paperext/data/cache/fulltext/8140724233d8ba06aeeeaad2a8357af4/fulltext.pdf"
    },
    "model_id": "parsepdf"
  }
}