{
  "pages": [
    {
      "markdown": "# Expressiveness and Learning of Hidden Quantum Markov Models \n\nSandeesh Adhikary*<br>University of Washington\n\nGeoff Gordon<br>Microsoft Research, Montréal\n\n\n#### Abstract\n\nExtending classical probabilistic reasoning using the quantum mechanical view of probability has been of recent interest, particularly in the development of hidden quantum Markov models (HQMMs) to model stochastic processes. However, there has been little progress in characterizing the expressiveness of such models and learning them from data. We tackle these problems by showing that HQMMs are a special subclass of the general class of observable operator models (OOMs) that do not suffer from the negative probability problem by design. We also provide a feasible retraction-based learning algorithm for HQMMs using constrained gradient descent on the Stiefel manifold of model parameters. We demonstrate that this approach is faster and scales to larger models than previous learning algorithms.\n\n\n## 1 Introduction and Related Work\n\nClassical probabilistic graphical models provide a principled framework for Bayesian reasoning, and there has been much interest in extending this framework by incorporating the mathematical formalism of quantum mechanics (Leifer and Poulin, 2008; Yeang, 2010; Leifer and Spekkens, 2013; Warmuth and Kuzmin, 2014). Hidden quantum Markov models (HQMMs) (Monras et al., 2010; Clark et al., 2015; Srinivasan et al., 2018b), have been some of the more well-investigated models; recent work by Srinivasan et al. (2018b) showed that every finitedimensional hidden Markov model (HMM) can also be modeled by a finite-dimensional HQMM, and empirically demonstrated some theoretical advantages of HQMMs over HMMs. A major motivation for investigating such\n\n## Siddarth Srinivasan* <br> Microsoft Research, Montréal <br> Georgia Institute of Technology\n\nByron Boots<br>University of Washington\n\n'quantum models' has been the promise of a more general and expressive class of probabilistic models. Yet, a clear characterization of the expressiveness of these models and a practical learning algorithm has remained lacking. These are precisely the problems we tackle in this paper.\n\nOur theoretical exploration of HQMMs is primarily centered around their relationship to the observable operator models (OOMs) developed by Jaeger (2000). OOMequivalents have been independently developed and are also referred to in the literature as uncontrolled predictive state representations (PSRs) (Singh et al., 2004), linearly dependent processes (Ito et al., 1992), and stochastic weighted automata (Balle et al., 2014; Thon and Jaeger, 2015). OOMs can be seen as a generalization of the well-known hidden Markov models (Rabiner, 1986), but despite their generality they lack a constructive definition. A valid OOM must never produce a negative probability for a sequence of observations, yet it is undecidable (Wiewiora, 2007) whether or not candidate set of OOM parameters will yield negative probabilities. This is known as the negative probability problem (NPP) of OOMs, and must be handled with heuristics in practice (Cohen et al., 2013). An alternative approach is to construct models that avoid the NPP by design, such as norm-observable operator models (NOOMs) (Zhao and Jaeger, 2010a) or quadratic weighted automata (Bailly, 2011). While NOOMs can simulate processes that no finite-dimensional HMM could model (such as the 'probability clock' (Zhao and Jaeger, 2010a)), it is unclear whether they have the broad expressiveness of OOMs; it isn't even known if they contain HMMs as a subclass. In this context, we make three main theoretical contributions in this paper: (i) we show how HQMMs can be seen as a generalization of NOOMs, (ii) we formulate the Liouville representation of HQMMs which uniquely characterizes the model and allows for direct comparison between HQMMs, and (iii) we show that every finite-dimensional HQMM is equivalent to a finite-dimensional OOM, with the special\n\n[^0]\n[^0]:    * denotes equal contribution"
    },
    {
      "markdown": "property that we can characterize the valid initial states as the spectraplex of Hermitian PSD matrices with trace 1.\nWe also present results on learning these models from data. We use the Kraus operator parameterization of HQMMs using matrices $\\left\\{\\mathbf{K}_{i}\\right\\}$ that satisfy the constraint $\\sum_{i} \\mathbf{K}_{i}^{\\dagger} \\mathbf{K}_{i}=\\mathbb{1}$. Stacking the operators $\\mathbf{K}_{i}$ vertically to form a matrix $\\boldsymbol{\\kappa}$, the constraint can be re-written as $\\boldsymbol{\\kappa}^{\\dagger} \\boldsymbol{\\kappa}=\\mathbb{1}$. The existing approach to learning HQMMs (Srinivasan et al., 2018b) yields feasible parameters by starting with an initial guess $\\kappa$ and iteratively finding unitary transformations that increase the likelihood of the data. However, this method is inefficient, often gets trapped in poor optima, and can only handle a small number of hidden states. The absence of a practical learning algorithm has been a bottleneck in the development of these models (Schuld et al., 2015). Our primary experimental contribution in this paper is the application and analysis of a viable approach to the learning problem: since $\\boldsymbol{\\kappa}$ lies on the Stiefel manifold (Stiefel, 1936; Edelman et al., 1998), we can directly learn feasible parameters by constraining gradient updates to lie on the manifold using a well-known retraction-based algorithm (Wen and Yin, 2013). We show that this approach is faster, finds better optima, and can handle more hidden states than the previous method.\n\n## 2 The Expressiveness of HQMMs\n\nIn general, the models we discuss are used to model sequential data and assume an evolving latent state that emits discrete observations at each time-step. We describe HMMs, OOMs, and NOOMs, and show how HQMMs can be derived as a generalization of NOOMs.\n\n### 2.1 Hidden Markov Models\n\nDefinition 1 (HMMs). An n-dimensional Hidden Markov Model with a set of discrete observations $\\mathcal{O}$ is a tuple ( $\\mathbb{R}^{n}, \\mathbf{A}, \\mathbf{C}, \\vec{x}_{0}$ ) where initial state $\\vec{x}_{0}$, transition matrix $\\mathbf{A}$, and emission matrix $\\mathbf{C}$ satisfy the following conditions:\n(i) Non-negative parameters: $\\vec{x}_{0} \\in \\mathbb{R}_{\\geq 0}^{n}, \\mathbf{A} \\in \\mathbb{R}_{\\geq 0}^{n \\times n}$, $\\mathbf{C} \\in \\mathbb{R}_{\\geq 0}^{|\\mathcal{O}| \\times n}$,\n(ii) Normalized initial state: $\\overrightarrow{1^{T}} \\vec{x}_{0}=1$,\n(iii) Column-stochastic operators: $\\overrightarrow{1^{T}} \\mathbf{A}=\\overrightarrow{1^{T}} \\mathbf{C}=\\overrightarrow{1^{T}}$.\n\nHMM belief states are always interpretable as probability distributions over hidden system states.\n\nAt each time-step, we update the belief state and condition on observation using the column-stochastic matrices $\\mathbf{A}$ and $\\mathbf{C}$ respectively:\n\n$$\n\\vec{x}_{t}^{\\prime}=\\mathbf{A} \\vec{x}_{t-1} \\quad \\vec{x}_{t}=\\frac{\\operatorname{diag}\\left(\\mathbf{C}_{\\left(y_{:}\\right)}\\right) \\vec{x}_{t}^{\\prime}}{\\overrightarrow{1^{T}} \\operatorname{diag}\\left(\\mathbf{C}_{\\left(y_{:}\\right)}\\right) \\vec{x}_{t}^{\\prime}}\n$$\n\nwhere $\\operatorname{diag}\\left(\\mathbf{C}_{y_{:}}\\right)$places the row $y$ of matrix $\\mathbf{C}$ in a diagonal matrix. We can also compute the probability of a sequence of observations $\\hat{y}=y_{1}, \\ldots, y_{t}$ from a given belief state $\\vec{x}$ as follows:\n\n$$\nP(\\hat{y})=\\overrightarrow{1^{T}} \\operatorname{diag}\\left(\\mathbf{C}_{\\left(y_{t} \\cdot\\right)}\\right) \\mathbf{A} \\cdots \\operatorname{diag}\\left(\\mathbf{C}_{\\left(y_{1} \\cdot\\right)}\\right) \\mathbf{A} \\vec{x}\n$$\n\n### 2.2 Observable Operator Models\n\nWe describe OOMs as a generalization of HMMs. Observe that the operations above can be equivalently represented by defining observable operators $\\mathbf{T}_{y}=\\operatorname{diag}\\left(\\mathbf{C}_{\\left(y_{:}\\right)}\\right) \\mathbf{A}$ for each observation $y$ :\n\n$$\n\\vec{x}_{t}=\\frac{\\mathbf{T}_{y} \\vec{x}_{t-1}}{\\overrightarrow{1^{T}} \\mathbf{T}_{y} \\vec{x}_{t-1}} \\quad P(\\hat{y})=\\overrightarrow{1^{T}} \\mathbf{T}_{y_{1}} \\cdots \\mathbf{T}_{y_{1}} \\vec{x}\n$$\n\nWe can arrive at OOMs by relaxing constraint (i) in Definition 1 (so entries in $\\vec{x}, \\mathbf{A}, \\mathbf{C}$ can be negative) and requiring only that the model always assign non-negative probabilities to observations. This allows us to define a standard OOM as follows:\nDefinition 2 (Standard OOMs (Jaeger, 2000)). An n-dimensional standard Observable Operator Model with a set of discrete observations $\\mathcal{O}$ is a tuple $\\left(\\mathbb{R}^{n},\\left\\{\\mathbf{T}_{y}\\right\\}_{y \\in \\mathcal{O}}, \\vec{x}_{0}\\right)$ where initial state $\\vec{x}_{0} \\in \\mathbb{R}^{n}$ and observable operators $\\left\\{\\mathbf{T}_{y}\\right\\}_{y \\in \\mathcal{O}} \\in \\mathbb{R}^{n \\times n}$ satisfy the following constraints:\n(i) Normalized initial state: $\\overrightarrow{1^{T}} \\vec{x}_{0}=1$,\n(ii) Normalized marginal over observations: $\\overrightarrow{1^{T}} \\sum_{y \\in \\mathcal{O}} \\mathbf{T}_{y}=\\overrightarrow{1^{T}}$,\n(iii) Non-negative probabilities: $\\overrightarrow{1^{T}} \\mathbf{T}_{y_{t}} \\ldots \\mathbf{T}_{y_{1}} \\vec{x}_{0} \\geq 0$ for all sequences $y_{1} \\ldots y_{t}$.\n\nNote that the above definition is non-constructive since it does not tell us what constraints we could place on model parameters or initial states to satisfy condition (iii) - this is the cost of relaxing the non-negativity constraint.\n\nIn fact, it is undecidable whether a given candidate OOM $\\left(\\mathbb{R}^{n},\\left(\\mathbf{T}_{y}\\right)_{y \\in \\mathcal{O}}, \\vec{x}_{0}\\right)$ satisfying conditions (i)-(ii) will violate condition (iii) (Wiewiora, 2007). This is the root of the infamous negative probability problem (NPP) in OOMs, since we cannot identify whether a learned model will assign negative probabilities to observations.\n\nJaeger (2000) further showed that HMM $\\subset$ OOM using the 'probability clock' OOM which requires an infinitedimensional HMM to model. The non-negativity constraint (i) from Definition 1 forces the largest eigenvalue of an observable operator $\\mathbf{T}_{y}$ of an HMM to be real (by the Perron-Frobenius theorem). However, negative entries in OOMs allow the largest eigenvalue to be complex, which allows the latent states (and hence conditional probabilities) to display oscillatory behaviour. Jaeger (2000) uses this property in their probability clock example."
    },
    {
      "markdown": "A useful conceptual characterization of a candidate OOM with parameters $\\left\\{\\mathbf{T}_{y}\\right\\}_{y \\in \\mathcal{O}}$ is the convex cone of valid initial states it admits, i.e., the initial states for which the model will never assign a negative probability for observations. If there is no such cone, the model is invalid. Indeed, Jaeger (2000) present the following alternative to condition (iii):\nProposition 1 (Jaeger (2000)). A tuple $\\left(\\mathbb{R}^{n},\\left(\\mathbf{T}_{y}\\right)_{y \\in \\mathcal{O}}, \\vec{x}_{0}\\right)$ satisfying conditions (i)-(ii) of Definition 2 is an OOM if and only if there exists a pointed convex cone $K$ such that:\n(i) Initial state is in the cone: $\\vec{x}_{0} \\in K$,\n(ii) Cone is closed under the operators: $\\mathbf{T}_{y} \\vec{x} \\in K$ for all $\\vec{x} \\in K$ and $y \\in \\mathcal{O}$,\n(iii) The sum of entries for any point in the cone is non-negative: $\\overrightarrow{1^{T}} \\vec{x} \\geq 0$ for all $\\vec{x} \\in K$.\n\nConditions (i) and (ii) guarantee that any initial state inside such a cone will stay inside the cone under action of $\\mathbf{T}_{y}$, and condition (iii) guarantees that any state inside the cone will evaluate to a non-negative probability. This characterization can also tell us which OOMs have equivalent HMMs: a finite-dimensional OOM has an equivalent finite-dimensional HMM if and only if $K$ is a $k$-polyhedral cone for some $k$, i.e., it is generated by some finite set of vectors (Jaeger, 2000). Proposition 1 also gives us a recipe to find OOMs that do not suffer from the NPP: select a desired convex cone of valid initial states and construct operators such that the cone is closed under their action.\n\nGeneral OOMs The standard OOMs given in Definition 2 are the original formulation by Jaeger (2000), which is stricter than necessary. Various equivalent formulations have been proposed, including as Sequential Systems (SS) by Thon and Jaeger (2015), uncontrolled predictive state representations (PSRs), or stochastic weighted automata (Balle et al., 2014). In this paper, we refer to the these as 'general OOMs'. The main difference is that the model parameters are no longer constrained to be real, and we don't force the state entries to sum to one; instead the state can be any vector as long as we can use a linear functional $\\sigma$ (which for standard OOMs was fixed to be $\\overrightarrow{1^{T}}$ ) to recover the probabilities. While the model parameters can be defined over arbitrary fields, we define general OOMs over the complex field as this allows us to eventually recover HQMMs.\nDefinition 3 (General OOMs (Thon and Jaeger, 2015)). An n-dimensional general Observable Operator Model with a set of discrete observations $\\mathcal{O}$ is a tuple $\\left(\\mathbb{C}^{n},\\left(\\boldsymbol{\\tau}_{y}\\right)_{y \\in \\mathcal{O}}, \\vec{x}_{0}, \\sigma\\right)$ where initial state $\\vec{x}_{0} \\in \\mathbb{C}^{n}$, observable operators $\\left\\{\\tau_{y}\\right\\}_{y \\in \\mathcal{O}} \\in \\mathbb{C}^{n \\times n}$, and a linear evaluation functional $\\vec{\\sigma} \\in \\mathbb{C}^{n}$ satisfy the following constraints:\n(i) Normalized Initial State: $\\vec{\\sigma}^{\\dagger} \\vec{x}_{0}=1$,\n(ii) Normalized marginal over observations: $\\vec{\\sigma}^{\\dagger} \\boldsymbol{\\tau}_{y_{t}} \\ldots \\boldsymbol{\\tau}_{y_{1}} x_{0}$ $=\\sum_{y \\in \\mathcal{O}} \\vec{\\sigma}^{\\dagger} \\boldsymbol{\\tau}_{y} \\boldsymbol{\\tau}_{y_{t}} \\ldots \\boldsymbol{\\tau}_{y_{1}} \\vec{x}_{0}$ for all sequences $y_{1} \\ldots y_{t}$,\n(iii) Non-negative probabilities: $\\vec{\\sigma}^{\\dagger} \\boldsymbol{\\tau}_{y_{t}} \\ldots \\boldsymbol{\\tau}_{y_{1}} \\vec{x}_{0} \\in[0,1]$ for all sequences $y_{1} \\ldots y_{t}$.\n\nFor such a model, the state update after observing $y \\in \\mathcal{O}$ and computing the probability of that observation are carried out as follows:\n\n$$\n\\vec{x}_{t}=\\frac{\\boldsymbol{\\tau}_{y} \\vec{x}_{t-1}}{\\vec{\\sigma}^{\\dagger} \\boldsymbol{\\tau}_{y} \\vec{x}_{t-1}} \\quad P(\\bar{y})=\\vec{\\sigma}^{\\dagger} \\boldsymbol{\\tau}_{y_{t}} \\cdots \\boldsymbol{\\tau}_{y_{1}} \\vec{x}\n$$\n\nAs shown in Proposition 13 of Thon and Jaeger (2015), every $n$-dimensional general OOM has an equivalent standard OOM that is a similarity transform away, i.e., we can find a similarity transform $\\mathbf{S}$ such that $\\left(\\mathbb{C}^{n},\\left(\\mathbf{S} \\boldsymbol{\\tau}_{y} \\mathbf{S}^{-1}\\right)_{y \\in \\mathcal{O}}, \\mathbf{S} \\vec{u}_{0}, \\vec{\\sigma} \\mathbf{S}^{-1}\\right)=\\left(\\mathbb{C}^{n},\\left(\\mathbf{T}_{y}\\right)_{y \\in \\mathcal{O}}, \\vec{v}_{0}, \\overrightarrow{1^{T}}\\right)$. We will use this equivalence to show that NOOMs and HQMMs are special cases of OOMs. Finally, we note that finite dimensional OOMs are the most expressive class of linear models capable of modeling any stochastic process whose 'system-dynamics' matrix (Singh et al., 2004) has finite rank (Zhao and Jaeger, 2010b). Hence these models are extremely powerful, although the NPP makes it challenging to use these models in practice.\n\n### 2.3 Norm-observable Operator Models\n\nNOOMs represent a class of models designed to avoid the NPP by construction. The central idea is to wrap the output of the model with the non-linear function $\\|\\cdot\\|^{2}$ so that it always returns non-negative values.\nDefinition 4 (NOOMs (Zhao and Jaeger, 2010b)). An n-dimensional Norm Observable Operator Model with a set of discrete observations $\\mathcal{O}$ is a tuple $\\left(\\mathbb{R}^{n},\\left(\\boldsymbol{\\phi}_{y}\\right)_{y \\in \\mathcal{O}}, \\vec{v}_{0}\\right)$ where initial state $\\vec{v}_{0} \\in \\mathbb{R}^{n}$ and observable operators $\\left\\{\\boldsymbol{\\phi}_{y}\\right\\}_{y \\in \\mathcal{O}} \\in \\mathbb{R}^{n \\times n}$ satisfy the following constraints:\n(i) Normalized initial state: $\\left\\|\\vec{v}_{0}\\right\\|_{2}^{2}=1$,\n(ii) Normalized marginal over observations: $\\sum_{y \\in \\mathcal{O}} \\boldsymbol{\\phi}_{y}^{\\dagger} \\boldsymbol{\\phi}_{y}=\\mathbb{1}$.\n\nThe updated state after observing $y \\in \\mathcal{O}$ and the probability of that observation can be computed as\n\n$$\n\\vec{v}_{t}=\\frac{\\boldsymbol{\\phi}_{y} \\vec{v}_{t-1}}{\\left\\|\\boldsymbol{\\phi}_{y_{t}} \\ldots \\boldsymbol{\\phi}_{y_{1}} \\vec{v}\\right\\|} \\quad P(\\bar{y})=\\left\\|\\boldsymbol{\\phi}_{y_{t}} \\ldots \\boldsymbol{\\phi}_{y_{1}} \\vec{v}\\right\\|^{2}\n$$\n\nAlthough any stochastic process can be represented as a NOOM in some inner product space, this space may be infinite dimensional (Zhao and Jaeger, 2010b). For practical purposes, we care about the expressiveness of finite-dimensional NOOMs. Zhao and Jaeger (2010b) showed that $\\mathrm{NOOM} \\subseteq \\mathrm{OOM}$, and once again used the\n\n[^0]\n[^0]:    $\\dagger$ is the complex conjugate transpose"
    },
    {
      "markdown": "ability of a real-valued NOOM operator to have complex eigenvalues in a NOOM probability clock to show that there are finite-dimensional NOOMs that cannot be modeled exactly by finite-dimensional HMMs.\nZhao and Jaeger (2010b) show that despite its non-linear form, NOOMs are equivalent to $n^{2}$-dimensional OOMs, and indeed we will build upon this approach to re-derive HQMMs. Zhao and Jaeger (2010a) use Kronecker product relationships for the 2 -norm (where $\\overrightarrow{\\mathrm{E}}$ is a vectorized identity matrix that implements a matrix trace operation) to show that sequence probabilities in a NOOM from Equation 5 can also be evaluated as:\n\n$$\nP(\\bar{y})=\\overrightarrow{\\mathrm{E}}_{n^{2}}^{T}\\left(\\boldsymbol{\\phi}_{y_{n}} \\otimes \\boldsymbol{\\phi}_{y_{n}}\\right) \\ldots\\left(\\boldsymbol{\\phi}_{y_{1}} \\otimes \\boldsymbol{\\phi}_{y_{1}}\\right)\\left(\\vec{v}_{0} \\otimes \\vec{v}_{0}\\right)\n$$\n\nNow, if we define $\\vec{\\sigma}=\\overrightarrow{\\mathrm{E}}_{n^{2}}, \\boldsymbol{\\tau}_{y}=\\boldsymbol{\\phi}_{y} \\otimes \\boldsymbol{\\phi}_{y}$, and the initial state $\\vec{\\omega}_{0} \\in \\mathbb{R}^{n^{2}}$ as $\\vec{\\omega}_{0}=\\vec{v}_{0} \\otimes \\vec{v}_{0}$, we get a general OOM $\\left(\\mathbb{C}^{n},\\left(\\boldsymbol{\\tau}_{y}\\right)_{y \\in \\mathcal{O}}, \\vec{\\omega}_{0}, \\vec{\\sigma}\\right)$. As shown by Zhao and Jaeger (2010b), this is a similarity transform of a standard OOM, with $\\mathbf{S}=\\mathbb{E}_{n^{2}}+\\frac{1}{n^{2}} \\overrightarrow{\\mathbf{I}}_{n^{2}}\\left(\\vec{\\sigma}^{T}-\\overrightarrow{\\mathbf{I}}_{n^{2}}^{T}\\right)$. Thus, NOOMs are not any more expressive than OOMs, i.e., $\\mathrm{NOOM} \\subseteq$ OOM.\n\n### 2.4 Hidden Quantum Markov Models\n\nPrevious work by Srinivasan et al. (2018b) derived HQMMs by generalizing HMMs using system-environment interactions (illustrated using a quantum circuit), and showed that every $n$-dimensional HMM can be modeled by an HQMM with no more than an $n^{2}$-dimensional hidden states. Here, we take a different approach; we will show how HQMMs can be defined through a series of natural generalizations of NOOMs in such a way that they also end up containing finite-dimensional HMMs. We do so by allowing parameters to be complex and expanding the concepts of NOOM states and operators using the representation in Equation 6.\n\nGeneralizing NOOM States We know from Equation 6 that the initial state $\\vec{\\omega}_{0}$ can viewed as a vectorized rank-1 Hermitian matrix $\\boldsymbol{\\rho}$, i.e., $\\vec{\\omega}=\\operatorname{vec}\\left(\\vec{v}_{0} \\vec{v}_{0}^{\\dagger}\\right)=\\operatorname{vec}(\\boldsymbol{\\rho})$. A natural generalization would be to let the initial state be a vectorized matrix of arbitrary rank, i.e., $\\boldsymbol{\\rho}_{0}=\\sum_{i} p_{i} \\vec{v}_{i} \\vec{v}_{i}^{\\dagger}$ instead. The normalization condition on the initial state can then be restated as $1=\\vec{\\sigma}^{\\dagger} \\vec{\\rho}_{0}=\\overrightarrow{\\mathrm{E}}_{n^{2}}^{*} \\vec{\\rho}_{0}=\\operatorname{tr}\\left(\\boldsymbol{\\rho}_{0}\\right)=\\sum_{i} p_{i}$.\nAs a linear combination of outer products of vectors with themselves, $\\boldsymbol{\\rho}$ must be Hermitian. We additionally assume that the constituent eigenvectors live in a Hilbert space $\\mathcal{H}$, so that $\\boldsymbol{\\rho}$ lives in a Liouville space, i.e., the outer product of two Hilbert spaces. Further, in the NOOM, $\\vec{v}_{0} \\vec{v}_{0}^{\\dagger}$ had a single eigenvalue of 1 . If we impose no further constraints, we could allow $p_{i}$ to be complex-valued or negative as long as the normalization condition above was satisfied. However, this could once again lead to negative probabilities when applying the evaluation $\\vec{\\sigma}$, and hence a non-constructive\nmodel. Thus, we impose a positive semi-definiteness (PSD) constraint on the initial state to guarantee that $p_{i} \\in \\mathbb{R}_{\\geq 0}$ so that $\\operatorname{tr}\\left(\\boldsymbol{\\rho}_{0}\\right)$ is real and non-negative. Essentially, we are now considering a model whose initial states $\\vec{\\rho}$ are vectorized arbitrary-rank Hermitian PSD matrices, which constitute a pointed convex cone. Such matrices are called density matrices in quantum mechanics (Nielsen and Chuang, 2010), and the imposition of the PSD constraint on the states is what allows these models to avoid the NPP.\n\nGeneralizing NOOM operators Having defined a convex cone of valid states, we now derive operators that ensure that the state always evolves inside the cone. We refer to such operators acting on our states in Liouville space as Liouville superoperators $\\left\\{\\mathbf{L}_{y}\\right\\}_{y \\in \\mathcal{O}}$. Condition (ii) in Definition 4 ensured that probabilities of observations computed by the NOOM were normalized, and the equivalent condition in the OOM representation in Equation 6 is that $\\vec{\\sigma}^{\\dagger}\\left(\\sum_{y \\in \\mathcal{O}} \\boldsymbol{\\tau}_{y}\\right)=\\vec{\\sigma}^{\\dagger}$. We impose a similar constraint (trace preservation or TP) on the superoperators to ensure we get a normalized distribution over observations. In addition to this, we further need to ensure that the probabilities assigned to observations are real and non-negative, i.e., the operators must always preserve the Hermitian PSD condition of the state. Finding a constructive way to impose these restrictions on Liouville superoperators is challenging, and it is easier to do so on the 'reshuffled' version of it called its Choi matrix (Wood et al., 2015). The reshuffle operation (Figure 1) involves reshaping the $n^{2}$-dimensional columns of the Liouville superoperator into $n \\times n$ matrices. Going across the columns of $\\mathbf{L}$ from left to right, we fill up the blocks of the Choi matrix column-first with these reshaped matrices (see Życzkowski and Bengtsson (2004) for further details). In the context of Hermitian preserving (HP) maps, there is no elegant way to also impose a simple PSD-preserving 'positivity' constraint (Choi, 1975; Pillis, 1967). Therefore, we must impose a slightly more restrictive complete positivity (CP) constraint which guarantees that the map $\\mathbf{L}_{y} \\otimes \\mathbb{I}$ is PSD-preserving for identity matrices of any dimension. In fact, Choi (1975) suggest that a CP map is the natural constructive generalization of 'positivity' for a linear HP map. We define L-HQMMs as a generalization of NOOMs with these constraints:\nDefinition 5 (L-HQMMs). An $n^{2}$-dimensional LiouvilleHidden Quantum Markov Model with a set of discrete observations $\\mathcal{O}$ is a tuple $\\left(\\mathbb{C}^{n^{2}},\\left(\\mathbf{L}_{y}\\right)_{y \\in \\mathcal{O}}, \\vec{\\rho}_{0}, \\overrightarrow{\\mathrm{E}}\\right)$ where the initial state $\\vec{\\rho}_{0} \\in \\mathbb{C}^{n^{2}}$ and Liouville superoperators $\\left\\{\\mathbf{L}_{y}\\right\\}_{y \\in \\mathcal{O}} \\in \\mathbb{C}^{n^{2} \\times n^{2}}$ with corresponding Choi matrices $\\left\\{\\mathbf{C}_{y}\\right\\}_{y \\in \\mathcal{O}}$ satisfy the following constraints:\n(i) $\\vec{\\rho}_{0}$ is a vectorized Hermitian PSD matrix of arbitrary rank,\n(ii) Normalized initial state: $\\overrightarrow{\\mathrm{E}}^{T} \\vec{\\rho}_{0}=1$,"
    },
    {
      "markdown": "(iii) $C P: \\mathbf{C}_{y} \\geq 0$ (Choi matrix is PSD).\n(iv) $T P: \\overrightarrow{\\mathbf{I}}^{T}\\left(\\sum_{y \\in \\mathcal{O}} \\mathbf{L}_{y}\\right)=\\overrightarrow{\\mathbf{I}}^{T}$,\n(v) $H P: \\mathbf{C}_{y}=\\mathbf{C}_{y}^{\\dagger}$,\n\nFor such a model, the state update after observing $y \\in \\mathcal{O}$ and computing the probability of that observation are:\n\n$$\n\\tilde{\\rho}_{t}=\\frac{\\mathbf{L}_{y} \\tilde{\\rho}_{t-1}}{\\overrightarrow{\\mathbf{I}}^{T} \\mathbf{L}_{y} \\tilde{\\rho}_{t-1}} \\quad P(\\tilde{y})=\\overrightarrow{\\mathbf{I}}^{T} \\mathbf{L}_{y_{t}} \\ldots \\mathbf{L}_{y_{t}} \\tilde{\\rho}\n$$\n\nThe exact relationship between HQMMs and OOMs was previously unknown, but this formulation of HQMMs allows us to state an important result:\nTheorem 1. $H Q M M \\subseteq O O M$, and the set of valid initial states for HQMMs is a spectruplex.\n\nProof. Setting $\\vec{\\sigma}=\\overrightarrow{\\mathbf{I}}$, L-HQMMs satisfy condition (i) of General OOMs laid out in Definition 3 by construction. Condition (ii) of Definition 3 is satisfied by the TP constraint on L-HQMMs. Next, the HP and CP constraints on L-HQMMs guarantee that $\\mathbf{L}_{y} \\vec{\\rho}$ always yields a vectorized Hermitian PSD matrix. The trace of this matrix is always real and non-negative, i.e., $\\overrightarrow{\\mathbf{I}}^{T} \\mathbf{L}_{y} \\vec{\\rho} \\geq 0$. We also have $\\overrightarrow{\\mathbf{I}}^{T} \\mathbf{L}_{y} \\tilde{\\rho}_{0} \\leq \\overrightarrow{\\mathbf{I}}^{T}\\left(\\sum_{y \\in \\mathcal{O}} \\mathbf{L}_{y}\\right) \\tilde{\\rho}_{0}=\\overrightarrow{\\mathbf{I}}^{T} \\tilde{\\rho}_{0}=1$, satisfying condition (iii) of Definition 3.\nThe valid initial states of L-HQMMs are Hermitian PSD matrices with unit trace. Hermitian PSD matrices form a convex cone, and the intersection of this cone with the linear affine subspace of trace 1 matrices is a spectrahedron known as a spectruplex.\n\nUsing the same similarity transform that we used for NOOMs $\\mathbf{S}=\\mathbf{I}_{n^{2}}+\\frac{1}{n^{2}} \\overline{\\mathbf{I}}_{n^{2}}\\left(\\vec{\\sigma}^{T}-\\overline{\\mathbf{I}}_{n^{2}}^{T}\\right)$, we can transform any $n^{2}$-dimensional L-HQMM into an equivalent standard OOM.\n\nIt is still an open question whether HQMMs are a proper subset of OOMs.\n\nAn alternate formulation of HQMMs Prior work on HQMMs have represented these models in the so-called operator-sum representation (Srinivasan et al., 2018b; Monras et al., 2010). While the notion of operating on vectorized matrices is fairly common in quantum information (and was implicitly used for HQMMs in Srinivasan et al. (2018a)), L-HQMMs are a novel formulation of HQMMs. We now derive the operator-sum representation of HQMMs from L-HQMMs, showing that the two are equivalent.\n\nFrom Definition 5, we know that any model equivalent to L-HQMMs must have CP, TP, and HP operators. From Choi's theorem (Choi, 1975), we know that any map which can be expressed in the operator-sum\nrepresentation $\\mathcal{K}(\\boldsymbol{\\rho})=\\sum_{w} \\mathbf{K}_{w} \\boldsymbol{\\rho} \\mathbf{K}_{w}^{\\dagger}$ is guaranteed to be CP , and will preserve the PSD nature of any input matrix. In the context of CP maps, the operator matrices $\\mathbf{K}_{w}$ are commonly called Kraus operators (Kraus, 1971). The quadratic application of operator preserves the Hermiticity of $\\boldsymbol{\\rho}$. Thus, the operator-sum representation is particularly appealing because it guarantees the CP and HP constraints by construction. Note that this representation of CP maps is merely a vectorization of the Liouville form\n\n$$\n\\operatorname{vec}\\left(\\sum_{w} \\mathbf{K}_{w} \\boldsymbol{\\rho} \\mathbf{K}_{w}^{\\dagger}\\right)=\\sum_{w}\\left(\\mathbf{K}_{w}^{\\prime} \\otimes \\mathbf{K}_{w}\\right) \\vec{\\rho}=\\mathbf{L} \\vec{\\rho}\n$$\n\nThus, the action of a Liouville superoperator $\\mathbf{L}_{y}$ corresponding to the observable $y$ on $\\vec{\\rho}$ can can be equivalently represented by a set of Kraus operators $\\left\\{\\mathbf{K}_{y, w_{y}}\\right\\}$ acting on the density matrix $\\boldsymbol{\\rho}$, where the cardinality of this set $\\left|w_{y}\\right|$ is determined by the Schmidt-rank (or Kraus-rank, as we soon explain) of $\\mathbf{L}_{y}$. The Schmidt-rank is analogous to the rank revealed by an SVD, but for a decomposition into a Kronecker product of two vector spaces.\nFinally, the operator-sum representation also provides a convenient constraint way of ensuring the TP constraint: the full set of Kraus operators across all observables must satisfy $\\sum_{y, w_{y}} \\mathbf{K}_{y, w_{y}}^{\\dagger} \\mathbf{K}_{y, w_{y}}=\\mathbb{1}$ (Nielsen and Chuang, 2010). Note that this condition essentially generalizes condition (ii) for NOOMs in Definition 4 to allow multiple operators per observable. We can now define HQMMs using the Kraus operator-sum representation, as given in Srinivasan et al. (2018b).\nDefinition 6 (K-HQMMs). An n-dimensional KrausHidden Quantum Markov Model with a set of discrete observations $\\mathcal{O}$ is a tuple $\\left(\\mathbb{C}^{n \\times n},\\left\\{\\mathbf{K}_{y, w_{y}}\\right\\}_{y \\in \\mathcal{O}}, \\boldsymbol{\\rho}_{0}, \\operatorname{tr}(\\cdot)\\right)$ where initial state $\\boldsymbol{\\rho}_{0} \\in \\mathbb{C}^{n \\times n}$ and Kraus operators $\\left\\{\\mathbf{K}_{y, w_{y}}\\right\\}_{y \\in \\mathcal{O}, w_{y} \\in \\mathbb{N}} \\in \\mathbb{C}^{n \\times n}$ satisfy the following constraints:\n(i) $\\boldsymbol{\\rho}_{0}$ is a Hermitian PSD matrix of arbitrary rank,\n(ii) Normalized Initial State: $\\operatorname{tr}\\left(\\boldsymbol{\\rho}_{0}\\right)=1$,\n(iii) Normalized marginal over observations (TP): $\\sum_{y, w} \\mathbf{K}_{y, w}^{\\dagger} \\mathbf{K}_{y, w}=\\mathbb{1}$.\n\nThe state update after observing $y$ is computed as\n\n$$\n\\boldsymbol{\\rho}_{t}=\\frac{\\sum_{w_{y}} \\mathbf{K}_{y, w_{y}} \\boldsymbol{\\rho}_{t-1} \\mathbf{K}_{y, w_{y}}^{\\dagger}}{\\operatorname{tr}\\left(\\sum_{w_{y}} \\mathbf{K}_{y, w_{y}} \\boldsymbol{\\rho}_{t-1} \\mathbf{K}_{y, w_{y}}^{\\dagger}\\right)}\n$$\n\nand probability of a given sequence is given by:\n$P(\\tilde{y})=\\operatorname{tr}\\left(\\sum_{w_{y_{t}}} \\mathbf{K}_{y_{t}, w_{y_{t}}}-\\left(\\sum_{w_{y_{t}}} \\mathbf{K}_{y_{t}, w_{y_{t}}} \\boldsymbol{\\rho}_{0} \\mathbf{K}_{y_{t}, w_{y_{t}}}^{\\dagger}\\right) \\ldots \\mathbf{K}_{y_{t}, w_{y_{t}}}^{\\dagger}\\right)$"
    },
    {
      "markdown": "The K-HQMM representation was used by Srinivasan et al. (2018b) to show that any $n$ dimensional HMM can be written as an equivalent $n^{2}$ dimensional KHQMM, while there were HQMMs like the NOOM probability clock (trivially a HQMM) that required infinite-dimensional HMMs; hence HMM $\\subset$ HQMM.\n\nUniqueness of L-HQMMs Note that the Kraus operator sum formulation of K-HQMMs does not uniquely define a CP map; it can be equivalently defined using different sets of Kraus operators (with possibly different cardinalities). Thus, it is not evident how one might compare two K-HQMMs. On the other hand, the Liouville superoperator is the unique representation of a CP map, and can be canonically factorized as follows (Wood et al., 2015; Miszczak, 2011):\n\n$$\n\\mathbf{L}=\\sum_{w} \\mathbf{K}_{w}^{*} \\otimes \\mathbf{K}_{w}=\\sum_{i=1}^{r} \\gamma_{i}\\left(\\mathbf{K}_{i}^{*} \\otimes \\mathbf{K}_{i}\\right)\n$$\n\nwhere $\\left\\{\\mathbf{K}_{w}\\right\\}$ is a set of arbitrary Kraus operators, $\\left\\{\\sqrt{\\gamma_{i}} \\mathbf{K}_{i}\\right\\}$ the set of canonical Kraus operators defining the CP map, and $r$ the 'Kraus-rank' of the CP map. It is a well known result that these factors can be computed directly from an SVD of the Choi matrix (the 'reshuffled' Liouville matrix); the $i$-th singular value and vector pair correspond to $\\gamma_{i}$ and vec $\\left(\\mathbf{K}_{i}\\right)$ (Wood et al., 2015; Miszczak, 2011). We illustrate this process in Figure 1. The Kraus-\n![img-0.jpeg](img-0.jpeg)\n\nFigure 1: Three equivalent formulations of a CP map: The unique canonical operator sum representation of a CP map can be obtained by performing an SVD of its Choi matrix, which is obtained by reshuffling its Liouville superoperator.\nrank of a CP map is equal to the rank of the Choi matrix, and is equal to the minimum number of Kraus operators required to express the operation. Since the Liouville superoperator (or the Choi matrix) uniquely defines a CP map, we can use this representations to compare two L-HQMMs.\n\nHQMMs \\& NOOMs We have shown that $n$ dimensional NOOMs form a subset of $n$-dimensional HQMMs through generalization. Prior work by Srinivasan\net al. (2018b) used 'HQMMs' and 'NOOMs' somewhat ambiguously, differentiating them primarily by the field over which they are definied ( $\\mathbb{R}$ or $\\mathbb{C}$ ). In this paper we have used the original formulation of NOOMs (Zhao and Jaeger, 2010b) to draw a clearer distinction, whereby NOOMs are simply HQMMs with rank-1 vectorized initial state and Kraus-rank 1 operators. Particularly, for a fixed latent dimension $n^{2}$ of the vectorized density matrix, an HQMM allows for a greater diversity of both states and dynamics.\nFirst, note that the valid states of HQMMs are Hermitian PSD matrices with unit trace, also known as mixed density matrices in quantum mechanics (Nielsen and Chuang, 2010). By contrast, the valid states for NOOMs correspond to the set of pure density operators (with rank 1). Since these operators encode the probability distribution of the latent state, we see that HQMM states can represent mixture distributions of NOOM states. Formally, the set of rank-1 density matrices are extremal points of the spectraplex defined by arbitrary rank density matrices. This gives us some geometric intuition for why HQMMs have a richer state space than NOOMs.\nSecond, HQMMs can have an arbitrary number of Kraus operators per observable while NOOMs are restricted to one to preserve rank-1 states. This indicates that the evolution associated with individual observations in an $n$-dimensional NOOM is restricted to dynamics corresponding to rank 1 Choi matrices. Thus, an $n$-dimensional HQMMs with arbitrary Kraus rank can encode richer dynamics than an $n$-dimensional NOOM.\n\n## 3 Learning HQMMs\n\nHaving characterized the expressiveness of HQMMs, we now turn to the task of learning them from data.\nThe Learning Problem We use the negative loglikelihood of the data as our loss function, which can be written as a function of the set of Kraus operators $\\left\\{\\mathbf{K}_{y, w}\\right\\}$ as follows (Srinivasan et al., 2018b):\n\n$$\n\\mathcal{L}=-\\ln \\operatorname{tr}\\left(\\sum_{w} \\mathbf{K}_{y_{n}, w} \\cdots\\left(\\sum_{w} \\mathbf{K}_{y_{1}, w} \\boldsymbol{\\rho}_{0} \\mathbf{K}_{y_{1}, w}^{\\dagger}\\right) \\ldots \\mathbf{K}_{y_{n}, w}^{\\dagger}\\right)\n$$\n\nNote that the learned Kraus operators must satisfy the TP constraint $\\sum_{y, w} \\mathbf{K}_{y, w}^{\\dagger} \\mathbf{K}_{y, w}=\\mathbb{I}$. The problem of learning a set of $N$ trace-preserving $n \\times n$ Kraus operators can equivalently be framed as one of learning a matrix $\\boldsymbol{\\kappa} \\in$ $\\mathbb{C}^{n N \\times n}$ on the Stiefel manifold i.e., that satisfy $\\boldsymbol{\\kappa}^{\\dagger} \\boldsymbol{\\kappa}=\\mathbb{I}$, where $\\boldsymbol{\\kappa}$ can be block-partitioned row-wise into the $N$ Kraus operators that parameterize the HQMM. Both the previous and this paper's approach begin with an initial guess $\\boldsymbol{\\kappa}_{0}$ with a pre-determined partitioning into the Kraus operators we wish to learn, and iteratively make changes to the guess to maximize the log-likelihood (a function of the Kraus operators)."
    },
    {
      "markdown": "The Previous Approach Since $\\boldsymbol{\\kappa}$ is a matrix with orthonormal columns, any initial guess $\\boldsymbol{\\kappa}_{0}$ is a unitary transformation away from the true $\\boldsymbol{\\kappa}^{*}$ that maximizes the log-likelihood. The existing method (Srinivasan et al., 2018b) iteratively finds a series of Givens rotations that locally increase the log-likelihood. However, a Givens rotation only changes two rows of $\\boldsymbol{\\kappa}$ at a time, making this approach prohibitively slow for learning large $\\boldsymbol{\\kappa}$ matrices. Furthermore, since these two rows are picked at random, this approach is not guaranteed to step towards the optimum at every iteration.\nRetraction-Based Optimization We propose directly learning $\\boldsymbol{\\kappa}$ using a gradient-based algorithm. Note that since $\\mathcal{L}$ is a function of complex matrices, the direction of steepest descent corresponds to the gradient with respect to the complex conjugate of the Kraus operators (A.Hjørungnes and D.Gesbert, 2007). Most existing algorithms that constrain gradient updates on the Stiefel manifold are either projection-like (which re-orthogonalize the naive gradient descent updates) or geodesic-like (which directly generate updates on the manifold itself) (Jiang and Dai, 2013). We picked the geodesic like algorithm proposed by Wen and Yin (2013) as it performed best on our datasets (see Appendix B for details).\nGiven a gradient $\\mathbf{G}$ of the loss function $\\mathcal{L}$ with respect to parameters $\\boldsymbol{\\kappa}$, we wish to find the trajectory $\\gamma(\\tau)$ for some step size $\\tau$ that corresponds to stepping along the direction of the gradient while staying on the Stiefel manifold. The Wen-Yin algorithm achieves this through retractions that smoothly map $\\mathbf{G}$ or any point on a manifold's tangent bundle onto the manifold itself, while preserving the descent direction at that point (Absil et al., 2007). The constrained update $\\gamma(\\tau)$ on the Stiefel manifold with respect to an initial feasible solution $\\boldsymbol{\\kappa}_{0}$ is\n\n$$\n\\gamma(\\tau)=\\boldsymbol{\\kappa}_{0}-\\tau \\mathbf{U}\\left(\\mathbb{I}+\\frac{\\tau}{2} \\mathbf{V}^{\\dagger} \\mathbf{U}\\right)^{-1} \\mathbf{V}^{\\dagger} \\kappa_{0}\n$$\n\nwhere $\\mathbf{U}=\\left[\\begin{array}{l}\\mathbf{G} \\\\ \\boldsymbol{\\kappa}_{0}\\end{array}\\right], \\mathbf{V}=\\left[\\begin{array}{l}\\boldsymbol{\\kappa}_{0} \\\\ -\\mathbf{G}\\end{array}\\right]$, and $\\mathbf{G}$ is the gradient at $\\boldsymbol{\\kappa}_{0}$. This update requires the inversion of a $2 n \\times 2 n$ matrix. $\\gamma(\\tau)$ is the trajectory obtained by smoothly retracting the gradient onto the manifold, giving the direction of steepest descent to feasibly optimize Equation 11. Consequently, Equation 12 guarantees that when $\\tau=0$, it has the same direction as $\\mathbf{G}$, and $\\gamma(\\tau)^{\\dagger} \\gamma(\\tau)=\\mathbb{I}$ for any $\\tau$.\n\nThis method can be combined with a gradient descent scheme (summarized in Algorithm 1) to learn feasible parameters for HQMMs. In our experiments with $N=|\\mathcal{O}| w$, and for a batch with $m$ sequences of length $l$, we compute the loss using Equation 11 in $O\\left(m l w n^{3}\\right)$ time, perform auto-differentiation, and obtain a retraction using Equation 12 in $O\\left(|\\mathcal{O}| w n^{3}\\right)$ time.\n\n## 4 Experimental Results\n\nTo show the superior performance of the retraction-based algorithm for constrained optimization on the Stiefel\n\nAlgorithm 1 Learning HQMMs using Constrained Optimization on the Stiefel Manifold\nInput: Training data $\\mathbf{Y} \\in \\mathbb{N}^{M \\times \\ell}$, where $M$ is the \\# of data points and $\\ell$ is the \\# of observed variables in the HQMM\nHyperparameters: $\\tau$ (learning rate), $\\alpha$ : (learning rate decay), $B$ (number of batches), $E$ (number of epochs)\nOutput: $\\left\\{\\mathbf{K}_{i}\\right\\}_{i=1}^{\\mid \\mathcal{O} \\mid w}$\n1: Initialize: Complex orthonormal matrix on Stiefel manifold $\\boldsymbol{\\kappa} \\in \\mathbb{C}^{|\\mathcal{O}| w n \\times n}$ and partition into Kraus operators $\\left\\{\\mathbf{K}_{i}\\right\\}_{i=1}^{|\\mathcal{O}| w}$, with $\\mathbf{K}_{i} \\in \\mathbb{C}^{n \\times n}$\n2: for epoch $=1: E$ do\n3: Partition training data $\\mathbf{Y}$ into $B$ batches $\\left\\{\\mathbf{Y}_{b}\\right\\}$\n4: for $b=1: B$ do\n5: $\\quad$ Compute gradient $\\mathbf{G}_{i} \\leftarrow \\frac{\\partial \\mathcal{E}}{\\partial \\mathbf{K}_{i}^{\\prime}}$ for batch $\\mathbf{Y}_{b}$ and loss function $\\mathcal{L}$\n6: $\\quad$ Compute $\\frac{\\partial \\mathcal{E}}{\\partial w}=\\mathbf{G} \\leftarrow\\left[\\begin{array}{lll}\\mathbf{G}_{1} & \\cdots & \\mathbf{G}_{|\\mathcal{O}| w}\\end{array}\\right]^{T}$\n7: $\\quad$ Construct $\\mathbf{U} \\leftarrow\\left[\\begin{array}{l}\\mathbf{G} \\\\ \\mathbf{G}\\end{array}\\right], \\mathbf{V} \\leftarrow\\lceil\\boldsymbol{\\kappa}\\rceil-\\mathbf{G} \\rceil$\n8: $\\quad$ Update $\\boldsymbol{\\kappa} \\leftarrow \\boldsymbol{\\kappa}-\\tau \\mathbf{U}\\left(\\mathbb{I}+\\frac{\\tau}{2} \\mathbf{V}^{\\dagger} \\mathbf{U}\\right)^{-1} \\mathbf{V}^{\\dagger} \\boldsymbol{\\kappa}$\n9: end for\n10: $\\quad$ Update learning rate $\\tau=\\alpha \\tau$\n11: Re-partition $\\boldsymbol{\\kappa}$ into $\\left\\{\\mathbf{K}_{i}\\right\\}$\n12: end for\n13: return $\\left\\{\\mathbf{K}_{i}\\right\\}$\nmanifold (COSM) over the previous Givens Search (GS) method in learning HQMMs, we evaluate their accuracy and run-time on two datasets. The first is the synthetic dataset used by Srinivasan et al. (2018b) (code obtained from Github) that was generated by an HMM. The second is a real-world dataset, on which the GS approach is prohibitively slow; demonstrating the scalability of COSM. In Appendix A, we also present results where COSM outperforms GS on the synthetic data used by Srinivasan et al. (2018b) that was generated by an HQMM representing a quantum mechanical process.\n\nTraining For all our HQMMs, we use the log-likelihood loss function from Equation 11. We initialize the latent state $\\boldsymbol{\\rho}_{0}$ as a random Hermitian PSD matrix using the QETLAB toolbox (Johnston, 2016), and $\\boldsymbol{\\kappa}$ as a random orthonormal matrix. Except for very small models, COSM is fairly robust to random initializations (see Appendix C). We compute the gradient of the loss function with respect to the complex conjugate of the Kraus operators using the Autograd package (which can handle complex differentiation), and vertically stack the gradients of the Kraus operators to construct the gradient $\\mathbf{G}$ of the matrix $\\kappa$. To smoothen the trajectory we apply momentum with $\\beta=0.9$ (Rumelhart et al., 1986; Qian,\n\n[^0]\n[^0]:    A preliminary version of these experimental results appeared in Adhikary et al. (2019). Code available at https://github.com/sandeshAdhikary/learning-hqmms-stiefel-manifold"
    },
    {
      "markdown": "1999), and re-normalize the gradient before and after the momentum update, making the magnitude of updates entirely dependent on step-size. We refer to HQMMs using the tuple $(n, s, w)-\\mathrm{HQMM}$, where $n$ is the number of hidden states, $s$ is the number of possible outputs (earlier denoted $|\\mathcal{O}|$ ), and $w$ is the number of Kraus operators per output, i.e., the Kraus-rank of the HQMM or dimension of the 'environment' variable (Srinivasan et al., 2018b). Consequently, for an $(n, s, w)-\\mathrm{HQMM}$ we have $\\boldsymbol{\\kappa} \\in \\mathbb{C}^{n s w \\times n}$. We also provide the performance of HMMs trained using the Expectation-Maximization (EM) algorithm (with 5 random restarts) for reference. Details of our hyperparameter tuning procedure and computing infrastructure are described in Appendix D.\n\nMetrics On the synthetic HMM dataset, we use a scaled log-likelihood (M. Zhao, 2007; Srinivasan et al., 2018b) independent of sequence length called description accuracy: $D A=f\\left(1+\\frac{\\log _{e} P(Y \\mid \\mathbf{D})}{\\ell}\\right)$, where $f(\\cdot)$ squishes the loglikelihood from $(-\\infty, 1]$ to $(-1,1)$ (with $f(x)=\\tanh (x / 8)$ for $x \\leq 0$, and $f(x)=x$ for $x>0$ ). When $D A=1$, the model predicted the sequence with perfect accuracy, and when $D A>0$, the model performed better than random. The error bars represent one standard deviation of the DA scores across many test samples. On the real-world dataset, we report the average accuracy for a classification problem.\n\n### 4.1 Synthetic HMM Data\n\nFor our first experiment, we generated data using the same synthetic HMM as Srinivasan et al. (2018b), with 6 hidden states and 6 possible outputs. We show two things with the experiments on this dataset: 1) COSM finds better optima than GS, and 2) COSM is much faster than GS - so much so that we could train larger HQMMs than were previously possible. We also investigate the effects of increasing model size by adding latent states $(n)$ versus increasing the Kraus-rank $(w)$.\n\nWe used the same 20 training and 10 validation sequences of length 3000 used by Srinivasan et al. (2018b), splitting up each sequence into 300 sequences and use a burn-in of 100. We trained HQMMs using the COSM approach for 60 epochs, and evaluated the model with the highest validation DA score on the test set. The results for this model are shown in Figure 2a and Figure 2b.\n\nCOSM finds better optima than GS As shown in Figure 2a, HQMMs (with $w=1$ ) learned using COSM achieve better optima than HQMMs learned using GS for all $n$. As described in Section 2.4, these models are essentially complex-valued NOOMs. We also confirm that as noted in Srinivasan et al. (2018b), small HQMMs $(n \\leq 5)$ can model this data better than small HMMs, although this doesn't hold for $n=6$. However, we can take advantage of the additional Kraus-rank hyperparameter $w$ available to HQMMs to further improve performance,\n![img-1.jpeg](img-1.jpeg)\n\nFigure 2: Test Set Performances on the Synthetic HMM Data: The dashed line represents the test set performance of the true model (a (6,6)-HMM) that generated the data.\nas shown in Figure 2b for $(5,6, w)-\\mathrm{HQMMs}$ (varying $w$ ). Also note that the number of parameters for an HQMM scales faster than for an HMM.\n\nCOSM is much faster than GS In Figure 3, we plot the test set DA versus CPU training time for the smallest and largest models trained. To ensure a fair comparison, we train both approaches on sequences of length 300 and a batch size of 30 . Note that we pre-tune hyperparameters on the validation set, and the graphs show the changing test DA as the models are trained with these hyperparameters (test DAs were not used to tune hyperparameters).\n\nFor all models, we see that COSM converges much faster than GS, and the difference in both speed and accuracy is especially pronounced for the larger models; COSM converges within a few hundred seconds, while GS yields very poor solutions even after 2000 seconds. As the GS method can take days to converge for large models, we could not directly calculate a precise speedup.\n\nSrinivasan et al. (2018b) proved that a (6,6,6)-HQMM should be sufficient to fully model a (6,6)-HMM, but the GS method was too slow to train this model. With COSM, we are able to show that this theoretical guarantee holds in practice. In fact, we find that in practice a $(5,6,3)-\\mathrm{HQMM}$ is sufficient to model our (6,6)-HMM.\n\n### 4.2 Splice Dataset\n\nFor our second experiment, we use the real-world splice dataset (Dheeru and Karra Taniskidou, 2017; Towell et al., 1991) consisting of DNA sequences of length 60, each element of which represents one of four nucleobases:"
    },
    {
      "markdown": "![img-2.jpeg](img-2.jpeg)\n\nFigure 3: COSM Learns More Accurate Models Faster than GS: Test DA versus training time for various ( $n, s, w$ )-HQMMs trained on the synthetic HMM data. COSM converges to a better optimum faster than GS for all models; the dashed line represents the DA of the true data generating model.\n\nAdenine (A), Cytosine (C), Guanine (G), and Thyamine (T). A DNA sequence typically consists of information encoded in sub-sequences (exons), that are separated by superfluous sub-sequences (introns). The task associated with this dataset is to classify sequences as having an exon-intron (EI) splice, an intron-exon (IE) splice, or neither (N), with 762, 765, and 1648 labeled examples for each label respectively. In addition to A, C, T and G, the raw dataset also contains some ambiguous characters, which we filter out prior to training. Our goal in this experiment is to demonstrate that we can use COSM to train HQMMs on real-world datasets which would have been too slow to train using GS.\n\nWe train a separate model for each of the three labels, and during test-time, choose the label corresponding to the model that assigned the highest likelihood to the given sequence. We train HQMMs using the COSM method and HMMs with the EM algorithm (with 5 random restarts) for reference. In Figure 4, we report the average classification accuracies across all labels obtained with 5 -fold cross validation. For reference, a random classifier achieves around $33.3 \\%$ accuracy.\nNote that 5 -fold cross-validation is prohibitively time consuming for GS, even for models with a modest number of parameters. However, we are able to learn these HQMMs with COSM. We also see that, (as before) there is a sizable marginal gain in DA when going from $w=1$ to $w=2$, with the benefits of increasing $w$ further being less clear. However unlike the previous experiment, we still see persistent gains by increasing $n$. Interpreting this in conjunction with the results in the previous section suggests that we have to tune both $n$ and $w$ depending on the dataset. We also find that for a given number of hidden states, COSM is able to learn an HQMM that outperforms the corresponding HMM, although this comes at the cost of a rapid scaling in the number of parameters.\n\n## 5 Conclusion\n\nWe showed that HQMMs are OOMs that generalize NOOMs, and that unlike prior approaches that avoid\n![img-3.jpeg](img-3.jpeg)\n\nFigure 4: Average 5-fold Test Set Performance on the Splice Dataset Test set accuracies (left) and number of parameters (right) for various HQMMs and HMMs trained using the COSM and EM algorithms respectively. Errorbars in the left graph represent the mean standard deviation across labels over the 5 folds.\nthe NPP by design, HQMMs are able to model arbitrary HMMs as well. HQMMs expand the convex cone of valid states from rank-1 PSD matrices in (complex valued) NOOMs to arbitrary rank Hermitian PSD matrices. We also formulated the unique Liouville representation of an HQMMs, which allows direct comparison between models, and also simplifies theoretical analysis connecting them to general OOMs. Future work could focus on identifying the exact relationship bewtween NOOMs and HMMs, and whether arbitrary OOMs can be converted to HQMMs.\n\nWe also introduced a retraction-based learning algorithm that directly constrains gradient updates to the Stiefel manifold to learn feasible HQMMs, and presented experimental results on a synthetic and a real-world"
    },
    {
      "markdown": "dataset. In the process, we showed that the proposed algorithm outperforms the prior approach in terms of both speed and accuracy, and so were able to train HQMMs that were previously too large to train. This also suggests that directly optimizing the parameters is a better strategy than finding small, local unitary rotations of the matrix on the Stiefel manifold. One downside is the rapid scaling of parameters in HQMMs, and it would be interesting to investigate approximations that may produce similar performance with far fewer parameters. It would also be useful to dynamically learn the Kraus-rank $w$ instead of tuning it as a hyperparameter. Other future work could develop new QGM models defined via Kraus operators, which can be learned using our approach.\n\n## References\n\nAbsil, P.-A., Mahony, R., and Sepulchre, R. (2007). Optimization Algorithms on Matrix Manifolds. Princeton University Press, Princeton, NJ, USA.\nAdhikary, S., Srinivasan, S., and Boots, B. (2019). Learning quantum graphical models using constrained gradient descent on the stiefel manifold. arXiv preprint arXiv:1903.03730.\nA.Hjorungnes and D.Gesbert (2007). Complex-Valued Matrix Differentiation: Techniques and Key Results. IEEE Transactions on Signal Processing, 55(6):2740-2746.\nBailly, R. (2011). Quadratic weighted automata: Spectral algorithm and likelihood maximization. In Asian Conference on Machine Learning, pages 147-163.\nBalle, B., Hamilton, W., and Pineau, J. (2014). Methods of moments for learning stochastic languages: Unified presentation and empirical comparison. In International Conference on Machine Learning, pages 1386-1394.\nChoi, M.-D. (1975). Completely positive linear maps on complex matrices. Linear Algebra and its Applications, 10(3):285 - 290.\nClark, L. A., Huang, W., Barlow, T. M., and Beige, A. (2015). Hidden quantum markov models and open quantum systems with instantaneous feedback. In ISCS 2014: Interdisciplinary Symposium on Complex Systems, pages 143-151. Springer.\nCohen, S. B., Stratos, K., Collins, M., Foster, D. P., and Ungar, L. (2013). Experiments with spectral learning of latent-variable pcfgs. In Proceedings of the 2013 conference of the North American chapter of the Association for Computational Linguistics: human language technologies, pages 148-157.\nDheeru, D. and Karra Taniskidou, E. (2017). UCI machine learning repository. Available at: http://archive.ics.uci.edu/ml.\nEdelman, A., Arias, T., and Smith, S. (1998). The geometry of algorithms with orthogonality constraints.\n\nSIAM Journal on Matrix Analysis and Applications, 20(2):303-353.\nGerlach, W. and Stern, O. (1922). Der experimentelle nachweis der richtungsquantelung im magnetfeld. Zeitschrift für Physik, 9(1):349-352.\nIto, H., Amari, S.-I., and Kobayashi, K. (1992). Identifiability of hidden markov information sources and their minimum degrees of freedom. IEEE transactions on information theory, 38(2):324-333.\nJaeger, H. (2000). Observable operator models for discrete stochastic time series. Neural computation, 12(6):1371-1398.\nJiang, B. and Dai, Y.-H. (2013). A framework of constraint preserving update schemes for optimization on stiefel manifold. Math. Program., 153:535-575.\nJohnston, N. (2016). QETLAB: A MATLAB toolbox for quantum entanglement, version 0.9. http://qetlab.com.\nKraus, K. (1971). General state changes in quantum theory. Annals of Physics, 64(2):311-335.\nLeifer, M. S. and Poulin, D. (2008). Quantum graphical models and belief propagation. Annals of Physics, 323(8):1899-1946.\nLeifer, M. S. and Spekkens, R. W. (2013). Towards a formulation of quantum theory as a causally neutral theory of Bayesian inference. Physical Review A, 88(5):052130.\nLi, L., Jamieson, K. G., DeSalvo, G., Rostamizadeh, A., and Talwalkar, A. S. (2017). Hyperband: A novel bandit-based approach to hyperparameter optimization. Journal of Machine Learning Research, 18:185:1-185:52.\nM. Zhao, H. J. (2007). Norm observable operator models. Technical report, Jacobs University.\nMiszczak, J. A. (2011). Singular value decomposition and matrix reorderings in quantum information theory. International Journal of Modern Physics C, 22(09):897-918.\nMouras, A., Beige, A., and Wiesner, K. (2010). Hidden Quantum Markov Models and non-adaptive read-out of many-body states. arXiv preprint arXiv:1002.2337.\nNielsen, M. A. and Chuang, I. L. (2010). Quantum Computation and Quantum Information: 10th Anniversary Edition. Cambridge University Press.\nPillis, J. (1967). Linear Transformations which Preserve Hermitian and Positive Semidefinite Operators. Pacific Journal of Mathematics.\nQian, N. (1999). On the momentum term in gradient descent learning algorithms. Neural Netw., 12(1):145-151.\nRabiner, L. R. (1986). An introduction to hidden markov models. ieee assp magazine, 3(1):4-16."
    },
    {
      "markdown": "Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986). Learning representations by back-propagating errors. Nature, 323:533-536.\nSchuld, M., Sinayskiy, I., and Petruccione, F. (2015). An introduction to quantum machine learning. Contemporary Physics, 56(2):172-185.\nSingh, S., James, M. R., and Rudary, M. R. (2004). Predictive state representations: A new theory for modeling dynamical systems. In Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence, UAI '04, pages 512-519, Arlington, Virginia, United States. AUAI Press.\nSrinivasan, S., Downey, C., and Boots, B. (2018a). Learning and Inference in Hilbert space with Quantum Graphical Models. In Advances in Neural Information Processing Systems 31.\nSrinivasan, S., Gordon, G., and Boots, B. (2018b). Learning hidden quantum markov models. In International Conference on Artificial Intelligence and Statistics, pages 1979-1987.\nStiefel, E. (1935-1936). Richtungsfelder und fernparallelismus in n-dimensionalem mannig faltigkeiten. Commentarii Math. Helvetici, 8(305-353).\nThon, M. and Jaeger, H. (2015). Links between multiplicity automata, observable operator models and predictive state representations - a unified learning framework. Journal of Machine Learning Research, 16:103-147.\nTowell, G. G., Noordewier, M. O., and Shavlik, J. W. (1991). Molecular biology (splicejunction gene sequences) data set. \"Available at https://archive.ics.uci.edu/ml/datasets\".\nWarmuth, M. K. and Kuzmin, D. (2014). A Bayesian probability calculus for density matrices. arXiv preprint arXiv:1408.3100.\nWen, Z. and Yin, W. (2013). A feasible method for optimization with orthogonality constraints. Mathematical Programming, 142(1-2):397-434.\nWiewiora, E. (2007). Modeling probability distributions with predictive state representations. PhD thesis, University of California, San Diego.\nWood, C. J., Biamonte, J. D., and Cory, D. G. (2015). Tensor networks and graphical calculus for open quantum systems. Quantum Info. Comput., 15(9-10):759-811.\nYeang, C.-H. (2010). A probabilistic graphical model of quantum systems. In Machine Learning and Applications (ICMLA), 2010 Ninth International Conference on, pages 155-162. IEEE.\nZhao, M. and Jaeger, H. (2010a). Norm-observable operator models. Neural Computation, 22:1927-1959.\nZhao, M.-J. and Jaeger, H. (2010b). Norm Observable Operator Models. Neural Computation, 22(7):1927-1959.\n\nŻyczkowski, K. and Bengtsson, I. (2004). On Duality between Quantum Maps and Quantum States. Open Systems \\& Information Dynamics, 11(1):3-42."
    },
    {
      "markdown": "## A Experiment on Synthetic HQMM Data\n\nAs an additional experiment on a purely quantum mechanical dataset, we compared the COSM and GS methods on data generated using the synthetic HQMM with 2 hidden states and 6 possible outputs in Srinivasan et al. (2018b). The data generation process is inspired by the well known Stern-Gerlach experiment (Gerlach and Stern, 1922) in quantum mechanics, and at least 4 hidden states are required to model it. Srinivasan et al. (2018b) demonstrated that HQMMs learned from such synthetic data showed in practice the same benefits that held in theory. Our goal is to verify that the COSM method performs at least as well as the GS method on a dataset well-suited to the HQMM model class.\n\nWe used the same synthetic dataset used by Srinivasan et al. (2018b), with 20 training and 10 validation sequences of length 3000 . We further split up each sequence into 300 sequences and use a burn-in of 100, instead of training on 3000 -length sequences with a burn-in of 1000 . This reduced training time without impacting accuracy or the amount of training data processed. We trained HQMMs using the COSM approach for 60 epochs, and saved the model that yielded the highest DA score on the validation set; we used this model to evaluate on the test set of 10 sequences of length 3000 (with burn-in 1000). The results for this model are shown in Figure 5. We see that the COSM method achieves slightly better DA compared to the GS method. We confirm that as seen in Srinivasan et al. (2018b), we need a 6 -state HMM to model this 2 -state HQMM.\n![img-4.jpeg](img-4.jpeg)\n\nFigure 5: Test Set Performance on the Synthetic HQMM Data: The dashed line represents the test set performance of the true model that generated the data. The GS and COSM methods were used to learn (2,6,1)HQMMs, while EM was used to learn HMM models with varying number of hidden states $(n)$. A 6 -state HMM model was needed to match a 2 -state HQMM.\n\n## B Updates on the Stiefel Manifold\n\nAlgorithms that constrain parameters on the Stiefel manifold generally are either projection-like (which\nre-orthogonalize the naive gradient descent updates) or geodesic-like (which directly generate updates on the manifold itself). Among geodesic-like algorithms, those proposed by Wen and Yin (2013) and Jiang and Dai (2013) are the current state-of-the-art approaches. In the regime of tall-and-skinny matrices in our problem, these two are theoretically equivalent and have the same computational complexity $O\\left(7 N n^{2}\\right)$, where $n$ is the latent dimension and $N=s w$. By comparison, the canonical gradient projection algorithm has a slightly lower computational complexity of $O\\left(3 N n^{2}\\right)$ (Jiang and Dai, 2013). We compared these three update schemes to project or retract gradients onto the Stiefel manifold. The exact update schemes for all three methods can be found in Jiang and Dai (2013).\n\nWe trained 9 HQMM models for both the synthetic HQMM and synthetic HMM datasets using these 3 update schemes. As shown in the results in Figures 6a and 6b, the three methods are very similar both in terms of speed and the final solution quality for our benchmark datasets. Since the Wen-Yin update was slightly faster, especially for larger models on the synthetic HQMM data, we used it over the alternatives.\n\n## C Sensitivity to Initialization\n\nThe COSM algorithm begins with an initial guess of the optimal parameters $\\boldsymbol{\\kappa}$ and a random intial density matrix $\\boldsymbol{\\rho}$. By 'burning-in' a reasonable number of initial entries in sequences, we minimize the effect of randomly initializing $\\boldsymbol{\\rho}$. To investigate the sensitivity of COSM to initializations of $\\boldsymbol{\\kappa}$, we trained models on the synthetic HQMM and HMM datasets over 3 random seeds. As shown in the results in Figure 7a and 7b, COSM is sensitive to random initializations for the smallest (2,6,1) model, but the variance in DA scores quickly decrease with an increase in model size, both as a function of $n$ and $w$. We observe even lower variance across different initializations for the synthetic HMM data in Figure 7b.\n\n## D Hyperparameter Selection\n\nTo facilitate a clear comparison with GS, we used the same batch size as in Srinivasan et al. (2018b), and tuned the step-size $\\tau$ and decay rate $\\alpha$ for all HQMM models. We started by manually tuning models, and identified that all models tended to converge to good solutions with the following hyperparameters: $\\tau=0.75$ and $\\alpha=0.92$ for the synthetic datasets, and $\\tau=0.8$ and $\\alpha=0.9$ for the splice dataset. We trained baseline models using these parameters, and then randomly searched for better configurations around these values.\n\nFor the synthetic datasets, we fixed the batch size at 20 and randomly sampled $\\tau$ between 0.55 and 0.95 , and $\\alpha$ between 0.9 and 0.99 . As we wanted to explore many"
    },
    {
      "markdown": "hyperparameter settings, we only trained on 3 random batches in every epoch. For the splice dataset, we fixed the batch size at 200 and randomly sampled $\\tau$ between 0.7 and 0.9 and $\\alpha$ between 0.88 and 0.92 . Since each splice model required learning three separate HQMMs across multiple folds, we tested fewer hyperparameter settings across a smaller search space. We also trained on a single random batch every epoch across 2 folds.\n\nGiven the large number of models that we needed to evaluate, we used the Hyperband scheduling technique (Li et al., 2017) to quickly sample through many hyperparameter configurations. For each model, we began by running 3 epochs for each of the $k$ randomly selected configurations, and removed $k / 3$ of them with the lowest validation DA scores. In the next round, we ran the remaining configurations for a larger number of iterations, and again removed the bottom third of the configurations with the lowest scores. We repeated this strategy until only one configuration remained, and saved the one with the highest validation DA throughout the tuning protocol. We searched across 27 and 9 random configurations for the synthetic and the splice datasets respectively. As an example, for the synthetic datasets we trained 27 models for 3 epochs, followed by the 9 best models for 9 epochs, followed by the 3 best models for 9 epochs, and the final best model for 27 epochs. In Table 1, we report the hyperparameters obtained through Hyperband that outperformed the default configuration. For models not listed in the table, the default configuration resulted in the best performance.\n\nAll our experiments were performed on a desktop with 8 Intel Core i7-7700K 4.20 GHz CPUs, and 31.3 GB RAM. All models are trained in MATLAB, but the gradient computation happens in Python.\n\n## E Estimating Speedup\n\nSince the GS method can take days to converge to the final solution for large models such as ( $6,6,6$ )-HQMM, it was not feasible to compute a direct speed up comparing its convergence time to COSM across most models. Thus, we estimate the speed-up offered by COSM by fitting a linear model to the DA trajectory of models learned by the GS method. Specifically, for a given HQMM model, we train both COSM and GS on the synthetic HMM data until one of them converges within a tolerance of $10^{-5}$ in DA scores. Since COSM always converges first, we take the DA scores achieved by GS in its last 10 steps and fit a linear model to it. We then extrapolate this linear model to estimate the time it would take for GS to reach some fraction of the solution DA reached by COSM. Note that a linear fit is an optimistic assumption of GS convergence time, meaning we are going to understate how much faster COSM is compared to GS. Finally, we estimate the speed up offered by COSM as the ratio of the (estimated) convergence time\n\nTable 1: Hyperparameter Selection The best performing step sizes $(\\tau)$ and decay rates $(\\alpha)$ for various COSM models. For models not listed here, the default hyperparameters $(\\tau=0.75, \\alpha=0.92)$ and $(\\tau=0.8, \\alpha=0.9)$ yielded the best results for the synthetic datasets and the splice dataset respectively.\n\n| Dataset | $n$ | $s$ | $w$ | $\\tau$ | $\\alpha$ |\n| :--: | :--: | :--: | :--: | :--: | :--: |\n| Synthetic HQMM | 2 | 6 | 1 | 0.75 | 0.92 |\n| Synthetic HMM | 2 | 6 | 1 | 0.95 | 0.99 |\n|  | 4 | 6 | 6 | 0.95 | 0.96 |\n|  | 5 | 6 | 1 | 0.55 | 0.96 |\n|  | 5 | 6 | 2 | 0.95 | 0.98 |\n|  | 5 | 6 | 6 | 0.95 | 0.99 |\n| Splice | 2 | 4 | 1 | 0.70 | 0.90 |\n|  | 2 | 4 | 2 | 0.85 | 0.92 |\n|  | 2 | 4 | 6 | 0.85 | 0.92 |\n|  | 4 | 4 | 1 | 0.90 | 0.92 |\n|  | 4 | 4 | 4 | 0.90 | 0.90 |\n|  | 6 | 4 | 4 | 0.70 | 0.90 |\n|  | 8 | 4 | 1 | 0.90 | 0.90 |\n\nfor GS and the actual convergence time for COSM. In Figure 8, we plot this estimated speed up with varying number of parameters (both as functions of $n$ and $w$ ) for different solution fractions. For a solution fraction of 1 , we record speedups greater than $150 \\times$ for the largest HQMMs trained. Furthermore, COSM offers comparable increase in speed up as parameters grow either by virtue of increasing the number of latent states $n$ or the Kraus-rank $w$."
    },
    {
      "markdown": "![img-5.jpeg](img-5.jpeg)\n\nFigure 6: Alternative Schemes to Constrain Updates on the Stiefel Manifold Validation set accuracies obtained for HQMMs trained using different update schemes. All schemes provide similar speed and accuracy, but the Wen-Yin update outperforms the others by a small margin."
    },
    {
      "markdown": "![img-6.jpeg](img-6.jpeg)\n\nFigure 7: COSM's Sensitivity to Random Initializations of $\\kappa$ Validation set accuracies obtained across 10 epochs for HQMMs trained on 3 different random initializations. COSM is sensitive to $\\boldsymbol{\\kappa}$ initialization for the smallest models, but is fairly robust for larger models."
    },
    {
      "markdown": "![img-7.jpeg](img-7.jpeg)\n\nFigure 8: Estimated Speedup of COSM over GS: Estimated speedups of COSM over GS for various solution fractions. As seen in the plots for solution fraction of 1, GS can take more than 150 times the convergence time for COSM to reach the latter's final solution quality."
    }
  ],
  "usage_info": {
    "pages_processed": 16,
    "doc_size_bytes": 1155464
  },
  "_metadata": {
    "model_version": "0.0.0",
    "llm_model": "mistral-ocr-2505",
    "query_data": {
      "pdf": "/Users/satyaortiz-gagne/CODE/paperext/data/cache/arxiv/1912.02098.pdf"
    },
    "model_id": "parsepdf"
  }
}