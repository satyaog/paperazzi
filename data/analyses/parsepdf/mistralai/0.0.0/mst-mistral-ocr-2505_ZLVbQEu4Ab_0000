{
  "pages": [
    {
      "markdown": "# When is Momentum Extragradient Optimal? A Polynomial-Based Analysis \n\nJunhyung Lyle Kim* jlylekim@rice.edu\nRice University, Department of Computer Science\nGauthier Gidel\ngauthier.gidel@umontreal.ca\nUniversité de Montréal, Department of Computer Science and Operations Research\nMila - Quebec AI Institute, Canada CIFAR AI Chair\nAnastasios Kyrillidis\nanastasios@rice.edu\nRice University, Department of Computer Science\nFabian Pedregosa\npedregosa@google.com\nGoogle DeepMind\nReviewed on OpenReview: https://openreview. net/forum?id=ZLVbQEu\\&Ab\n\n\n#### Abstract\n\nThe extragradient method has gained popularity due to its robust convergence properties for differentiable games. Unlike single-objective optimization, game dynamics involve complex interactions reflected by the eigenvalues of the game vector field's Jacobian scattered across the complex plane. This complexity can cause the simple gradient method to diverge, even for bilinear games, while the extragradient method achieves convergence. Building on the recently proven accelerated convergence of the momentum extragradient method for bilinear games (Azizian et al., 2020b), we use a polynomial-based analysis to identify three distinct scenarios where this method exhibits further accelerated convergence. These scenarios encompass situations where the eigenvalues reside on the (positive) real line, lie on the real line alongside complex conjugates, or exist solely as complex conjugates. Furthermore, we derive the hyperparameters for each scenario that achieve the fastest convergence rate.\n\n\n## 1 Introduction\n\nWhile most machine learning problems are formulated as minimization problems, a growing number of works rely instead on game formulations that involve multiple players and objectives. Examples of such problems include generative adversarial networks (GANs) (Goodfellow et al., 2014), actor-critic algorithms (Pfau \\& Vinyals, 2016), sharpness aware minimization (Foret et al., 2021), and fine-tuning language models from human feedback (Munos et al., 2023). This increasing interest in game formulations motivates further theoretical exploration of differentiable games.\n\nOptimizing differentiable games presents challenges absent in minimization problems due to the interplay of multiple players and objectives. Notably, the game Jacobian's eigenvalues are distributed on the complex plane, exhibiting richer dynamics compared to single-objective minimization, where the Hessian eigenvalues are restricted to the real line. Consequently, even for simple bilinear games, standard algorithms like the gradient method fail to converge (Mescheder et al., 2018; Balduzzi et al., 2018; Gidel et al., 2019).\n\nFortunately, the extragradient method (EG), originally introduced by Korpelevich Korpelevich (1976), offers a solution. Unlike the gradient method, EG demonstrably converges for bilinear games (Tseng, 1995). This\n\n[^0]\n[^0]:    * Authors after JLK are listed in alphabetical order.\n\n    This paper extends Kim et al. (2022) presented at the NeurIPS 2022 Optimization for Machine Learning Workshop."
    },
    {
      "markdown": "has sparked extensive research analyzing EG from different perspectives, including variational inequality (Gidel et al., 2018; Gorbunov et al., 2022), stochastic (Li et al., 2021), and distributed (Liu et al., 2020; Beznosikov et al., 2021) settings.\n\nMost existing works, including those mentioned earlier, analyze EG and relevant algorithms by assuming some structure on the objectives, such as (strong) monotonicity or Lipschitzness (Solodov \\& Svaiter, 1999; Tseng, 1995; Daskalakis \\& Panageas, 2018; Ryu et al., 2019; Azizian et al., 2020a). Such assumptions, in the context of differentiable games, confine the distribution of the eigenvalues of the game Jacobian; for instance, strong monotonicity implies a lower bound on the real part of the eigenvalues, and the Lipschitz assumption implies an upper bound on the magnitude of the eigenvalues of the Jacobian.\n\nBuilding upon the limitations of prior assumptions, Azizian et al. (2020b) showed that the key factor for effectively analyzing game dynamics lies in the spectrum of the Jacobian on the complex plane. Through a polynomial-based analysis, they demonstrated that first-order methods can sometimes achieve faster rates using momentum. This is achieved by replacing the smoothness and monotonicity assumptions with more precise assumptions on the distribution of the Jacobian eigenvalues, represented by simple shapes like ellipses or line segments. Notably, Azizian et al. (2020b) proved that for bilinear games, the extragradient method with momentum achieves an accelerated convergence rate.\n\nIn this work, we take a different approach by asking the reverse question: for what shapes of the Jacobian spectrum does the momentum extragradient (MEG) method achieve optimal performance? This reverse analysis allows us to study the behavior of MEG in specific settings depending on the hyperparameter setup, encompassing:\n\n- Minimization, where all Jacobian eigenvalues lie on the positive real line.\n- Regularized bilinear games, where all eigenvalues are complex conjugates.\n- Intermediate case, where eigenvalues are both on the real line and as complex conjugates (illustrated in Figure 1).\n\nOur contributions can be summarized as follows:\n\n- Characterizing MEG convergence modes: We derive the residual polynomials of MEG for affine game vector fields and identify three distinct convergence modes based on hyperparameter settings. This analysis can then be applied to different eigenvalue structures of the Jacobian (see Theorem 3).\n- Optimal hyperparameters and convergence rates: For each eigenvalue structure, we derive the optimal hyperparameters of MEG and its (asymptotic) convergence rates. For minimization, MEG exhibits \"super-acceleration,\" where a constant improvement upon classical lower bound rate is attained, ${ }^{1}$ similarly to the gradient method with momentum (GDM) with cyclical step sizes (Goujaud et al., 2022). For the other two cases involving imaginary eigenvalues, MEG exhibits accelerated convergence rates with the derived optimal hyperparameters.\n- Comparison with other methods. We compare MEG's convergence rates with gradient (GD), GDM, and extragradient (EG) methods. For the considered game classes, none of these methods achieve (asymptotically) accelerated rates (Corollaries 1 and 2), unlike MEG. In Section 7, we validate our findings through numerical experiments, including scenarios with slight deviations from our initial assumptions.\n\n\n# 2 Problem Setup and Related Work \n\nFollowing Letcher et al. (2019); Balduzzi et al. (2018), we define the $n$-player differentiable game as a family of twice continuously differentiable losses $\\ell_{i}: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}$, for $i=1, \\ldots, n$. The player $i$ controls the parameter $w^{(i)} \\in \\mathbb{R}^{d_{i}}$. We denote the concatenated parameters by $w=\\left[w^{(1)}, \\ldots, w^{(n)}\\right] \\in \\mathbb{R}^{d}$, where $d=\\sum_{i=1}^{n} d_{i}$.\n\n[^0]\n[^0]:    ${ }^{1}$ Note that achieving this improvement is possible by having additional information beyond just the largest (smoothness) and smallest (strong convexity) eigenvalues of the Jacobian."
    },
    {
      "markdown": "![img-0.jpeg](img-0.jpeg)\n\nFigure 1: Convergence rates of $M E G$ in terms of the game Jacobian eigenvalues. The step sizes for MEG, $h$ and $\\gamma$, and the momentum parameter $m$ are set up according to each case of Theorem 3, illustrating three distinct convergence modes of MEG. For each case, the red line indicates the robust region (c.f., Definition 1) where MEG achieves the optimal convergence rate.\n\nFor this problem, a Nash equilibrium satisfies: $w^{(i)^{*}} \\in \\arg \\min _{w^{(i)} \\in \\mathbb{R}^{d_{i}}} \\ell_{i}\\left(w^{(i)}, w^{(\\neg i)^{*}}\\right) \\quad \\forall i \\in\\{1, \\ldots, n\\}$, where the notation ${ }^{(\\neg i)}$ denotes all indices except for $i$. We also define the vector field $v$ of the game as the concatenation of the individual gradients: $v(w)=\\left[\\nabla_{w^{(1)}} \\ell_{1}(w) \\cdots \\nabla_{w^{(n)}} \\ell_{n}(w)\\right]^{\\top}$, and denote its associated Jacobian with $\\nabla v$.\n\nUnfortunately, finding Nash equilibria for general games remains an intractable problem (Shoham \\& LeytonBrown, 2008; Letcher et al., 2019). ${ }^{2}$ Therefore, instead of directly searching for Nash equilibria, we focus on finding stationary points of the game's vector field $v$. This approach is motivated by the fact that any Nash equilibrium necessarily corresponds to a stationary point of the gradient dynamics. In other words, we aim to solve the following problem:\n\n$$\n\\text { Find } w^{*} \\in \\mathbb{R}^{d} \\quad \\text { such that } \\quad v\\left(w^{*}\\right)=0\n$$\n\nNotation. $\\mathfrak{N}(z)$ and $\\mathfrak{I}(z)$ respectively denote the real and the imaginary part of a complex number $z$. The spectrum of a matrix $M$ is denoted by $\\operatorname{Sp}(M)$, and its spectral radius by $\\rho(M):=\\max \\{|\\lambda|: \\lambda \\in \\operatorname{Sp}(M)\\}$. $M \\succ 0$ denotes that $M$ is a positive-definite matrix. $\\mathbb{C}_{+}$denotes the complex plane with positive real part, and $\\mathbb{R}_{+}$denotes positive real numbers.\n\n# 2.1 Related Work \n\nThe extragradient method, originally introduced in Korpelevich (1976), is a popular algorithm for solving (unconstrained) variational inequality problems in (1) (Gidel et al., 2018). There are several works that study the convergence rate of EG for (strongly) monotone problems (Tseng, 1995; Solodov \\& Svaiter, 1999; Nemirovski, 2004; Monteiro \\& Svaiter, 2010; Mokhtari et al., 2020; Gorbunov et al., 2022). Under similar settings, stochastic variants of EG are studied in Palaniappan \\& Bach (2016); Hsieh et al. (2019; 2020); Li et al. (2021). However, as mentioned earlier, assumptions like (strong) monotonicity or Lipchtizness may not accurately represent how Jacobian eigenvalues are distributed.\n\nInstead, we make more fine-grained assumptions on these eigenvalues, to obtain the optimal hyperparameters and convergence rates for MEG via polynomial-based analysis. Such analysis dates back to the development of the conjugate gradient method (Hestenes \\& Stiefel, 1952), and is still actively used; for instance, to derive lower bounds (Arjevani \\& Shamir, 2016), to develop accelerated decentralized algorithms (Berthier et al., 2020), and to analyze average-case performance (Pedregosa \\& Scieur, 2020; Domingo-Enrich et al., 2021).\n\nOn that end, we use the following lemma (Chihara, 2011), which elucidates the connection between firstorder methods and (residual) polynomials, when the vector field $v$ is affine. First-order methods are the ones in which the sequence of iterates $w_{t}$ is in the span of previous gradients: $w_{t} \\in w_{0}+\\operatorname{span}\\left\\{v\\left(w_{0}\\right), \\ldots, v\\left(w_{t-1}\\right)\\right\\}$.\n\n[^0]\n[^0]:    ${ }^{2}$ Formulating Nash equilibrium search as a nonlinear complementarity problem makes it inherently difficult, classified as PPAD-hard (Daskalakis et al., 2009; Letcher et al., 2019)."
    },
    {
      "markdown": "Lemma 1 (Chihara (2011)). Let $w_{t}$ be the iterate generated by a first-order method after $t$ iterations, with $v(w)=A w+b$. Then, there exists a real polynomial $P_{t}$, of degree at most $t$, satisfying:\n\n$$\nw_{t}-w^{\\star}=P_{t}(A)\\left(w_{0}-w^{\\star}\\right)\n$$\n\nwhere $P_{t}(0)=1$, and $v\\left(w^{\\star}\\right)=A w^{\\star}+b=0$.\nBy taking $\\ell_{2}$-norms, (2) further implies the following worst-case convergence rate:\n\n$$\n\\left\\|w_{t}-w^{\\star}\\right\\|=\\left\\|P_{t}(A)\\left(w_{0}-w^{\\star}\\right)\\right\\| \\leqslant\\left\\|P_{t}\\left(Z \\Lambda Z^{-1}\\right)\\right\\| \\cdot\\left\\|w_{0}-w^{\\star}\\right\\| \\leqslant \\sup _{\\lambda \\in \\mathcal{S}^{\\star}}\\left|P_{t}(\\lambda)\\right| \\cdot\\|Z\\|\\left\\|Z^{-1}\\right\\| \\cdot\\left\\|w_{0}-w^{\\star}\\right\\|\n$$\n\nwhere $A=Z \\Lambda Z^{-1}$ is the diagonalization of $A,{ }^{3}$ and the constant $\\|Z\\|\\left\\|Z^{-1}\\right\\|$ disappears if $A$ is a normal matrix. Hence, the worst-case convergence rate of a first-order method can be analyzed by studying the associated residual polynomial $P_{t}$ evaluated at the eigenvalues $\\lambda$ of the Jacobian $\\nabla v=A$, distributed over the set $S^{\\star}$.\n\nUnlocking Faster Rates Through Fine-Grained Spectral Shapes. While Azizian et al. (2020b) characterized lower bounds and optimality for certain first-order methods under simple spectral shapes, we posit that a more granular understanding of $\\mathcal{S}^{\\star}$ could unlock even faster convergence rates. By meticulously analyzing the residual polynomials of MEG, we identify specific spectral shapes where MEG exhibits optimal performance. This approach resonates with recent advancements in optimization literature (Oymak, 2021; Goujaud et al., 2022), which demonstrate that knowledge beyond merely the largest and smallest eigenvalues (i.e., smoothness and strong convexity) can lead to accelerated convergence in convex smooth minimization.\n\n# 3 Momentum Extragradient via Chebyshev Polynomials \n\nIn this section, we delve into the intricate dynamics of the momentum extragradient method (MEG) by harnessing the power of residual polynomials and Chebyshev polynomials.\nMEG iterates according to the following update rule:\n\n$$\n(\\mathrm{MEG}) \\quad w_{t+1}=w_{t}-h v\\left(w_{t}-\\gamma v\\left(w_{t}\\right)\\right)+m\\left(w_{t}-w_{t-1}\\right)\n$$\n\nwhere $h$ is the step size, $\\gamma$ is the extrapolation step size, and $m$ is the momentum parameter.\nThe extragradient method (EG), which serves as the foundation for MEG, was originally proposed by Korpelevich (1976) for saddle point problems. It has garnered renewed interest due to its remarkable ability to converge in certain differentiable games, such as bilinear games, where the standard gradient method falters (Gidel et al., 2019; Azizian et al., 2020b;a).\nFor completeness, we remind the gradient method with momentum (GDM):\n\n$$\n(\\mathrm{GDM}) \\quad w_{t+1}=w_{t}-h v\\left(w_{t}\\right)+m\\left(w_{t}-w_{t-1}\\right)\n$$\n\nfrom which the gradient method (GD) can be obtained by setting $m=0$.\nAs a first-order method (Arjevani \\& Shamir, 2016; Azizian et al., 2020b), MEG's behavior can be elegantly analyzed through the lens of residual polynomials, as established in Lemma 1. The following theorem unveils the specific residual polynomials associated with MEG:\nTheorem 1 (Residual polynomials of MEG and their Chebyshev representation). Consider the momentum extragradient method (MEG) in (4) with a vector field of the form $v(w)=A w+b$. The residual polynomials associated with MEG can be expressed as follows:\n\n$$\n\\tilde{P}_{0}(\\lambda)=1, \\quad \\tilde{P}_{1}(\\lambda)=1-\\frac{h \\lambda(1-\\gamma \\lambda)}{1+m}, \\quad \\text { and } \\quad \\tilde{P}_{t+1}(\\lambda)=(1+m-h \\lambda(1-\\gamma \\lambda)) \\tilde{P}_{t}(\\lambda)-m \\tilde{P}_{t-1}(\\lambda)\n$$\n\n[^0]\n[^0]:    ${ }^{3}$ Note that almost all matrices are diagonalizable over $\\mathbb{C}$, in the sense that the set of non-diagonalizable matrices has Lebesgue measure zero (Hetzel et al., 2007)."
    },
    {
      "markdown": "Remarkably, these polynomials can be elegantly rewritten in terms of Chebyshev polynomials of the first and second kind, denoted by $T_{t}(\\cdot)$ and $U_{t}(\\cdot)$, respectively:\n\n$$\nP_{t}^{M E G}(\\lambda)=m^{t / 2}\\left(\\frac{2 m}{1+m} T_{t}(\\sigma(\\lambda))+\\frac{1-m}{1+m} U_{t}(\\sigma(\\lambda))\\right), \\text { where } \\sigma(\\lambda) \\equiv \\sigma(\\lambda ; h, \\gamma, m)=\\frac{1+m-h \\lambda(1-\\gamma \\lambda)}{2 \\sqrt{m}}\n$$\n\nThe term $\\sigma(\\lambda)$, which encapsulates the interplay between step sizes, momentum, and eigenvalues, is referred to as the link function.\n\nThe residual polynomials of MEG and GDM, intriguingly, share a similar structure but differ in their link functions. Below are the residual polynomials of GDM, expressed in Chebyshev polynomials (Pedregosa, 2020):\n\n$$\nP_{t}^{\\mathrm{GDM}}(\\lambda)=m^{t / 2}\\left(\\frac{2 m}{1+m} T_{t}(\\xi(\\lambda))+\\frac{1-m}{1+m} U_{t}(\\xi(\\lambda))\\right), \\quad \\text { where } \\quad \\xi(\\lambda)=\\frac{1+m-h \\lambda}{2 \\sqrt{m}}\n$$\n\nNotice that the residual polynomials of MEG in (6) and that of GDM in (7) are identical, except for the link functions $\\sigma(\\lambda)$ and $\\xi(\\lambda)$, which enter as arguments in $T_{t}(\\cdot)$ and $U_{t}(\\cdot)$.\nThe differences in these link functions are paramount because the behavior of Chebyshev polynomials hinges decisively on their argument's domain:\nLemma 2 (Goujaud \\& Pedregosa (2022)). Let $z$ be a complex number, and let $T_{t}(\\cdot)$ and $U_{t}(\\cdot)$ be the Chebyshev polynomials of the first and second kind, respectively. The sequence $\\left\\{\\left|\\frac{2 m}{1+m} T_{t}(z)+\\frac{1-m}{1+m} U_{t}(z)\\right|\\right\\}_{t \\geqslant 0}$ grows exponentially in $t$ for $z \\notin[-1,1]$, while for $z \\in[-1,1]$, the following bounds hold:\n\n$$\n\\left|T_{t}(z)\\right| \\leqslant 1 \\quad \\text { and } \\quad\\left|U_{t}(z)\\right| \\leqslant t+1\n$$\n\nTherefore, to study the optimal convergence behavior of MEG, we are interested in the case where the set of step sizes and the momentum parameters lead to $|\\sigma(\\lambda ; h, \\gamma, m)| \\leqslant 1$ so that we can use the bounds in (8). We will refer to those sets of eigenvalues and hyperparameters as the robust region, as defined below.\nDefinition 1 (Robust region of MEG). Consider the MEG method in (4) expressed via Chebyshev polynomials, as in (6). We define the set of eigenvalues and hyperparameters such that the image of the link function $\\sigma(\\lambda ; h, \\gamma, m)$ lies in the interval $[-1,1]$ as the robust region, and denote it with $\\sigma^{-1}([-1,1])$.\n\nAlthough polynomial-based analysis requires the assumption that the vector field is affine, it captures intuitive insights into how various algorithms behave in different settings, as we remark below.\nRemark 1. From the definition of $\\xi(\\lambda)$ in (7), one can infer why negative momentum can help the convergence of GDM (Gidel et al., 2019) when $\\lambda \\in \\mathbb{R}_{+}$: it forces GDM to stay within the robust region, $|\\xi(\\lambda)| \\leqslant 1$. One can also infer the divergence of GDM in the presence of complex eigenvalues, unless, for instance, complex momentum is used (Lorraine et al., 2022). Similarly, the residual polynomial of GD is $P_{t}^{G D}(\\lambda)=(1-h \\lambda)^{t}$ (Goujaud $\\mathcal{E}$ Pedregosa, 2022, Example 4.2), and can easily diverge in the presence of complex eigenvalues, which can potentially be alleviated by using complex step sizes. On the contrary, thanks to the quadratic link function of MEG in (6), it can converge for much wider subsets of complex eigenvalues.\n\nBy analyzing the residual polynomials of MEG, we can also characterize the asymptotic convergence rate of MEG for any combination of hyperparameters, as summarized in the next theorem.\nTheorem 2 (Asymptotic convergence rate of MEG). Suppose $v(w)=A w+b$. The asymptotic convergence rate of MEG in (4) is: ${ }^{4}$\n\n$$\n\\limsup _{t \\rightarrow \\infty} \\sqrt[3]{\\frac{\\left\\|w_{t}-w^{\\star}\\right\\|}{\\left\\|w_{0}-w^{\\star}\\right\\|}}= \\begin{cases}\\sqrt[4]{m}, & \\text { if } \\quad \\bar{\\sigma} \\leqslant 1 \\quad \\text { (robust region); } \\\\ \\sqrt[4]{m}\\left(\\bar{\\sigma}+\\sqrt{\\bar{\\sigma}^{2}-1}\\right)^{1 / 2}, & \\text { if } \\quad \\bar{\\sigma} \\in\\left(1, \\frac{1+m}{2 \\sqrt{m}}\\right) \\\\ \\geqslant 1 \\text { (no convergence), } & \\text { otherwise, }\\end{cases}\n$$\n\nwhere $\\bar{\\sigma}=\\sup _{\\lambda \\in \\mathcal{S}^{*}}|\\sigma(\\lambda ; h, \\gamma, m)|$, and $\\sigma(\\lambda ; h, \\gamma, m) \\equiv \\sigma(\\lambda)$ is the link function of MEG defined in (6).\n\n[^0]\n[^0]:    ${ }^{4}$ The reason why we take the $2 t$-th root is to normalize by the number of vector field computations; we compare in Section 4 the asymptotic rate of MEG in (9) with other gradient methods that use a single vector field computation in the recurrences, such as GD and GDM."
    },
    {
      "markdown": "Optimal hyperparameters for MEG that we obtain in Section 4 minimize the asymptotic convergence rate above. Note that the optimal hyperparameters vary based on the set $\\mathcal{S}^{*}$, which we detail in Section 3.2.\n\n# 3.1 Three Modes of the Momentum Extragradient \n\nWithin the robust region of MEG, we can compute its worst-case convergence rate based on (3) as follows:\n\n$$\n\\begin{aligned}\n\\sup _{\\lambda \\in \\mathcal{S}^{*}}\\left|P_{t}^{\\mathrm{MEG}}(\\lambda)\\right| & \\stackrel{(6)}{\\leqslant} m^{t / 2}\\left(\\frac{2 m}{1+m} \\sup _{\\lambda \\in \\mathcal{S}^{*}}\\left|T_{t}(\\sigma(\\lambda))\\right|+\\frac{1-m}{1+m} \\sup _{\\lambda \\in \\mathcal{S}^{*}}\\left|U_{t}(\\sigma(\\lambda))\\right|\\right) \\\\\n& \\leqslant{ }^{(8)} m^{t / 2}\\left(\\frac{2 m}{1+m}+\\frac{1-m}{1+m}(t+1)\\right) \\leqslant m^{t / 2}(t+1)\n\\end{aligned}\n$$\n\nSince the Chebyshev polynomial expressions of MEG in (6) and that of $\\mathrm{GDM}^{5}$ are identical except for the link functions, the convergence rate in (10) applies to both MEG and GDM, as long as the link functions $|\\sigma(\\lambda)|$ and $|\\xi(\\lambda)|$ are bounded by 1 . As a result, we see that the asymptotic convergence rate in (9) only depends on the momentum parameter $m$, when the hyperparameters are restricted to the robust region. This fact was utilized in tuning GDM for strongly convex quadratic minimization (Zhang \\& Mitliagkas, 2019).\nThe robust region of MEG can be described with the four extreme points below (derivation in the appendix):\n\n$$\n\\sigma^{-1}(-1)=\\frac{1}{2 \\gamma} \\pm \\sqrt{\\frac{1}{4 \\gamma^{2}}-\\frac{(1+\\sqrt{m})^{2}}{h \\gamma}}, \\quad \\text { and } \\quad \\sigma^{-1}(1)=\\frac{1}{2 \\gamma} \\pm \\sqrt{\\frac{1}{4 \\gamma^{2}}-\\frac{(1-\\sqrt{m})^{2}}{h \\gamma}}\n$$\n\nThe above four points and their intermediate values characterize the set of Jacobian eigenvalues $\\lambda$ that can be mapped to $[-1,1]$. The distribution of these eigenvalues can vary in three different modes depending on the selected hyperparameters of MEG, as stated in the following theorem.\nTheorem 3. Consider the momentum extragradient method in (4), expressed with the Chebyshev polynomials as in (6). Then, the robust region (c.f., Definition 1) have the following three modes:\n\n- Case 1: If $\\frac{h}{4 \\gamma} \\geqslant(1+\\sqrt{m})^{2}$, then $\\sigma^{-1}(-1)$ and $\\sigma^{-1}(1)$ are all real numbers;\n- Case 2: If $(1-\\sqrt{m})^{2} \\leqslant \\frac{h}{4 \\gamma}<(1+\\sqrt{m})^{2}$, then $\\sigma^{-1}(-1)$ are complex, and $\\sigma^{-1}(1)$ are real;\n- Case 3: If $(1-\\sqrt{m})^{2}>\\frac{h}{4 \\gamma}$, then $\\sigma^{-1}(-1)$ and $\\sigma^{-1}(1)$ are all complex numbers.\n\nRemark 2. Theorem 3 offers guidance on how to set up the hyperparameters for MEG. This depends on the Jacobian spectra of the game problem being considered. For instance, if one observes only real eigenvalues (i.e., the problem is in fact minimization), the main step size $h$ should be at least $4 \\times$ larger than the extrapolation step size $\\gamma$, based on the condition $\\frac{h}{4 \\gamma} \\geqslant(1+\\sqrt{m})^{2}$.\n\nWe illustrate Theorem 3 in Figure 1. We first set the hyperparameters according to each condition in Theorem 3. We then discretize the interval $[-1,1]$, and plot $\\sigma^{-1}([-1,1])$ for each case, represented by red lines. We can see the quadratic link function induced by MEG allows interesting eigenvalue dynamics to be mapped onto the $[-1,1]$ segment, such as the cross-shape observed in Case 2. Moreover, although MEG exhibits the best rates within the robust region, it does not necessarily diverge outside of it, as in the second case of Theorem 2. We illustrate the convergence region of MEG measured by $\\sqrt[n]{\\left|P_{t}(\\bar{\\lambda})\\right|}<1$ from (6) for $t=2000$, with different colors indicating varying convergence rates, which slow down as one moves away from the robust region. Interestingly, Figure 1 (right) shows that MEG can also converge in the absence of monotonicity (i.e., in the presence of Jacobian eigenvalues with negative real part) (Gorbunov et al., 2023).\n\n### 3.2 Robust Region-Induced Problem Cases\n\nWe classify problem classes into three distinct cases based on Theorem 3, each reflecting a different mode of the robust region (Figure 2):\n\n[^0]\n[^0]:    ${ }^{5}$ Asymptotically, GDM enjoys $\\sqrt{m}$ convergence rate instead of the $\\sqrt[4]{m}$ of MEG, as it uses a single vector field computation per iteration instead of the two. However, these are not directly comparable, as the values of $m$ that correspond to the robust region are not the same."
    },
    {
      "markdown": "![img-1.jpeg](img-1.jpeg)\n\nFigure 2: Illustration of the three spectrum models where MEG achieves accelerated convergence rates.\n\nCase 1: The problem reduces to minimization, where the Jacobian eigenvalues are distributed on the (positive) real line, but as a union of two intervals. We can model such spectrum as:\n\n$$\n\\operatorname{Sp}(\\nabla v) \\subset \\mathcal{S}_{1}^{\\star}=\\left[\\mu_{1}, L_{1}\\right] \\cup\\left[\\mu_{2}, L_{2}\\right] \\subset \\mathbb{R}_{+}\n$$\n\nAbove generalizes the Hessian spectrum that arise in minimizing $\\mu$-strongly convex and $L$-smooth functions, i.e. $\\lambda \\in[\\mu, L]$. This spectrum can be obtained from (12) by setting $\\mu_{1}=\\mu, L_{2}=L$, and $L_{1}=\\mu_{2}$. It was empirically observed that, during DNN training, sometimes a few eigenvalues of the Hessian have significantly larger magnitudes (Papyan, 2020). In such cases, (12) can be more precise than a single interval $[\\mu, L]$. In particular, Goujaud et al. (2022) utilized (12), and showed that the GDM with alternating step sizes can achieve a (constant factor) improvement over the traditional lower bound for strongly convex and smooth quadratic objectives.\n\nIn Section 4, we show that MEG enjoys similar improvement. To show that, we define the following quantities following Goujaud et al. (2022), which will be used to obtain the convergence rate of MEG in (18) for this problem class.\n\n$$\n\\zeta:=\\frac{L_{2}+\\mu_{1}}{L_{2}-\\mu_{1}}=\\frac{1+\\tau}{1-\\tau}, \\quad \\text { and } \\quad R:=\\frac{\\mu_{2}-L_{1}}{L_{2}-\\mu_{1}} \\in[0,1)\n$$\n\nHere, $\\zeta$ is the ratio between the center of $\\mathcal{S}_{1}^{\\star}$ and its radius, and $\\tau:=L_{2} / \\mu_{1}$ is the inverse condition number. $R$ is the relative gap of $\\mu_{2}-L_{1}$ and $L_{2}-\\mu_{1}$, which becomes 0 if $\\mu_{2}=L_{1}$ (i.e., $\\mathcal{S}_{1}^{\\star}$ becomes $\\left.\\left[\\mu_{1}, L_{2}\\right]\\right)$.\n\nCase 2: In this case, the Jacobian eigenvalues are distributed both on the real line and as complex conjugates, exhibiting a cross-shaped spectrum. We model this spectrum as:\n\n$$\n\\operatorname{Sp}(\\nabla v) \\subset \\mathcal{S}_{2}^{\\star}=[\\mu, L] \\cup\\left\\{z \\in \\mathbb{C}: \\mathfrak{R}(z)=c^{\\prime}>0, \\mathcal{I}(z) \\in[-c, c]\\right\\}\n$$\n\nThe first set $[\\mu, L]$ denotes a segment on the real line, reminiscent of the Hessian spectrum for minimizing $\\mu$-strongly convex and $L$-smooth functions. The second set has a fixed real component $\\left(c^{\\prime}>0\\right)$, along with imaginary components symmetric across the real line (i.e., complex conjugates), as the Jacobian is real.\n\nThis is a strict generalization of the purely imaginary interval $\\pm[a i, b i]$ commonly considered in the bilinear games literature (Liang \\& Stokes, 2019; Azizian et al., 2020b; Mokhtari et al., 2020). While many recent papers on bilinear games cite GANs (Goodfellow et al., 2014) as a motivation, the work in Berard et al. (2020, Figure 4) empirically shows that the spectrum of GANs is not contained in the imaginary axis; the cross-shaped spectrum model above might be closer to some of the observed GAN spectra.\n\nCase 3: In this case, the Jacobain eigenvalues are distributed only as complex conjugates, with a fixed real component, exhibiting a shifted imaginary spectrum. We model this spectrum as:\n\n$$\n\\operatorname{Sp}(\\nabla v) \\subset \\mathcal{S}_{3}^{\\star}=[c+a i, c+b i] \\cup[c-a i, c-b i] \\subset \\mathbb{C}_{+}\n$$\n\nAgain, (15) generalizes bilinear games, where the spectrum reduces to $\\pm[a i, b i]$ with $c=0$."
    },
    {
      "markdown": "Examples of Cases 2 and 3 in quadratic games. To understand these spectra better, we provide examples using quadratic games. Consider the following two player quadratic game, where $x \\in \\mathbb{R}^{d_{1}}$ and $y \\in \\mathbb{R}^{d_{2}}$ are the parameters controlled by each player, whose loss functions respectively are:\n\n$$\n\\ell_{1}(x, y)=\\frac{1}{2} x^{\\top} S_{1} x+x^{\\top} M_{12} y+x^{\\top} b_{1} \\quad \\text { and } \\quad \\ell_{2}(x, y)=\\frac{1}{2} y^{\\top} S_{2} y+y^{\\top} M_{21} x+y^{\\top} b_{2}\n$$\n\nwhere $S_{1}, S_{2} \\succ 0$. Then, the vector field can be written as:\n\n$$\nv(x, y)=\\left[\\begin{array}{l}\nS_{1} x+M_{12} y+b_{1} \\\\\nM_{21} x+S_{2} y+b_{2}\n\\end{array}\\right]=A w+b, \\text { where } A=\\left[\\begin{array}{cc}\nS_{1} & M_{12} \\\\\nM_{21} & S_{2}\n\\end{array}\\right], w=\\left[\\begin{array}{l}\nx \\\\\ny\n\\end{array}\\right], \\text { and } b=\\left[\\begin{array}{l}\nb_{1} \\\\\nb_{2}\n\\end{array}\\right]\n$$\n\nIf $S_{1}=S_{2}=0$ and $M_{12}=-M_{21}^{\\top}$, the game Jacobian $\\nabla v=A$ has only purely imaginary eigenvalues (Azizian et al., 2020b, Lemma 7), recovering bilinear games.\n\nAs the second and the third spectrum models in (14) and (15) generalize bilinear games, we can consider more complex quadratic games, where $S_{1}$ and $S_{2}$ does not have to be 0 . Specifically, when $M_{12}=-M_{21}^{\\top}$, and they share common bases with $S_{1}$ and $S_{2}$ specified in the below proposition, then $\\operatorname{Sp}(A)$ has a cross-shaped spectrum in (14) of Case 2 and a shifted imaginary spectrum in (15) of Case 3.\nProposition 1. Let $A$ be a matrix of the form $\\left[\\begin{array}{cc}S_{1} & B \\\\ -B^{\\top} & S_{2}\\end{array}\\right]$, where $S_{1}, S_{2} \\succ 0$. Without loss of generality, assume that $\\operatorname{dim}\\left(S_{1}\\right)>\\operatorname{dim}\\left(S_{2}\\right)=d$. Then,\n\n- Case 2: $\\operatorname{Sp}(A)$ has a cross-shape if there exist orthonormal matrices $U, V$ and diagonal matrices $D_{1}, D_{2}$ such that $S_{1}=U \\operatorname{diag}\\left(a, \\ldots, a, D_{1}\\right) U^{\\top}, S_{2}=V \\operatorname{diag}(a, \\ldots, a) V^{\\top}$, and $B=U D_{2} V^{\\top}$.\n- Case 3: $\\operatorname{Sp}(A)$ has a shifted imaginary shape if there exist orthonormal matrices $U, V$ and diagonal matrix $D_{2}$ such that $S_{1}=U \\operatorname{diag}(a, \\ldots, a) U^{\\top}, S_{2}=V \\operatorname{diag}(a, \\ldots, a) V^{\\top}$, and $B=U D_{2} V^{\\top}$.\n\nWe can interpret Case 3 as a regularized bilinear game, where $S_{1}$ and $S_{2}$ are diagonal matrices with a constant eigenvalue. This implies that the players cannot control their parameter $x$ and $y$ arbitrarily, which can be seen in the loss functions in (16), where $S_{1}$ and $S_{2}$ appears in terms $x^{\\top} S_{1} x$ and $y^{\\top} S_{2} y$. Case 2 can be interpreted similarly, but player 1 (without loss of generality) has more flexibility in its parameter choice due to the additional diagonal matrix $D_{1}$ in the eigenvalue decomposition of $S_{1}$.\n\n# 4 Optimal Parameters and Convergence Rates \n\nIn this section, we obtain the optimal hyperparameters of MEG (in the sense that they achieve the fastest asymptotic convergence rate), for each spectrum model discussed in the previous section.\n\nCase 1: minimization. When the condition in Case 1 of Theorem 3 holds (i.e., $\\frac{h}{4 \\gamma} \\geqslant\\left(1+\\sqrt{m}\\right)^{2}$ ), both $\\sigma^{-1}(-1)$ and $\\sigma^{-1}(1)$ (and their intermediate values), line on the real line, forming a union of two intervals (see Figure 1, left). The robust region in this case, denoted $\\sigma_{\\text {Case }_{1}}^{-1}([-1,1])$, is expressed as:\n\n$$\n\\left[\\frac{1}{2 \\gamma}-\\sqrt{\\frac{1}{4 \\gamma^{2}}-\\frac{(1-\\sqrt{m})^{2}}{h \\gamma}}, \\frac{1}{2 \\gamma}-\\sqrt{\\frac{1}{4 \\gamma^{2}}-\\frac{(1+\\sqrt{m})^{2}}{h \\gamma}}\\right] \\bigcup\\left[\\frac{1}{2 \\gamma}+\\sqrt{\\frac{1}{4 \\gamma^{2}}-\\frac{(1+\\sqrt{m})^{2}}{h \\gamma}}, \\frac{1}{2 \\gamma}+\\sqrt{\\frac{1}{4 \\gamma^{2}}-\\frac{(1-\\sqrt{m})^{2}}{h \\gamma}}\\right] \\subset \\mathbb{R}_{+}\n$$\n\nFor this case, the optimal hyperparameters of MEG in terms of the worst-case asymptotic convergence rate in (9) can be set as below.\nTheorem 4 (Case 1). Consider solving (1) for games where the Jacobian has the spectrum in (12). For this problem, the optimal hyperparameters for the momentum extragradient method in (4) are:\n\n$$\nh=\\frac{4\\left(\\mu_{1}+L_{2}\\right)}{\\left(\\sqrt{\\mu_{2}+L_{1}}+\\sqrt{\\mu_{1}+L_{2}}\\right)^{2}}, \\gamma=\\frac{1}{\\mu_{1}+L_{2}}=\\frac{1}{\\mu_{2}+L_{1}}, \\quad \\text { and } m=\\left(\\frac{\\sqrt{\\mu_{2} L_{1}}-\\sqrt{\\mu_{1} L_{2}}}{\\sqrt{\\mu_{2} L_{1}}+\\sqrt{\\mu_{1} L_{2}}}\\right)^{2}=\\left(\\frac{\\sqrt{\\zeta^{2}-R^{2}}-\\sqrt{\\zeta^{2}-1}}{\\sqrt{\\zeta^{2}-R^{2}}+\\sqrt{\\zeta^{2}-1}}\\right)^{2}\n$$"
    },
    {
      "markdown": "Recalling (9), we immediately get the asymptotic convergence rate from Theorem 4. Further, this formula can be simplified in the ill-conditioned regime, where the inverse condition number $\\tau:=\\mu_{1} / L_{2} \\rightarrow 0$ :\n\n$$\n\\sqrt[4]{m}=\\left(\\frac{\\sqrt{\\zeta^{2}-R^{2}}-\\sqrt{\\zeta^{2}-1}}{\\sqrt{\\zeta^{2}-R^{2}}+\\sqrt{\\zeta^{2}-1}}\\right)^{1 / 2} \\underset{\\tau \\rightarrow 0}{=} 1-\\frac{2 \\sqrt{\\tau}}{\\sqrt{1-R^{2}}}+o(\\sqrt{\\tau})\n$$\n\nFrom (18), we see that MEG achieves an accelerated convergence rate $1-O(\\sqrt{\\tau})$, which is known to be \"optimal\" for this function class, and can be asymptotically achieved by $\\mathrm{GDM}^{6}$ (Polyak, 1987) (see also Theorem 8 with $\\theta=1$ ). Surprisingly, this rate can be further improved by the factor $\\sqrt{1-R^{2}}$, exhibiting \"super-acceleration\" phenomenon enjoyed by GDM with (optimal) cyclical step sizes (Goujaud et al., 2022). Note that achieving this improvement is possible by having additional information beyond just the largest $\\left(L_{2}\\right)$ and smallest $\\left(\\mu_{1}\\right)$ eigenvalues of the Hessian.\n\nCase 2: cross-shaped spectrum. If the condition in Case 2 of Theorem 3 is satisfied (i.e., $(1-\\sqrt{m})^{2} \\leqslant$ $\\frac{h}{4 \\gamma}<(1+\\sqrt{m})^{2}$ ), then $\\sigma^{-1}(-1)$ are complex, while $\\sigma^{-1}(1)$ are real (c.f., Figure 1, middle). We can write the robust region $\\sigma_{\\text {Case }_{2}}^{-1}([-1,1])$ as:\n\nHere, the first interval lies on $\\mathbb{R}_{+}$, as the square root term is real; conversely, in the second interval, the square root term is imaginary, with the fixed real component: $\\frac{1}{2 \\gamma}$. We summarize the optimal hyperparameters for this case in the next theorem.\nTheorem 5 (Case 2). Consider solving (1) for games where the Jacobian has a cross-shaped spectrum as in (14). For this problem, the optimal hyperparameters for the momentum extragradient method in (4) are:\n\n$$\nh=\\frac{16(\\mu+L)}{\\left(\\sqrt{4 c^{2}+(\\mu+L)^{2}}+\\sqrt{4 \\mu L}\\right)^{2}}, \\quad \\gamma=\\frac{1}{\\mu+L}, \\quad \\text { and } \\quad m=\\left(\\frac{\\sqrt{4 c^{2}+(\\mu+L)^{2}}-\\sqrt{4 \\mu L}}{\\sqrt{4 c^{2}+(\\mu+L)^{2}}+\\sqrt{4 \\mu L}}\\right)^{2}\n$$\n\nWe get the asymptotic rate from Theorem 5, which simplifies in the ill-conditioned regime $\\tau:=\\mu / L \\rightarrow 0$ as:\n\n$$\n\\sqrt[4]{m}=\\left(\\frac{\\sqrt{4 c^{2}+(\\mu+L)^{2}}-\\sqrt{4 \\mu L}}{\\sqrt{4 c^{2}+(\\mu+L)^{2}}+\\sqrt{4 \\mu L}}\\right)^{1 / 2} \\underset{\\tau \\rightarrow 0}{=} 1-\\frac{2 \\sqrt{\\tau}}{\\sqrt{(2 c / L)^{2}+1}}+o(\\sqrt{\\tau})\n$$\n\nWe see that MEG achieves accelerated convergence rate $1-O(\\sqrt{\\mu / L})$, as long as $c=O(L)$. We remark that this rate is optimal in the following sense. The lower bound for the problems with cross-shaped spectrum in (14) must be slower than the existing ones for minimizing $\\mu$-strongly convex and $L$-smooth functions, as the former is strictly more general. Since we reach the same asymptotic optimal rate, this must be optimal.\n\nCase 3: shifted imaginary spectrum. Lastly, if the condition in Case 3 of Theorem 3 is satisfied (i.e., $\\frac{h}{4 \\gamma}<(1-\\sqrt{m})^{2}$ ), then $\\sigma^{-1}(-1)$ and $\\sigma^{-1}(1)$ (and the intermediate values) are all complex conjugates (c.f., Figure 1, right). We can write the robust region $\\sigma_{\\text {Case }_{3}}^{-1}([-1,1])$ as:\n\n$$\n\\left[\\frac{1}{2 \\gamma}+\\sqrt{\\frac{1}{4 \\gamma^{2}}-\\frac{(1+\\sqrt{m})^{2}}{h \\gamma}}, \\frac{1}{2 \\gamma}+\\sqrt{\\frac{1}{4 \\gamma^{2}}-\\frac{(1-\\sqrt{m})^{2}}{h \\gamma}}\\right] \\bigcup\\left[\\frac{1}{2 \\gamma}-\\sqrt{\\frac{1}{4 \\gamma^{2}}-\\frac{(1-\\sqrt{m})^{2}}{h \\gamma}}, \\frac{1}{2 \\gamma}-\\sqrt{\\frac{1}{4 \\gamma^{2}}-\\frac{(1+\\sqrt{m})^{2}}{h \\gamma}}\\right] \\subset \\mathbb{C}_{+}\n$$\n\nWe modeled such spectrum in (15), which generalizes bilinear games, where the spectrum reduces to $\\pm[a i, b i]$ (i.e., with $c=0$ ). We summarize the optimal hyperparameters for this case below.\n\n[^0]\n[^0]:    ${ }^{6}$ Precisely, GDM with optimal step size and momentum asymptotically achieve $1-2 \\sqrt{\\tau}+o(\\sqrt{\\tau})$ convergence rate, as $\\tau \\rightarrow 0$ (Goujaud \\& Pedregosa, 2022, Proposition 3.3)."
    },
    {
      "markdown": "Theorem 6 (Case 3). Consider solving (1) for games where the Jacobian has a shifted imaginary spectrum in (15). For this problem, the optimal hyperparameters for the momentum extragradient method in (4) are:\n\n$$\nh=\\frac{8 c}{\\left(\\sqrt{c^{2}+a^{2}}+\\sqrt{c^{2}+b^{2}}\\right)^{2}}, \\quad \\gamma=\\frac{1}{2 c}, \\quad \\text { and } \\quad m=\\left(\\frac{\\sqrt{c^{2}+b^{2}}-\\sqrt{c^{2}+a^{2}}}{\\sqrt{c^{2}+b^{2}}+\\sqrt{c^{2}+a^{2}}}\\right)^{2}\n$$\n\nSimilarly to before, we compute the asymptotic convergence rate from Theorem 4 using (9).\n\n$$\n\\sqrt[4]{m}=\\left(\\frac{\\sqrt{c^{2}+b^{2}}-\\sqrt{c^{2}+a^{2}}}{\\sqrt{c^{2}+b^{2}}+\\sqrt{c^{2}+a^{2}}}\\right)^{1 / 2}=\\left(1-\\frac{2 \\sqrt{c^{2}+a^{2}}}{\\sqrt{c^{2}+b^{2}}+\\sqrt{c^{2}+a^{2}}}\\right)^{1 / 2}\n$$\n\nNote that by setting $c=0$, the rate in (20) matches the lower bound of bilinear game: $\\sqrt{\\frac{b-a}{b+a}}$ (Azizian et al., 2020b, Proposition 5). Further, with $c>0$, the convergence rate in (20) improves, highlighting the contrast between vanilla bilinear games and their regularized counterpart.\nRemark 3. Notice that the optimal momentum $m$ in both Theorems 5 and 6 are positive. This is in contrast to Gidel et al. (2019), where the gradient method with negative momentum is studied. This difference elucidates distinct dynamics of how momentum interacts with the gradient and the extragradient methods.\n\n# 5 Comparison with Other Methods \n\nHaving established MEG's asymptotic convergence rates for various spectrum models, we now compare it with other first-order methods, including GD, GDM, and EG.\n\nComparison with GD and EG. Building upon the fixed-point iteration framework established by Polyak (1987), Azizian et al. (2020a) interpret both GD and EG as fixed-point iterations. Within this framework, iterates of a method are generated according to:\n\n$$\nw_{t+1}=F\\left(w_{t}\\right), \\quad \\forall t \\geqslant 0\n$$\n\nwhere $F: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}^{d}$ is an operator representing the method. However, analyzing this scheme in general settings poses challenges due to the potential nonlinearity of $F$.\n\nTo address this, under conditions of twice differentiability of $F$ and proximity of $w$ to the stationary point $w^{\\star}$, the analysis can be simplified by linearizing $F$ around $w^{\\star}$ :\n\n$$\nF(w) \\approx F\\left(w^{\\star}\\right)+\\nabla F\\left(w^{\\star}\\right)\\left(w-w^{\\star}\\right)\n$$\n\nThen, for $w_{0}$ in a neighborhood of $w^{\\star}$, one can obtain an asymptotic convergence rate of (21) by studying the spectral radius of the Jacobian at the solution: $\\rho\\left(\\nabla F\\left(w^{\\star}\\right)\\right) \\leqslant \\rho^{\\star}<1$. This implies that (21) locally converges linearly to $w^{\\star}$ at the rate $O\\left(\\left(\\rho^{\\star}+\\varepsilon\\right)^{t}\\right)$ for $\\varepsilon \\geqslant 0$. Further, if $F$ is linear, $\\varepsilon=0$ (Polyak, 1987).\nThe corresponding fixed point operators $F_{h}^{\\mathrm{GD}}$ and $F_{h}^{\\mathrm{EG}}$ of GD and $\\mathrm{EG}^{\\dagger}$ respectively are:\n\n$$\n\\begin{aligned}\n& (\\mathrm{GD}) w_{t+1}=w_{t}-h v\\left(w_{t}\\right)=F_{h}^{\\mathrm{GD}}\\left(w_{t}\\right), \\text { and } \\\\\n& (\\mathrm{EG}) w_{t+1}=w_{t}-h v\\left(w_{t}-h v\\left(w_{t}\\right)\\right)=F_{h}^{\\mathrm{EG}}\\left(w_{t}\\right)\n\\end{aligned}\n$$\n\nThe local convergence rate can then be obtained by bounding the spectral radius of the Jacobian of the operators under certain assumptions. We summarize the relevant results below.\nTheorem 7 (Azizian et al. (2020a); Gidel et al. (2019)). Let $w^{\\star}$ be a stationary point of $v$. Further, assume the eigenvalues of $\\nabla v\\left(w^{\\star}\\right)$ all have positive real parts. Then, denoting $\\mathcal{S}^{\\star}:=S p\\left(\\nabla v\\left(w^{\\star}\\right)\\right)$,\n\n1. For the gradient method in (22) with step size $h=\\min _{\\lambda \\in \\mathcal{S}^{\\star}} \\mathfrak{R}(1 / \\lambda)$, it satisfies: ${ }^{8}$\n\n$$\n\\rho\\left(\\nabla F_{h}^{G D}\\left(w^{\\star}\\right)\\right)^{2} \\leqslant 1-\\min _{\\lambda \\in \\mathcal{S}^{\\star}} \\mathfrak{R}\\left(\\frac{1}{\\lambda}\\right) \\min _{\\lambda \\in \\mathcal{S}^{\\star}} \\mathfrak{R}(\\lambda)\n$$\n\n[^0]\n[^0]:    ${ }^{7}$ Azizian et al. (2020a) assumes that EG uses the same step size $h$ for both the main and the extrapolation steps.\n    ${ }^{8}$ Note that the spectral radius $\\rho$ is squared, but asymptotically is almost the same as $\\sqrt{1-x} \\leqslant 1-x / 2$."
    },
    {
      "markdown": "2. For the extragradient method in (23) with step size $h=\\left(4 \\sup _{\\lambda \\in \\mathcal{S}^{*}}|\\lambda|\\right)^{-1}$, it satisfies:\n\n$$\n\\rho\\left(\\nabla F_{h}^{E G}\\left(w^{\\star}\\right)\\right)^{2} \\leqslant 1-\\frac{1}{4}\\left(\\frac{\\min _{\\lambda \\in \\mathcal{S}^{*}} \\mathfrak{R}(\\lambda)}{\\sup _{\\lambda \\in \\mathcal{S}^{*}}|\\lambda|}+\\frac{1}{16} \\frac{\\min _{\\lambda \\in \\mathcal{S}^{*}}|\\lambda|^{2}}{\\sup _{\\lambda \\in \\mathcal{S}^{*}}|\\lambda|^{2}}\\right)\n$$\n\nWe can determine the convergence rate of GD and EG by using Theorem 7 since all three cases of our spectrum models in (12), (14), and (15) meet the condition that the eigenvalues of $\\nabla v\\left(w^{\\star}\\right)$ have positive real parts. The following corollary summarizes this result.\nCorollary 1. With the conditions in Theorem 7, for each case of the Jacobian spectrum $\\mathcal{S}_{1}^{\\star}, \\mathcal{S}_{2}^{\\star}$, and $\\mathcal{S}_{3}^{\\star}$, the gradient method in (22) and the extragradient method in (23) satisfy the following:\n\n- Case 1: $\\operatorname{Sp}(\\nabla v) \\subset \\mathcal{S}_{1}^{\\star}=\\left[\\mu_{1}, L_{1}\\right] \\cup\\left[\\mu_{2}, L_{2}\\right] \\in \\mathbb{R}_{+}$:\n\n$$\n\\rho\\left(\\nabla F_{h}^{G D}\\left(w^{\\star}\\right)\\right)^{2} \\leqslant 1-\\frac{\\mu_{1}}{L_{2}}, \\quad \\text { and } \\quad \\rho\\left(\\nabla F_{h}^{E G}\\left(w^{\\star}\\right)\\right)^{2} \\leqslant 1-\\frac{1}{4}\\left(\\frac{\\mu_{1}}{L_{2}}+\\frac{\\mu_{1}^{2}}{16 L_{2}^{2}}\\right)\n$$\n\n- Case 2: $\\operatorname{Sp}(\\nabla v) \\subset \\mathcal{S}_{2}^{\\star}=[\\mu, L] \\cup\\left\\{z \\in \\mathbb{C}: \\mathfrak{R}(z)=c^{\\prime}>0, \\mathfrak{I}(z) \\in[-c, c]\\right\\}$ :\n\n$$\n\\begin{aligned}\n& \\rho\\left(\\nabla F_{h}^{G D}\\left(w^{\\star}\\right)\\right)^{2} \\leqslant \\begin{cases}1-\\frac{2 \\mu}{4 c^{2} /(L-\\mu)+(L-\\mu)} & \\text { if } c \\geqslant \\sqrt{\\frac{L^{2}-\\mu^{2}}{4}} \\\\\n1-\\frac{\\mu}{L} & \\text { otherwise. }\\end{cases} \\\\\n& \\rho\\left(\\nabla F_{h}^{E G}\\left(w^{\\star}\\right)\\right)^{2} \\leqslant \\begin{cases}1-\\frac{1}{4}\\left(\\frac{\\mu}{\\sqrt{c^{2}+((L-\\mu) / 2)^{2}}}+\\frac{\\mu^{2}}{16\\left(c^{2}+((L-\\mu) / 2)^{2}\\right)}\\right) & \\text { if } c \\geqslant \\sqrt{\\frac{3 L^{2}+2 L \\mu-\\mu^{2}}{4}} \\\\\n1-\\frac{1}{4}\\left(\\frac{\\mu}{L}+\\frac{\\mu^{2}}{16 L^{2}}\\right) & \\text { otherwise. }\\end{cases}\n\\end{aligned}\n$$\n\n- Case 3: $\\operatorname{Sp}(\\nabla v) \\subset \\mathcal{S}_{3}^{\\star}=[c+a i, c+b i] \\cup[c-a i, c-b i] \\in \\mathbb{C}_{+}$:\n\n$$\n\\rho\\left(\\nabla F_{h}^{G D}\\left(w^{\\star}\\right)\\right)^{2} \\leqslant 1-\\frac{c^{2}}{c^{2}+b^{2}}, \\quad \\text { and } \\quad \\rho\\left(\\nabla F_{h}^{E G}\\left(w^{\\star}\\right)\\right)^{2} \\leqslant 1-\\frac{1}{4}\\left(\\frac{c}{\\sqrt{c^{2}+b^{2}}}+\\frac{c^{2}+a^{2}}{16\\left(c^{2}+b^{2}\\right)}\\right)\n$$\n\nIn Case 1, we see from (26) that both GD and EG have convergence rates $1-O\\left(\\mu_{1} / L_{2}\\right)=1-O(\\tau)$. MEG, on the other hand, has an accelerated convergence rate of $1-O(\\sqrt{\\tau})$, as well as an additional constant improvement by a factor of $\\sqrt{1-R^{2}}$, as we showed in (18). Moving on to Case 2, we showed in (19) that MEG enjoys an accelerated convergence rate of $1-O(\\sqrt{\\mu / L})$ as long as $c=O(L)$. However, both GD and EG in (27) have non-accelerated convergence under the same condition. Lastly, for Case 3, we showed in (20) that MEG achieves an asymptotic rate that matches the known lower bound for bilinear games: $\\sqrt{\\frac{b-a}{b+a}}$, with $c=0$; further, the rate of MEG improves if $c>0$. On the contrary, GD and EG suffer from slower rates, as shown in (28).\nComparison with GDM. We now compare the convergence rate of MEG with that of GDM, which iterates as in (5). In Azizian et al. (2020b), it was shown that GD is the optimal method for games where the Jacobian eigenvalues are within a disc in the complex plane. This suggests that acceleration is not possible for this type of problem. ${ }^{9}$ On the other hand, it is well-known that GDM achieves an accelerated convergence rate for strongly-convex (quadratic) minimization, where the eigenvalues of the Hessian lie on the (strictly positive) real line segment (Polyak, 1987). Hence, Azizian et al. (2020b) studies the intermediate case, where the Jacobian eigenvalues are within an ellipse, which can be thought of as the real segment $[\\mu, L]$ perturbed with $\\epsilon$ in an elliptic way. That is, they consider the spectral shape: ${ }^{10}$\n\n$$\nK_{\\epsilon}=\\left\\{z \\in \\mathbb{C}:\\left(\\frac{\\Re z-(\\mu+L) / 2}{(L-\\mu) / 2}\\right)^{2}+\\left(\\frac{3 z}{\\epsilon}\\right)^{2} \\leqslant 1\\right\\}\n$$\n\nSimilarly to GD and EG above, in Azizian et al. (2020b), GDM is interpreted as a fixed point iteration: ${ }^{11}$\n\n$$\nw_{t+1}=w_{t}-h v\\left(w_{t}\\right)+m\\left(w_{t}-w_{t-1}\\right)=F^{\\mathrm{GDM}}\\left(w_{t}, w_{t-1}\\right)\n$$\n\nTo study the convergence rate of GDM, we use the following theorem from Azizian et al. (2020b):\n\n[^0]\n[^0]:    ${ }^{9}$ Yet, one can consider the case where, e.g., a cross-shape is contained in a disc. Then, by knowing more fine-grained structure of the Jacobian spectrum, MEG can have faster convergence in (19).\n    ${ }^{10} \\mathrm{~A}$-visual illustration of this ellipse can be found in Azizian et al. (2020b, Figure 2).\n    ${ }^{11}$ As GDM updates $w_{t+1}$ using both $w_{t}$ and $w_{t-1}$, Azizian et al. (2020b) uses an augmented fixed point operator; see Lemma 2 in that work for details."
    },
    {
      "markdown": "Theorem 8 (Azizian et al. (2020b)). Define $\\epsilon(\\mu, L)$ as $\\epsilon(\\mu, L) / L=(\\mu / L)^{\\theta}=\\tau^{\\theta}$ with $\\theta>0$ and $a \\wedge b=$ $\\min (a, b)$. If $\\operatorname{Sp}\\left(\\nabla F^{G D M}\\left(w^{\\star}, w^{\\star}\\right)\\right) \\subset K_{\\epsilon}$, and when $\\tau \\rightarrow 0$, it satisfies:\n\n$$\n\\rho\\left(\\nabla F^{G D M}\\left(w^{\\star}, w^{\\star}\\right)\\right) \\leqslant \\begin{cases}1-2 \\sqrt{\\tau}+O\\left(\\tau^{\\theta \\wedge 1}\\right), & \\text { if } \\theta>\\frac{1}{2} \\\\ 1-2(\\sqrt{2}-1) \\sqrt{\\tau}+O(\\tau), & \\text { if } \\theta=\\frac{1}{2} \\\\ 1-\\tau^{1-\\theta}+O\\left(\\tau^{1 \\wedge(2-3 \\theta)}\\right), & \\text { if } \\theta<\\frac{1}{2}\\end{cases}\n$$\n\nwhere the hyperparametes $h$ and $m$ are functions of $\\mu, L$, and $\\epsilon$ only.\nFor Case 1, GDM converges at the rate $1-2 \\sqrt{\\tau}+O(\\tau)$ (i.e., with $\\theta=1$ from the above), which is always slower than the rate of MEG in (18) by the factor of $\\sqrt{1-R^{2}}$. For Case 2, we see from Theorem 8 that GDM achieves an accelerated rate, i.e., $1-O(\\sqrt{\\tau})$, until $\\theta=\\frac{1}{2}$. In other words, the biggest elliptic perturbation $\\epsilon$ where GDM permits the accelerated rate is $\\epsilon=\\sqrt{\\mu} L .^{12}$ We interpret Theorem 8 for games with cross-shaped Jacobian spectrum in (14) and shifted imaginary spectrum in (15) in the following corollary.\nCorollary 2. Consider the gradient method with momentum, interpreted as fixed point iteration as in (29). For games with cross-shaped Jacobian spectrum in (14) with $c=\\frac{L-\\mu}{2}$, GDM cannot achieve an accelerated rate when $\\frac{L-\\mu}{2}=c>\\epsilon=\\sqrt{\\mu L}$. Since $L>\\mu$, this further implies $\\frac{L}{\\mu}>\\sqrt{5}$. That is, when the condition number exceeds $\\sqrt{5} \\approx 2.236$, GDM cannot achieve an accelerated convergence rate. On the contrary, as we showed in (19), MEG can converge at an accelerated rate in the ill-conditioned regime.\n\nThe convergence rate of GDM for Case 3 cannot be determined from Theorem 8, as this theorem assumes the spectrum model of the real line segment $[\\mu, L]$ with $\\epsilon$ perturbation (along the imaginary axis), while $\\mathcal{S}_{4}^{*}$ in (15) has a fixed real component. Instead, we utilize the link function of GDM in (7) to show that it is unlikely for GDM to stay in the robust region: $\\xi^{-1}([-1,1])$.\nProposition 2. Consider solving (1) for games where the Jacobian has a shifted imaginary spectrum in (15), using the gradient method with momentum in (5). For any complex number $z=p+q i \\in \\mathbb{C}_{+}$, if $\\frac{2(1+\\omega)}{h}<p$, then GDM cannot stay in the robust region, i.e., $|\\xi(\\lambda)|>1$.\n\nNote that the condition $\\frac{2(1+\\omega)}{h}<p$ is hard to avoid even for small $p$, considering $h$ is usually a small value.\n\n# 6 Local Convergence for Non-affine Vector Fields \n\nThe optimal hyperparameters of MEG for each spectrum model and the associated convergence rate we obtained in Section 4 are attainable when the vector field is affine. A natural question is, then, what can we say about the convergence rate of MEG when the vector field is not affine? To that end, we provide the local convergence of MEG by restarting the momentum, as detailed below.\nLet us consider the operator $G$ representing the MEG in (4) such that:\n\n$$\n\\left[w_{t+1}, w_{t}\\right]=G\\left(\\left[w_{t}, w_{t-1}\\right]\\right) \\quad \\text { and } \\quad G\\left(\\left[w^{\\star}, w^{\\star}\\right]\\right)=\\left[w^{\\star}, w^{\\star}\\right]\n$$\n\nIn addition, we assume that $w_{1}=w_{0}-\\frac{h}{1+\\omega} v\\left(w_{0}-\\gamma v\\left(w_{0}\\right)\\right)$, in order to induce the residual polynomials from Theorem 1; see also its proof and Algorithm 1 in the appendix. Now let us consider the following algorithm:\n\n$$\n\\begin{aligned}\n{\\left[w_{t k+i+1}, w_{t k+i}\\right] } & =G\\left(\\left[w_{t k+i}, w_{t k+i-1}\\right]\\right) \\text { for } 1 \\leqslant i \\leqslant k-1, \\quad \\text { and then } \\\\\nw_{(t+1) k+1} & =w_{(t+1) k}-\\frac{h}{1+\\omega} v\\left(w_{(t+1) k}-\\gamma v\\left(w_{(t+1) k}\\right)\\right)\n\\end{aligned}\n$$\n\nIn other words, we repeat MEG for $k$ steps, and then restart the momentum at $\\left[w_{(t+1) k+1}, w_{(t+1) k}\\right]$. The local convergence of the restarted MEG is established in the next theorem.\nTheorem 9 (Local convergence). Let $G: \\mathbb{R}^{2 d} \\rightarrow \\mathbb{R}^{2 d}$ be the continuously differentiable operator representing the momentum extragradient method (MEG) in (4). Let $w^{\\star}$ be a stationary point. Let $w_{t}$ denote the output of MEG, which enjoys a convergence rate of the form $\\left\\|w_{t}-w^{\\star}\\right\\|=C(1-\\varphi)^{t}(t+1)\\left\\|w_{0}-w^{\\star}\\right\\|$ for some $0<\\varphi<1$ when the vector field is affine. Further, consider restarting the momentum of MEG after running\n\n[^0]\n[^0]:    ${ }^{12}$ Observe that $(\\mu / L)^{1 / 2}=\\epsilon(\\mu, L) / L \\Longrightarrow \\epsilon(\\mu, L)=\\sqrt{\\mu} L$."
    },
    {
      "markdown": "$k$ steps, as in (31). Then, for each $\\varepsilon>0$, there exists $k>0$ and $\\delta>0$ such that, for all initializations $w_{0}$ satisfying $\\left\\|w_{0}-w^{*}\\right\\| \\leqslant \\delta$, the restarted MEG satisfies:\n\n$$\n\\left\\|w_{t}-w^{*}\\right\\|=O\\left((1-\\varphi+\\varepsilon)^{t}\\right)\\left\\|w_{0}-w^{*}\\right\\|\n$$\n\n# 7 Experiments \n\n![img-2.jpeg](img-2.jpeg)\n\nFigure 3: Illustration of the game Jacobian spectra and the performance of different algorithms considered. Jacobian spectrum in the first plot matches $\\mathcal{S}_{2}^{*}$ in (14) precisely, while that in the third plot inexactly follows $\\mathcal{S}_{2}^{*}$. The second (fourth) plot shows the performance of different algorithms for solving quadratic games in (16) with the Jacobian spectrum following the first (third) plot.\n\nIn this section, we perform numerical experiments to optimize a game when the Jacobian has a cross-shaped spectrum in (14). We focus on this spectrum as it may be the most challenging case, involving both real and complex eigenvalues (c.f., Theorem 3). To test the robustness, we consider two cases where the Jacobian spectrum exactly follows $\\mathcal{S}_{2}^{*}$ in (14), as well as the inexact case. We illustrate them in Figure 3.\nWe focus on two-player quadratic games, where player 1 controls $x \\in \\mathbb{R}^{d_{1}}$ and player 2 controls $y \\in \\mathbb{R}^{d_{2}}$ with loss functions in (16). In our setting, the corresponding vector field in (17) satisfies $M_{12}=-M_{21}^{\\top}$, but $S_{1}$ and $S_{2}$ can be nonzero symmetric matrices. Further, the Jacobian $\\nabla v=A$ has the cross-shaped eigenvalue structure in (14), with $c=\\frac{L-\\mu}{4}$ (c.f., Proposition 1, Case 2). For the problem constants, we use $\\mu=1$, and $L=200$. The optimum $\\left[x^{4} y^{*}\\right]^{\\top}=w^{*} \\in \\mathbb{R}^{200}$ is generated using the standard normal distribution. For simplicity, we assume $b=\\left[b_{1} b_{2}\\right]^{\\top}=\\left[\\begin{array}{ll}0 & 0\\end{array}\\right]^{\\top}$. For the algorithms, we compare GD in (22), GDM in (5), EG in (23), and MEG in (4). All algorithms are initialized with 0 . We plot the experimental results in Figure 3.\n\nFor MEG (optimal), we set the hyperparameters using Theorem 5. For GD (theory) and EG (theory), we set the hyperparameters using Theorem 7, both for the exact and the inexact settings. For GDM (grid search), we perform a grid search of $h^{\\mathrm{GDM}}$ and $m^{\\mathrm{GDM}}$, and choose the best-performing ones, as Theorem 8 does not give a specific form for hyperparameter setup. Specifically, we consider $0.005 \\leqslant h^{\\mathrm{GDM}} \\leqslant 0.015$ with $10^{-3}$ increment, and $0.01 \\leqslant m^{\\mathrm{GDM}} \\leqslant 0.99$ with $10^{-2}$ increment. In addition, as Theorem 7 might be conservative, we conduct grid searches for GD and EG as well. For GD (grid search), we use the same setup as $h^{\\mathrm{GDM}}$. For EG (grid search), we use $0.001 \\leqslant h^{\\mathrm{EG}} \\leqslant 0.05$ with $10^{-4}$ increment.\n\nThere are several remarks to make. First, although the third plot in Figure 3 does not exactly follow the spectrum model in (14), MEG still works well with the optimal hyperparameters from Theorem 5. As expected, MEG (optimal) required more iterations in the inexact case compared to the exact case. Second, compared to other algorithms, MEG (optimal) indeed exhibits a significantly faster rate of convergence, even when compared to other methods that use grid-search hyperparameter tuning, supporting our theoretical findings in Section 4. Third, while EG (theory) is slower than GD (theory), which confirms Corrolary 1, EG (grid search) can be tuned to converge faster via grid search. Lastly, even though the best performance of GDM (grid search) is obtained through grid search, one can see the GD (grid search) obtains a slightly faster convergence rate than GDM (grid search), confirming Corollay 2."
    },
    {
      "markdown": "# 8 Conclusion \n\nIn the study of differentiable games, finding stationary points efficiently is crucial. This work analyzes the momentum extragradient method, revealing three distinct convergence modes dependent on the Jacobian eigenvalue distribution. Through a polynomial-based analysis, we derive optimal hyperparameters for each mode, achieving accelerated asymptotic convergence rates. We compared the obtained rates with other firstorder methods and showed that the considered methods do not achieve the accelerated convergence rate. Notably, our initial analysis for affine vector fields extends to guarantee local convergence rates on twicedifferentiable vector fields. Numerical experiments on quadratic games validate our theoretical findings.\n\n## Acknowledgments\n\nThe authors would like to thank Fangshuo Liao, Baptiste Goujaud, Damien Scieur, Miri Son, and Giorgio Young for their useful discussions and feedback.\n\nThis work is supported by NSF FET: Small No. 1907936, NSF MLWiNS CNS No. 2003137 (in collaboration with Intel), NSF CMMI No. 2037545, NSF CAREER award No. 2145629, NSF CIF No. 2008555, Rice InterDisciplinary Excellence Award (IDEA), and the Canada CIFAR AI Chairs program.\n\n## References\n\nYossi Arjevani and Ohad Shamir. On the iteration complexity of oblivious first-order optimization algorithms. In International Conference on Machine Learning. PMLR, 2016.\n\nWaiss Azizian, Ioannis Mitliagkas, Simon Lacoste-Julien, and Gauthier Gidel. A tight and unified analysis of gradient-based methods for a whole spectrum of differentiable games. In International Conference on Artificial Intelligence and Statistics. PMLR, 2020a.\n\nWaïss Azizian, Damien Scieur, Ioannis Mitliagkas, Simon Lacoste-Julien, and Gauthier Gidel. Accelerating smooth games by manipulating spectral shapes. In International Conference on Artificial Intelligence and Statistics. PMLR, 2020b.\n\nDavid Balduzzi, Sebastien Racaniere, James Martens, Jakob Foerster, Karl Tuyls, and Thore Graepel. The mechanics of n-player differentiable games. In International Conference on Machine Learning. PMLR, 2018 .\n\nHugo Berard, Gauthier Gidel, Amjad Almahairi, Pascal Vincent, and Simon Lacoste-Julien. A closer look at the optimization landscapes of generative adversarial networks. In International Conference on Learning Representations, 2020.\n\nRaphaël Berthier, Francis Bach, and Pierre Gaillard. Accelerated gossip in networks of given dimension using Jacobi polynomial iterations. SIAM Journal on Mathematics of Data Science, 2020.\n\nAleksandr Beznosikov, Pavel Dvurechensky, Anastasia Koloskova, Valentin Samokhin, Sebastian U Stich, and Alexander Gasnikov. Decentralized local stochastic extra-gradient for variational inequalities. arXiv preprint arXiv:2106.08315, 2021.\n\nTheodore S Chihara. An introduction to orthogonal polynomials. Courier Corporation, 2011.\nConstantinos Daskalakis and Ioannis Panageas. The limit points of (optimistic) gradient descent in min-max optimization. Advances in neural information processing systems, 31, 2018.\n\nConstantinos Daskalakis, Paul W Goldberg, and Christos H Papadimitriou. The complexity of computing a nash equilibrium. SIAM Journal on Computing, 2009.\n\nCarles Domingo-Enrich, Fabian Pedregosa, and Damien Scieur. Average-case acceleration for bilinear games and normal matrices. In International Conference on Learning Representations, 2021."
    },
    {
      "markdown": "Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for efficiently improving generalization. In International Conference on Learning Representations, 2021.\n\nGauthier Gidel, Hugo Berard, Gaëtan Vignoud, Pascal Vincent, and Simon Lacoste-Julien. A variational inequality perspective on generative adversarial networks. arXiv preprint arXiv:1802.10551, 2018.\n\nGauthier Gidel, Reyhane Askari Hemmat, Mohammad Pezeshki, Rémi Le Priol, Gabriel Huang, Simon Lacoste-Julien, and Ioannis Mitliagkas. Negative momentum for improved game dynamics. In The 22nd International Conference on Artificial Intelligence and Statistics. PMLR, 2019.\n\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems, volume 27, 2014.\n\nEduard Gorbunov, Nicolas Loizou, and Gauthier Gidel. Extragradient method: O (1/k) last-iterate convergence for monotone variational inequalities and connections with cocoercivity. In International Conference on Artificial Intelligence and Statistics. PMLR, 2022.\n\nEduard Gorbunov, Adrien Taylor, Samuel Horváth, and Gauthier Gidel. Convergence of proximal point and extragradient-based methods beyond monotonicity: the case of negative comonotonicity. In International Conference on Machine Learning. PMLR, 2023.\n\nBaptiste Goujaud and Fabian Pedregosa. Cyclical step-sizes, 2022. URL http://fa.bianp.net/blog/2022/ cyclical/.\n\nBaptiste Goujaud, Damien Scieur, Aymeric Dieuleveut, Adrien B Taylor, and Fabian Pedregosa. Superacceleration with cyclical step-sizes. In International Conference on Artificial Intelligence and Statistics. PMLR, 2022.\n\nMagnus R Hestenes and Eduard Stiefel. Methods of conjugate gradients for solving. Journal of research of the National Bureau of Standards, 49(6):409, 1952.\n\nAndrew J Hetzel, Jay S Liew, and Kent E Morrison. The probability that a matrix of integers is diagonalizable. The American Mathematical Monthly, 114(6):491-499, 2007.\n\nYu-Guan Hsieh, Franck Iutzeler, Jérôme Malick, and Panayotis Mertikopoulos. On the convergence of singlecall stochastic extra-gradient methods. Advances in Neural Information Processing Systems, 32, 2019.\n\nYu-Guan Hsieh, Franck Iutzeler, Jérôme Malick, and Panayotis Mertikopoulos. Explore aggressively, update conservatively: Stochastic extragradient methods with variable stepsize scaling. Advances in Neural Information Processing Systems, 33, 2020.\n\nJunhyung Lyle Kim, Gauthier Gidel, Anastasios Kyrillidis, and Fabian Pedregosa. Momentum extragradient is optimal for games with cross-shaped spectrum. In OPT 2022: Optimization for Machine Learning (NeurIPS 2022 Workshop), 2022.\n\nGalina M Korpelevich. The extragradient method for finding saddle points and other problems. Matecon, 1976.\n\nPeter Lancaster and Hanafi K Farahat. Norms on direct sums and tensor products. mathematics of computation, 1972.\n\nAlistair Letcher, David Balduzzi, Sébastien Racaniere, James Martens, Jakob Foerster, Karl Tuyls, and Thore Graepel. Differentiable game mechanics. The Journal of Machine Learning Research, 2019.\n\nChris Junchi Li, Yaodong Yu, Nicolas Loizou, Gauthier Gidel, Yi Ma, Nicolas Le Roux, and Michael I Jordan. On the convergence of stochastic extragradient for bilinear games with restarted iteration averaging. arXiv preprint arXiv:2107.00464, 2021."
    },
    {
      "markdown": "Tengyuan Liang and James Stokes. Interaction matters: A note on non-asymptotic local convergence of generative adversarial networks. In The 22nd International Conference on Artificial Intelligence and Statistics, pp. 907-915. PMLR, 2019.\n\nMingrui Liu, Wei Zhang, Youssef Mroueh, Xiaodong Cui, Jarret Ross, Tianbao Yang, and Payel Das. A decentralized parallel algorithm for training generative adversarial nets. Advances in Neural Information Processing Systems, 2020.\n\nJonathan P. Lorraine, David Acuna, Paul Vicol, and David Duvenaud. Complex momentum for optimization in games. In Proceedings of The 25th International Conference on Artificial Intelligence and Statistics, volume 151 of Proceedings of Machine Learning Research, 2022.\n\nLars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for gans do actually converge? In International conference on machine learning. PMLR, 2018.\n\nAryan Mokhtari, Asuman Ozdaglar, and Sarath Pattathil. A unified analysis of extra-gradient and optimistic gradient methods for saddle point problems: Proximal point approach. In International Conference on Artificial Intelligence and Statistics. PMLR, 2020.\n\nRenato DC Monteiro and Benar Fux Svaiter. On the complexity of the hybrid proximal extragradient method for the iterates and the ergodic mean. SIAM Journal on Optimization, 20(6):2755-2787, 2010.\n\nRémi Munos, Michal Valko, Daniele Calandriello, Mohammad Gheshlaghi Azar, Mark Rowland, Zhaohan Daniel Guo, Yunhao Tang, Matthieu Geist, Thomas Mesnard, Andrea Michi, et al. Nash learning from human feedback. arXiv preprint arXiv:2312.00886, 2023.\n\nArkadi Nemirovski. Prox-method with rate of convergence o (1/t) for variational inequalities with lipschitz continuous monotone operators and smooth convex-concave saddle point problems. SIAM Journal on Optimization, 2004.\n\nSamet Oymak. Provable super-convergence with a large cyclical learning rate. IEEE Signal Processing Letters, 28, 2021.\n\nBalamurugan Palaniappan and Francis Bach. Stochastic variance reduction methods for saddle-point problems. Advances in Neural Information Processing Systems, 2016.\n\nVardan Papyan. Traces of class/cross-class structure pervade deep learning spectra. The Journal of Machine Learning Research, 21, 2020.\n\nFabian Pedregosa. Momentum: when Chebyshev meets Chebyshev, 2020. URL http://fa.bianp.net/ blog/2020/momentum/.\n\nFabian Pedregosa and Damien Scieur. Acceleration through spectral density estimation. In Proceedings of the 37th International Conference on Machine Learning. PMLR, November 2020.\n\nDavid Pfau and Oriol Vinyals. Connecting generative adversarial networks and actor-critic methods. arXiv preprint arXiv:1610.01945, 2016.\n\nBoris T Polyak. Introduction to optimization. optimization software. Inc., Publications Division, New York, $1: 32,1987$.\n\nErnest K Ryu, Kun Yuan, and Wotao Yin. ODE analysis of stochastic gradient methods with optimism and anchoring for minimax problems. arXiv preprint arXiv:1905.10899, 2019.\n\nYoav Shoham and Kevin Leyton-Brown. Multiagent systems: Algorithmic, game-theoretic, and logical foundations. Cambridge University Press, 2008.\n\nMikhail V Solodov and Benar F Svaiter. A hybrid approximate extragradient-proximal point algorithm using the enlargement of a maximal monotone operator. Set-Valued Analysis, 7(4):323-345, 1999."
    },
    {
      "markdown": "Paul Tseng. On linear convergence of iterative methods for the variational inequality problem. Journal of Computational and Applied Mathematics, 1995.\n\nJian Zhang and Ioannis Mitliagkas. Yellowfin and the art of momentum tuning. Proceedings of Machine Learning and Systems, 1, 2019."
    },
    {
      "markdown": "# A Missing Proofs in Section 3 \n\n## A. 1 Proof of Lemma 1\n\nProof of Lemma 1 can be found for example in Azizian et al. (2020b, Section B).\n\n## A. 2 Proof of Theorem 1\n\nTo obtain the residual polynomials of $\\mathrm{MEG}, w_{1}$ has to be set slightly differently from the rest of the iterates, as we write in the pseudocode below:\n\n```\nAlgorithm 1: Momentum extragradient (MEG) method\nInput: Initialization \\(w_{0}\\), hyperparameters \\(h, \\gamma, m\\).\nSet: \\(w_{1}=w_{0}-\\frac{h}{1+m} v\\left(w_{0}-\\gamma v\\left(w_{0}\\right)\\right)\\)\nfor \\(t=1,2, \\ldots\\) do\n    \\(w_{t+1}=w_{t}-h v\\left(w_{t}-\\gamma v\\left(w_{t}\\right)\\right)+m\\left(w_{t}-w_{t-1}\\right)\\)\nend\n```\n\n\n## Derivation of the first part\n\nProof. We want to find the residual polynomial $\\tilde{P}_{t}(A)$ of the extragradient with momentum (MEG) in (4). That is, we want to find\n\n$$\nw_{t}-w^{\\star}=\\tilde{P}_{t}(A)\\left(w_{0}-w^{\\star}\\right)\n$$\n\nwhere $\\left\\{w_{t}\\right\\}_{t=0}$ is the iterates generated by MEG, which is possible by Lemma 1, as MEG is a first-order method (Arjevani \\& Shamir, 2016; Azizian et al., 2020b). We now prove this is by induction. To do so, we will use the following properties. First, note that as we are looking for a stationary point, it holds that $v\\left(w^{\\star}\\right)=0$. Further, as $v$ is linear by the assumption of Lemma 1, it holds that $v(w)=A\\left(w-w^{\\star}\\right)$.\nBase case. For $t=0, \\tilde{P}_{0}(A)$ is a degree-zero polynomial, and hence equals $I_{d}$, which denotes the identity matrix. Thus, $w_{0}-w^{\\star}=I_{d}\\left(w_{0}-w^{\\star}\\right)$ holds true.\n\nFor completeness, we also prove when $t=1$. In that case, observe that MEG in proceeds as $w_{1}=w_{0}-$ $\\frac{h}{1+m} v\\left(w_{0}-\\gamma v\\left(w_{0}\\right)\\right)$. Subtracting $w^{\\star}$ on both sides, we have:\n\n$$\n\\begin{aligned}\nw_{1}-w^{\\star} & =w_{0}-w^{\\star}-\\frac{h}{1+m} v\\left(w_{0}-\\gamma v\\left(w_{0}\\right)\\right) \\\\\n& =w_{0}-w^{\\star}-\\frac{h}{1+m} v\\left(w_{0}-\\gamma A\\left(w_{0}-w^{\\star}\\right)\\right) \\\\\n& =w_{0}-w^{\\star}-\\frac{h}{1+m} A\\left(w_{0}-\\gamma A\\left(w_{0}-w^{\\star}\\right)-w^{\\star}\\right) \\\\\n& =w_{0}-w^{\\star}-\\frac{h}{1+m} A\\left(w_{0}-w^{\\star}\\right)+\\frac{h \\gamma}{1+m} A^{2}\\left(w_{0}-w^{\\star}\\right) \\\\\n& =\\left(I_{d}-\\frac{h}{1+m} A+\\frac{h \\gamma}{1+m} A^{2}\\right)\\left(w_{0}-w^{\\star}\\right) \\\\\n& =\\left(I_{d}-\\frac{h}{1+m} A\\left(I_{d}-\\gamma A\\right)\\right)\\left(w_{0}-w^{\\star}\\right) \\\\\n& =\\tilde{P}_{1}(A)\\left(w_{0}-w^{\\star}\\right)\n\\end{aligned}\n$$"
    },
    {
      "markdown": "Induction step. As the induction hypothesis, assume $\\tilde{P}_{t}$ satisfies (32). We want to prove this holds for $t+1$. We have:\n\n$$\n\\begin{aligned}\nw_{t+1} & =w_{t}-h v\\left(w_{t}-\\gamma v\\left(w_{t}\\right)\\right)+m\\left(w_{t}-w_{t-1}\\right) \\\\\n& =w_{t}-h v\\left(w_{t}-\\gamma A\\left(w_{t}-w^{\\star}\\right)\\right)+m\\left(w_{t}-w_{t-1}\\right) \\\\\n& =w_{t}-h A\\left(w_{t}-\\gamma A\\left(w_{t}-w^{\\star}\\right)-w^{\\star}\\right)+m\\left(w_{t}-w_{t-1}\\right) \\\\\n& =w_{t}-h A\\left(w_{t}-w^{\\star}\\right)+h \\gamma A^{2}\\left(w_{t}-w^{\\star}\\right)+m\\left(w_{t}-w_{t-1}\\right) \\\\\n& =w_{t}-h A\\left(I_{d}-\\gamma A\\right)\\left(w_{t}-w^{\\star}\\right)+m\\left(w_{t}-w_{t-1}\\right)\n\\end{aligned}\n$$\n\nSubtracting $w^{\\star}$ on both sides, we have:\n\n$$\n\\begin{aligned}\nw_{t+1}-w^{\\star} & =w_{t}-w^{\\star}-h A\\left(I_{d}-\\gamma A\\right)\\left(w_{t}-w^{\\star}\\right)+m\\left(w_{t}-w_{t-1}\\right) \\\\\n& =\\left(I_{d}-h A\\left(I_{d}-\\gamma A\\right)\\right)\\left(w_{t}-w^{\\star}\\right)+m\\left(w_{t}-w^{\\star}-\\left(w_{t-1}-w^{\\star}\\right)\\right) \\\\\n& \\stackrel{(32)}{=}\\left(I_{d}-h A\\left(I_{d}-\\gamma A\\right)\\right) \\tilde{P}_{t}(A)\\left(w_{0}-w^{\\star}\\right)+m\\left(\\tilde{P}_{t}(A)\\left(w_{0}-w^{\\star}\\right)-\\tilde{P}_{t-1}(A)\\left(w_{0}-w^{\\star}\\right)\\right) \\\\\n& =\\left(I_{d}+m I_{d}-h A\\left(I_{d}-\\gamma A\\right)\\right) \\tilde{P}_{t}(A)\\left(w_{0}-w^{\\star}\\right)-m \\tilde{P}_{t-1}(A)\\left(w_{0}-w^{\\star}\\right) \\\\\n& =\\tilde{P}_{t+1}(A)\\left(w_{0}-w^{\\star}\\right)\n\\end{aligned}\n$$\n\nwhere in the third equality, we used the induction hypothesis in (32).\n\n# Derivation of the second part in (6) \n\nProof. We show $P_{t}=\\tilde{P}_{t}$ for all $t$ via induction.\nBase case. For $t=0$, by the definition of Chebyshev polynomials of the first and the second kinds, we have $T_{0}(\\lambda)=U_{0}(\\lambda)=1$. Thus,\n\n$$\n\\begin{aligned}\nP_{0}(\\lambda) & =m^{0}\\left(\\frac{2 m}{1+m} T_{0}(\\sigma(\\lambda))+\\frac{1-m}{1+m} U_{0}(\\sigma(\\lambda))\\right) \\\\\n& =\\frac{2 m}{1+m}+\\frac{1-m}{1+m}=1=\\tilde{P}_{0}(\\lambda)\n\\end{aligned}\n$$\n\nAgain, for completeness, we prove when $t=1$ as well. In that case, by the definition of Chebyshev polynomials of the first and the second kinds, we have $T_{1}(\\lambda)=\\lambda$, and $U_{1}(\\lambda)=2 \\lambda$. Therefore,\n\n$$\n\\begin{aligned}\nP_{1}(\\lambda) & =m^{t / 2}\\left(\\frac{2 m}{1+m} T_{1}(\\sigma(\\lambda))+\\frac{1-m}{1+m} U_{1}(\\sigma(\\lambda))\\right) \\\\\n& =m^{t / 2}\\left(\\frac{2 m}{1+m} \\sigma(\\lambda)+\\frac{1-m}{1+m} \\cdot 2 \\cdot \\sigma(\\lambda)\\right) \\\\\n& =m^{t / 2}\\left(\\frac{2 \\sigma(\\lambda)}{1+m}\\right) \\\\\n& =1-\\frac{h \\lambda(1-\\gamma \\lambda)}{1+m}=\\tilde{P}_{1}(\\lambda)\n\\end{aligned}\n$$"
    },
    {
      "markdown": "Induction step. As the induction hypothesis, assume that $P_{t}=\\tilde{P}_{t}$ for $t$. In this step, we show that the same holds for $t+1$.\n\n$$\n\\begin{aligned}\nP_{t+1}= & m^{(t+1) / 2}\\left[\\frac{2 m}{1+m} T_{t+1}(\\sigma(\\lambda))+\\frac{1-m}{1+m} U_{t+1}(\\sigma(\\lambda))\\right] \\\\\n= & m^{(t+1) / 2}\\left[\\frac{2 m}{1+m}\\left(2 \\sigma(\\lambda) T_{t}(\\sigma(\\lambda))-T_{t-1}(\\sigma(\\lambda))\\right)\\right. \\\\\n& \\left.\\quad+\\frac{1-m}{1+m}\\left(2 \\sigma(\\lambda) U_{t}(\\sigma(\\lambda)-U_{t-1}(\\sigma(\\lambda))\\right)\\right]\\right] \\\\\n= & 2 \\sigma(\\lambda) \\cdot m^{1 / 2} \\cdot \\underbrace{m^{t / 2}\\left(\\frac{2 m}{1+m} T_{t}(\\sigma(\\lambda))+\\frac{1-m}{1+m} U_{t}(\\sigma(\\lambda))\\right)}_{P_{t}(\\lambda)} \\\\\n& -m \\cdot \\underbrace{m^{(t-1) / 2}\\left(\\frac{2 m}{1+m} T_{t-1}(\\sigma(\\lambda))+\\frac{1-m}{1+m} U_{t-1}(\\sigma(\\lambda))\\right)}_{P_{t-1}(\\lambda)} \\\\\n= & 2 \\sigma(\\lambda) \\cdot \\sqrt{m} \\cdot \\tilde{P}_{t}(\\lambda)-m \\cdot \\tilde{P}_{t-1}(\\lambda) \\\\\n= & (1+m-h \\lambda(1-\\gamma \\lambda)) \\tilde{P}_{t}(\\lambda)-m \\tilde{P}_{t-1}(\\lambda)\n\\end{aligned}\n$$\n\nwhere in the second to last equality we use the induction hypothesis.\n\n# A. 3 Proof of Lemma 2 \n\nProof of Lemma 2 can be found in Goujaud \\& Pedregosa (2022).\n\n## A. 4 Proof of Theorem 2\n\nProof. We first recall that using (3), we can upper bound the worst-case convergence rate as:\n\n$$\n\\begin{aligned}\n\\sup _{\\lambda \\in \\mathcal{S}^{*}}\\left|P_{t}(\\lambda)\\right| & \\stackrel{(6)}{=} \\sup _{\\lambda \\in \\mathcal{S}^{*}}\\left|m^{t / 2}\\left(\\frac{2 m}{1+m} T_{t}(\\sigma(\\lambda))+\\frac{1-m}{1+m} U_{t}(\\sigma(\\lambda))\\right)\\right| \\\\\n& \\leqslant m^{t / 2}\\left(\\frac{2 m}{1+m} \\sup _{\\lambda \\in \\mathcal{S}^{*}}\\left|T_{t}(\\sigma(\\lambda))\\right|+\\frac{1-m}{1+m} \\sup _{\\lambda \\in \\mathcal{S}^{*}}\\left|U_{t}(\\sigma(\\lambda))\\right|\\right)\n\\end{aligned}\n$$\n\nNow, denote $\\bar{\\sigma}:=\\sup _{\\lambda \\in \\mathcal{S}^{*}}|\\sigma(\\lambda ; h, \\gamma, m)|$. For the first case, if $\\bar{\\sigma} \\leqslant 1$, both $T_{t}(x)$ and $U_{t}(x)$ behave nicely, per Lemma 2. Thus, we have\n\n$$\n(33) \\stackrel{(8)}{\\leqslant} m^{t / 2}\\left(\\frac{2 m}{1+m}+\\frac{1-m}{1+m}(t+1)\\right) \\leqslant m^{t / 2}(t+1) \\Longrightarrow \\limsup _{t \\rightarrow \\infty}\\left(m^{t / 2}(t+1)\\right)^{\\frac{1}{2 t}}=\\sqrt[4]{m}\n$$\n\nFor the second case, we use the following expressions of Chebyshev polynomials:\n\n$$\n\\begin{aligned}\n& T_{n}(x)=\\frac{\\left(x-\\sqrt{x^{2}-1}\\right)^{n}+\\left(x+\\sqrt{x^{2}-1}\\right)^{n}}{2}, \\quad \\text { and } \\\\\n& U_{n}(x)=\\frac{\\left(x+\\sqrt{x^{2}-1}\\right)^{n+1}-\\left(x-\\sqrt{x^{2}-1}\\right)^{n+1}}{2 \\sqrt{x^{2}-1}}\n\\end{aligned}\n$$\n\nTherefore, in the second case where $\\bar{\\sigma}>1$, we have both $T_{n}(x)$ and $U_{n}(x)$ growing at rate $\\left(x+\\sqrt{x^{2}-1}\\right)^{n}$. Hence, we have:\n\n$$\n(33) \\leqslant O\\left(m^{t / 2}\\left(\\bar{\\sigma}+\\sqrt{\\bar{\\sigma}^{2}-1}\\right)^{t}\\right) \\Longrightarrow \\limsup _{t \\rightarrow \\infty}\\left(m^{t / 2}\\left(\\bar{\\sigma}+\\sqrt{\\bar{\\sigma}^{2}-1}\\right)^{t}\\right)^{\\frac{1}{2 t}}=\\sqrt[4]{m}\\left(\\bar{\\sigma}+\\sqrt{\\bar{\\sigma}^{2}-1}\\right)^{1 / 2}\n$$"
    },
    {
      "markdown": "Finally, in order for MEG to converge in the second case, we need:\n\n$$\n\\sqrt[4]{m}\\left(\\bar{\\sigma}+\\sqrt{\\bar{\\sigma}^{2}-1}\\right)^{1 / 2}<1\n$$\n\nwhich is equivalent to\n\n$$\n\\bar{\\sigma} \\leqslant \\frac{\\sqrt{m}(m+1)}{2 m}=\\frac{m+1}{2 \\sqrt{m}}\n$$\n\n# A. 5 Derivation of extreme points of robust region in (11) \n\nWe first write a general formula for inverting a quadratic function. For $f(x)=a x^{2}+b x+c$, its inverse is given by:\n\n$$\n\\begin{aligned}\nf(x) & =a x^{2}+b x+c:=y \\\\\nf^{-1}(y) & =\\frac{-b \\pm \\sqrt{b^{2}-4 a(c-y)}}{2 a}\n\\end{aligned}\n$$\n\nwith some abuse of notation (i.e., $f^{-1}$ above is not a function).\nApplying the above to the link function of MEG in (6), we get\n\n$$\n\\sigma^{-1}(y)=\\frac{1}{2 \\gamma} \\pm \\sqrt{\\frac{1}{4 \\gamma^{2}}-\\frac{1+m}{h \\gamma}+\\frac{2 \\sqrt{m}}{h \\gamma} \\cdot y}\n$$\n\nWith this formula, we can plug in 1 and -1 to get:\n\n$$\n\\sigma^{-1}(-1)=\\frac{1}{2 \\gamma} \\pm \\sqrt{\\frac{1}{4 \\gamma^{2}}-\\frac{(1+\\sqrt{m})^{2}}{h \\gamma}} \\text { and } \\sigma^{-1}(1)=\\frac{1}{2 \\gamma} \\pm \\sqrt{\\frac{1}{4 \\gamma^{2}}-\\frac{(1-\\sqrt{m})^{2}}{h \\gamma}}\n$$\n\n## A. 6 Proof of Theorem 3\n\nProof. We analyze each case separately.\nCase 1: There are two square roots: $\\sqrt{\\frac{1}{4 \\gamma^{2}}-\\frac{(1-\\sqrt{m})^{2}}{h \\gamma}}$ and $\\sqrt{\\frac{1}{4 \\gamma^{2}}-\\frac{(1+\\sqrt{m})^{2}}{h \\gamma}}$. The second one is real if:\n\n$$\n\\frac{1}{4 \\gamma^{2}} \\geqslant \\frac{(1+\\sqrt{m})^{2}}{h \\gamma} \\Longrightarrow \\frac{h \\gamma}{4 \\gamma^{2}}=\\frac{h}{4 \\gamma} \\geqslant(1+\\sqrt{m})^{2}\n$$\n\nwhich implies the first is real, as $(1+\\sqrt{m})^{2} \\geqslant(1-\\sqrt{m})^{2}$.\n\nCase 3: There are two square roots: $\\sqrt{\\frac{1}{4 \\gamma^{2}}-\\frac{(1-\\sqrt{m})^{2}}{h \\gamma}}$ and $\\sqrt{\\frac{1}{4 \\gamma^{2}}-\\frac{(1+\\sqrt{m})^{2}}{h \\gamma}}$. The first one is complex if:\n\n$$\n\\frac{1}{4 \\gamma^{2}}<\\frac{(1-\\sqrt{m})^{2}}{h \\gamma} \\Longrightarrow \\frac{h \\gamma}{4 \\gamma^{2}}=\\frac{h}{4 \\gamma}<(1-\\sqrt{m})^{2}\n$$\n\nwhich implies the second is complex, as $(1+\\sqrt{m})^{2} \\geqslant(1-\\sqrt{m})^{2}$.\n\nCase 2: This case follows automatically from the above two cases."
    },
    {
      "markdown": "# A. 7 Proof of Proposition 1 \n\nProof. Define $D_{3}=\\operatorname{diag}(a, \\ldots, a)$ of dimensions $d \\times d$. Let us prove that if there exists $U, V$ orthonormal matrices and $D_{1}, D_{2}$ matrices with non-zeros coefficients only on the diagonal such that (with a slight abuse of notation)\n\n$$\nS_{1}=U \\operatorname{diag}\\left(D_{3}, D_{1}\\right) U^{\\top}, S_{2}=V D_{3} V^{\\top}, \\quad \\text { and } \\quad B=U D_{2} V^{\\top}\n$$\n\nthen the spectrum of $A$ is crossed shaped. In that case, we have\n\n$$\n\\begin{aligned}\nA & =\\left[\\begin{array}{cc}\nU\\left[D_{3} ; D_{1}\\right] U^{\\top} & U D_{2} V^{\\top} \\\\\n-V D_{2}^{\\top} U^{\\top} & V D_{3} V^{\\top}\n\\end{array}\\right] \\\\\n& =\\left[\\begin{array}{cc}\nU & 0 \\\\\n0 & V\n\\end{array}\\right]\\left[\\begin{array}{cc}\n{\\left[D_{3} ; D_{1}\\right]} & D_{2} \\\\\n-D_{2}^{\\top} & D_{3}\n\\end{array}\\right]\\left[\\begin{array}{cc}\nU & 0 \\\\\n0 & V\n\\end{array}\\right]^{\\top}\n\\end{aligned}\n$$\n\nNow by considering the basis $W=\\left(\\left(U_{1}, 0\\right),\\left(0, V_{1}\\right), \\ldots,\\left(U_{d_{v}}, 0\\right),\\left(0, V_{d_{v}}\\right),\\left(U_{d_{v}+1}, 0\\right), \\ldots,\\left(U_{d}, 0\\right)\\right)$ we have that $A$ can be block diagonalized in that basis as\n\n$$\nA=W \\operatorname{diag}\\left(\\left[\\begin{array}{cc}\na & {\\left[D_{2}\\right]_{11}} \\\\\n-\\left[D_{2}\\right]_{11} & a\n\\end{array}\\right], \\ldots,\\left[\\begin{array}{cc}\na & {\\left[D_{2}\\right]_{d_{v}} d_{v}} \\\\\n-\\left[D_{2}\\right]_{d_{v} d_{v}} & a\n\\end{array}\\right],\\left[D_{1}\\right]_{1}, \\ldots,\\left[D_{1}\\right]_{d_{u}-d_{v} d_{u}-d_{v}}\\right) W^{\\top}\n$$\n\nNow, notice that\n\n$$\n\\operatorname{Sp}\\left(\\left[\\begin{array}{cc}\na & -b \\\\\nb & a\n\\end{array}\\right]\\right)=\\{a \\pm b i\\}\n$$\n\nsince the associated characteristic polynomial of the above matrix is:\n\n$$\n(a-\\lambda)^{2}+b^{2}=0 \\Longrightarrow a-\\lambda= \\pm b i \\Longrightarrow \\lambda=a \\pm b i\n$$\n\nHence, using (35) in the formulation of $A$ in (34), we have that the spectrum of $A$ is cross-shaped.\n\n## B Missing Proofs in Section 4\n\n## B. 1 Proof of Theorem 4\n\nProof. We write the conditions required for Theorem 5 below:\n\n$$\n\\begin{aligned}\n& \\frac{1}{2 \\gamma}-\\sqrt{\\frac{1}{4 \\gamma^{2}}-\\frac{(1-\\sqrt{m})^{2}}{h \\gamma}}=\\mu_{1} \\\\\n& \\frac{1}{2 \\gamma}-\\sqrt{\\frac{1}{4 \\gamma^{2}}-\\frac{(1+\\sqrt{m})^{2}}{h \\gamma}}=L_{1} \\\\\n& \\frac{1}{2 \\gamma}+\\sqrt{\\frac{1}{4 \\gamma^{2}}-\\frac{(1+\\sqrt{m})^{2}}{h \\gamma}}=\\mu_{2}, \\quad \\text { and } \\\\\n& \\frac{1}{2 \\gamma}+\\sqrt{\\frac{1}{4 \\gamma^{2}}-\\frac{(1-\\sqrt{m})^{2}}{h \\gamma}}=\\mu_{2}\n\\end{aligned}\n$$\n\nBy adding (37) and (38) (or equivalently by ading (36) and (39)), we get\n\n$$\n\\gamma=\\frac{1}{\\mu_{1}+L_{2}}=\\frac{1}{\\mu_{2}+L_{1}}\n$$"
    },
    {
      "markdown": "From (36), we have:\n\n$$\n\\begin{aligned}\n\\frac{1}{2 \\gamma}+\\sqrt{\\frac{1}{4 \\gamma^{2}}-\\frac{(1-\\sqrt{m})^{2}}{h \\gamma}} & =\\mu_{1} \\\\\n\\frac{1}{4 \\gamma^{2}}-\\frac{(1-\\sqrt{m})^{2}}{h \\gamma} & =\\left(\\frac{1}{2 \\gamma}-\\mu_{1}\\right)^{2} \\\\\n\\frac{(1-\\sqrt{m})^{2}}{h} & =\\mu_{1}\\left(1-\\gamma \\mu_{1}\\right) \\\\\nh=\\frac{(1-\\sqrt{m})^{2}}{\\mu_{1}\\left(1-\\gamma \\mu_{1}\\right)} & =\\frac{(1-\\sqrt{m})^{2}\\left(\\mu_{1}+L_{2}\\right)}{\\mu_{1} L_{2}}\n\\end{aligned}\n$$\n\nSimilarly, from (38), we have:\n\n$$\n\\begin{aligned}\n\\frac{1}{2 \\gamma}+\\sqrt{\\frac{1}{4 \\gamma^{2}}-\\frac{(1+\\sqrt{m})^{2}}{h \\gamma}} & =\\mu_{2} \\\\\n\\frac{1}{4 \\gamma^{2}}-\\frac{(1+\\sqrt{m})^{2}}{h \\gamma} & =\\left(\\frac{\\mu_{2}-L_{1}}{2}\\right)^{2} \\\\\n\\left(\\frac{\\mu_{2}+L_{1}}{2}\\right)^{2}-\\left(\\frac{\\mu_{2}-L_{1}}{2}\\right)^{2} & =\\mu_{2} L_{1}=\\frac{(1+\\sqrt{m})^{2}}{h \\gamma}\n\\end{aligned}\n$$\n\nCombining (41) and (42), and solving for $m$, we get\n\n$$\n\\begin{aligned}\n\\mu_{2} L_{1}(1-\\sqrt{m})^{2} & =\\mu_{1} L_{2}(1+\\sqrt{m})^{2} \\\\\nm & =\\left(\\frac{\\sqrt{\\mu_{2} L_{1}}-\\sqrt{\\mu_{1} L_{2}}}{\\sqrt{\\mu_{2} L_{1}}+\\sqrt{\\mu_{1} L_{2}}}\\right)^{2} \\stackrel{(13)}{=}\\left(\\frac{\\sqrt{\\zeta^{2}-R^{2}}-\\sqrt{\\zeta^{2}-1}}{\\sqrt{\\zeta^{2}-R^{2}}+\\sqrt{\\zeta^{2}-1}}\\right)^{2}\n\\end{aligned}\n$$\n\nFinally, plugging (43) back to (41), we get:\n\n$$\n\\begin{aligned}\nh & =\\frac{(1-\\sqrt{m})^{2}\\left(\\mu_{1}+L_{2}\\right)}{\\mu_{1} L_{2}} \\\\\n& =\\frac{4 \\mu_{1} L_{2}}{\\left(\\sqrt{\\mu_{2}+L_{1}}+\\sqrt{\\mu_{1}+L_{2}}\\right)^{2}} \\cdot \\frac{\\mu_{1}+L_{2}}{\\mu_{1} L_{2}} \\\\\n& =\\frac{4\\left(\\mu_{1}+L_{2}\\right)}{\\left(\\sqrt{\\mu_{2}+L_{1}}+\\sqrt{\\mu_{1}+L_{2}}\\right)^{2}}\n\\end{aligned}\n$$\n\n# B. 2 Proof of Theorem 5 \n\nProof. We write the conditions required for Theorem 5 below:\n\n$$\n\\begin{aligned}\n\\frac{1}{2 \\gamma}-\\sqrt{\\frac{1}{4 \\gamma^{2}}-\\frac{(1-\\sqrt{m})^{2}}{h \\gamma}} & =\\mu \\\\\n\\frac{1}{2 \\gamma}+\\sqrt{\\frac{1}{4 \\gamma^{2}}-\\frac{(1-\\sqrt{m})^{2}}{h \\gamma}} & =L, \\quad \\text { and } \\\\\n\\sqrt{\\frac{(1+\\sqrt{m})^{2}}{h \\gamma}-\\frac{1}{4 \\gamma^{2}}} & =c\n\\end{aligned}\n$$\n\nFirst, by adding (44) and (45), we get:\n\n$$\n\\frac{1}{\\gamma}=\\mu+L \\Longrightarrow \\gamma=\\frac{1}{\\mu+L}\n$$"
    },
    {
      "markdown": "Plugging (47) back into (44), we have:\n\n$$\n\\begin{aligned}\n\\frac{1}{2 \\gamma}-\\sqrt{\\frac{1}{4 \\gamma^{2}}-\\frac{(1-\\sqrt{m})^{2}}{h \\gamma}} & =\\mu \\\\\n\\frac{\\mu+L}{2}-\\mu & =\\sqrt{\\left(\\frac{\\mu+L}{2}\\right)^{2}-\\frac{(1-\\sqrt{m})^{2}(\\mu+L)}{h}} \\\\\n\\left(\\frac{L-\\mu}{2}\\right)^{2} & =\\left(\\frac{\\mu+L}{2}\\right)^{2}-\\frac{(1-\\sqrt{m})^{2}(\\mu+L)}{h} \\\\\n\\frac{(1-\\sqrt{m})^{2}(\\mu+L)}{h} & =\\left(\\frac{\\mu+L}{2}\\right)^{2}-\\left(\\frac{L-\\mu}{2}\\right)^{2}=\\mu L \\\\\nh & =\\frac{(1-\\sqrt{m})^{2}(\\mu+L)}{\\mu L}\n\\end{aligned}\n$$\n\nPlugging (47) and (48) into (46), we have:\n\n$$\n\\begin{aligned}\n\\sqrt{\\frac{(1+\\sqrt{m})^{2}}{h \\gamma}-\\frac{1}{4 \\gamma^{2}}} & =c \\\\\n\\sqrt{\\frac{(1+\\sqrt{m})^{2} \\cdot \\mu L}{(1-\\sqrt{m})^{2}}-\\left(\\frac{\\mu+L}{2}\\right)^{2}} & =c \\\\\n\\frac{(1+\\sqrt{m})^{2} \\cdot \\mu L}{(1-\\sqrt{m})^{2}} & =c^{2}+\\left(\\frac{\\mu+L}{2}\\right)^{2}=\\frac{4 c^{2}+(\\mu+L)^{2}}{4} \\\\\n\\frac{(1+\\sqrt{m})^{2}}{(1-\\sqrt{m})^{2}} & =\\frac{4 c^{2}+(\\mu+L)^{2}}{4 \\mu L} \\\\\n(1+\\sqrt{m}) \\sqrt{4 \\mu L} & =(1-\\sqrt{m}) \\sqrt{4 c^{2}+(\\mu+L)^{2}} \\\\\n\\sqrt{m}\\left(\\sqrt{4 c^{2}+(\\mu+L)^{2}}+\\sqrt{4 \\mu L}\\right) & =\\sqrt{4 c^{2}+(\\mu+L)^{2}}-\\sqrt{4 \\mu L} \\\\\n\\sqrt{m} & =\\frac{\\sqrt{4 c^{2}+(\\mu+L)^{2}}-\\sqrt{4 \\mu L}}{\\sqrt{4 c^{2}+(\\mu+L)^{2}}+\\sqrt{4 \\mu L}}\n\\end{aligned}\n$$\n\nFinally, to simplify (48) further, from (49), we have:\n\n$$\n1-\\sqrt{m}=\\frac{4 \\sqrt{\\mu} L}{\\sqrt{4 c^{2}+(\\mu+L)^{2}}+\\sqrt{4 \\mu L}}\n$$\n\nHence, from (48),\n\n$$\nh=\\frac{(\\mu+L)(1-\\sqrt{m})^{2}}{\\mu L}=\\frac{\\frac{16 \\mu L(\\mu+L)}{\\left(\\sqrt{4 c^{2}+(\\mu+L)^{2}}+\\sqrt{4 \\mu L}\\right)^{2}}}{\\mu L}=\\frac{16(\\mu+L)}{\\left(\\sqrt{4 c^{2}+(\\mu+L)^{2}}+\\sqrt{4 \\mu L}\\right)^{2}}\n$$"
    },
    {
      "markdown": "# B. 3 Proof of Theorem 6 \n\nProof. We write the conditions required for (6) below:\n\n$$\n\\begin{aligned}\n& \\frac{1}{2 \\gamma}+\\sqrt{\\frac{1}{4 \\gamma^{2}}-\\frac{(1+\\sqrt{m})^{2}}{h \\gamma}}=c+b i \\\\\n& \\frac{1}{2 \\gamma}-\\sqrt{\\frac{1}{4 \\gamma^{2}}-\\frac{(1+\\sqrt{m})^{2}}{h \\gamma}}=c+a i \\\\\n& \\frac{1}{2 \\gamma}+\\sqrt{\\frac{1}{4 \\gamma^{2}}-\\frac{(1+\\sqrt{m})^{2}}{h \\gamma}}=c-a i, \\quad \\text { and } \\\\\n& \\frac{1}{2 \\gamma}+\\sqrt{\\frac{1}{4 \\gamma^{2}}-\\frac{(1-\\sqrt{m})^{2}}{h \\gamma}}=c-b i\n\\end{aligned}\n$$\n\nFirst, we can see from all cases that the optimal $\\gamma$ is\n\n$$\n\\gamma=\\frac{1}{2 c}\n$$\n\n(51) and (54) equivalently imply\n\n$$\n\\begin{aligned}\n\\sqrt{\\frac{(1+\\sqrt{m})^{2}}{h \\gamma}-\\frac{1}{4 \\gamma^{2}}} & =b \\\\\n(1+\\sqrt{m})^{2} & =h \\gamma b^{2}+\\frac{h}{4 \\gamma}=\\frac{h\\left(c^{2}+b^{2}\\right)}{2 c} \\\\\nh & =\\frac{2 c(1+\\sqrt{m})^{2}}{c^{2}+b^{2}}\n\\end{aligned}\n$$\n\nSimilarly, (52) and (53) imply\n\n$$\n\\begin{aligned}\n\\sqrt{\\frac{(1-\\sqrt{m})^{2}}{h \\gamma}-\\frac{1}{4 \\gamma^{2}}} & =a \\\\\n\\frac{(1-\\sqrt{m})^{2}}{h \\gamma} & =a^{2}+\\frac{1}{4 \\gamma^{2}}=a^{2}+c^{2} \\\\\n\\frac{(1-\\sqrt{m})^{2}\\left(c^{2}+b^{2}\\right)}{(1+\\sqrt{m})^{2}} & =a^{2}+c^{2} \\\\\n(1-\\sqrt{m}) \\sqrt{c^{2}+b^{2}} & =(1+\\sqrt{m}) \\sqrt{c^{2}+a^{2}} \\\\\n\\sqrt{m} & =\\frac{\\sqrt{c^{2}+b^{2}}-\\sqrt{c^{2}+a^{2}}}{\\sqrt{c^{2}+b^{2}}+\\sqrt{c^{2}+a^{2}}}=1-\\frac{2 \\sqrt{c^{2}+a^{2}}}{\\sqrt{c^{2}+b^{2}}+\\sqrt{c^{2}+a^{2}}}\n\\end{aligned}\n$$\n\nPlugging (57) to (56), we get\n\n$$\nh=\\frac{2 c(1+\\sqrt{m})^{2}}{c^{2}+b^{2}}=\\frac{8 c}{\\left(\\sqrt{c^{2}+b^{2}}+\\sqrt{c^{2}+a^{2}}\\right)^{2}}\n$$\n\n## C Missing Proofs in Sections 5\n\n## C. 1 Proof of Corollary 1\n\nProof. To compute the convergence rates of GD and EG from Theorem 7 applied to each spectrum model in (12), (14), and (15), we need to compute $\\min _{\\lambda \\in \\Delta} \\mathfrak{R}(1 / \\lambda)$ and $\\min _{\\lambda \\in \\Delta} \\mathfrak{R}(\\lambda)$ for GD. Similarly for EG, we need to compute additionally $\\min _{\\lambda \\in \\Delta} \\cdot|\\lambda|, \\min _{\\lambda \\in \\Delta} \\cdot|\\lambda|^{2}$, and $\\sup _{\\lambda \\in \\Delta} \\cdot|\\lambda|^{2}$."
    },
    {
      "markdown": "Case 1: It's straightforward to compute\n\n$$\n\\min _{\\lambda \\in \\mathcal{S}_{1}^{*}} \\mathfrak{R}(1 / \\lambda)=1 / L_{2}, \\quad \\text { and } \\quad \\min _{\\lambda \\in \\mathcal{S}_{1}^{*}} \\mathfrak{R}(\\lambda)=\\mu_{1}\n$$\n\nThus, GD for Case 1 has the rate\n\n$$\n1-\\frac{\\mu_{1}}{L_{2}}=1-\\tau\n$$\n\nFor EG, it's also simple to obtain\n\n$$\n\\min _{\\lambda \\in \\mathcal{S}_{1}^{*}}|\\lambda|=L_{2}, \\quad \\min _{\\lambda \\in \\mathcal{S}_{1}^{*}}|\\lambda|^{2}=\\mu_{1}^{2}, \\quad \\text { and } \\quad \\sup _{\\lambda \\in \\mathcal{S}_{1}^{*}}|\\lambda|^{2}=L_{2}^{2}\n$$\n\nThus, EG for Case 1 has the rate\n\n$$\n1-\\frac{1}{4}\\left(\\frac{\\mu_{1}}{L_{2}}+\\frac{1}{16}\\left(\\frac{\\mu_{1}}{L_{2}}\\right)^{2}\\right)\n$$\n\nCase 2: For a complex number $z=p+q i \\in \\mathbb{C}$, we can compute $\\mathfrak{R}(1 / z)$ as:\n\n$$\n\\frac{1}{z}=\\frac{1}{p+q i}=\\frac{p-q i}{p^{2}+q^{2}}=\\frac{p}{p^{2}+q^{2}}-\\frac{q}{p^{2}+q^{2}} i \\Longrightarrow \\mathfrak{R}\\left(\\frac{1}{z}\\right)=\\frac{p}{p^{2}+q^{2}}\n$$\n\nThe four extreme points of the cross-shaped spectrum model in (14) are:\n\n$$\n\\mu=\\mu+0 i, \\quad L=L+0 i, \\quad \\text { and } \\quad \\frac{L-\\mu}{2} \\pm c i\n$$\n\nHence, $\\mathfrak{R}(1 / z)$ for each of the above points is:\n\n$$\n\\begin{aligned}\n\\mathfrak{R}\\left(\\frac{1}{\\mu}\\right) & =\\frac{\\mu}{\\mu^{2}}=\\frac{1}{\\mu} \\\\\n\\mathfrak{R}\\left(\\frac{1}{L}\\right) & =\\frac{L}{L^{2}}=\\frac{1}{L}, \\quad \\text { and } \\\\\n\\mathfrak{R}\\left(\\frac{1}{\\frac{L-\\mu}{2} \\pm c i}\\right) & =\\frac{\\frac{L-\\mu}{2}}{\\left(\\frac{L-\\mu}{2}\\right)^{2}+c^{2}} \\\\\n& =\\frac{2(L-\\mu)}{4 c^{2}+(L-\\mu)^{2}}\n\\end{aligned}\n$$\n\nTherefore, $\\min _{\\lambda \\in \\mathcal{S}_{2}^{*}} \\mathfrak{R}\\left(\\frac{1}{\\lambda}\\right)=\\frac{1}{L}$. As $\\mu<L$, we only need to compare the last two values. Observe that:\n\n$$\n\\begin{aligned}\nc & >\\sqrt{\\frac{L^{2}-\\mu^{2}}{4}} \\\\\n4 c^{2} & >(L-\\mu)(L+\\mu) \\\\\n4 c^{2} & >2 L(L-\\mu)-(L-\\mu)^{2} \\\\\n\\frac{1}{L} & >\\frac{2(L-\\mu)}{4 c^{2}+(L-\\mu)^{2}}\n\\end{aligned}\n$$\n\nTherefore,\n\n$$\n\\min _{\\lambda \\in \\mathcal{S}_{2}^{*}} \\mathfrak{R}\\left(\\frac{1}{\\lambda}\\right)= \\begin{cases}\\frac{2(L-\\mu)}{4 c^{2}+(L-\\mu)^{2}} & \\text { if } \\quad c>\\sqrt{\\frac{L^{2}-\\mu^{2}}{4}} \\\\ \\frac{1}{L} & \\text { otherwise }\\end{cases}\n$$"
    },
    {
      "markdown": "For $\\min _{\\lambda \\in \\mathcal{S}_{2}^{*}} \\mathfrak{R}(\\lambda)$, it's straightforward from the definition that\n\n$$\n\\min _{\\lambda \\in \\mathcal{S}_{2}^{*}} \\mathfrak{R}(\\lambda)=\\mu\n$$\n\nThus, GD for Case 2 has the rate\n\n$$\n\\begin{cases}1-\\frac{2 \\mu(L-\\mu)}{4 c^{2}+(L-\\mu)^{2}} & \\text { if } \\quad c>\\sqrt{\\frac{L^{2}-\\mu^{2}}{4}} \\\\ 1-\\frac{\\mu}{L} & \\text { otherwise }\\end{cases}\n$$\n\nSimilarly for EG, we need to compute $\\min _{\\lambda \\in \\mathcal{S}_{2}^{*}} \\mathfrak{R}(\\lambda)$, which was computed above; additionally, we need to compute $\\min _{\\lambda \\in \\mathcal{S}_{2}^{*}}|\\lambda|, \\min _{\\lambda \\in \\mathcal{S}_{2}^{*}}|\\lambda|^{2}$, and $\\sup _{\\lambda \\in \\mathcal{S}_{2}^{*}}|\\lambda|^{2}$. For $z=p+q i \\in \\mathbb{C},|z|=\\sqrt{p^{2}+q^{2}}$. Hence, we have\n\n$$\n|\\mu+0 i|=\\mu, \\quad|L+0 i|=L, \\quad \\text { and } \\quad\\left|\\frac{L-\\mu}{2} \\pm c i\\right|=\\sqrt{c^{2}+\\left(\\frac{L-\\mu}{2}\\right)^{2}}\n$$\n\nObserve that:\n\n$$\n\\begin{gathered}\nc>\\sqrt{\\frac{3 L^{2}+2 L \\mu-\\mu^{2}}{4}} \\\\\nc^{2}>L^{2}-\\frac{L^{2}-2 L \\mu+\\mu^{2}}{4} \\\\\nc^{2}+\\left(\\frac{L-\\mu}{2}\\right)^{2}>L^{2}\n\\end{gathered}\n$$\n\nThus, for $\\sup _{\\lambda \\in \\mathcal{S}_{2}^{*}}|\\lambda|$, we have\n\n$$\n\\sup _{\\lambda \\in \\mathcal{S}_{2}^{*}}|\\lambda|= \\begin{cases}\\sqrt{c^{2}+\\left(\\frac{L-\\mu}{2}\\right)^{2}} & \\text { if } \\quad c>\\sqrt{\\frac{3 L^{2}+2 L \\mu-\\mu^{2}}{4}} \\\\ L & \\text { otherwise }\\end{cases}\n$$\n\nfrom which $\\sup _{\\lambda \\in \\mathcal{S}_{2}^{*}}|\\lambda|^{2}$ can also be obtained. Lastly, $\\min _{\\lambda \\in \\mathcal{S}_{3}^{*}}|\\lambda|^{2}=\\mu^{2}$, as we know $\\mu<L$, and $(L-\\mu) / 2$ is the center of $[\\mu, L]$.\nCombining all three, we get the rate of EG for Case 3 is\n\n$$\n\\begin{cases}1-\\frac{1}{4}\\left(\\frac{\\mu}{\\sqrt{c^{2}+\\left(\\frac{L-\\mu}{2}\\right)^{2}}}+\\frac{\\mu^{2}}{16\\left(c^{2}+\\left(\\frac{L-\\mu}{2}\\right)^{2}\\right)}\\right) & \\text { if } c \\geqslant \\sqrt{\\frac{3 L^{2}+2 L \\mu-\\mu^{2}}{4}} \\\\ 1-\\frac{1}{4}\\left(\\frac{\\mu}{L}+\\frac{\\mu^{2}}{16 L^{2}}\\right) & \\text { otherwise }\\end{cases}\n$$\n\nCase 3: Since (15) has fixed real component, $\\min _{\\lambda \\in \\mathcal{S}_{2}^{*}} \\mathfrak{R}(\\lambda)=c$.\nFor $\\min _{\\lambda \\in \\mathcal{S}_{3}^{*}} \\mathfrak{R}(1 / \\lambda)$ we can compute compare\n\n$$\n\\mathfrak{R}\\left(\\frac{1}{c+a i}\\right)=\\frac{c}{c^{2}+a^{2}}>\\frac{c}{c^{2}+b^{2}}=\\mathfrak{R}\\left(\\frac{1}{c+b i}\\right)\n$$\n\nsince $a<b$ from (15). Thus, GD for Case 3 has the rate\n\n$$\n1-\\frac{c^{2}}{c^{2}+b^{2}}\n$$\n\nFor EG, it's also simple to obtain\n\n$$\n\\min _{\\lambda \\in \\mathcal{S}_{2}^{*}}|\\lambda|=\\sqrt{c^{2}+b^{2}}, \\quad \\min _{\\lambda \\in \\mathcal{S}_{3}^{*}}|\\lambda|^{2}=c^{2}+a^{2}, \\quad \\text { and } \\quad \\sup _{\\lambda \\in \\mathcal{S}_{3}^{*}}|\\lambda|^{2}=c^{2}+b^{2}\n$$\n\nThus, EG has the rate\n\n$$\n1-\\frac{1}{4}\\left(\\frac{c}{\\sqrt{c^{2}+b^{2}}}+\\frac{1}{16} \\frac{\\left(c^{2}+a^{2}\\right)}{\\left(c^{2}+b^{2}\\right)}\\right)\n$$"
    },
    {
      "markdown": "# C. 2 Proof of Corollary 2 \n\nProof. Per Theorem 8, the largest $\\epsilon$ that permits acceleration for GDM is $\\epsilon=\\sqrt{\\mu L}$. Therefore, in the special case of (14) we consider, i.e., when $c=\\frac{L-\\mu}{2}$, GDM cannot achieve acceleration if $\\frac{L-\\mu}{2}>\\sqrt{\\mu L}$. Hence, we have:\n\n$$\n\\begin{aligned}\n\\frac{L-\\mu}{2} & >\\sqrt{\\mu L} \\\\\nL-\\mu & >2 \\sqrt{\\mu L} \\\\\nL^{2}+\\mu^{2} & >6 \\mu L>6 \\mu^{2} \\quad(\\because L>\\mu) \\\\\nL & >\\sqrt{5} \\mu .\n\\end{aligned}\n$$\n\n## C. 3 Proof of Proposition 2\n\nProof. For an arbitrary complex number $p+q i$ with $p>0$, and using the link function of GDM from (7), we have\n\n$$\n\\begin{aligned}\n|\\xi(p+q i)|=\\sqrt{\\left(\\frac{1+m-h p}{2 \\sqrt{m}}\\right)^{2}+\\left(\\frac{h q}{2 \\sqrt{m}}\\right)^{2}} & \\leqslant 1 \\\\\n\\frac{(1+m-h p)^{2}+h^{2} q^{2}}{4 m} & \\leqslant 1 \\\\\n(1+m-h p)^{2}+h^{2} q^{2} & \\leqslant 4 m \\\\\n(1-m)^{2}+h p(h p-2(1+m))+h^{2} q^{2} & \\leqslant 0 \\\\\n\\frac{(1-m)^{2}+h^{2} q^{2}}{h p} & \\leqslant 2(1+m)-h p\n\\end{aligned}\n$$\n\nNotice that the LHS is positive. Therefore, if the RHS is negative, the above inequality cannot hold. In other words, if $\\frac{2(1+m)}{h}<p$, GDM cannot stay in the robust region. This is very hard to satisfy, even with a small $p$.\n\n## D Missing Proofs in Section 6\n\nLet us consider an affine vector field $v(w)=A w+b$ and its associated augmented MEG linear operator $J$ :\n\n$$\n\\left[\\begin{array}{c}\nw_{t+1}-w^{\\star} \\\\\nw_{t}-w^{\\star}\n\\end{array}\\right]=J\\left[\\begin{array}{c}\nw_{t}-w^{\\star} \\\\\nw_{t-1}-w^{\\star}\n\\end{array}\\right] \\quad \\text { with } \\quad J=\\left[\\begin{array}{c}\n(1+\\beta) I_{d}-h A\\left(I_{d}-\\gamma A\\right) \\\\\nI_{d}\n\\end{array}\\right] \\quad \\begin{array}{c}\n-\\beta I_{d} \\\\\n0_{d}\n\\end{array}\\right]\n$$\n\nwhere $I_{d}$ and $0_{d}$ respectively stand for the identity and the null matrices. To show the local convergence of (restarted) MEG for non-affine vector fields in Theorem 9, we first establish the following lemma, which connects the augmented state and the non-augmented one.\nLemma 3. Let $P_{t}^{M E G}$ be the residual polynomial associated with $t$ updates of MEG (c.f., Theorem 1). Let $J$ be defined as (58). If $w_{1}=w_{0}-\\frac{h}{1+m} v\\left(w_{0}-\\gamma v\\left(w_{0}\\right)\\right)$, we then have\n\n$$\nJ^{t}\\left[\\begin{array}{l}\nw_{1}-w^{\\star} \\\\\nw_{0}-w^{\\star}\n\\end{array}\\right]=\\left[\\begin{array}{l}\nP_{t}^{M E G}(A)\\left(w_{1}-w^{\\star}\\right) \\\\\nP_{t}^{M E G}(A)\\left(w_{0}-w^{\\star}\\right)\n\\end{array}\\right]\n$$\n\nConsequently, if we denote $z_{t+1}:=\\left[w_{t+1}, w_{t}\\right]$ and $z_{*}:=\\left[w^{\\star}, w^{\\star}\\right]$, we have\n\n$$\n\\left\\|z_{t+1}-z_{*}\\right\\| \\leqslant C(t+1)(1-\\varphi)^{t}\\left\\|z_{0}-z_{*}\\right\\|\n$$"
    },
    {
      "markdown": "Proof. Let us express $J^{t}$ such that\n\n$$\nJ^{t}=\\left[\\begin{array}{ll}\nP_{t}^{11}(A) & P_{t}^{12}(A) \\\\\nP_{t}^{21}(A) & P_{t}^{22}(A)\n\\end{array}\\right], \\quad \\text { and } \\quad J^{t}\\left[\\begin{array}{l}\nw_{1}-w^{\\star} \\\\\nw_{0}-w^{\\star}\n\\end{array}\\right]=\\left[\\begin{array}{l}\nP_{t}^{11}(A)\\left(w_{0}-w^{\\star}\\right)+P_{t}^{12}(A)\\left(w_{0}-w^{\\star}\\right) \\\\\nP_{t}^{21}(A)\\left(w_{0}-w^{\\star}\\right)+P_{t}^{22}(A)\\left(w_{0}-w^{\\star}\\right)\n\\end{array}\\right]\n$$\n\nBy writing $J^{t+1}=J J^{t}$ and using the block-matrix form of $J$ in (58), we get that for any $t \\geqslant 0$,\n\n$$\n\\begin{aligned}\nP_{t+1}^{11}(A) & =((1+\\beta) I_{d}-h A\\left(I_{d}-\\gamma A\\right)) P_{t}^{11}(A)-\\beta P_{t}^{21}(A) \\\\\nP_{t+1}^{21}(A) & =P_{t}^{11}(A) \\\\\nP_{t+1}^{12}(A) & =((1+\\beta) I_{d}-h A\\left(I_{d}-\\gamma A\\right)) P_{t}^{12}(A)-\\beta P_{t}^{22}(A) \\\\\nP_{t+1}^{22}(A) & =P_{t}^{12}(A)\n\\end{aligned}\n$$\n\nHence, we have that,\n\n$$\n\\begin{aligned}\n& P_{t+1}^{11}(A) \\stackrel{(62)}{=}\\left((1+\\beta) I_{d}-h A\\left(I_{d}-\\gamma A\\right)\\right) P_{t}^{11}(A)-\\beta P_{t-1}^{11}(A) \\\\\n& P_{t+1}^{12}(A) \\stackrel{(63)}{=}\\left((1+\\beta) I_{d}-h A\\left(I_{d}-\\gamma A\\right)\\right) P_{t}^{12}(A)-\\beta P_{t-1}^{12}(A)\n\\end{aligned}\n$$\n\nWe claim that\n\n$$\nP_{t}^{11}(A)+P_{t}^{12}(A)=P_{t}^{\\mathrm{MEG}}(A) \\quad \\text { for all } \\quad t \\geqslant 0\n$$\n\nWe prove this via induction.\nFor the base case, using the fact that $w_{1}=w_{0}-\\frac{h}{1+m}\\left(w_{0}-\\gamma v\\left(w_{0}\\right)\\right)$, we have that\n\n$$\n\\begin{aligned}\n& \\left(P_{1}^{11}(A)+P_{1}^{12}(A)\\right)\\left(w_{0}-w^{\\star}\\right)=w_{1}-w^{\\star}=\\left(I_{d}-\\frac{h}{1+m}\\left(I_{d}-\\gamma A\\right)\\right)\\left(w_{0}-w^{\\star}\\right)=P_{1}^{\\mathrm{MEG}}(A)\\left(w_{0}-w^{\\star}\\right) \\\\\n& \\left(P_{0}^{11}(A)+P_{0}^{12}(A)\\right)\\left(w_{0}-w^{\\star}\\right)=\\left(P_{1}^{21}(A)+P_{1}^{22}(A)\\right)\\left(w_{0}-w^{\\star}\\right)=I_{d}\\left(w_{0}-w^{\\star}\\right)=P_{0}^{\\mathrm{MEG}}(A)\\left(w_{0}-w^{\\star}\\right)\n\\end{aligned}\n$$\n\nTo show the induction step, by adding (64) and (65), we get\n\n$$\n\\begin{aligned}\nP_{t+1}^{11}(A)+P_{t+1}^{12}(A) & =\\left((1+\\beta) I_{d}-h A\\left(I_{d}-\\gamma A\\right)\\right)\\left(P_{t}^{11}(A)+P_{t}^{12}(A)\\right)-\\beta\\left(P_{t-1}^{11}(A)+P_{t-1}^{12}(A)\\right) \\\\\n& \\stackrel{(66)}{=}\\left((1+\\beta) I_{d}-h A\\left(I_{d}-\\gamma A\\right)\\right) P_{t}^{\\mathrm{MEG}}(A)-\\beta P_{t-1}^{\\mathrm{MEG}}(A)\n\\end{aligned}\n$$\n\nwhere in the last step we used the induction hypothesis. Also notice $P_{t+1}^{11}(A)+P_{t+1}^{12}(A)=P_{t+1}^{\\mathrm{MEG}}$ on the left-hand side.\n\nHence we have for any $t \\geqslant 0$,\n\n$$\n\\left(P_{t}^{11}(A)+P_{t}^{12}(A)\\right)\\left(w_{0}-w^{\\star}\\right)=P_{t}^{\\mathrm{MEG}}(A)\\left(w_{0}-w^{\\star}\\right)\n$$\n\nTherfore, going back to (61), we have:\n\n$$\n\\begin{aligned}\n{\\left[\\begin{array}{c}\nw_{t+1}-w^{\\star} \\\\\nw_{t}-w^{\\star}\n\\end{array}\\right]=J^{t}\\left[\\begin{array}{l}\nw_{1}-w^{\\star} \\\\\nw_{0}-w^{\\star}\n\\end{array}\\right] } & =\\left[\\begin{array}{l}\n\\left(P_{t}^{11}(A)+P_{t}^{12}(A)\\right)\\left(w_{0}-w^{\\star}\\right) \\\\\n\\left(P_{t}^{21}(A)+P_{t}^{22}(A)\\right)\\left(w_{0}-w^{\\star}\\right)\n\\end{array}\\right] \\\\\n& \\stackrel{(62)}{=}\\left[\\begin{array}{c}\n\\left(P_{t}^{11}(A)+P_{t}^{12}(A)\\right)\\left(w_{0}-w^{\\star}\\right) \\\\\n\\left(P_{t-1}^{11}(A)+P_{t-1}^{12}(A)\\right)\\left(w_{0}-w^{\\star}\\right)\n\\end{array}\\right] \\\\\n& \\stackrel{(66)}{=}\\left[\\begin{array}{l}\nP_{t}^{\\mathrm{MEG}}(A)\\left(w_{0}-w^{\\star}\\right) \\\\\nP_{t-1}^{\\mathrm{MEG}}(A)\\left(w_{0}-w^{\\star}\\right)\n\\end{array}\\right] \\\\\n& \\text { Thm. } 1 .\\left[\\begin{array}{c}\nP_{t}^{\\mathrm{MEG}}(A)\\left(w_{0}-w^{\\star}\\right) \\\\\nP_{t}^{\\mathrm{MEG}}(A)\\left(w_{-1}-w^{\\star}\\right)\n\\end{array}\\right] \\\\\n& =\\left[\\begin{array}{cc}\nP_{t}^{\\mathrm{MEG}}(A) & 0 \\\\\n0 & P_{t}^{\\mathrm{MEG}}(A)\n\\end{array}\\right] \\cdot\\left[\\begin{array}{c}\nw_{0}-w^{\\star} \\\\\nw_{-1}-w^{\\star}\n\\end{array}\\right] \\\\\n& =P_{t}^{\\mathrm{MEG}}(A) \\otimes I_{2} \\cdot\\left[\\begin{array}{c}\nw_{0}-w^{\\star} \\\\\nw_{-1}-w^{\\star}\n\\end{array}\\right]\n\\end{aligned}\n$$"
    },
    {
      "markdown": "where we use the convention that $w_{0}=w_{-1}$. Finally, using the fact that $\\|A \\otimes B\\|=\\|A\\|\\|B\\|$ for $\\ell_{2}$-operator norm (Lancaster \\& Farahat, 1972), we have\n\n$$\n\\left\\|z_{t+1}-z_{*}\\right\\| \\leqslant\\left\\|P_{t}^{\\mathrm{MEG}}(A)\\right\\|\\left\\|z_{0}-z_{*}\\right\\| \\stackrel{(10)}{\\leqslant} C(t+1)(1-\\varphi)^{t}\\left\\|z_{0}-z_{*}\\right\\|\n$$\n\n# D. 1 Proof of Theorem 9 \n\nProof. We first recall the restarted MEG algorithm we consider in (31):\n\n$$\n\\begin{aligned}\n{\\left[w_{t k+i+1}, w_{t k+i}\\right] } & =G\\left(\\left[w_{t k+i}, w_{t k+i-1}\\right]\\right) \\quad \\text { for } \\quad 1 \\leqslant i \\leqslant k-1, \\quad \\text { and then } \\\\\nw_{(t+1) k+1} & =w_{(t+1) k}-\\frac{h}{1+m} v\\left(w_{(t+1) k}-\\gamma v\\left(w_{(t+1) k}\\right)\\right)\n\\end{aligned}\n$$\n\nIn other words, we repeat MEG for $k$ steps, and then re-start the momentum at $\\left[w_{(t+1) k+1}, w_{(t+1) k}\\right]$.\nWe can analyze this method as follows, where we denote $z_{t}:=\\left[w_{t}, w_{t-1}\\right]$ and $z_{*}=\\left[w^{\\star}, w^{\\star}\\right]$ :\n\n$$\n\\begin{aligned}\n\\left\\|z_{(t+1) k}-z_{*}\\right\\| & =\\left\\|G^{(k)}\\left(z_{t k}\\right)-z_{*}\\right\\| \\\\\n& \\stackrel{(68)}{=}\\left\\|\\nabla G^{(k)}\\left(\\tilde{z}_{t k}\\right)\\left(z_{t k}-z_{*}\\right)\\right\\| \\\\\n& \\leqslant\\left\\|\\nabla G^{(k)}\\left(z_{*}\\right)\\left(z_{t k}-z_{*}\\right)\\right\\|+\\left\\|\\left(\\nabla G^{(k)}\\left(\\tilde{z}_{t k}\\right)-\\nabla G^{(k)}\\left(z_{*}\\right)\\right)\\left(z_{t k}-z_{*}\\right)\\right\\| \\\\\n& \\stackrel{(60)}{\\leqslant} C(k+1)(1-\\varphi)^{k}\\left\\|z_{t k}-z_{*}\\right\\|+\\left\\|\\nabla G^{(k)}\\left(\\tilde{z}_{t k}\\right)-\\nabla G^{(k)}\\left(z_{*}\\right)\\right\\|\\left\\|\\left(z_{t k}-z_{*}\\right)\\right\\|,\n\\end{aligned}\n$$\n\nwhere in the second line we use the Mean Value Theorem:\n\n$$\n\\begin{aligned}\n\\exists \\tilde{z}_{t k} \\in\\left[z_{t k}, z_{*}\\right] \\quad \\text { such that } \\quad G^{(k)}\\left(z_{t k}\\right) & =G^{(k)}\\left(z_{*}\\right)+\\nabla G^{(k)}\\left(\\tilde{z}_{t k}\\right)\\left(z_{t k}-z_{*}\\right) \\\\\n& =z_{*}+\\nabla G^{(k)}\\left(\\tilde{z}_{t k}\\right)\\left(z_{t k}-z_{*}\\right) \\quad \\text { (since } z_{*} \\text { is the fixed point). }\n\\end{aligned}\n$$\n\nIn the fourth line we used the fact that $\\nabla G^{(k)}\\left(z_{*}\\right)\\left(z_{t k}-z_{*}\\right)$ exactly correspond to $k$ updates of MEG when the vector field is affine, as well as Lemma 3 to account for the augmented state.\nNow let us consider $\\varphi>\\varepsilon>0$ and $k$ large enough such that $C(k+1)(1-\\varphi)^{k} \\leqslant\\left(1-\\varphi+\\frac{\\varepsilon}{2}\\right)^{k}$. Since $\\nabla G$ is assumed to be continuous, $\\nabla G^{(k)}$ is continuous too. Therefore, there exists $\\delta>0$ such that $\\left\\|z_{t k}-z_{*}\\right\\| \\leqslant \\delta$ implies $\\left\\|\\nabla G^{(k)}\\left(\\tilde{z}_{t k}\\right)-\\nabla G^{(k)}\\left(z_{*}\\right)\\right\\| \\leqslant \\varepsilon^{\\prime}$. In particular, choose $\\varepsilon^{\\prime}=(1-\\varphi+\\varepsilon)^{k}-\\left(1-\\varphi+\\frac{\\varepsilon}{2}\\right)^{k} \\sim \\frac{k \\varepsilon}{2(1-\\varphi)}$. Then, we have\n\n$$\n\\begin{aligned}\n\\left\\|z_{(t+1) k}-z_{*}\\right\\| & \\leqslant C(k+1)(1-\\varphi)^{k}\\left\\|z_{t k}-z_{*}\\right\\|+\\left\\|\\nabla G^{(k)}\\left(\\tilde{z}_{t k}\\right)-\\nabla G^{(k)}\\left(z_{*}\\right)\\right\\|\\left\\|z_{t k}-z_{*}\\right\\| \\\\\n& \\leqslant\\left(1-\\varphi+\\frac{\\varepsilon}{2}\\right)^{k}\\left\\|z_{t k}-z_{*}\\right\\|+\\varepsilon^{\\prime}\\left\\|z_{t k}-z_{*}\\right\\| \\\\\n& \\leqslant(1-\\varphi+\\varepsilon)^{k}\\left\\|z_{t k}-z_{*}\\right\\|<\\left\\|z_{t k}-z_{*}\\right\\|<\\left\\|z_{0}-z_{*}\\right\\|\n\\end{aligned}\n$$\n\nFrom the above, we can conclude that for all $\\varepsilon>0$, there exists $k>0$ and $\\delta>0$ such that, for all initialization satisfying $\\left\\|w_{0}-w^{\\star}\\right\\| \\leqslant \\delta$, the restarted MEG described above satisfies:\n\n$$\n\\left\\|w_{t}-w^{\\star}\\right\\|=O\\left((1-\\varphi+\\varepsilon)^{t}\\right)\\left\\|w_{0}-w^{\\star}\\right\\|\n$$"
    }
  ],
  "usage_info": {
    "pages_processed": 30,
    "doc_size_bytes": 1563121
  },
  "_metadata": {
    "model_version": "0.0.0",
    "llm_model": "mistral-ocr-2505",
    "query_data": {
      "pdf": "/Users/satyaortiz-gagne/CODE/paperext/data/cache/fulltext/9541fb5c4a4fa30edda5a507ed1cea49/fulltext.pdf"
    },
    "model_id": "parsepdf"
  }
}