{
  "pages": [
    {
      "markdown": "# On the Identifiability of Quantized Factors \n\nVitória Barin-Pacela<br>FAIR, Meta; Mila; DIRO, Université de Montréal<br>Kartik Ahuja<br>FAIR, Meta<br>Simon Lacoste-Julien<br>Mila; DIRO, Université de Montréal; Canada CIFAR AI Chair\n\n## Pascal Vincent\n\nFAIR, Meta; Mila; DIRO, Université de Montréal; CIFAR\n\nEditors: Francesco Locatello and Vanessa Didelez\n\n\n#### Abstract\n\nDisentanglement aims to recover meaningful latent ground-truth factors from the observed distribution solely, and is formalized through the theory of identifiability. The identifiability of independent latent factors has been proven to be impossible in the unsupervised i.i.d. setting under a general nonlinear map from factors to observations. In this work, however, we demonstrate that it is possible to recover quantized latent factors under a generic nonlinear diffeomorphism. We only assume that the latent factors have independent discontinuities in their density, without requiring the factors to be statistically independent. We introduce this novel form of identifiability, termed quantized factor identifiability, and provide a comprehensive proof of the recovery of the quantized factors.\n\n\nKeywords: identifiability, disentanglement, causal representation learning, quantized representations, discrete representations\n\n## 1. Introduction\n\nA large part of intelligence is based on the ability to make sense of observed sensory data without explicit supervision. The goal of representation learning is, thus, to detect and model relevant structure in the distribution of observed data, and expose it into useful compact representations, to facilitate generalization and sample-efficient learning of subsequent tasks. One long-standing goal in that respect has been that of structuring the representation into disentangled factors (Bengio et al., 2013). These may be conceived of as \"natural\" ground truth, descriptive, or causal variables that underlie the observations. A vector representation consisting of recovered disentangled factors may be viewed as corresponding to a natural Cartesian coordinate system for the observations, whereby each varying factor is associated with an axis.\n\nIdentifiability theory formalizes the foundations of disentanglement by precisely delimiting the conditions under which it is possible. Unsupervised disentanglement of latent factors has been found impossible in the general nonlinear setting in the absence of further inductive bias (Locatello et al., 2019). This result echoes an older identification impossibility result on nonlinear Independent Component Analysis (Hyvärinen and Pajunen, 1999). As a result, much subsequent work has sidestepped the issue either via stronger inductive biases, such as more restrictive assumptions on the function that maps latent factors to observations (Buchholz et al., 2022; Kivva et al., 2022; Ahuja et al., 2022c; Brady et al., 2023; Lachapelle et al., 2023), for instance sparsity of its Jacobian (Moran"
    },
    {
      "markdown": "et al., 2022; Zheng et al., 2022; Zheng and Zhang, 2023), or by turning to weakly supervised disentanglement, using some form of additional information (see related works in Appendix A).\n\nProvided that they corresponds to valid assumptions, inductive biases should undoubtedly be used in practice whenever available, as well as any additional supervisory signals. However, in the present theoretical work, we revisit and tackle the problem of fully unsupervised identifiability of latent factors, the most challenging setting. We assume a generic smooth invertible nonlinear mapping: a diffeomorphism. No additional assumptions are made on the mapping, and the assumption of the factors being mutually independent is also discarded.\n\nGiven the previous theoretical impossibility results for unsupervised identifiability under a diffeomorphism, a shift in our approach was necessary. We relax the notion of identifiability of continuous factors to that of identifiability of quantized continuous factors.\n\nThe promise of quantized, grid-like representations has been argued empirically in both machine learning and neuroscience. It has been suggested that the brain of humans and other animals organizes spatial knowledge and relational concepts into codes that have an hexagonal grid-like pattern (Constantinescu et al., 2016; Whittington et al., 2020). In representation learning, vector quantization has shown enormous success in image generation (van den Oord et al., 2017). Concurrent studies investigate empirically this explicit relationship with disentanglement (Hsu et al., 2023), and further explore quantization in grid-structured representations (Mentzer et al., 2024; Irie et al., 2023; Friede et al., 2023).\n\nHowever, none of these provide a supporting identifiability theory for quantized factors. In the present work, we first formalize this novel relaxed form of identifiability. We, then, provide a full proof of the identifiability of quantized factors under a general diffeomorphism. This is achieved by assuming, rather than the mutual independence of factors, the presence of independent discontinuities in the joint probability density ${ }^{1}$ of the latent factors.\n\nOur contributions are the following:\n\n- We introduce and formalize a novel relaxed form of representation identifiability: quantized factor identifiability.\n- We provide the first proof of representation identifiability under a general diffeomorphic map, which sets itself apart from the impossibility results that dominate the field.\nWe hope that this novel theoretical foundation may provide useful insights to develop algorithms of practical relevance for robustly learning disentangled representations.\n\n\n# 2. From precise factor identifiability to quantized factor identifiability \n\nIn this section, we define and contrast the standard form of factor identifiability, which we term \"precise factor identifiability\", with our new relaxed \"quantized factor identifiability\" paradigm.\n\n### 2.1. Setup\n\nWe suppose that we have access to observations in $\\mathcal{X} \\subset \\mathbb{R}^{D}$. They are realizations of the vector random variable $X=\\left(X_{1}, \\ldots, X_{D}\\right)$, which is assumed to be a transformation of a real vector of unobserved latent factors $Z=\\left(Z_{1}, \\ldots, Z_{d}\\right)$, i.e. $X=f(Z)$, via a bijective mapping $f: \\mathcal{Z} \\rightarrow \\mathcal{X}$ where $\\mathcal{Z} \\subset \\mathbb{R}^{d}$. The mapping $f$ is called the mixing map, which is unknown but is assumed to belong to a broad function class. The latent factors $Z$ follow a distribution represented by the probability\n\n[^0]\n[^0]:    1. More precisely, these are non-removable discontinuities in the PDF, as will be elaborated in Section 5.1."
    },
    {
      "markdown": "![img-0.jpeg](img-0.jpeg)\n\nFigure 1: Recovery of quantized factors. Left: The true (continuous) latent factors $Z_{1}$ and $Z_{2}$ are not independent, but their joint probability density $p_{Z}$ has independent discontinuities: sharp changes in the density that are aligned with the axes and form a grid. Middle: The factors get warped and entangled by the diffeomorphism $f$ into observations $X$, but the discontinuities in their density survive in the observed space. Right: We can learn a diffeomorphism $g$ that yields a density $p_{Z^{\\prime}}$ having axis-aligned discontinuities. This suffices to recover a grid whose cells match the initial grid's cells (up to possible permutation and axis reversal). Pink cell example: the points $Z^{\\prime}$ in cell $(3,2)$ originated from the points $Z$ in cell $(3,2)$. To construct these cells, the quantization of each continuous factor to an integer depends on thresholds based on the location of the discontinuities. The quantizations of $Z_{1}^{\\prime}$ and $Z_{2}^{\\prime}$ match precisely the quantizations of $Z_{1}$ and $Z_{2}$, up to possible permutation and axis reversal. This summarizes the identifiability of quantized factors under diffeomorphisms.\ndensity function (PDF) $p_{Z}$, which is also unknown but is typically subject to assumptions. This induces a distribution for $X$ whose PDF is denoted by $p_{X}$. The mapping $g: \\mathcal{X} \\rightarrow \\mathcal{Z}$ approximates $f^{-1}$ at the optimum.\n\nThe setup is summarized in the following diagram.\n![img-1.jpeg](img-1.jpeg)\n\nIn identifiability theory, the distribution of observations $p_{X}$ is supposedly known. Alternatively, for this level of precision, observed samples from $p_{X}$ can be considered with the sample size approaching infinity. Identifiability theorems also need to make clear assumptions on the mixing map $f$ and on the density of factors $p_{X}$.\n\nIn the remainder of this section, we first formalize the usual factor identifiability as well as our proposed relaxation to quantized factor identifiability in the general case. Subsequently, we focus on the case where $f$ is a diffeomorphism.\n\n# 2.2. Precise Factor Identifiability \n\nThe usual, precise factor identifiability theorems amount to statements of the following form:"
    },
    {
      "markdown": "Precise Identifiability of Factors: Knowledge of $p_{X}$ is sufficient to determine a reverse mapping $g: \\mathbb{R}^{D} \\rightarrow \\mathbb{R}^{d}$ that will yield recovered factors $\\left(Z_{1}^{\\prime}, \\ldots, Z_{d}^{\\prime}\\right)=g(X)$ that correspond one-to-one to the ground-truth factors $\\left(Z_{1}, \\ldots, Z_{d}\\right)$, up to permutation and component-wise invertible transformations (ideally monotonic).\n\nFormally: there exists an indices permutation function $\\sigma$ and invertible scalar functions $\\gamma_{i}$ such that $\\forall i \\in\\{1, \\ldots, d\\}, \\gamma_{i}\\left(Z_{i}^{\\prime}\\right)=Z_{j}$ with $j=\\sigma(i)$. Precise factor identifiability theorems require specifying assumptions on $f$ and on $p_{Z}$.\n\n# 2.3. Quantization of factors \n\nLet us now specify how the factors can be quantized. For simplicity, we consider that each factor is a real-valued scalar. A real number $z$ may be quantized to an integer based on a tuple of real thresholds $T$ via the following quantization operation:\n\n$$\nQ(z ; T)=\\sum_{k=1}^{|T|} \\mathbf{1}_{z \\geq T_{k}}\n$$\n\nFor example, consider $z \\in[0,4]$ and $T=(0.5,2.0)$. Then $Q(z ; T)=\\mathbf{1}_{z \\geq 0.5}+\\mathbf{1}_{z \\geq 2.0}$. So $Q(z ; T)= \\begin{cases}0, & 0 \\leq z<0.5 \\\\ 1, & 0.5 \\leq z<2 \\\\ 2, & 2 \\leq z \\leq 4\\end{cases}$\n\nWe also define quantization with order reversal as $Q^{-}(z ; T)=\\sum_{k=1}^{|T|} \\mathbf{1}_{z \\leq T_{k}}$. For convenience, we will use the notation $Q^{(s)}$ to mean $Q$ if $s=+1$ and $Q^{-}$if $s=-1$.\n\nThe set of specific thresholds used for quantizing a random variable $Z_{i}$ is typically derived from some properties of its distribution $p_{Z_{i}}$ (e.g. a set of $|T|$ specific quantiles). We will consider the more general case, where the thresholds for quantizing $Z_{i}$ might be determined not only based on $p_{Z_{i}}$, but more generally, on $i$ and on the joint probability density of all factors $p_{Z}$. The operation returning a set of thresholds to be used for a factor $Z_{i}$ is denoted by $\\mathcal{T}\\left(p_{Z}, i\\right)$. Thus, the quantization of $Z_{i}$ may be written as: $q_{i}\\left(Z_{i}\\right)=Q\\left(Z_{i} ; \\mathcal{T}\\left(p_{Z}, i\\right)\\right)$.\n\n### 2.4. Quantized Factor Identifiability\n\nQuantized factor identifiability theorems will be statements of the following form:\nIdentifiability of Quantized Factors: Knowledge of $p_{X}$ is sufficient to determine a reverse mapping $g: \\mathbb{R}^{D} \\rightarrow \\mathbb{R}^{d}$ that will yield recovered factors $\\left(Z_{1}^{\\prime}, \\ldots, Z_{d}^{\\prime}\\right)=g(X)$ such that their quantization $\\left(q_{1}^{\\prime}\\left(Z_{1}^{\\prime}\\right), \\ldots, q_{d}^{\\prime}\\left(Z_{d}^{\\prime}\\right)\\right)$ will correspond one-to-one to the quantized ground-truth factors $\\left(q_{1}\\left(Z_{1}\\right), \\ldots, q_{d}\\left(Z_{d}\\right)\\right)$, up to possible permutation of indices and order reversal.\n\nFormally, there exists an indices permutation function $\\sigma$ and order-reversal indicators $s_{i} \\in$ $\\{-1,+1\\}$ such that: $\\forall i \\in\\{1, \\ldots, d\\}, q_{i}^{\\prime}\\left(Z_{i}^{\\prime}\\right)=q_{j}\\left(Z_{j}\\right)$, with $j=\\sigma(i)$, where $q_{j}$ and $q_{i}^{\\prime}$ are monotonic quantization functions. We can, more precisely, define $q_{j}$ as $q_{j}\\left(Z_{j}\\right)=Q\\left(Z_{j} ; \\mathcal{T}\\left(p_{Z}, j\\right)\\right)$, and $q_{i}^{\\prime}\\left(Z_{i}^{\\prime}\\right)=Q^{s_{i}}\\left(Z_{i}^{\\prime} ; \\mathcal{T}\\left(p_{Z^{\\prime}}, i\\right)\\right)$. The precise operation $\\mathcal{T}$ that determines how the quantization"
    },
    {
      "markdown": "thresholds are obtained from properties of the distributions remain to be specified by the particular quantized factor identifiability theorem.\n\nHence, quantized factor identifiability theorems require specifying assumptions on $f$, assumptions on $p_{Z}$, as well as a precise quantization operation. For the quantization to be meaningful, it should produce at least two non-empty bins. That is, for any given factor, the respective factor samples will be mapped to at least two different quantized values. Quantization to a single all-encompassing bin is trivially identifiable and useless.\n\nWe highlight that quantized factor identifiability does not intend to prove identifiability when the true factors take a discrete set of values. Instead, we define a relaxed form of identifiability for continuous ground-truth factors. Quantization leads to a loss of precision/resolution, resulting in a coarser identification.\n\n# 3. What to assume on $p_{Z}$ when $f$ is a diffeomorphism \n\nFrom now on, we will turn our attention to the case where the mixing map $f$ is assumed to be a general diffeomorphism, that is, a continuously differentiable function with a continuously differentiable inverse. For the remaining of the paper, we will assume that all diffeomorphisms considered are smooth, i.e. $C^{\\infty}$, in order to simplify the statements. The goal is to learn the approximate inverse diffeomorphism $g$. First, let us discuss what assumptions we should make on the distribution of factors $p_{Z}$ that may yield a positive identifiability result.\n\n### 3.1. Disentanglement, independence, and discontinuities\n\nDisentanglement has been equated to finding statistically independent factors (Khemakhem et al., 2020a), but statistical independence has been criticized as an unrealistic and problematic assumption (Träuble et al., 2021; Dittadi et al., 2021; Roth et al., 2023) whose association to disentanglement is misleading. For example, the usual descriptive factors with which we describe scenes are usually not statistically independent. Consider the variables color, shape, and background: bananas tend to be yellow; cows tend to be on grass backgrounds; camels tend to be on sand.\n\nMoreover, a primary interest for learning disentangled representations is as an enabler of robust generalization under distribution shifts. From this perspective, we should aim for factor discovery approaches that are stable and insensitive to broad changes in the (unknown) distribution of the factors, such as whether they happen to be independent or correlated in the data. Aiming for extracting statistically independent factors will, by construction, be very sensitive to this, which goes contrary to the desired robustness.\n\nLastly, and most importantly for our goal of characterizing what form of unsupervised identification is possible under a diffeomorphism, assuming statistical independence is insufficient, as previous impossibility results for nonlinear ICA have shown (Hyvärinen and Pajunen, 1999). This is fundamentally due to the extreme flexibility of diffeomorphisms. Even more discouraging, Buchholz et al. (2022) have shown that even if we knew $p_{Z}$ precisely, we could not achieve precise factor identifiability. This is because a diffeomorphism can move data points along isosurfaces of $p_{Z}$ while keeping the same $p_{Z}$, thus rendering entangled representations indistinguishable from disentangled ones.\n\nTo prevent this movement along isolines of $p_{Z}$ from taking points from one region to another of the factor space, there could be barriers of discontinuity separating different regions of $p_{Z}$. We"
    },
    {
      "markdown": "develop this justification for the need for discontinuities more precisely in Appendix H, based on the result from Buchholz et al. (2022).\n\nAnother perspective to consider is that discontinuities are among the few characteristics of a density that diffeomorphisms can neither erase nor create (Theorem 4). Hence, they are good candidates for holding cues in $p_{Z}$ guaranteed to survive in some form when mapped to $X$ via any diffeomorphism. Thus, if they were indicative of coordinate axes in $Z$, there is a prospect of recovering them from the resulting discontinuities in $p_{X}$.\n\nTherefore, to enable the identifiability of quantized factors under a diffeomorphism, we will not assume that $p_{Z}$ implies statistically independent factors, but rather that it has independent discontinuities, which we define precisely in the next section.\n\n# 3.2. Independent discontinuities in the probability density \n\nHere, we contrast the statistical independence of factors with our approach, the independence of discontinuities. We will assume that there are discontinuities in the PDF of the factors, and that the location of these discontinuities in the density of any given factor is independent of the values of all the other factors.\n\nDefinition 1 Let $\\mathcal{S}$ be the support of $p_{Z}$. We say that $p_{Z}$ has an independent discontinuity at $Z_{i}=\\tau$ when every point in the intersection of the coordinate hyperplane $\\left\\{\\mathbf{z}_{i}=\\tau\\right\\}$ with $\\mathcal{S}$ is a non-removable discontinuity of $p_{Z}$. Formally, this independent discontinuity at $Z_{i}=\\tau$ is defined as the set $\\Gamma_{\\mathcal{S}}(i, \\tau)=\\left\\{\\mathbf{z} \\in \\mathcal{S} \\mid \\mathbf{z}_{i}=\\tau\\right\\}$ under the condition that $\\forall \\mathbf{z} \\in \\Gamma_{\\mathcal{S}}(i, \\tau), p_{Z}$ has a non-removable discontinuity at $\\mathbf{z}$.\n\nSuch discontinuities are \"independent\" in the sense that we have a discontinuity at $Z_{i}=\\tau$ regardless of the values taken by the other factors. Only the locations of the discontinuities in the density need to be independent of the other factors. This does not impose statistical independence of the factors, nor anything else wherever the density is continuous. Thus, assuming the presence of independent discontinuities can accommodate statistically independent factors as well as correlated factors.\n\nGeometrically, an independent discontinuity in $p_{Z}$ corresponds to a coordinate hyperplane restricted to the support of $p_{Z}$. This hyperplane is orthogonal to the $Z_{i}$ axis and parallel to all the other axes. We will, thus, interchangeably call it an independent discontinuity or axis-aligned discontinuity.\n\nFor our theorems, we will further require that the interior of the support of the density is connected. A connected independent discontinuity that splits this support in two is said to be an axis-separator (formally defined in Section 5.2). If the set of all non-removable discontinuities of $p_{Z}$ is the union of a finite set of such axis-separators, with at least one along each axis, then we say that they form an axis-aligned grid. Figure 1 (left) gives an example of two factors that are clearly not statistically independent but that have independent discontinuities in their PDF, appearing to the eye as axis-aligned discontinuities along each axis. Altogether, they from an axis-aligned grid.\n\nIndependent discontinuities are striking landmarks in the PDF landscape $p_{Z}$ that remain detectable in $p_{X}$ and $p_{Z^{\\prime}}$. A diffeomorphic map, even though it can warp the space in almost arbitrary ways, will not be able to erase such discontinuities. These are the robust cues that we can rely on to achieve quantized factor disentanglement under a diffeomorphism."
    },
    {
      "markdown": "# 4. Overview of the main quantized identifiability result \n\nIn a nutshell:\n\n## Assumptions\n\n- $f$ is a diffeomorphism\n- $\\left(Z_{1}, \\ldots, Z_{d}\\right) \\sim p_{Z}$ are $d$ continuous random variables.\n- The interior of the support of $p_{Z}$ is a connected set.\n- The set of non-removable discontinuities of $p_{Z}$ is the union of a finite set of independent discontinuities - at least one along each dimension - that together form an axis-aligned grid. This grid must also possess a backbone (precisely defined in the next section).\n\nQuantized factor identifiability theorem Under the above assumptions:\n\n- It suffices to learn a diffeomorphism $g$ yielding $Z^{\\prime}=g(X)$ such that the PDF of $p_{Z^{\\prime}}$ has independent discontinuities forming an axis-aligned grid.\n- Then, the quantized reconstructed factors $\\left(q_{1}^{\\prime}\\left(Z_{1}^{\\prime}\\right), \\ldots, q_{d}^{\\prime}\\left(Z_{d}^{\\prime}\\right)\\right)$ will correspond one-toone to the quantized ground-truth factors $\\left(q_{1}\\left(Z_{1}\\right), \\ldots, q_{d}\\left(Z_{d}\\right)\\right)$, up to possible permutation of indices (and order reversal).\n- The quantization thresholds used for $q_{i}$ and $q_{i}^{\\prime}$ are obtained as the locations of the independent discontinuities.\n\nThis result is illustrated in Figure 1. The formal Quantized factor identifiability theorem is Theorem 17, found in section 5.3 together with its proof. It builds on two other theorems: the Non-removable discontinuity preservation theorem (Theorem 4 in Section 5.1) and the Grid structure recovery theorem (Theorem 15 in Section 5.2) and its corollary. The following section presents these theorems and the required definitions in their logical order.\n\n## 5. Main theorems\n\nMost of the theory will concern the diffeomorphism $h:=g \\circ f$ that maps $Z$ to $Z^{\\prime}$.\n\n### 5.1. Non-removable discontinuity preservation theorem\n\nWe will show that discontinuities in the PDF are preserved by a a diffeomorphism. However, one subtlety is that the PDF corresponding to a given distribution is not unique, as elaborated in Appendix G. The many PDFs representing the same distribution actually form an equivalence class, whose elements may take arbitrarily different values on sets of points of measure zero. So not all the discontinuities in a PDF are meaningful. Since we care about observable characteristics of the actual distribution, we must focus on aspects of the PDF that are immune to erasure by changes of measure zero. We use the following definitions:\n\nDefinition 2 Removable discontinuity: A PDF $p$ has a removable discontinuity at $z$ if $p$ is discontinuous at $z$ but there exists another $p^{\\prime}$ in the same equivalence class (i.e. $p$ and $p^{\\prime}$ yield the exact same probability measure) that is continuous at $z$."
    },
    {
      "markdown": "Definition 3 Non-removable discontinuity: A PDF $p$ has a non-removable discontinuity at $z$ if $p$ is discontinuous at $z$ but this discontinuity is not removable. Equivalently, all PDFs in the equivalence class of $p$ are discontinuous at $z$. Note that a non-removable discontinuitiy is a property of an equivalence class of PDFs, thus of the distribution, not just of a single PDF.\n\nTheorem 4 Non-removable discontinuity preservation theorem. Let $Z$ be a latent random variable with values in $\\mathcal{Z} \\subset \\mathbb{R}^{d}$, whose distribution is represented by a PDF $p_{Z}$. Let $h: \\mathcal{Z} \\rightarrow \\mathcal{Z}^{\\prime} \\subset \\mathbb{R}^{d}$ be a diffeomorphism, and let $Z^{\\prime}=h(Z)$ be a transformed random variable whose distribution is represented by a probability density function $p_{Z^{\\prime}}$. Then, $p_{Z^{\\prime}}$ has a non-removable discontinuity at a point $z^{\\prime}$ if and only if $p_{Z}$ has a non-removable discontinuity at the point $z=h^{-1}\\left(z^{\\prime}\\right)$.\n\nProof: Appendix E.\n\n# 5.2. Grid structure recovery theorem and corollary \n\n### 5.2.1. DEFINITION OF GRID STRUCTURE\n\nThe notions we use to define the grid structure are related to usual hyperplanes and hypersurfaces of $\\mathbb{R}^{d}$, but they are restricted to a connected subset $\\mathcal{S}$ of $\\mathbb{R}^{d}$. In our setting, $\\mathcal{S}$ will be the interior of the support of the density on which the grids can be defined. Let $\\mathcal{S} \\subset \\mathbb{R}^{d}$ be a connected open smooth submanifold of dimension $d$ ( $\\mathcal{S}$ such as an open $d$-ball). We will use the following definitions, which are illustrated in Appendix D.\nDefinition 5 The splitting of a set $\\mathcal{S}$ by another set $\\mathcal{C}$, denoted $\\operatorname{split}(\\mathcal{S}, \\mathcal{C})$, is the set of connected components of $\\mathcal{S} \\backslash \\mathcal{C}$.\n\nDefinition 6 We say that $\\mathcal{C}$ splits $\\mathcal{S}$ in two to mean $|\\operatorname{split}(\\mathcal{S}, \\mathcal{C})|=2$ (we denote the cardinality of a countable set $A$ by $|A|$, and similarly for the number of elements in an ordered list or a tuple).\n\nDefinition 7 We say that $\\mathcal{C}$ is a separator of $\\mathcal{S}$ if $\\mathcal{C}$ is a connected subset of $\\mathcal{S}$ and $\\mathcal{C}$ splits $\\mathcal{S}$ in two. The two connected components that result from the split are called the two halves resulting form the split, denoted $\\mathcal{C}^{+}$and $\\mathcal{C}^{-}$, i.e. $\\left\\{\\mathcal{C}^{+}, \\mathcal{C}^{-}\\right\\}=\\operatorname{split}(\\mathcal{S}, \\mathcal{C})$.\n\nDefinition $8 \\mathcal{C}$ is a smooth separator of $\\mathcal{S}$ if $\\mathcal{C}$ is a separator of $\\mathcal{S}$ and is a smooth hypersurface of $\\mathcal{S}$ (i.e. a smooth embedded submanifold of dimension $d-1$ ).\n\nDefinition 9 An axis-separator of $\\mathcal{S}$ is a special case of smooth separator of $\\mathcal{S}$ that is the intersection of $\\mathcal{S}$ with an axis-aligned hyperplane of $\\mathbb{R}^{d}$ (a coordinate hyperplane). It can be defined as $\\mathcal{H}=$ $\\Gamma_{\\mathcal{S}}(i, \\tau)=\\left\\{z \\in \\mathcal{S} \\mid z_{i}=\\tau\\right\\}$ (Figure 7). Because it is a separator, it splits $\\mathcal{S}$ in two halves $\\Gamma_{\\mathcal{S}}^{+}(i, \\tau)=\\left\\{z \\in \\mathcal{S} \\mid z_{i}>\\tau\\right\\}$ and $\\Gamma_{\\mathcal{S}}^{-}(i, \\tau)=\\left\\{z \\in \\mathcal{S} \\mid z_{i}<\\tau\\right\\}$, which are each nonempty and connected (Figure 8).\n\nDefinition 10 An axis-separator-set $\\mathcal{G}$ on $\\mathcal{S}$ is a finite set of axis separators of $\\mathcal{S}$.\nDefinition 11 An axis-aligned grid $G \\subset \\mathcal{S}$ is a subset of $\\mathcal{S}$ that can be obtained as a union of all the separators in an axis-separator-set $\\mathcal{G}$. i.e. $G=\\cup \\mathcal{G}=\\cup_{H \\in \\mathcal{G}} H$.\nNote the important distinction we make between a grid, which is a subset of $\\mathcal{S}$ and hence a set of points, and an axis-separator-set, which is a set of axis separators (which themselves are sets of"
    },
    {
      "markdown": "points). An axis-separator-set thus has more explicit structure than a grid. The proof we will unroll depends conceptually on the ability to rebuild, in several steps, the entire grid internal structure, starting from only the unstructured grid as a set of points. The first step of this program will be the recoverability of axis-separator-set from grid.\n\nDefinition 12 A parallel-separator-set is a set of axis-separators all defined on the same $i^{\\text {th }}$ axis (and are thus parallel). In particular, we denote the subset of axis-separator set $\\mathcal{G}$ that are all defined on the $i^{\\text {th }}$ axis as $\\mathcal{G}^{(i)}$.\n\nDefinition 13 A discrete coordination $\\mathbf{A}$ is a tuple $\\mathbf{A}=\\left(\\mathbf{A}_{1}, \\ldots, \\mathbf{A}_{d}\\right)$ where each $\\mathbf{A}_{i}$ is itself a tuple of real numbers in increasing order $\\mathbf{A}_{i}=\\left(\\mathbf{A}_{i, 1}, \\ldots, \\mathbf{A}_{i, n_{i}}\\right)$ such that $\\mathbf{A}_{i, k+1}>\\mathbf{A}_{i, k}$. These represent the coordinates of axis-separators along each of the $d$ coordinate axes (Figure 10).\nNote: $\\mathbf{A}_{i}$ contains the list of quantization thresholds to quantize the $i^{\\text {th }}$ coordinate (or factor) as $Q\\left(Z_{i} ; \\mathbf{A}_{i}\\right)$, as defined in equation 1.\nA discrete coordination defines the entire grid structure. One can easily obtain the various constituent sets from it:\n(a) the individual separators ( $\\approx$ \"hyperplanes\") $\\Gamma_{\\mathcal{S}}\\left(i, \\mathbf{A}_{i, k}\\right)$, and their positive and negative halves ( $\\approx$ \"half-spaces\") $\\Gamma_{\\mathcal{S}}^{+}\\left(i, \\mathbf{A}_{i, k}\\right)$ and $\\Gamma_{\\mathcal{S}}^{-}\\left(i, \\mathbf{A}_{i, k}\\right)$ respectively;\n(b) the parallel-separator-sets $\\mathcal{G}^{(1)}, \\ldots, \\mathcal{G}^{(d)}$, where $\\mathcal{G}^{(i)}=\\left\\{\\Gamma_{\\mathcal{S}}\\left(i, \\mathbf{A}_{i, k}\\right)\\right\\}_{k=1}^{|\\mathbf{A}_{i}|}$;\n(c) the axis-separator-set $\\mathcal{G}=\\mathcal{G}^{(1)} \\cup \\ldots \\cup \\mathcal{G}^{(d)}$;\n(d) the grid $G=\\operatorname{grid}_{\\mathcal{S}}(\\mathbf{A})=\\cup \\mathcal{G}$.\n\nDefinition 14 A backbone $\\mathcal{H}^{*}$ of a grid is a list $\\mathcal{H}^{*}=\\left(\\mathcal{H}_{1}^{*}, \\ldots, \\mathcal{H}_{d}^{*}\\right)$ of $d$ separators of that grid, each defined on the corresponding axis, that have a non-empty intersection (they meet at a single point $z^{*}$ ). That is, for $\\mathcal{H}_{1}^{*} \\in \\mathcal{G}^{(1)}, \\ldots, \\mathcal{H}_{d}^{*} \\in \\mathcal{G}^{(d)}$, we must have $\\bigcap_{i=1}^{d} \\mathcal{H}_{i}^{*}=\\left\\{z^{*}\\right\\}$. In addition, for $\\mathcal{H}^{*}$ to be a backbone, it is also required that each of its separators $\\mathcal{H}_{i}^{*}$ intersect all the other separators $H \\in \\mathcal{G}^{(j)}$ of the grid that are defined on the other axes $j \\neq i$ (those not in the same parallel-separator-set); namely, $\\forall i, \\forall j \\neq i, \\forall H \\in \\mathcal{G}^{(j)}, \\mathcal{H}_{i}^{*} \\cap H \\neq \\emptyset$ (example in Figure 11).\nA backbone functions as a set of \"main axes\", and we will require a proper grid to have at least one backbone. This is a weaker requirement than requiring a \"complete grid\" where each separator of the grid would be required to intersect all the separators that are not in the same parallel-separator-set. Here, we require only that the separators of the backbone intersect all the other separators on the other axes of the grid.\n\n# 5.2.2. GRID STRUCTURE PRESERVATION AND RECOVERY THEOREM \n\nTheorem 15 Grid structure preservation and recovery theorem. Let $h: \\mathcal{S} \\subset \\mathbb{R}^{d} \\rightarrow \\mathcal{S}^{\\prime} \\subset \\mathbb{R}^{d}$ be a diffeomorphism, where both $\\mathcal{S}$ and $\\mathcal{S}^{\\prime}$ are open connected subsets of $\\mathbb{R}^{d}$. Suppose we have an axis-aligned grid $G \\subset \\mathcal{S}$, associated with its axis-separator-set $\\mathcal{G}$ and discrete coordination $\\mathbf{A}$, that is, $G=\\operatorname{grid}_{\\mathcal{S}}(\\mathbf{A})$. While the grid does not need to be \"complete\", we suppose that $\\mathcal{G}$ has at least one backbone. Now, suppose that we have another axis-aligned grid in $\\mathcal{S}^{\\prime}$, associated with its discrete coordination $\\mathbf{B}$, with $G^{\\prime}=\\operatorname{grid}_{\\mathcal{S}^{\\prime}}(\\mathbf{B})$. Suppose $G^{\\prime}=h(G)$. Then, there exists a permutation function $\\sigma$ over dimension indexes $1, \\ldots, d$ and a direction reversal vector $s \\in\\{-1,+1\\}^{d}$ such that $\\forall j \\in\\{1, \\ldots, d\\}, i=\\sigma^{-1}(j), K=\\left|\\mathbf{A}_{i}\\right|=\\left|\\mathbf{B}_{j}\\right|, \\forall k \\in\\{1, \\ldots, K\\}, \\forall z^{\\prime} \\in \\mathcal{S}^{\\prime}$,"
    },
    {
      "markdown": "If $s_{i}=+1$, then:\n\n$$\n\\left\\{\\begin{array}{l}\nz_{j}^{\\prime}=\\mathbf{B}_{j, k} \\Longleftrightarrow h^{-1}\\left(z^{\\prime}\\right)_{i}=\\mathbf{A}_{i, k} \\\\\nz_{j}^{\\prime}>\\mathbf{B}_{j, k} \\Longleftrightarrow h^{-1}\\left(z^{\\prime}\\right)_{i}>\\mathbf{A}_{i, k} \\\\\nz_{j}^{\\prime}<\\mathbf{B}_{j, k} \\Longleftrightarrow h^{-1}\\left(z^{\\prime}\\right)_{i}<\\mathbf{A}_{i, k}\n\\end{array}\\right.\n$$\n\nIf $s_{i}=-1$, then:\n\n$$\n\\left\\{\\begin{array}{l}\nz_{j}^{\\prime}=\\mathbf{B}_{j, k} \\Longleftrightarrow h^{-1}\\left(z^{\\prime}\\right)_{i}=\\mathbf{A}_{i, K-k+1} \\\\\nz_{j}^{\\prime}>\\mathbf{B}_{j, k} \\Longleftrightarrow h^{-1}\\left(z^{\\prime}\\right)_{i}<\\mathbf{A}_{i, K-k+1} \\\\\nz_{j}^{\\prime}<\\mathbf{B}_{j, k} \\Longleftrightarrow h^{-1}\\left(z^{\\prime}\\right)_{i}>\\mathbf{A}_{i, K-k+1}\n\\end{array}\\right.\n$$\n\nPrinciple of the proof Starting from the premise $G^{\\prime}=h(G)$, we know that $h$ maps every point of $G$ to a point of $G^{\\prime}$. The proof recovers the entire underlying grid structure in 3 steps:\nStep 1 recover a one-to-one mapping of the individual separators: $\\mathcal{G}^{\\prime}=h(\\mathcal{G})$.\nStep 2 recover the partition into subsets of parallel separators (each subset associated to an axis): $\\mathcal{G}^{\\prime(j)}=h\\left(\\mathcal{G}^{(i)}\\right)$ (with permutation $j=\\sigma(i)$ ).\nStep 3 show that the ordering of the separators in a parallel-separators-set is preserved (up to possible order reversal):\n$\\left[h\\left(\\Gamma_{\\mathcal{S}}\\left(i, \\mathbf{A}_{i, 1}\\right)\\right), \\ldots, h\\left(\\Gamma_{\\mathcal{S}}\\left(i, \\mathbf{A}_{i, K}\\right)\\right)\\right]=\\left[\\Gamma_{\\mathcal{S}^{\\prime}}\\left(j, \\mathbf{B}_{j, 1}\\right), \\ldots, \\Gamma_{\\mathcal{S}^{\\prime}}\\left(j, \\mathbf{B}_{j, K}\\right)\\right]$ or in reversed order $\\left[h\\left(\\Gamma_{\\mathcal{S}}\\left(i, \\mathbf{A}_{i, 1}\\right)\\right), \\ldots, h\\left(\\Gamma_{\\mathcal{S}}\\left(i, \\mathbf{A}_{i, K}\\right)\\right)\\right]=\\left[\\Gamma_{\\mathcal{S}^{\\prime}}\\left(j, \\mathbf{A}_{j, K}\\right), \\ldots, \\Gamma_{\\mathcal{S}^{\\prime}}\\left(j, \\mathbf{A}_{j, 1}\\right)\\right]$. And similarly, the ordering of the halves corresponding to each of these separators is preserved. Knowing to which half (either $\\Gamma_{\\mathcal{S}^{\\prime}}^{+}(j, \\tau)$ or $\\Gamma_{\\mathcal{S}^{\\prime}}^{-}(j, \\tau)$ ) a point $\\mathbf{z}^{\\prime}$ belongs tells us whether $z_{j}^{\\prime}$ is above or below the threshold $\\tau$.\nFor example, seeing that $z_{j}^{\\prime}>\\mathbf{B}_{j, k}$ tells us that $z^{\\prime} \\in \\Gamma_{\\mathcal{S}^{\\prime}}^{+}\\left(j, \\mathbf{B}_{j, k}\\right)$, which implies from step 3 (in the case of no order reversal) that its preimage $z=h^{-1}\\left(z^{\\prime}\\right)$ belongs to $\\Gamma_{\\mathcal{S}}^{+}\\left(i, \\mathbf{A}_{i, k}\\right)$, which yields $z_{i}>\\mathbf{A}_{i, k}$. This is what Theorem 15 expresses. We refer the reader to Appendix F for the full proof.\n\nCorollary 16 Recovery of quantized factors. Under the same premises as Theorem 15, consider random variables $Z$ and $Z^{\\prime}=h(Z)$. Using the quantization operation $Q$ (previously defined in Section 2.3, equation 1), we recover quantized factors up to permutation $\\sigma$ of the axes and possible direction reversal indicated by $s: \\forall i \\in 1, \\ldots, d, Q\\left(Z_{i} ; \\mathbf{A}_{i}\\right)=Q^{s_{i}}\\left(Z_{j}^{\\prime} ; \\mathbf{B}_{j}\\right)$ with $j=\\sigma(i)$.\n\nProof $Z^{\\prime}=h(Z)$ implies that $Z_{i}=h^{-1}\\left(Z^{\\prime}\\right)_{i}$.\nNow if $s_{i}=+1$, Theorem 15 yields $Z_{j}^{\\prime} \\geq \\mathbf{B}_{j, k} \\Longleftrightarrow Z_{i} \\geq \\mathbf{A}_{i, k}$.\nThus, $Q\\left(Z_{i}, \\mathbf{A}_{i}\\right)=\\sum_{k} \\mathbf{1}_{Z_{i} \\geq \\mathbf{A}_{i, k}}=\\sum_{k} \\mathbf{1}_{Z_{i}^{\\prime} \\geq \\mathbf{B}_{j, k}}=Q\\left(Z_{j}^{\\prime} ; \\mathbf{B}_{j}\\right)$.\nSimilarly, if $s_{i}=-1$, Theorem 15 yields $Z_{j}^{\\prime} \\leq \\mathbf{B}_{j, k} \\Longleftrightarrow Z_{i} \\geq \\mathbf{A}_{i, k}$.\nThus, $Q\\left(Z_{i}, \\mathbf{A}_{i}\\right)=\\sum_{k} \\mathbf{1}_{Z_{i} \\geq \\mathbf{A}_{i, k}}=\\sum_{k} \\mathbf{1}_{Z_{i}^{\\prime} \\leq \\mathbf{B}_{j, k}}=Q^{-}\\left(Z_{j}^{\\prime} ; \\mathbf{B}_{j}\\right)$.\nSo in both cases, we have $Q\\left(Z_{i} ; \\mathbf{A}_{i}\\right)=Q^{s_{i}}\\left(Z_{j}^{\\prime} ; \\mathbf{B}_{j}\\right)$.\n\n# 5.3. Quantized factor identifiability theorem \n\nTheorem 17 Quantized factors identifiability theorem. Let $Z$ be a latent random variable with values in $\\mathcal{Z} \\subset \\mathbb{R}^{d}$ and whose PDF is $p_{\\mathcal{Z}}$. Let $f: \\mathcal{Z} \\rightarrow \\mathcal{X} \\subset \\mathbb{R}^{D}$ be a diffeomorphism, and $X=f(Z)$ be the observed random variable. Assume that the support of the PDF $p_{Z}$ is an open connected set ${ }^{2}$.\n\n[^0]\n[^0]:    2. Alternatively, if the support is not open, we can consider its interior."
    },
    {
      "markdown": "Further assume that $p_{Z}$ has at least one connected independent discontinuity in each dimension, such that the set of non-removable discontinuities of $p_{Z}$ forms an axis-aligned grid with a backbone. Let $\\mathbf{A}$ be the discrete coordination of this grid. Then, there exists a diffeomorphism $g: \\mathcal{X} \\rightarrow \\mathcal{Z}^{\\prime}$ yielding a variable $Z^{\\prime}=g(X)$ such that the set of non-removable discontinuities of the PDF $p_{Z^{\\prime}}$ is an axis-aligned grid. Consider any such diffeomorphism $g$, and let $\\mathbf{B}$ be the discrete coordination of its resulting axis-aligned grid. Then, there exists a permutation function $\\sigma$ over the dimension indexes $1, \\ldots, d$, and a direction reversal vector $s \\in\\{-1,+1\\}^{d}$ such that $q_{j}^{\\prime}\\left(Z_{j}^{\\prime}\\right)=q_{i}\\left(Z_{i}\\right)$ with $i=\\sigma^{-1}(j)$, where $q_{j}^{\\prime}\\left(Z_{j}^{\\prime}\\right)=Q^{s_{i}}\\left(Z_{j}^{\\prime} ; \\mathbf{B}_{j}\\right)$ and $q_{i}\\left(Z_{i}\\right)=Q\\left(Z_{i} ; \\mathbf{A}_{i}\\right)$. In other words, the quantized factors in $Z^{\\prime}$ agree with the quantized factors in $Z$, up to permutation and possible axis reversal.\n\nProof Note that existence is trivial (it suffices to take $g=f^{-1}$, which yields $Z^{\\prime}=Z$ ). But the fact that any $g$ that yields a PDF whose non-removable discontinuities form an axis-aligned grid will have this property can now easily be proven from our previous results. It suffices to consider $h=g \\circ f$ to be a diffeomorphism (the composition of two diffeomorphisms), so that $Z^{\\prime}=h(Z)$, and to combine the non-removable discontinuity preservation theorem (Thm. 4) with the grid structure preservation and recovery theorem (Thm. 15). Let $G=\\operatorname{grid}_{\\mathcal{S}}(\\mathbf{A})$ and $G^{\\prime}=\\operatorname{grid}_{\\mathcal{S}}(\\mathbf{B})$ be the set of nonremovable discontinuity points of $p_{Z}$ and $p_{Z^{\\prime}}$, respectively. From the non-removable discontinuity preservation theorem, we have that $G^{\\prime}=h(G)$. And from the grid structure preservation and recovery theorem and its corollary, we have that $G^{\\prime}=h(G)$ implies that there exists a permutation function $\\sigma$ over dimension indexes $1, \\ldots, d$ and a direction reversal vector $s \\in\\{-1,+1\\}^{d}$ such that $Q^{s_{i}}\\left(Z_{j}^{\\prime} ; \\mathbf{B}_{j}\\right)=Q\\left(Z_{i} ; \\mathbf{A}_{i}\\right)$ with $i=\\sigma^{-1}(j)$. We have, thus, proved that the quantized factors of $Z^{\\prime}$ agree with the quantized factors of $Z$, up to permutation and axis reversal.\n\n# 6. Independent discontinuities in real-world disentangled factors \n\nWe have motivated independent discontinuities as a theoretical requirement to be able to identify quantized factors even after they passed through highly flexible diffeomorphic maps. In real data under finite samples, we can only hope for a smooothed density estimate that will never show true discontinuities, but merely sharp changes (gradients of large magnitude) in the density. Also if one is willing to assume a slightly less flexible map, such as Lipschitz, the requirement for true discontinuities may likely be relaxed to merely sharp changes.\n\nStill, one may wonder why and how such sharp density changes could appear in latent factors of real-world data. Here is a simple example: due to gravity, people and most objects tend\n![img-2.jpeg](img-2.jpeg)\n\nFigure 2: Grid structure observed in the PDF of the NASA exoplanet dataset (standardized log of factors).\nto be either in a standing or lying position. One will seldom see them with a $45^{\\circ}$ pitch angle irrespective of how other factors appear (e.g. background color). This results in a sharp change (discontinuity) in the PDF of the pitch angle factor, independent of the values of the other factors."
    },
    {
      "markdown": "This is an example of a density jump due to a physical equilibrium point, of which we can expect many variants in nature.\n\nEmpirically, we found evidence of independent sharp density changes forming a grid structure in descriptive factors of the NASA Exoplanet Archive (Akeson et al., 2013). Figure 2 shows the magnitude of the gradient of the density of the factors stellar magnitude and planet radius. Locations of high magnitude gradient show an axis-aligned grid, compatible with independent jumps in the density similar to the synthetic data from Figure 1. We provide another evidence of axis-alignment in real motion-capture data in Appendix C.\n\n# 7. Experiments \n\n![img-3.jpeg](img-3.jpeg)\n\nFigure 3: The true latent factors (a) do not have factorized support and are correlated. The observations (b) are the result of a linear map applied to the factors. Our method (c) obtains a factorized representation corresponding to the ground-truth factors. Both linear ICA (d) and Hausdorff Factorized Support (e) Roth et al. (2023) fail to learn the axisaligned true latent factors.\n\nWe develop a criterion for learning axis-aligned discontinuities and present a proof-of-concept experiment for the case where the mixing map $f$ is linear. This is a simple prototype to demonstrate the feasibility of quantized identification based on independent discontinuities in the PDF, and how it can be advantageous compared to other methods already in the linear case. We present a tentative criterion for nonlinear transformations in Appendix B.2, but it remains to be thoroughly tested and experimented. We reserve the proposal and analysis of a full practical criterion for future work."
    },
    {
      "markdown": "The method we present here aims to align the gradients of the joint density of the factors with the axes. We perform a density estimation $\\hat{p}_{\\sigma}$ of $Z$ and use it to obtain the gradients $\\frac{\\partial \\log \\hat{p}_{\\sigma}}{\\partial z}$. Gradients of high magnitude hint at potential discontinuities. We encourage the alignment of these gradient vectors with the standard basis vectors (axes) by maximizing their cosine similarity. The algorithm and experimental setup is detailed in Appendix B.1.\n\nFigure 3 presents the visualization of the reconstruction of the latent variables for our model, compared to Linear ICA (using the FastICA algorithm (Hyvärinen, 1999)) and to Hausdorff Factorized Support (HFS) (Roth et al., 2023), for the case where the ground-truth latent factors are neither independent nor have a factorized support. The results show that the true latent structure is well-reconstructed by our model. The learned factor grid is axis-aligned and corresponds to the original grid up to permutation and axis reversal, as anticipated by the theory. The quantized cells are correctly identified up to this indeterminacy. Meanwhile, both FastICA and Hausdorff Factorized Support learn the factors up to a rotation and shearing (besides permutation and scaling), because their reconstruction is not axis-aligned. Appendix B. 1 presents the results in the case where the support is factorized and we remark that our model is again able to axis-align the factors, while even HFS' reconstruction does not present factorized support ${ }^{3}$.\n\n# 8. Conclusion and future work \n\nIn this theoretical work, we have introduced the novel paradigm of quantized factor identifiability. We have then shown that fully unsupervised identifiability of quantized factors is possible under diffeomorphisms. This is significant given that the prevailing literature is dominated by impossibility results. We are able to achieve the identification of quantized factors, provided that we assume independent discontinuities in the latent factor's distribution (which naturally form a grid). The novel relaxed (weaker) form of identifiability is meant as a step towards more realistic assumptions for disentanglement: no restrictive inductive bias on the mapping and no assumed independence of factors, rather aiming for potential causal footprints (Lopez-Paz et al., 2017).\n\nHowever, there are important limitations to this theory, the most obvious being that it requires actual discontinuities. This is required due to the flexibility of general diffeomorphisms, as justified in Appendix H. Future work shall try to relax this to just sharp (but not infinitely sharp) changes in the density, under slightly less general Lipschitz smooth mappings.\n\nWhen moving to the finite sample setting, we must resort to density estimation, which yields a smoothed estimate of $p_{X}$, and as a result, discontinuities will become non-infinite sharp changes. These \"softer\" discontinuities can still be detected by considering the magnitude of the gradient of the density (as we display in Figure 2). The development of an effective practical training criterion and algorithm to train a nonlinear reverse mapping $g$ to recover an axis-aligned grid is left for future work (Appendix B. 2 proposes a possible starting direction).\n\n[^0]\n[^0]:    3. The code for reproducing these results is available at https://github.com/facebookresearch/quantized_identifiability/."
    },
    {
      "markdown": "# Acknowledgments \n\nThe authors thank Léon Bottou for sharing his original motivation for discontinuities in the density from a causal perspective, as well as David Lopez-Paz for related discussions on causal footprints. The present work only barely mentions these motivations due to its identifiability focus, but we are grateful for Léon's and David's encouragement in exploring this direction. The authors thank Diane Bouchacourt for discussions on Theorem 1 and the experimental validation of the theory on real-world datasets, as well as Mark Ibrahim for his contribution to early discussions while this project was taking shape. We also thank Sébastien Lachapelle for feedback on this project, and Mohammad Pezeshki for feedback on the paper.\n\nVitória Barin-Pacela is partially supported by the Canada CIFAR AI Chair Program, as well as a grant from Samsung Electronics Co., Ldt., administered by Mila, in support of her PhD studies at the University of Montreal. Simon Lacoste-Julien and Pascal Vincent are CIFAR Associate Fellows in the Learning in Machines \\& Brains program.\n\nThis research has made use of the NASA Exoplanet Archive, which is operated by the California Institute of Technology, under contract with the National Aeronautics and Space Administration under the Exoplanet Exploration Program.\n\n## References\n\nKartik Ahuja, Jason S Hartford, and Yoshua Bengio. Weakly Supervised Representation Learning with Sparse Perturbations. In Advances in Neural Information Processing Systems, 2022a.\n\nKartik Ahuja, Divyat Mahajan, Vasilis Syrgkanis, and Ioannis Mitliagkas. Towards efficient representation identification in supervised learning. In 1st Conference on Causal Learning and Reasoning, 2022b.\n\nKartik Ahuja, Yixin Wang, Divyat Mahajan, and Yoshua Bengio. Interventional causal representation learning. In 40th International Conference on Machine Learning, 2022c.\nR. L. Akeson, X. Chen, D. Ciardi, M. Crane, J. Good, M. Harbut, E. Jackson, S. R. Kane, A. C. Laity, S. Leifer, M. Lynn, D. L. McElroy, M. Papin, P. Plavchan, S. V. Ramí rez, R. Rey, K. von Braun, M. Wittman, M. Abajian, B. Ali, C. Beichman, A. Beekley, G. B. Berriman, S. Berukoff, G. Bryden, B. Chan, S. Groom, C. Lau, A. N. Payne, M. Regelson, M. Saucedo, M. Schmitz, J. Stauffer, P. Wyatt, and A. Zhang. The NASA exoplanet archive: Data and tools for exoplanet research. Publications of the Astronomical Society of the Pacific, 125(930):989-999, 2013.\n\nYoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798-1828, 2013.\n\nJack Brady, Roland S. Zimmermann, Yash Sharma, Bernhard Schölkopf, and Wieland and von Kügelgen, Julius Brendel. Provably Learning Object-Centric Representations. In International Conference on Machine Learning, 2023.\n\nJohann Brehmer, Pim de Haan, Phillip Lippe, and Taco S Cohen. Weakly supervised causal representation learning. In Advances in Neural Information Processing Systems, 2022."
    },
    {
      "markdown": "Simon Buchholz, Michel Besserve, and Bernhard Schölkopf. Function Classes for Identifiable Nonlinear Independent Component Analysis. In Conference on Neural Information Processing Systems, 2022.\n\nMarek Capinski and Peter Ekkehard Kopp. Measure, Integral and Probability. Springer Undergraduate Mathematics Series. Springer London, 2013. ISBN 9781447106456.\n\nPierre Comon. Independent component analysis, a new concept? Signal processing, 36(3):287-314, 1994.\n\nAlexandra O. Constantinescu, Jill X. O’Reilly, and Timothy E. J. Behrens. Organizing conceptual knowledge in humans with a gridlike code. Science, 352(6292):1464-1468, 2016.\n\nAndrea Dittadi, Frederik Träuble, Francesco Locatello, Manuel Wuthrich, Vaibhav Agrawal, Ole Winther, Stefan Bauer, and Bernhard Schölkopf. On the Transfer of Disentangled Representations in Realistic Settings. In International Conference on Learning Representations, 2021.\n\nManfredo P. do Carmo. Differential Geometry of Curves and Surfaces. Prentice-Hall, 1976.\nDavid Friede, Christian Reimers, Heiner Stuckenschmidt, and Mathias Niepert. Learning disentangled discrete representations. arXiV preprint arXiv:2307.14151, 2023.\n\nLuigi Gresele, Julius Von Kügelgen, Vincent Stimper, Bernhard Schölkopf, and Michel Besserve. Independent mechanism analysis, a new concept? In Advances in Neural Information Processing Systems, 2021.\n\nHermanni Hälvä, Sylvain Le Corff, Luc Lehéricy, Jonathan So, Yongjie Zhu, Elisabeth Gassiat, and Aapo Hyvarinen. Disentangling Identifiable Features from Noisy Data with Structured Nonlinear ICA. In Advances in Neural Information Processing Systems, 2021.\n\nKyle Hsu, Will Dorrell, James C. R. Whittington, Jiajun Wu, and Chelsea Finn. Disentanglement via Latent Quantization. In Neural Information Processing Systems, 2023.\n\nAntti Hyttinen, Vitória Barin-Pacela, and Aapo Hyvärinen. Binary independent component analysis: a non-stationarity-based approach. In 38th Conference on Uncertainty in Artificial Intelligence, 2022.\n\nAapo Hyvärinen. Fast and robust fixed-point algorithms for independent component analysis. IEEE Transactions on Neural Networks, 10(3):626-634, 1999.\n\nAapo Hyvarinen and Hiroshi Morioka. Unsupervised feature extraction by time-contrastive learning and nonlinear ICA. In Advances in Neural Information Processing Systems, volume 29, 2016.\n\nAapo Hyvarinen and Hiroshi Morioka. Nonlinear ICA of temporally dependent stationary sources. In Artificial Intelligence and Statistics, 2017.\n\nAapo Hyvärinen and Petteri Pajunen. Nonlinear independent component analysis: Existence and uniqueness results. Neural networks, 12(3):429-439, 1999."
    },
    {
      "markdown": "Aapo Hyvärinen, Hiroaki Sasaki, and Richard E. Turner. Nonlinear ICA Using Auxiliary Variables and Generalized Contrastive Learning. In International Conference on Artificial Intelligence and Statistics, 2019.\n\nKazuki Irie, Róbert Csordás, and Jürgen Schmidhuber. Topological Neural Discrete Representation Learning à la Kohonen. In ICML 2023 Workshop: Sampling and Optimization in Discrete Space, 2023.\n\nIlyes Khemakhem, Diederik Kingma, Ricardo Monti, and Aapo Hyvarinen. Variational autoencoders and nonlinear ICA: A unifying framework. In International Conference on Artificial Intelligence and Statistics, 2020a.\n\nIlyes Khemakhem, Ricardo Monti, Diederik Kingma, and Aapo Hyvarinen. Ice-beem: Identifiable conditional energy-based deep models based on nonlinear ICA. Advances in Neural Information Processing Systems, 2020b.\n\nBohdan Kivva, Goutham Rajendran, Pradeep Kumar Ravikumar, and Bryon Aragam. Identifiability of deep generative models without auxiliary information. In Advances in Neural Information Processing Systems, 2022.\n\nDaniel A. Klain and Gian-Carlo Rota. Introduction to Geometric Probability. Cambridge University Press, 1997.\n\nDavid A. Klindt, Lukas Schott, Yash Sharma, Ivan Ustyuzhaninov, Wieland Brendel, Matthias Bethge, and Dylan Paiton. Towards nonlinear disentanglement in natural data with temporal sparse coding. In International Conference on Learning Representations, 2021.\n\nSébastien Lachapelle, Pau Rodriguez, Yash Sharma, Katie E Everett, Rémi Le Priol, Alexandre Lacoste, and Simon Lacoste-Julien. Disentanglement via mechanism sparsity regularization: A new principle for nonlinear ICA. In Conference on Causal Learning and Reasoning, 2022.\n\nSébastien Lachapelle, Divyat Mahajan, Ioannis Mitliagkas, and Simon Lacoste-Julien. Additive decoders for latent variables identification and cartesian-product extrapolation. In Conference on Neural Information Processing Systems, 2023.\n\nJohn M. Lee. Introduction to Smooth Manifolds, volume 218. Springer, second edition, 2012.\nLek-Heng Lim, Ken Sze-Wai Wong, and Ke Ye. The Grassmannian of affine subspaces. Foundations of Computational Mathematics, 21:537—574, 2021.\n\nPhillip Lippe, Sara Magliacane, Sindy Löwe, Yuki M Asano, Taco Cohen, and Stratis Gavves. CITRIS: Causal identifiability from temporal intervened sequences. In International Conference on Machine Learning, 2022.\n\nFrancesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard Schölkopf, and Olivier Bachem. Challenging common assumptions in the unsupervised learning of disentangled representations. In International Conference on Machine Learning, 2019.\n\nFrancesco Locatello, Ben Poole, Gunnar Rätsch, Bernhard Schölkopf, Olivier Bachem, and Michael Tschannen. Weakly-supervised disentanglement without compromises. In International Conference on Machine Learning, 2020."
    },
    {
      "markdown": "David Lopez-Paz, Robert Nishihara, Soumith Chintalah, Bernhard Schölkopf, and Léon Bottou. Discovering Causal Signals in Images. In Computer Vision and Pattern Recognition, 2017.\n\nJerrold E. Marsden and Michael J. Hoffman. Elementary Classical Analysis. W. H. Freeman, 1993.\nFabian Mentzer, David Minnen, Eirikur Agustsson, and Michael Tschannen. Finite Scalar Quantization: VQ-VAE Made Simple. In International Conference on Learning Representations, 2024.\n\nGemma E. Moran, Dhanya Sridhar, Yixin Wang, and David M. Blei. Identifiable Deep Generative Models via Sparse Decoding. Transactions on Machine Learning Research, 2022.\n\nKarsten Roth, Mark Ibrahim, Zeynep Akata, Pascal Vincent, and Diane Bouchacourt. Disentanglement of Correlated Factors via Hausdorff Factorized Support. In International Conference on Learning Representations, 2023.\nA. Taleb and C. Jutten. Source separation in post-nonlinear mixtures. IEEE Transactions on Signal Processing, 47(10):2807-2820, 1999.\n\nFrederik Träuble, Elliot Creager, Niki Kilbertus, Francesco Locatello, Andrea Dittadi, Anirudh Goyal, Bernhard Schölkopf, and Stefan Bauer. On disentangled representations learned from correlated data. In Proceedings of the 38th International Conference on Machine Learning, 2021.\n\nAaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. In Advances in Neural Information Processing Systems, 2017.\n\nYixin Wang and Michael I Jordan. Desiderata for representation learning: A causal perspective. arXiv preprint arXiv:2109.03795, 2021.\n\nJames CR Whittington, Timothy H Muller, Shirley Mark, Guifen Chen, Caswell Barry, Neil Burgess, and Timothy EJ Behrens. The tolman-eichenbaum machine: Unifying space and relational memory through generalisation in the hippocampal formation. Cell, 183(5), 2020.\n\nWeiran Yao, Guangyi Chen, and Kun Zhang. Learning latent causal dynamics. arXiv preprint arXiv:2202.04828, 2022.\n\nYujia Zheng and Kun Zhang. Generalizing Nonlinear ICA Beyond Structural Sparsity. In 37th Conference on Neural Information Processing Systems, 2023.\n\nYujia Zheng, Ignavier Ng, and Kun Zhang. On the Identifiability of Nonlinear ICA: Sparsity and Beyond. In 36th Conference on Neural Information Processing Systems, 2022."
    },
    {
      "markdown": "# APPENDIX \n\n## Appendix A. Related work\n\nWe categorize the existing literature on causal representation learning into the following two categories: i) the theory imposes assumptions on both the mixing map and the latent factors, leading to typically fully unsupervised models; ii) the theory imposes assumptions on the distribution of latent factors and not strictly on the mixing map, leading to models that mostly require weak supervision or auxiliary variables. None of these studies considered the recovery of quantized factors like we do in this work.\n\nIdentifiability of latent factors in the unsupervised i.i.d setting: In linear Independent Component Analysis (ICA), Comon (1994) established that under a linear and invertible mixing map and independent non-Gaussian latent factors, these latent factors ca be identified up to order and scale indeterminacies. Beyond the linear case, Taleb and Jutten (1999) analyze a post-nonlinear mapping, obtaining the same indeterminacies as in linear mixtures. Gresele et al. (2021) demonstrated that with independent latent factors and a mixing function that adheres to the independent mechanism assumption, some of the non-identifiability counterexamples highlighted in Hyvärinen and Pajunen (1999) can be avoided. Expanding on the role of mixing maps, Buchholz et al. (2022) scrutinized different classes of maps that restrict the Jacobian of the mixing maps. Their study specifically focused on conformal maps and orthogonal coordinate transformations. Kivva et al. (2022) proposes that when the mixing map is piece-wise linear and the latent distribution is a Gaussian mixture (with latent components conditionally independent given a discrete, unobserved confounder), the true latent factors can be identified up to scaling and permutation as well. Ahuja et al. (2022c) asserted that the true latent factors can be identified, barring permutation and scaling errors, when the mixing map is polynomial and latent factors satisfy the support independence assumption, as proposed in Wang and Jordan (2021); Roth et al. (2023). Brady et al. (2023); Lachapelle et al. (2023) obtain identifiability for additive decoders, while Moran et al. (2022); Zheng et al. (2022); Zheng and Zhang (2023) obtain identifiability by assuming a sparse structure on the Jacobian of the mixing function.\n\nIdentifiability of latent factors with weak supervision: Research in this category largely makes assumptions on the latent distribution but imposes few constraints on the mixing map. To compensate for this lack of restrictions, these studies necessitate additional information, typically in one of two forms: a) identification driven by auxiliary information (e.g., labels, time stamps), or b) identification driven by weak supervision (e.g., data augmentations) (Hyvarinen and Morioka, 2017; Hyvärinen et al., 2019; Hyvarinen and Morioka, 2016). A key example of auxiliary informationdriven identification is the work on identifiable variational autoencoders (Khemakhem et al., 2020a), which assumes the existence of an additionally observed variable such that the latent variables are conditionally independent given it, and the conditional probability density of the latent variable given this auxiliary variable comes from an exponential family. This work has been expanded upon in several subsequent studies (Khemakhem et al., 2020b; Lachapelle et al., 2022; Ahuja et al., 2022b; Hyttinen et al., 2022), which modify some of its assumptions. Locatello et al. (2020); Klindt et al. (2021) assumed access to paired data, which can emerge from data augmentation or natural video frames with sparse changes, resulting in supervision-driven identification. Several follow-up studies (Hälvä et al., 2021; Ahuja et al., 2022a; Brehmer et al., 2022; Yao et al., 2022; Lippe et al., 2022) have built upon this work, moving beyond the independence assumptions on the latent factors and incorporating general transition dynamics."
    },
    {
      "markdown": "# Appendix B. Practical criterion \n\n## B.1. Proof-of-concept experimentation for linear maps\n\n## Synthetic data generation:\n\nWe generate a grid of points by establishing a prior for each cell, such that the sum of the priors of all the cells equals 1 . We define a $4 \\times 4$ grid, the position of each separator being drawn uniformly inside the range of the grid. In order to generate correlated data, first we draw the prior probabilities from a standard Uniform distribution. Then, we redefine the prior probability of the cells in the diagonal to be higher than the probability of the other cells, followed by normalization. The dataset is composed of 50,000 samples from this distribution. In the dataset with unfactorized support (Figure 3 ), the correlation coefficient between the true factors of variation is of 0.61 .\n\n## Algorithm:\n\nThe steps for implementing the training criterion are:\n\n1. Randomly initialize a parametric mapping $g: \\mathcal{X} \\rightarrow \\mathcal{Z}$ to be learned.\n2. From the matrix of observed samples $\\mathbf{X}$ of size $n \\times D$, compute the matrix of estimated latent variables $\\mathbf{Z}=g(\\mathbf{X})$ of size $n \\times d$ - where $g$ is applied separately to each row ${ }^{4}$.\n3. Estimate the density of $\\mathbf{Z}$ using a kernel density estimator (Parzen window) $\\hat{p}_{\\sigma}$ and compute the gradient $\\mathbf{V}_{i, \\cdot}=\\frac{\\partial \\log \\hat{p}_{\\sigma}}{\\partial z}\\left(\\mathbf{Z}_{i, \\cdot}\\right)$ at every point of $\\mathbf{Z}$.\n4. Define importance weighting terms $\\alpha$ based on the gradient magnitudes $\\alpha_{i}=\\frac{\\left\\|V_{i,}\\right\\|}{\\sum_{j=1}^{n}\\left\\|V_{i^{\\prime},}\\right\\|}$. A large magnitude of the gradient indicates a sharp jump, hence, this weight indicates how close the sample is to a density jump, that is, how likely it belongs to an axis-separator of the grid.\nLet $\\overline{\\mathbf{V}}_{i}=\\frac{\\mathbf{V}_{i}}{\\left\\|\\mathbf{V}_{i}\\right\\|}$ be the normalized version of $\\mathbf{V}_{i}$. For the individual gradient vectors to be axis-aligned, the maximum cosine similarity with the canonical axis vectors should be maximized:\n\n$$\n\\operatorname{maximize} \\max _{j \\in\\{1, \\ldots, d\\}}\\left|\\operatorname{cosim}\\left(V_{i \\cdot}, \\mathbf{1}_{j}\\right)\\right|=\\max _{j} \\frac{\\left|\\mathbf{V}_{i j}\\right|}{\\left\\|\\mathbf{V}_{i \\cdot}\\right\\|_{2}}=\\frac{\\left\\|\\mathbf{V}_{i \\cdot}\\right\\|_{\\infty}}{\\left\\|\\mathbf{V}_{i}\\right\\|_{2}}=\\left\\|\\overline{\\mathbf{V}}_{i \\cdot}\\right\\|_{\\infty}\n$$\n\nThen, the final loss function to be optimized over all the points is:\n\n$$\n\\operatorname{minimize} \\ell_{\\text {grad-axis }}=-\\sum_{i=1}^{n} \\alpha_{i}\\left\\|\\overline{\\mathbf{V}}_{i,}\\right\\|_{\\infty}\n$$\n\n## Details of model and algorithm - Dataset with unfactorized support:\n\nWe minimize this loss using stochastic gradient descent with a learning rate of 0.1 , momentum of 0.9 , and a batch size of 5000 samples. Mini-batches are employed due to the high memory cost of loading the full dataset. The results displayed are for when the training loss stops decreasing.\n\nThe kernel density estimation employs a bandwidth of 0.1 . With finite samples, we use a density estimator $\\hat{p}_{Z^{\\prime}}$, since we do not have access to the exact $p_{Z^{\\prime}}$. We remark that any density estimation will result in some smoothing of the true distribution. So even if there were real discontinuities in the exact density, they will appear as smoothed discontinuities: the gradients of the density have large magnitude, not infinite magnitude.\n\n## Hausdorff Factorized Support training details:\n\nWe train HFS using the hausdorff hard distance approximation which is used throughout the experiments from Roth et al. (2023). In this simple linear case, we simply optimize to minimize\n\n[^0]\n[^0]:    4. Note that we dropped the apostrophe ' in $\\mathbf{Z}$ to lighten notation."
    },
    {
      "markdown": "![img-4.jpeg](img-4.jpeg)\n\nFigure 4: When the true latent factors (4(a)) are correlated, our method (4(e)) obtains a factorized representation corresponding to the ground-truth factors, as opposed to linear ICA (4(c)) and Hausdorff Factorized Support (4(d)) which reconstruct the factors up to a rotation."
    },
    {
      "markdown": "the Hausdorff distance between the learned factors and their counterpart with factorized support. We did not find an advantage in using the reconstruction term as the representation does not collapse into a single point. Training is done using stochastic gradient descent with a step size of 0.0001 and a batch size of 5000 samples.\n\nExperiment with factorized support: We conduct a similar study on a dataset in which the support of the true factors is factorized. In this dataset (Figure 4, the true factors have a correlation coefficient of 0.64 . In this experiment, we attempt to have a fair comparison with HFS and demonstrate that using (discontinuity) information from inside the support can help achieve better axis-alignment (and as a result, better factorized support) of the learned factors of variation.\n\nWe compare our model with linear ICA and show that our model is able to learn a factorized representation of the factors, while Fast ICA (Hyvärinen, 1999) fails due to the correlation of the factors violating the independence assumption, as illustrated in Figure 4. HFS also learns the reconstructed factors up to a rotation (but no shearing), even though its factorized support assumption is satisfied, showing that in this case our criterion is effective in aligning the factors with the axes.\n\n# B.2. Towards a criterion for nonlinear maps \n\nAlignment of discontinuities in the joint density For nonlinear maps, only encouraging the gradients to be axis aligned does not suffice because the distortions yield a curved latent space. It is also desirable to straighten this deformed grid, which can then be axis-aligned. Here, we outline a few terms that could encourage this behavior in the training dynamics. We can align both the point samples and their gradient vectors. Moreover, the alignment comes in two forms: local alignment in a neighborhood of points, and alignment to the axes.\n\n- Gradient local alignment term: encourage pairs of neighboring points of high gradient magnitude to have gradients aligned by maximizing their cosine similarity. We can make the criterion a weighted average of cosine similarities, with significant weights only if they are neighboring points and both have large gradient magnitudes):\n\n$$\n\\begin{aligned}\n\\beta_{i, i^{\\prime}} & =\\alpha_{i} \\alpha_{i^{\\prime}} \\exp \\left(-\\frac{1}{2 \\sigma_{2}^{2}}\\left\\|\\mathbf{Z}_{i}-\\mathbf{Z}_{i^{\\prime}}\\right\\|^{2}\\right) \\\\\n\\bar{\\beta}_{i, i^{\\prime}} & =\\frac{\\beta_{i, i^{\\prime}}}{\\sum_{i, i^{\\prime}} \\beta_{i, i^{\\prime}}} \\\\\n\\operatorname{maximize} & \\sum_{i, i^{\\prime}} \\bar{\\beta}_{i, i^{\\prime}} \\operatorname{cosim}\\left(\\mathbf{V}_{i}, \\mathbf{V}_{i^{\\prime}}\\right) \\\\\n\\text { i.e. } & \\operatorname{minimize} \\ell_{\\text {grad-local }}=-\\sum_{i, i^{\\prime}} \\bar{\\beta}_{i, i^{\\prime}}\\left\\langle\\tilde{\\mathbf{V}}_{i}, \\tilde{\\mathbf{V}}_{i^{\\prime}}\\right\\rangle\n\\end{aligned}\n$$\n\n- Points local axis alignment term: encourages neighboring points with large density gradient magnitude to lie on or close to the same axis separator. For this, it suffices that they share one of their coordinates. In other words, is suffices to minimize the minimum over coordinates of the squared difference:\n\n$$\n\\operatorname{minimize} \\ell_{\\text {points-local }}=\\sum_{i, i^{\\prime}} \\bar{\\beta}_{i, i^{\\prime}} \\min _{j}\\left(\\frac{\\mathbf{Z}_{i j}-\\mathbf{Z}_{i^{\\prime} j}}{\\left\\|\\mathbf{Z}_{i}-\\mathbf{Z}_{i^{\\prime}}\\right\\|}\\right)^{2}\n$$"
    },
    {
      "markdown": "- Points-gradient-orthogonality term: encourages the gradient vector to be orthogonal to the vectors joining neighboring points by penalizing their squared cosine similarity:\n\n$$\n\\operatorname{minimize} \\ell_{\\text {points-grad }}=\\sum_{i, i^{\\prime}} \\bar{\\beta}_{i, i^{\\prime}}\\left(\\left\\langle\\overline{\\mathbf{V}}_{i}, \\frac{\\mathbf{Z}_{i^{\\prime}}-\\mathbf{Z}_{i}}{\\left\\|\\mathbf{Z}_{i^{\\prime}}-\\mathbf{Z}_{i}\\right\\|}\\right\\rangle\\right)^{2}\n$$\n\nWe can, then, define a training criterion that is a weighted sum of these terms (with appropriate sign), possibly together with the minimization of a reconstruction error $\\ell_{\\text {rec }}$ (from a decoder network $\\hat{f}$ that tries to reconstruct $\\mathbf{X}$ from $\\mathbf{Z}$ ).\n\n$$\n\\ell_{\\mathrm{rec}}=\\frac{1}{n} \\sum_{i=1}^{n}\\left\\|\\hat{f}\\left(\\mathbf{Z}_{i}\\right)-\\mathbf{X}_{i}\\right\\|^{2}\n$$\n\nThe complete loss to minimize is, thus,\n\n$$\nL(\\theta)=\\lambda_{1} \\ell_{\\text {grad-local }}+\\lambda_{2} \\ell_{\\text {grad-axis }}+\\lambda_{3} \\ell_{\\text {points-local }}+\\lambda_{4} \\ell_{\\text {points-grad }}+\\lambda_{5} \\ell_{\\text {rec }}\n$$\n\nwhere $\\theta$ is the set of (network) parameters of both encoder $g$ and decoder $\\hat{f}$.\n\n# Appendix C. Additional evidence of axis-aligned discontinuities in real data \n\n![img-5.jpeg](img-5.jpeg)\n\nFigure 5: Evidence of axis-aligned discontinuities found in the CMU Motion Capture Dataset.\nFigure 5 presents additional evidence of axis-aligned discontinuities in real data from the CMU Motion Capture dataset (obtained from mocap.cs.cmu.edu). We preprocess the variables to obtain the angles of the joints of the body in different frames captured. In particular, the angle of the left elbow is the angle formed by the markers \"LELB\", \"LUPA\", and \"LWRA\". The variable angle(shoulders) measures the angle of the shoulders (defined by the markers \"RSHO\" and \"LSHO\") with respect to the vertical axis. Then these angles are standardized. In the plot, we can observe axis-aligned discontinuities in green, which represents a high magnitude of the gradient of the density.\n\n## Appendix D. Illustrations of the definitions\n\nFigures $7,8,10,9$, and 11 illustrate the concepts used in the definitions of section 5.2."
    },
    {
      "markdown": "![img-6.jpeg](img-6.jpeg)\n(a) Independent non-Gaussian factors\n![img-7.jpeg](img-7.jpeg)\n(b) Latent factors whose PDF has axis-aligned discontinuities\n![img-8.jpeg](img-8.jpeg)\n(c) Underlying grid structure\n\nFigure 6: Illustration of different kinds of assumptions on the distribution of latent factors. Left: samples from traditional assumption of independent non-Gaussian factors (here using a truncated Laplace distribution). Middle: samples from a distribution that follows our assumption of axis-aligned discontinuities in the probability density. Right: Underlying grid structure revealing discontinuities in the density landscape as colored axis-separators, forming a grid. Traditional independence assumption yields non-identifiability result under a general nonlinear smooth mapping (diffeomorphism). Our assumption yields, under a diffeomorphism, provable recovery of a discretized coordinate system. It allows to map back observed points into the proper latent grid cell - a novel relaxed form of identifiability, which we term quantized factor identifiability.\n![img-9.jpeg](img-9.jpeg)\n\nFigure 7: Axis separators $\\mathcal{H}_{1}, \\ldots, \\mathcal{H}_{6}$ of $\\mathcal{S}$ (Definition 9)."
    },
    {
      "markdown": "![img-10.jpeg](img-10.jpeg)\n\nFigure 8: An axis-separator of $\\mathcal{S}$ splits $\\mathcal{S}$ in two halves $\\Gamma_{\\mathcal{S}}^{+}(i, \\tau)=\\left\\{z \\in \\mathcal{S} \\mid z_{i}>\\tau\\right\\}$ and $\\Gamma_{\\mathcal{S}}^{-}(i, \\tau)=$ $\\left\\{z \\in \\mathcal{S} \\mid z_{i}<\\tau\\right\\}$, which are each nonempty and connected (Definition 9).\n![img-11.jpeg](img-11.jpeg)\n\nFigure 9: Axis-separator of $\\mathcal{S}$ counterexample: Even though the support of the set on the left is connected, $\\mathcal{H}_{2}$ splits it into three parts, not two halves, so it does not satisfy the axisseparator condition (Definition 9)."
    },
    {
      "markdown": "![img-12.jpeg](img-12.jpeg)\n\nFigure 10: A discrete coordination $\\mathbf{A}$ is a tuple $\\mathbf{A}=\\left(\\mathbf{A}_{1}, \\ldots, \\mathbf{A}_{d}\\right)$ where each $\\mathbf{A}_{i}$ is itself a tuple of real numbers in increasing order $\\mathbf{A}_{i}=\\left(\\mathbf{A}_{i, 1}, \\ldots, \\mathbf{A}_{i, n_{i}}\\right)$ such that $\\mathbf{A}_{i, k+1}>\\mathbf{A}_{i, k}$. These represent the coordinates of axis-separators along each of the $d$ coordinate axes (Definition 13).\n![img-13.jpeg](img-13.jpeg)\n\nFigure 11: Left: The set of axis separators in dark blue are a backbone since they intersect all the others (Definition 14). Right: The set of separators in dark blue is not a backbone since the horizontal axis separator does not intersect the right-most vertical separator.\n\n# Appendix E. Proof of non-removable discontinuity preservation (Theorem 4) \n\nProof Let us denote $J_{h}(z)=\\frac{\\partial h}{\\partial z}(z)$ the Jacobian of $h$, and $J_{h^{-1}}\\left(z^{\\prime}\\right)=\\frac{\\partial h^{-1}}{\\partial z^{\\prime}}\\left(z^{\\prime}\\right)$ the Jacobian of $h^{-1}$. Suppose $p_{Z}$ is one of the PDFs of $Z$, from this we can obtain a PDF of $Z^{\\prime}$ using the change of variable formula: $p_{Z^{\\prime}}\\left(z^{\\prime}\\right)=p_{Z}\\left(h^{-1}\\left(z^{\\prime}\\right)\\right)\\left|\\operatorname{det} J_{h^{-1}}\\left(z^{\\prime}\\right)\\right|$. Symmetrically, we can say that if $p_{Z^{\\prime}}$ is a PDF of $Z^{\\prime}$, we obtain a PDF version of $Z$ as follows: $p_{Z}(z)=p_{Z^{\\prime}}(h(z))\\left|\\operatorname{det} J_{h}(z)\\right|$."
    },
    {
      "markdown": "Suppose the PDF $p_{Z}$ has a non-removable discontinuity at $z_{0}$. Pick one of the PDFs of $Z^{\\prime}$, let us call it $p_{Z^{\\prime}}$. There are three possibilities for what could happen at $Z^{\\prime}=h\\left(z_{0}\\right)$.\n\n- $p_{Z^{\\prime}}$ is continuous at $h\\left(z_{0}\\right)$. We can apply the change of variables formula and obtain a PDF of $Z$ that is given as $p_{Z}(z)=p_{Z^{\\prime}}(h(z)) \\mid$ det $J_{h}(z) \\mid$. Since the RHS is a product of two terms that are continuous at $z_{0}$, we conclude that $p_{Z}$ is continuous at $z_{0}$. This contradicts the fact that $p_{Z}$ has a non-removable discontinuity at $z_{0}$.\n- $p_{Z^{\\prime}}$ is discontinuous at $h\\left(z_{0}\\right)$ but the discontinuity is removable. Therefore, there exists a PDF $p_{Z^{\\prime}}$ that is continuous at $h\\left(z_{0}\\right)$. We can now follow the same argument as the above bullet to construct a PDF of $Z$ that is continuous at $z_{0}$, which would contradict the fact that $p_{Z}$ has a removable discontinuity at $z_{0}$.\n- Finally, we are only left with the case that $p_{Z^{\\prime}}$ has a non-removable discontinuity at $h\\left(z_{0}\\right)$, which is what we set out to prove.\n\n\n# Appendix F. Proof of grid structure preservation and recovery (Theorem 15) \n\n## F.1. Proof of Step 1 - Recovery of all separators\n\nKnowing, from Theorem 4, that the set of points making up the axis-aligned grid $G$ maps through $h$ to the set of points making up the axis-aligned grid $G^{\\prime}$ (i.e. $G^{\\prime}=h(G)$ ), our first major step consists in establishing that the axis-separators that make up $G$ (i.e. the elements of $\\mathcal{G}$ ) map one-to-one to the axis-separators that make up $G^{\\prime}$ (i.e. the elements of $\\mathcal{G}^{\\prime}$ ). We can denote this simply as $G^{\\prime}=h(G) \\Longrightarrow \\mathcal{G}^{\\prime}=h(\\mathcal{G})$.\n\n## OVERVIEW\n\nThe high-level proof is as follows:\n\n- Since $H \\in \\mathcal{G}$ is a connected smooth hypersurface in $\\mathcal{S}$, and a $C^{\\infty}$-diffeomorphism maps connected sets to connected sets and smooth hypersurfaces to smooth hypersurfaces, we get that $h(H)$ is a connected smooth hypersurface in $\\mathcal{S}^{\\prime}$.\n- From Theorem 4, we also know that $h(H) \\subset G^{\\prime}$.\n- Next, we establish that the only smooth connected hypersurfaces in $\\mathcal{S}^{\\prime}$ that are included in $G^{\\prime}$ are necessarily subsets of a single axis-separator of $\\mathcal{G}^{\\prime}$. This is fundamentally due to the fact that a connected smooth hypersurface cannot spread from one separator of the grid to another along their orthogonal intersection, as it would no longer be smooth (having a \"kink\"), so it has to stay within a single separator.\n- We conclude that $h(H)$ is necessarily a subset of a single axis-separator $H^{\\prime} \\in \\mathcal{G}^{\\prime}$.\n- We, then, show that not only is $h(H)$ a subset of a single axis-separator $H^{\\prime} \\in \\mathcal{G}^{\\prime}$, but that it has to be that entire separator. From the previous point, the reverse diffeomorphism $h^{-1}$ must map back the would-be remaining part of $H^{\\prime}$ (i.e. $H^{\\prime} \\backslash h(H) \\neq \\emptyset$ ) to a subset of the same separator as it maps back $h(H)$, i.e. to $H$. But this leads to a contradiction, since that remaining part did not come from $H$ initially. (See proof of Lemma 24 in Appendix).\n- We have thus shown that $H \\in \\mathcal{G} \\Longrightarrow h(H) \\in \\mathcal{G}^{\\prime}$. It suffices to apply this result in the other direction using $h^{-1}$ to establish the converse. We thus have a bijection: the one-to-one mapping we needed to prove. Which we can write succinctly as $\\mathcal{G}^{\\prime}=h(\\mathcal{G})$."
    },
    {
      "markdown": "# F.1.1. DETAILED PROOF OF STEP 1 - RECOVERY OF ALL SEPARATORS \n\nThe goal of step 1 is to establish that the axis-separators that make up $G$ (i.e. the elements of $\\mathcal{G}$ ) map one-to-one to the axis-separators that make up $G^{\\prime}$ (i.e. the elements of $\\mathcal{G}^{\\prime}$ ). We can denote this simply as $G^{\\prime}=h(G) \\Longrightarrow \\mathcal{G}^{\\prime}=h(\\mathcal{G})$. We provide a detailed proof here. Note that we always assume finite axis-separator sets.\n\n## PRELIMINARIES\n\nWhenever we say hypersurface, it is always defined as a $d-1$ dimensional regular submanifold embedded in $d$-dimensional ambient space $\\mathcal{S} \\subset \\mathbb{R}^{d}$, where $\\mathcal{S}$ is a $d$-dimensional connected open submanifold of $\\mathbb{R}^{d}$. In our application $\\mathcal{S}$ will be the interior of the support of the density we consider.\n\n- Definition: Intersection set. given a grid $G=\\cup \\mathcal{G}=\\cup_{H \\in \\mathcal{G}} H$, we define its intersection set $I(\\mathcal{G})$ as the set of points that belong to intersections of 2 or more distinct separators of $\\mathcal{G}$. Formally: $I(\\mathcal{G})=\\cup_{H \\in \\mathcal{G}, H^{\\prime} \\in \\mathcal{G}, H^{\\prime} \\neq H}\\left(H \\cap H^{\\prime}\\right)$.\n- Definition: Exclusive point. We say that a point $z$ is exclusive to a separator $H$ of a grid $G=\\cup \\mathcal{G}$ if it belongs to $H$ but does not belong to any other separator of the grid (i.e. it does not belong to $I$ ). Similarly we will say that a set is exclusive to a separator if all its elements are exclusive points of that separator. The set of points of a separator $H$ that are exclusive to it will be denoted $\\bar{H}=H \\backslash I$.\n- Definition: Tangent space we view the tangent spaces to hypersurfaces embedded in an ambient space included in $\\mathbb{R}^{d}$ literally as affine subspaces of $\\mathbb{R}^{d}$, i.e. we use the traditional view ${ }^{5}$ of tangent space (do Carmo, 1976), which is a natural generalization of the notion of a plane tangent to a surface at a point, to higher dimensional hypersurfaces embedded in $\\mathbb{R}^{d}$. The tangent space at $z \\in A$ to a hypersurface $A$ will be denoted $T^{A}(z)=T_{z} A$. A smooth hypersurface has the property that it has at every $z \\in A$ a well-defined tangent space $T^{A}(z)=T_{z} A$ of the same dimension as the hypersurface. When $A$ is a smooth hypersurface, $T^{A}$ is a smooth map $T^{A}: A \\rightarrow \\operatorname{Graff}_{d-1}\\left(\\mathbb{R}^{d}\\right)$ that maps any point $z$ of $A$ to a point of the affine-Grassmannian manifold (Klain and Rota, 1997; Lim et al., 2021) $\\operatorname{Graff}_{d-1}\\left(\\mathbb{R}^{d}\\right)$, i.e. the space of all $d-1$ dimensional affine-subspaces of $\\mathbb{R}^{d}$. Since $T^{A}$ is a continuous map between smooth manifolds, $T^{A}(z)$ will be continuous in any local (or global) parametrization of $A$ around $z$. (continuity based on the topology of the affine-Grassmannian manifold for comparing tangent spaces as affine-subspaces of $\\mathbb{R}^{d}$ ).\nNote that the tangent space to any axis-separator $H$ is a constant: it is the affine subspace confounded with the hyperplane that includes the separator, and will be denoted $\\mathcal{T}_{H}$. i.e. we have $\\forall z \\in H, T^{H}(z)=T_{z} H=\\mathcal{T}_{H}$. Note also that with this affine subspace definition of tangent space, $\\mathcal{T}_{H}$ is different for every separator $H$ of an axis-aligned grid: $\\forall H_{1} \\in \\mathcal{G}, \\forall H_{2} \\in$ $\\mathcal{G}, \\mathcal{T}_{H_{1}}=\\mathcal{T}_{H_{2}} \\Leftrightarrow H_{1}=H_{2}$.\n\n- Useful properties: We will also use the following properties that are either well-established differential geometry knowledge or straightforward corollaries thereof (Lee, 2012).\n- Property 1: A $C^{\\infty}$-diffeomorphism maps a smooth hypersurface to a smooth hypersurface.\n- Property 2: A diffeomorphism maps a path-connected set to a path-connected set.\n\n5. This traditional extrinsic view of tangent space is preferred here to more modern definitions, because it simplifies a step in our proof. It is also arguably easier to intuit and follow for readers who may not be familiar with differential geometry."
    },
    {
      "markdown": "- Property 3: Smooth connected hypersurfaces in $\\mathbb{R}^{d}$ have a $d-1$ dimensional tangent space that is well-defined all over the hypersurface and continuous (in the sense defined above, see tangent space).\n- Property 4: A non-empty open subset of a smooth hypersurface in ambient space is itself a smooth hypersurface in ambient space.\n- Property 5: A hypersurface that is a subset of another hypersurface has at every of its points the same tangent space as the hypersurface it is a subset of.\n\n\n# DETAILED PROOF \n\nLemma 18 No subset of the intersection set $I(\\mathcal{G})$ of an axis-aligned grid $G=\\cup \\mathcal{G}$ can be a hypersurface in ambient space.\n\nProof Consider $I(\\mathcal{G})$ the intersection set of a grid $G=\\cup \\mathcal{G}$. Formally:\n$I(\\mathcal{G})=\\cup_{H \\in \\mathcal{G}, H^{\\prime} \\in \\mathcal{G}, H^{\\prime} \\neq H}\\left(H \\cap H^{\\prime}\\right)$. Each $H \\cap H^{\\prime}$, if it is non-empty, is the intersection of two orthogonal (thus transversal) connected hypersurfaces (i.e. $d-1$ dimensional submanifolds embedded in ambient space), so that their intersection can be at most a $d-2$ dimensional embedded submanifold of ambient space. The union of a finite number of at most $d-2$ dimensional submanifolds cannot be more than $d-2$ dimensional, so $I(\\mathcal{G})$ cannot be more than $d-2$ dimensional. Consequently no subset of $I(\\mathcal{G})$ can be more than $d-2$ dimensional, thus it cannot be a hypersurface in ambient space.\n\nLemma 19 Let $A$ be a connected smooth hypersurface included in an axis-alined grid $G=\\cup \\mathcal{G}$ with axis-separator set $\\mathcal{G}$. Let $z \\in A$. All open neighborhoods of $z$ in $A$ will necessarily contain at least one point that is exclusive to a separator of $\\mathcal{G}$.\n\nProof An open neighborhood $\\mathcal{B}_{z}^{A}$ of $z$ in $A$ is an open subset of $A$, thus from Property $4, \\mathcal{B}_{z}^{A}$ is a hypersurface in ambient space. From Lemma 18 no subset of $I(\\mathcal{G})$, (the set of points of $G$ that belong to more than one separator) can be a hypersurface. So $\\mathcal{B}_{z}^{A}$ cannot be a subset of $I(\\mathcal{G})$, i.e. it must contain at least one point exclusive to a separator of $\\mathcal{G}$.\n\nLemma 20 Let $A$ be a connected smooth hypersurface included in an axis-alined grid $G=\\cup \\mathcal{G}$ with axis-separator set $\\mathcal{G}$. Let $z$ be a point of $A$ that is exclusive to a separator $H \\in \\mathcal{G}$ (i.e. $z \\in H \\backslash I(\\mathcal{G})$ : it belongs to no other separator of $\\mathcal{G}$ ), then there exists an open connected neighborhood $\\mathcal{B}_{z}^{A}$ of $z$ in $A$ that is exclusive to $H$.\n\nProof We reason using the usual Euclidean distance in $\\mathbb{R}^{d}$. Consider an open $d$-ball $\\mathcal{B}_{z}^{d}$ in $\\mathbb{R}^{d}$ centered on $z$ and whose radius $\\epsilon$ is chosen to be less than the smallest distance of $z$ to any other separator, i.e. such that $0<\\epsilon<\\inf _{z^{\\prime} \\in(G \\backslash H)}\\left\\|z-z^{\\prime}\\right\\|$. Since $z$ is exclusive to separator $H$ and the number of separators is finite, this distance will be greater than 0 . Then all points of $G$ within a distance less than $\\epsilon$ of $z$ will necessarily belong exclusively to $H$, i.e. $\\mathcal{B}_{z}^{d} \\cap G \\subset \\bar{H}$, where $\\bar{H}=H \\backslash I(\\mathcal{G})$. Now we can choose a sufficiently small connected open neighborhood $\\mathcal{B}_{z}^{A}$ of $z$ in $A$ so that the distance in ambient space between $z$ and any other point of $\\mathcal{B}_{z}^{A}$ is less than $\\epsilon$. Thus $\\mathcal{B}_{z}^{A} \\subset \\mathcal{B}_{z}^{d}$. Since we also have $\\mathcal{B}_{z}^{A} \\subset A \\subset G$ this implies that $\\mathcal{B}_{z}^{A} \\subset \\mathcal{B}_{z}^{d} \\cap G$ and consequently that $\\mathcal{B}_{z}^{A} \\subset \\bar{H}$. We have thus shown that there exists an open connected neighborhood of $z$ in $A$ that is exclusive to $H$."
    },
    {
      "markdown": "Lemma 21 Let $A$ be a connected smooth hypersurface included in an axis-alined grid $G=\\cup \\mathcal{G}$ with axis-separator set $\\mathcal{G}$. Then for any point $z \\in A$ there exists a non-empty open subset $B$ whose boundary contains $z$ and such that $B$ is a non-empty open subset exclusive to one of the separators.\n\nProof There are two cases to consider for $z$ : either $z$ is an exclusive point of a separator of the grid, or it is an intersection point of separators (belonging to $I(\\mathcal{G})$ ).\n\nFirst case: $z$ is a point exclusive to a separator $H \\subset \\mathcal{G}$.\nThen, by Lemma 20, we know that there exists an open connected neighborhood $\\mathcal{B}_{z}^{A}$ of $z$ in $A$ that is exclusive to $H$. We can then easily pick an open subset $B$ of $\\mathcal{B}_{z}^{A}$ whose boundary contains $z$ (For instance, pick a close neighbor $z_{1}$ of $z$ in $\\mathcal{B}_{z}^{A}$, and construct $B$ as the intersection of $\\mathcal{B}_{z}^{A}$ with an open ball centered on $z_{1}$ and of radius $\\left\\|z_{1}-z\\right\\|$ ). $B$ is an open subset exclusive to $H$, the separator that $z$ belongs to.\n\nSecond case: $z$ is not exclusive to any separator of the grid.\nLet $\\mathcal{G}=\\left\\{H_{1}, \\ldots, H_{k}\\right\\}$ be the finite set of separators of grid $G=\\cup \\mathcal{G}$. Let $\\bar{H}_{i}=H_{i} \\backslash I(\\mathcal{G})$ be the corresponding subset of exclusive points to each separator $H_{i}$, and let $\\bar{A}_{i}=\\bar{H}_{i} \\cap A$, for each $i \\in\\{1, \\ldots, k\\}$. So $\\bar{A}_{i}$, if it is not empty, will contain only points exclusive to $H_{i}$. From Lemma 20 we deduce that every point of $\\bar{A}_{i}$ has an open neighborhood in $A$ exclusive to $H_{i}$ : this open neighborhood is thus included in $A \\cap \\bar{H}_{i}$ and is thus a subset of $\\bar{A}_{i}$. We have thus shown that every point of $\\bar{A}_{i}$ has an open neighborhood in $A$ that is included in $\\bar{A}_{i}$. From this we conclude that each $\\bar{A}_{i}$ is an open subset (possibly empty) of $A$.\nWe know that $z$ belongs to none of the $\\bar{A}_{i}$, since it is not exclusive to any separator. Now we will show that $z$ belongs to the boundary of at least one of the $\\bar{A}_{i}$. We will reason using the metric $d^{A}$ induced on embedded submanifold $A \\subset \\mathbb{R}^{d}$ by the usual Euclidean metric in ambient space $\\mathbb{R}^{d}$. Let $\\epsilon=\\min _{i \\in\\{1, \\ldots, k\\}} d^{A}\\left(z, \\bar{A}_{i}\\right)$. We can use a min since it is over a finite number $k$ of separators. Note that $d^{A}\\left(z, \\bar{A}_{i}\\right)=\\inf _{z^{\\prime} \\in \\bar{A}_{i}} d^{A}\\left(z, z^{\\prime}\\right)$ will be $+\\infty$ if $\\bar{A}_{i}$ is empty, by the definition of the infimum. If $\\epsilon$ was strictly greater than 0 , then this would mean that no point of $A$ exclusive to any separator would be at a distance strictly less than $\\epsilon$ from $z$ (since any point of $A$ exclusive to a separator belongs to one of the $\\bar{A}_{i}$ ). Thus the open ball $\\mathcal{B}^{A}(z, \\epsilon)=\\left\\{z^{\\prime} \\in A, d^{A}\\left(z, z^{\\prime}\\right)<\\epsilon\\right\\}$ would not contain any point exclusive to any separator. But this would contradict Lemma 19. So necessarily $\\epsilon=0$. This implies that there is at least one of the $\\bar{A}_{i}$ whose distance to $z$ is 0 , i.e. there exists a $k^{*} \\in\\{0, \\ldots, k\\}$ such that $d^{A}\\left(z, \\bar{A}_{k^{*}}\\right)=0$. Since $z \\notin \\bar{A}_{k^{*}}$ we conclude that $z$ belongs to the boundary of this $\\bar{A}_{k^{*}}$. Moreover this $\\bar{A}_{k^{*}}$ is non-empty (otherwise that distance would be $+\\infty$ ). It is thus an open-subset of $A$, exclusive to separator $H_{k}$. We have thus established that there exists a non-empty open subset of $A$ exclusive to one of the separators, and whose boundary contains $z$.\n\nLemma 22 Let $A$ be a connected smooth hypersurface included in an axis-alined grid $G=\\cup \\mathcal{G}$ with axis-separator set $\\mathcal{G}$. Let $\\gamma$ be a continuous path, included in $A$, that starts at a point $z_{1}$, where $z_{1}$ is exclusive to a separator $H \\in \\mathcal{G}$. Then $\\gamma$ will necessarily be included entirely in $H$.\n\nProof Consider path $\\gamma:[0,1] \\rightarrow A$, where $\\gamma(0)=z_{1}$ is exclusive to $H$. From Lemma 21, for each point $\\gamma(t) \\in A$, there exists an open subset $B_{t}$ of $A$ whose boundary contains $\\gamma(t)$ and such that $B_{t}$ is an open subset exclusive to one of the separators. Let us call this separator $H_{t}$ (Note that there may be multiple possible choices for $B_{t}$ and $H_{t}$ ). Consider any point $z \\in B_{t}$. Since $B_{t}$ is a non-empty open subset of smooth hypersurface $A$, by Property 4 it is a hypersurface and by Property $5 B_{t}$"
    },
    {
      "markdown": "and $A$ will have the same tangent space, so that $T_{z} B_{t}=T_{z} A$. Since $B_{t}$ is also a subset of smooth hypersurface $H_{t}$, we have by Property 5 that $T_{z} B_{t}=T_{z} H_{t}$. Thus $T_{z} A=T_{z} B_{t}=T_{z} H_{t}$. Now the tangent space to any axis-separator $H^{\\prime}$ is the constant $\\mathcal{T}_{H^{\\prime}}$. We can thus write, for any $z \\in B_{t}$, $T_{z} A=T_{z} B_{t}=T_{z} H_{t}=\\mathcal{T}_{H_{t}}$. Since $A$ is a connected smooth hypersurface, it has a continuous and well defined tangent space at every point (see Property 3 and how we defined tangent space). Thus, if the tangent space is constant on an open subset $B_{t} \\subset A$, it will have that same constant value at its boundary. So the tangent space to $A$ at point $\\gamma(t)$, which belongs to the boundary of $B_{t}$, will also be $T_{\\gamma(t)} A=\\mathcal{T}_{H_{t}}$. For the same reason of the continuity of the tangent space of a path-connected smooth hypersurface $A$, we cannot have, along the curve $\\gamma(t)$, an abrupt change in the tangent space $T_{\\gamma(t)} A$, consequently $\\mathcal{T}_{H_{t}}$ cannot change abruptly along the path. The only way for it not to change abruptly is that $H_{t}$ stays constant along the path: $H_{t}=\\mathrm{constant} \\forall t \\in[0,1]$. In other words, for any point $\\gamma(t)$ along the path there must exist an open subset $B_{t}$ of $A$ that is included in and exclusive to the same constant separator along the path. Now, if $z_{1}=\\gamma(0)$ is exclusive to a separator $H$, then $H_{0}=H$ and we must thus have $H_{t}=H, \\forall t \\in[0,1]$. We have thus shown that if the path starts at a point $z_{1}=\\gamma(0)$ which is exclusive to a separator $H$, then all points of the path necessarily belong to $H$ (though not necessarily exclusively to $H$ ). Thus, the path is entirely included in $H$.\n\nLemma 23 A path-connected smooth hypersurface $A$ included in an axis-alined grid $G=\\cup \\mathcal{G}$ is necessarily a subset of one separator of $\\mathcal{G}$.\n\nProof Let $z_{0}$ be a point of $A$ that is exclusive to a separator $H \\in \\mathcal{G}$. We know from Lemma 19 that such a point exists. Since $A$ is path-connected, there exists in $A$ a continuous path connecting $z_{0}$ to any point $z \\in A$. Thus, Lemma 22 leads to conclude that $\\forall z \\in A, z \\in H$. Thus $A \\subset H$.\n\nLemma 24 Let $G=\\cup \\mathcal{G}$ be an axis-aligned grid in $\\mathcal{S}$. Let $h: \\mathcal{S} \\rightarrow \\mathcal{S}^{\\prime}$ be a diffeomorphism. Let $G^{\\prime}=\\cup \\mathcal{G}^{\\prime}$ be an axis-aligned grid in $\\mathcal{S}^{\\prime}$. If $h(G)=G^{\\prime}$, the image of a separator $H_{1} \\in \\mathcal{G}$ by the diffeomorphism $h$ will be a separator $H^{\\prime} \\in \\mathcal{G}^{\\prime}$, i.e. $H_{1} \\in \\mathcal{G} \\Longrightarrow h\\left(H_{1}\\right) \\in \\mathcal{G}^{\\prime}$.\n\nProof An axis-separator $H_{1} \\subset G$ is a path-connected smooth hypersurface. From Property 1 and Property 2, its image by the diffeomorphism $h$ will be a path-connected smooth hypersurface $h\\left(H_{1}\\right) \\subset G^{\\prime}$. So $h\\left(H_{1}\\right)$ is a path-connected smooth hypersurface included in axis-aligned grid $G^{\\prime}$. Consequently, Lemma 23 guarantees that we have $h\\left(H_{1}\\right) \\subset H^{\\prime}$ for some $H^{\\prime} \\in \\mathcal{G}^{\\prime}$. We will now prove that $h\\left(H_{1}\\right)=H^{\\prime}$. Suppose by contradiction that $h\\left(H_{1}\\right) \\subsetneq H^{\\prime}$, and let $B^{\\prime}=H^{\\prime}-h\\left(H_{1}\\right) \\neq \\emptyset$. Similarly, if we apply the reverse diffeomorphism $h^{-1}$, we will have $h^{-1}\\left(H^{\\prime}\\right) \\subset H_{2}$ for some $H_{2} \\in \\mathcal{G}$. Consequently, the two disjoint sets composing $H^{\\prime}=B^{\\prime} \\cup h\\left(H_{1}\\right)$ will both map back to subsets of $H_{2}$, i.e. $h^{-1}\\left(B^{\\prime}\\right) \\subset H_{2}$ and $h^{-1}\\left(h\\left(H_{1}\\right)\\right) \\subset H_{2}$. The latter can be rewritten as $H_{1} \\subset H_{2}$, which implies $H_{2}=H_{1}$ since no two distinct separators of $\\mathcal{G}$ are included in one another. So we have $h^{-1}\\left(B^{\\prime}\\right) \\subset H_{1}$. Thus $h\\left(h^{-1}\\left(B^{\\prime}\\right)\\right) \\subset h\\left(H_{1}\\right)$, hence $B^{\\prime} \\subset h\\left(H_{1}\\right)$. We had defined $B^{\\prime}$ as $B^{\\prime}=H^{\\prime}-h\\left(H_{1}\\right) \\neq \\emptyset$ but a non-empty $B^{\\prime}$ cannot at the same time correspond to a set from which we removed $h\\left(H_{1}\\right)$ and be included in $h\\left(H_{1}\\right)$. We have a contradiction, so we cannot have $h\\left(H_{1}\\right) \\subsetneq H^{\\prime}$, therefore $h\\left(H_{1}\\right)=H^{\\prime}$.\n\nProposition 25 The diffeomorphism $h$ maps separators in $\\mathcal{G}$ one-to-one to separators in $\\mathcal{G}^{\\prime}$, i.e. $h(G)=G^{\\prime} \\Longrightarrow h(\\mathcal{G})=\\mathcal{G}^{\\prime}$."
    },
    {
      "markdown": "Proof We have shown in Lemma 24 that $H \\in \\mathcal{G} \\Longrightarrow h(H) \\in \\mathcal{G}^{\\prime}$. It suffices to apply this result in the other direction using $h^{-1}$ to establish the converse. We thus have a bijection: the one-to-one mapping we needed to prove. Which we can write succinctly $h(\\mathcal{G})=\\mathcal{G}^{\\prime}$.\n\n# F.2. Step 2 - Recovery of partition into sets of parallel separators \n\nWe have established in step 1 that we recover the set of all separators $\\mathcal{G}^{\\prime}=h(\\mathcal{G})$. Our next step is to recover its partition into subsets of parallel separators (each subset associated to an axis): $\\mathcal{G}^{\\prime(j)}=h\\left(\\mathcal{G}^{(i)}\\right)$ (with permutation $j=\\sigma(i)$ ).\n\n## Proof FOR STEP 2\n\nConsider $d$ separators forming a backbone of $\\mathcal{G}$, recall that a backbone is constituted of $d$ distinct axis-separators that intersect in a single point, i.e. $\\mathcal{H}_{1}^{*} \\in \\mathcal{G}^{(1)}, \\ldots, \\mathcal{H}_{d}^{*} \\in \\mathcal{G}^{(d)},\\bigcap_{i=1}^{d} \\mathcal{H}_{i}^{*}=\\left\\{z^{*}\\right\\}$. We have that $\\forall j \\neq i, \\mathcal{H}_{i}^{*} \\neq \\mathcal{H}_{j}^{*} \\Longrightarrow \\forall j \\neq i, h\\left(\\mathcal{H}_{i}^{*}\\right) \\neq h\\left(\\mathcal{H}_{j}^{*}\\right)$.\nWe also have that $\\bigcap_{i=1}^{d} \\mathcal{H}_{i}^{*}=\\left\\{z^{*}\\right\\} \\Longrightarrow \\bigcap_{i=1}^{d} h\\left(\\mathcal{H}_{i}^{*}\\right)=\\left\\{h\\left(z^{*}\\right)\\right\\}$ (as $h$ is a bijection).\nMoreover, we know from step 1 that $\\mathcal{H}_{i}^{*} \\in \\mathcal{G} \\Longrightarrow h\\left(\\mathcal{H}_{i}^{*}\\right) \\in \\mathcal{G}^{\\prime}$. In short, the $h\\left(\\mathcal{H}_{1}^{*}\\right), \\ldots, h\\left(\\mathcal{H}_{d}^{*}\\right)$ are $d$ distinct separators, each an element of $\\mathcal{G}^{\\prime}$, that intersect in a single point $h\\left(z^{*}\\right)$. The only sets of $d$ distinct separators in $\\mathcal{G}^{\\prime}$ that pass through a same point are $d$ separators defined along each of the $d$ different axes of $\\mathcal{Z}^{\\prime}=\\mathbb{R}^{d}$. Thus there exists a permutation $\\sigma$ such that for such backbone separators, $\\mathcal{H}_{i}^{*} \\in \\mathcal{G}^{(i)} \\Longrightarrow h\\left(\\mathcal{H}_{i}^{*}\\right) \\in \\mathcal{G}^{\\prime(\\sigma(i))}$.\n\nNow consider any other separator $H \\in \\mathcal{G}^{(i)}$. From the definition of the backbone, we know that $H \\cap \\mathcal{H}_{j}^{*} \\neq \\emptyset, \\forall j \\neq i$.\nThis implies that $h(H) \\cap h\\left(\\mathcal{H}_{j}^{*}\\right) \\neq \\emptyset, \\forall j \\neq i$. The fact that $h(H)$ intersects a separator $h\\left(\\mathcal{H}_{j}^{*}\\right) \\in$ $\\mathcal{G}^{\\prime(\\sigma(j))}$ implies that it does not belong to parallel-separator-set $\\mathcal{G}^{\\prime(\\sigma(j))}$. Thus $\\forall j \\neq i, h(H) \\notin$ $\\mathcal{G}^{\\prime(\\sigma(j))}$. So there is just one parallel separator set left which $h(H)$ can belong to: $h(H) \\in \\mathcal{G}^{\\prime(\\sigma(i))}$. In short, we have proved that $H \\in \\mathcal{G}^{(i)} \\Longrightarrow h(H) \\in \\mathcal{G}^{\\prime(\\sigma(i))}$.\n\nSince distinct separators map to distinct separators, and each has to belong to exactly one of the $\\mathcal{G}^{(k)}$, this mapping is a bijection and we can write $H \\in \\mathcal{G}^{(i)} \\Longleftrightarrow h(H) \\in \\mathcal{G}^{\\prime(\\sigma(i))}$, or in short $h\\left(\\mathcal{G}^{(i)}\\right)=\\mathcal{G}^{\\prime(j)}$ with $j=\\sigma(i)$.\n\n## F.3. Step 3 - Recovery of coordinate ordering\n\nThe last step consists in showing that the ordering of the separators in a parallel-separators-set is preserved (up to possible order reversal).\n\n## Proof FOR STEP 3 - OVERVIEW\n\nThe gist of the proof is as follows (a complete detailed proof is provided in Appendix F.3.1):\nWe first establish that $h$ preserves separators and halves. This follows directly from the preservation of inclusion, connectedness and set operations under diffeomorphisms. Then, we use the fact that inclusion defines a strict order relationship between positive halves associated to a coordination, and similarly between negative halves. As inclusion is preserved by a diffeomorphism, this order relationship is preserved. We can use this to show that the order implied by $\\mathbf{A}_{i}$ is either conserved, as is, in $\\mathbf{B}_{j}$ (negative halves of coordination $\\mathbf{A}$ being mapped to negative halves of $\\mathbf{B}$ ) or simply"
    },
    {
      "markdown": "reversed (negative halves of $\\mathbf{A}$ are being mapped to positive halves of $\\mathbf{B}$ ). This directly yields the result of the main Theorem (15).\n\n# F.3.1. DETAILED PROOF FOR STEP 3 \n\n## PRELIMINARY LEMMA\n\nLemma 26 Preservation of separator and halves under a diffeomorphism: If $h$ is a diffeomorphism and $\\mathcal{C}$ is a separator of $\\mathcal{S}$ that splits it in two halves $\\mathcal{C}^{+}$and $\\mathcal{C}^{-}$, then $h(\\mathcal{C})$ is a separator of $h(\\mathcal{S})$ that splits it in two halves $h\\left(\\mathcal{C}^{+}\\right)$and $h\\left(\\mathcal{C}^{-}\\right)$\nFormally:\n\n$$\n\\begin{aligned}\n& \\mathcal{C} \\subset \\mathcal{S}, \\mathcal{C}, \\text { connected, } \\operatorname{split}(\\mathcal{S}, \\mathcal{C})=\\left\\{\\mathcal{C}^{+}, \\mathcal{C}^{-}\\right\\} \\\\\n\\Longleftrightarrow & h(\\mathcal{C}) \\subset h(\\mathcal{S}), h(\\mathcal{C}), \\text { connected, } \\operatorname{split}(h(\\mathcal{S}), h(\\mathcal{C}))=\\left\\{h\\left(\\mathcal{C}^{+}\\right), h\\left(\\mathcal{C}^{-}\\right)\\right\\}\n\\end{aligned}\n$$\n\nProof This follows from preservation of inclusion, connectedness, and set operations (union, intersection, difference) under a diffeomorphism.\nFormally: $\\mathcal{C} \\subset \\mathcal{S} \\Longrightarrow h(\\mathcal{C}) \\subset h(\\mathcal{S})$.\n$\\mathcal{C}^{+}$and $\\mathcal{C}^{-}$being the connected components of $\\mathcal{S}-\\mathcal{C}$ implies that $\\mathcal{C}^{+}$and $\\mathcal{C}^{-}$are each connected, and that $\\mathcal{S} \\backslash \\mathcal{C}=\\mathcal{C}^{+} \\cup \\mathcal{C}^{-}$, where $\\mathcal{C}^{+} \\cup \\mathcal{C}^{-}$is not connected.\nEach of $\\mathcal{S}, \\mathcal{C}, \\mathcal{C}^{+}, \\mathcal{C}^{-}$connected $\\Longrightarrow$ Each of $\\mathcal{S}, h(\\mathcal{C}), h\\left(\\mathcal{C}^{+}\\right), h\\left(\\mathcal{C}^{-}\\right)$connected.\n$\\mathcal{S} \\backslash \\mathcal{C}=\\mathcal{C}^{+} \\cup \\mathcal{C}^{-} \\Longrightarrow h(\\mathcal{S}) \\backslash h(\\mathcal{C})=h\\left(\\mathcal{C}^{+}\\right) \\cup h\\left(\\mathcal{C}^{-}\\right)$\n$\\mathcal{C}^{+} \\cup \\mathcal{C}^{-}$not connected $\\Longrightarrow h\\left(\\mathcal{C}^{+}\\right) \\cup h\\left(\\mathcal{C}^{-}\\right)$not connected.\nThat $h\\left(\\mathcal{C}^{+}\\right) \\cup h\\left(\\mathcal{C}^{-}\\right)$is not connected but $h\\left(\\mathcal{C}^{+}\\right)$and $h\\left(\\mathcal{C}^{-}\\right)$are each connected, implies that $h\\left(\\mathcal{C}^{+}\\right)$ and $h\\left(\\mathcal{C}^{-}\\right)$are the two connected components of $h\\left(\\mathcal{C}^{+}\\right) \\cup h\\left(\\mathcal{C}^{-}\\right)$i.e. of $h(\\mathcal{S})-h(\\mathcal{C})$.\nThis implies that $\\operatorname{split}(h(\\mathcal{S}), h(\\mathcal{C}))=\\left\\{h\\left(\\mathcal{C}^{+}\\right), h\\left(\\mathcal{C}^{-}\\right)\\right\\}$. The implication in the other direction can be obtained in the by applying the same reasoning using $h^{-1}$.\n\n## PROOF OF STEP 3\n\nLet $j=\\sigma(i)$ and $K=\\left|\\mathbf{A}_{i}\\right|=\\left|\\mathbf{B}_{j}\\right|$ and denote the corresponding set of axis separators as\n$\\mathcal{A}=\\left\\{\\Gamma_{\\mathcal{S}}\\left(i, \\mathbf{A}_{i, 1}\\right), \\ldots, \\Gamma_{\\mathcal{S}}\\left(i, \\mathbf{A}_{i, K}\\right)\\right\\}$ and $\\mathcal{B}=\\left\\{\\Gamma_{\\mathcal{S}^{\\prime}}\\left(j, \\mathbf{B}_{j, 1}\\right), \\ldots, \\Gamma_{\\mathcal{S}^{\\prime}}\\left(j, \\mathbf{B}_{j, K}\\right)\\right\\}$\nand denote the corresponding sets of halves:\n$\\mathcal{A}^{+}=\\left\\{\\Gamma_{\\mathcal{S}}^{+}\\left(i, \\mathbf{A}_{i, 1}\\right), \\ldots, \\Gamma_{\\mathcal{S}}^{+}\\left(i, \\mathbf{A}_{i, K}\\right)\\right\\}, \\mathcal{A}^{-}=\\left\\{\\Gamma_{\\mathcal{S}}^{-}\\left(i, \\mathbf{A}_{i, 1}\\right), \\ldots, \\Gamma_{\\mathcal{S}}^{-}\\left(i, \\mathbf{A}_{i, K}\\right)\\right\\}, \\mathcal{A}^{ \\pm}=\\mathcal{A}^{+} \\cup$\n$\\mathcal{A}^{-}$\nand $\\mathcal{B}^{+}=\\left\\{\\Gamma_{\\mathcal{S}^{\\prime}}^{+}\\left(j, \\mathbf{B}_{j, 1}\\right), \\ldots, \\Gamma_{\\mathcal{S}^{\\prime}}^{+}\\left(j, \\mathbf{B}_{j, K}\\right)\\right\\}, \\mathcal{B}^{-}=\\left\\{\\Gamma_{\\mathcal{S}^{\\prime}}^{-}\\left(j, \\mathbf{B}_{j, 1}\\right), \\ldots, \\Gamma_{\\mathcal{S}^{\\prime}}^{-}\\left(j, \\mathbf{B}_{j, K}\\right)\\right\\}, \\mathcal{B}^{ \\pm}=$ $\\mathcal{B}^{+} \\cup \\mathcal{B}^{-}$\n\nProof Step 2, states that $h(\\mathcal{A})=\\mathcal{B}$.\nAnd we have from the above Lemma that\n\n$$\n\\begin{gathered}\n\\operatorname{split}(\\mathcal{S}, \\mathcal{C})=\\left\\{\\mathcal{C}^{+}, \\mathcal{C}^{-}\\right\\} \\\\\n\\Longleftrightarrow \\operatorname{split}(h(\\mathcal{S}), h(\\mathcal{C}))=\\left\\{h\\left(\\mathcal{C}^{+}\\right), h\\left(\\mathcal{C}^{-}\\right)\\right\\}\n\\end{gathered}\n$$\n\nthus the equality of the sets of separators $h(\\mathcal{A})=\\mathcal{B}$ obtained in Proof Step 2 implies an equality of the sets of halves:\n\n$$\nh\\left(\\mathcal{A}^{ \\pm}\\right)=\\mathcal{B}^{ \\pm}\n$$"
    },
    {
      "markdown": "Now, the only halves, among all halves, that do not include any of the separators are $\\Gamma_{\\mathcal{S}}^{-}\\left(i, \\mathbf{A}_{i, 1}\\right)$ and $\\Gamma_{\\mathcal{S}}^{+}\\left(i, \\mathbf{A}_{i, K}\\right)$ i.e. formally:\n\n$$\n\\left\\{\\mathcal{C} \\in \\mathcal{A} \\mid \\forall \\mathcal{H} \\in \\mathcal{A}^{ \\pm}, \\mathcal{C} \\cap \\mathcal{H}=\\emptyset\\right\\}=\\left\\{\\Gamma_{\\mathcal{S}}^{-}\\left(i, \\mathbf{A}_{i, 1}\\right), \\Gamma_{\\mathcal{S}}^{+}\\left(i, \\mathbf{A}_{i, K}\\right)\\right\\}\n$$\n\nthis property will naturally translate to their mapping by the diffeomorphism $h$ (due to the preservation of inclusion an intersections)\nhence\n\n$$\n\\left\\{\\mathcal{C} \\in h(\\mathcal{A}) \\mid \\forall \\mathcal{H} \\in h\\left(\\mathcal{A}^{ \\pm}\\right), \\mathcal{C} \\cap \\mathcal{H}=\\emptyset\\right\\}=\\left\\{h\\left(\\Gamma_{\\mathcal{S}}^{-}\\left(i, \\mathbf{A}_{i, 1}\\right)\\right), h\\left(\\Gamma_{\\mathcal{S}}^{+}\\left(i, \\mathbf{A}_{i, K}\\right)\\right)\\right\\}\n$$\n\ni.e.\n\n$$\n\\left\\{\\mathcal{C} \\in \\mathcal{B} \\mid \\forall \\mathcal{H} \\in \\mathcal{B}^{ \\pm}, \\mathcal{C} \\cap \\mathcal{H}=\\emptyset\\right\\}=\\left\\{h\\left(\\Gamma_{\\mathcal{S}}^{-}\\left(i, \\mathbf{A}_{i, 1}\\right)\\right), h\\left(\\Gamma_{\\mathcal{S}}^{+}\\left(i, \\mathbf{A}_{i, K}\\right)\\right)\\right\\}\n$$\n\nbut we also have, similarly,\n\n$$\n\\left\\{\\mathcal{C} \\in \\mathcal{B} \\mid \\forall \\mathcal{H} \\in \\mathcal{B}^{ \\pm}, \\mathcal{C} \\cap \\mathcal{H}=\\emptyset\\right\\}=\\left\\{\\Gamma_{\\mathcal{S}^{\\prime}}^{-}\\left(i, \\mathbf{B}_{i, 1}\\right)\\right), \\Gamma_{\\mathcal{S}^{\\prime}}^{+}\\left(i, \\mathbf{B}_{i, K}\\right)\\right)\\right\\}\n$$\n\nFrom this we conclude that:\n\n$$\n\\left\\{h\\left(\\Gamma_{\\mathcal{S}}^{-}\\left(i, \\mathbf{A}_{i, 1}\\right)\\right), h\\left(\\Gamma_{\\mathcal{S}}^{+}\\left(i, \\mathbf{A}_{i, K}\\right)\\right)\\right\\}=\\left\\{\\Gamma_{\\mathcal{S}^{\\prime}}^{-}\\left(i, \\mathbf{B}_{i, 1}\\right)\\right), \\Gamma_{\\mathcal{S}^{\\prime}}^{+}\\left(i, \\mathbf{B}_{i, K}\\right)\\right)\\right\\}\n$$\n\nThus we have either one of two cases:\nCase 1: $h\\left(\\Gamma_{\\mathcal{S}}^{-}\\left(i, \\mathbf{A}_{i, 1}\\right)\\right)=\\Gamma_{\\mathcal{S}^{\\prime}}^{-}\\left(i, \\mathbf{B}_{i, 1}\\right)$ and $h\\left(\\Gamma_{\\mathcal{S}}^{+}\\left(i, \\mathbf{A}_{i, K}\\right)\\right)=\\Gamma_{\\mathcal{S}^{\\prime}}^{+}\\left(i, \\mathbf{B}_{i, K}\\right)$. We associate this case with $s_{i}=+1$.\n\nCase 2: $h\\left(\\Gamma_{\\mathcal{S}}^{-}\\left(i, \\mathbf{A}_{i, 1}\\right)\\right)=\\Gamma_{\\mathcal{S}^{\\prime}}^{+}\\left(i, \\mathbf{B}_{i, K}\\right)$ and $h\\left(\\Gamma_{\\mathcal{S}}^{+}\\left(i, \\mathbf{A}_{i, K}\\right)\\right)=\\Gamma_{\\mathcal{S}^{\\prime}}^{-}\\left(i, \\mathbf{B}_{i, 1}\\right)$. We associate this case with $s_{i}=-1$.\nCase 1: $s_{i}=+1, h\\left(\\Gamma_{\\mathcal{S}}^{-}\\left(i, \\mathbf{A}_{i, 1}\\right)\\right)=\\Gamma_{\\mathcal{S}^{\\prime}}^{-}\\left(i, \\mathbf{B}_{i, 1}\\right)$ and $h\\left(\\Gamma_{\\mathcal{S}}^{+}\\left(i, \\mathbf{A}_{i, K}\\right)\\right)=\\Gamma_{\\mathcal{S}^{\\prime}}^{+}\\left(i, \\mathbf{B}_{i, K}\\right) \\quad$ The halfspaces in $\\mathcal{A}^{ \\pm}$that include $\\Gamma_{\\mathcal{S}}^{-}\\left(i, \\mathbf{A}_{i, 1}\\right)$ are only the $\\Gamma_{\\mathcal{S}}^{-}$, formally:\n\n$$\n\\left\\{\\mathcal{H} \\in \\mathcal{A}^{ \\pm} \\mid \\Gamma_{\\mathcal{S}}^{-}\\left(i, \\mathbf{A}_{i, 1}\\right) \\subset \\mathcal{H}\\right\\}=\\mathcal{A}^{-}\n$$\n\nthis relationship will be maintained under a diffeomorphism $h$ i.e.\n\n$$\n\\left\\{\\mathcal{H} \\in h\\left(\\mathcal{A}^{ \\pm}\\right) \\mid h\\left(\\Gamma_{\\mathcal{S}}^{-}\\left(i, \\mathbf{A}_{i, 1}\\right)\\right) \\subset \\mathcal{H}\\right\\}=h\\left(\\mathcal{A}^{-}\\right)\n$$\n\nthus, since $h\\left(\\mathcal{A}^{ \\pm}\\right)=\\mathcal{B}^{ \\pm}$and $h\\left(\\Gamma_{\\mathcal{S}}^{-}\\left(i, \\mathbf{A}_{i, 1}\\right)\\right)=\\Gamma_{\\mathcal{S}^{\\prime}}^{-}\\left(i, B_{i, 1}\\right)$ this can be rewritten as\n\n$$\n\\begin{aligned}\n\\left\\{\\mathcal{H} \\in \\mathcal{B}^{ \\pm} \\mid \\Gamma_{\\mathcal{S}^{\\prime}}^{-}\\left(i, \\mathbf{B}_{i, 1}\\right) \\subset \\mathcal{H}\\right\\} & =h\\left(\\mathcal{A}^{-}\\right) \\\\\n\\mathcal{B}^{-} & =h\\left(\\mathcal{A}^{-}\\right)\n\\end{aligned}\n$$\n\nor, written less compactly:\n\n$$\n\\left\\{h\\left(\\Gamma_{\\mathcal{S}}^{-}\\left(i, \\mathbf{A}_{i, 1}\\right)\\right), \\ldots, h\\left(\\Gamma_{\\mathcal{S}}^{-}\\left(i, \\mathbf{A}_{i, K}\\right)\\right)\\right\\}=\\left\\{\\Gamma_{\\mathcal{S}^{\\prime}}^{-}\\left(j, \\mathbf{B}_{j, 1}\\right), \\ldots, \\Gamma_{\\mathcal{S}^{\\prime}}^{-}\\left(j, \\mathbf{B}_{j, K}\\right)\\right\\}\n$$\n\nFurthermore, strict inclusion defines an order relationship between the elements of $\\mathcal{A}^{-}$, which will be preserved under the diffeomorphism, and thus defines a strict ordering between them:"
    },
    {
      "markdown": "$$\n\\begin{gathered}\n\\Gamma_{\\mathcal{S}}^{-}\\left(i, \\mathbf{A}_{i, 1}\\right) \\subsetneq \\Gamma_{\\mathcal{S}}^{-}\\left(i, \\mathbf{A}_{i, 2}\\right) \\subsetneq \\ldots \\subsetneq \\Gamma_{\\mathcal{S}}^{-}\\left(i, \\mathbf{A}_{i, K}\\right) \\\\\n\\Longrightarrow h\\left(\\Gamma_{\\mathcal{S}}^{-}\\left(i, \\mathbf{A}_{i, 1}\\right)\\right) \\subsetneq h\\left(\\Gamma_{\\mathcal{S}}^{-}\\left(i, \\mathbf{A}_{i, 2}\\right)\\right) \\subsetneq \\ldots \\subsetneq h\\left(\\Gamma_{\\mathcal{S}}^{-}\\left(i, \\mathbf{A}_{i, K}\\right)\\right)\n\\end{gathered}\n$$\n\nwe know that the $h\\left(\\Gamma_{\\mathcal{S}}^{-}\\left(i, A_{i, k}\\right)\\right)$ are the elements of $\\mathcal{B}^{-}$(as we have just sown that $\\mathcal{B}^{-}=h\\left(\\mathcal{A}^{-}\\right)$), i.e. the $\\Gamma_{\\mathcal{S}^{\\prime}}^{-}\\left(j, \\mathbf{B}_{j, k}\\right)$. Their order is defined uniquely by strict inclusion as\n\n$$\n\\Gamma_{\\mathcal{S}^{\\prime}}^{-}\\left(j, \\mathbf{B}_{j, 1}\\right) \\subsetneq \\Gamma_{\\mathcal{S}^{\\prime}}^{-}\\left(j, \\mathbf{B}_{j, 2}\\right) \\subsetneq \\ldots \\subsetneq \\Gamma_{\\mathcal{S}^{\\prime}}^{-}\\left(j, \\mathbf{B}_{j, K}\\right)\n$$\n\nthus we can conclude not only (as we showed with $\\mathcal{B}^{-}=h\\left(\\mathcal{A}^{-}\\right)$) that\n\n$$\n\\left\\{h\\left(\\Gamma_{\\mathcal{S}}^{-}\\left(i, \\mathbf{A}_{i, 1}\\right)\\right), \\ldots, h\\left(\\Gamma_{\\mathcal{S}}^{-}\\left(i, \\mathbf{A}_{i, K}\\right)\\right)\\right\\}=\\left\\{\\Gamma_{\\mathcal{S}^{\\prime}}^{-}\\left(j, \\mathbf{B}_{j, 1}\\right), \\ldots, \\Gamma_{\\mathcal{S}^{\\prime}}^{-}\\left(j, \\mathbf{B}_{j, K}\\right)\\right\\}\n$$\n\nbut also that their ordering is preserved i.e.\n\n$$\n\\left(h\\left(\\Gamma_{\\mathcal{S}}^{-}\\left(i, \\mathbf{A}_{i, 1}\\right)\\right), \\ldots, h\\left(\\Gamma_{\\mathcal{S}}^{-}\\left(i, \\mathbf{A}_{i, K}\\right)\\right)\\right)=\\left(\\Gamma_{\\mathcal{S}^{\\prime}}^{-}\\left(j, \\mathbf{B}_{j, 1}\\right), \\ldots, \\Gamma_{\\mathcal{S}^{\\prime}}^{-}\\left(j, \\mathbf{B}_{j, K}\\right)\\right)\n$$\n\nor expressed differently:\n\n$$\n\\forall k \\in\\{1, \\ldots, K\\}, h\\left(\\Gamma_{\\mathcal{S}}^{-}\\left(i, \\mathbf{A}_{i, k}\\right)\\right)=\\Gamma_{\\mathcal{S}^{\\prime}}^{-}\\left(j, \\mathbf{B}_{j, k}\\right)\n$$\n\nit is straightforward to conclude from this that we also have\n\n$$\n\\begin{aligned}\n& \\forall k \\in\\{1, \\ldots, K\\} \\\\\n& h\\left(\\Gamma_{\\mathcal{S}}^{-}\\left(i, \\mathbf{A}_{i, k}\\right)\\right)=\\Gamma_{\\mathcal{S}^{\\prime}}^{-}\\left(j, \\mathbf{B}_{j, k}\\right) \\\\\n& h\\left(\\Gamma_{\\mathcal{S}}^{+}\\left(i, \\mathbf{A}_{i, k}\\right)\\right)=\\Gamma_{\\mathcal{S}^{\\prime}}^{+}\\left(j, \\mathbf{B}_{j, k}\\right) \\\\\n& h\\left(\\Gamma_{\\mathcal{S}}\\left(i, \\mathbf{A}_{i, k}\\right)\\right)=\\Gamma_{\\mathcal{S}^{\\prime}}\\left(j, \\mathbf{B}_{j, k}\\right)\n\\end{aligned}\n$$\n\nor stated differently, that:\n\n$$\n\\begin{aligned}\n& \\forall k \\in\\{1, \\ldots, K\\}, \\forall z \\in \\mathcal{S} \\\\\n& z \\in \\Gamma_{\\mathcal{S}}^{-}\\left(i, \\mathbf{A}_{i, k}\\right) \\Longleftrightarrow h(z) \\in \\Gamma_{\\mathcal{S}^{\\prime}}^{-}\\left(j, \\mathbf{B}_{j, k}\\right) \\\\\n& z \\in \\Gamma_{\\mathcal{S}}^{+}\\left(i, \\mathbf{A}_{i, k}\\right) \\Longleftrightarrow h(z) \\in \\Gamma_{\\mathcal{S}^{\\prime}}^{+}\\left(j, \\mathbf{B}_{j, k}\\right) \\\\\n& z \\in \\Gamma_{\\mathcal{S}}\\left(i, \\mathbf{A}_{i, k}\\right) \\Longleftrightarrow h(z) \\in \\Gamma_{\\mathcal{S}^{\\prime}}\\left(j, \\mathbf{B}_{j, k}\\right)\n\\end{aligned}\n$$\n\nor equivalently\n\n$$\n\\begin{aligned}\n& \\forall k \\in\\{1, \\ldots, K\\}, \\forall z^{\\prime} \\in \\mathcal{S}^{\\prime} \\\\\n& h^{-1}\\left(z^{\\prime}\\right) \\in \\Gamma_{\\mathcal{S}}^{-}\\left(i, \\mathbf{A}_{i, k}\\right) \\Longleftrightarrow z^{\\prime} \\in \\Gamma_{\\mathcal{S}^{\\prime}}^{-}\\left(j, \\mathbf{B}_{j, k}\\right) \\\\\n& h^{-1}\\left(z^{\\prime}\\right) \\in \\Gamma_{\\mathcal{S}}^{+}\\left(i, \\mathbf{A}_{i, k}\\right) \\Longleftrightarrow z^{\\prime} \\in \\Gamma_{\\mathcal{S}^{\\prime}}^{+}\\left(j, \\mathbf{B}_{j, k}\\right) \\\\\n& h^{-1}\\left(z^{\\prime}\\right) \\in \\Gamma_{\\mathcal{S}}\\left(i, \\mathbf{A}_{i, k}\\right) \\Longleftrightarrow z^{\\prime} \\in \\Gamma_{\\mathcal{S}^{\\prime}}\\left(j, \\mathbf{B}_{j, k}\\right)\n\\end{aligned}\n$$\n\nwhich we may also write"
    },
    {
      "markdown": "$$\n\\begin{aligned}\n& \\forall k \\in\\{1, \\ldots, K\\}, \\forall z^{\\prime} \\in \\mathcal{S}^{\\prime} \\\\\n& z_{j}^{\\prime}<\\mathbf{B}_{j, k} \\Longleftrightarrow h^{-1}\\left(z^{\\prime}\\right)_{i}<\\mathbf{A}_{i, k} \\\\\n& z_{j}^{\\prime}>\\mathbf{B}_{j, k} \\Longleftrightarrow h^{-1}\\left(z^{\\prime}\\right)_{i}>\\mathbf{A}_{i, k} \\\\\n& z_{j}^{\\prime}=\\mathbf{B}_{j, k} \\Longleftrightarrow h^{-1}\\left(z^{\\prime}\\right)_{i}=\\mathbf{A}_{i, k}\n\\end{aligned}\n$$\n\nwhich is what we needed to prove in the main grid structure recovery theorem.\nCase 2: axis reversal $s_{i}=-1, h\\left(\\Gamma_{\\mathcal{S}}^{-}\\left(i, \\mathbf{A}_{i, 1}\\right)\\right)=\\Gamma_{\\mathcal{S}^{\\prime}}^{+}\\left(i, \\mathbf{B}_{i, K}\\right)$ and $h\\left(\\Gamma_{\\mathcal{S}}^{+}\\left(i, \\mathbf{A}_{i, K}\\right)\\right)=\\Gamma_{\\mathcal{S}^{\\prime}}^{-}\\left(i, \\mathbf{B}_{i, 1}\\right)$ We can follow the exact same reasoning steps as in case 1, starting from $h\\left(\\Gamma_{\\mathcal{S}}^{-}\\left(i, \\mathbf{A}_{i, 1}\\right)\\right)=$ $\\Gamma_{\\mathcal{S}^{\\prime}}^{+}\\left(i, \\mathbf{B}_{i, K}\\right)$ :\n\n- to first show that $h\\left(\\mathcal{A}^{-}\\right)=\\mathcal{B}^{+}$i.e.\n\n$$\n\\left\\{h\\left(\\Gamma_{\\mathcal{S}}^{-}\\left(i, \\mathbf{A}_{i, 1}\\right)\\right), \\ldots, h\\left(\\Gamma_{\\mathcal{S}}^{-}\\left(i, \\mathbf{A}_{i, K}\\right)\\right)\\right\\}=\\left\\{\\Gamma_{\\mathcal{S}^{\\prime}}^{+}\\left(j, \\mathbf{B}_{j, 1}\\right), \\ldots, \\Gamma_{\\mathcal{S}^{\\prime}}^{+}\\left(j, \\mathbf{B}_{j, K}\\right)\\right\\}\n$$\n\n- then use the preservation of the order relation defined by inclusion of halves to establish that\n\n$$\n\\left(h\\left(\\Gamma_{\\mathcal{S}}^{-}\\left(i, \\mathbf{A}_{i, 1}\\right)\\right), \\ldots, h\\left(\\Gamma_{\\mathcal{S}}^{-}\\left(i, \\mathbf{A}_{i, K}\\right)\\right)\\right)=\\left(\\Gamma_{\\mathcal{S}^{\\prime}}^{+}\\left(j, \\mathbf{B}_{j, K}\\right), \\ldots, \\Gamma_{\\mathcal{S}^{\\prime}}^{+}\\left(j, \\mathbf{B}_{j, 1}\\right)\\right)\n$$\n\n- thus that\n\n$$\n\\begin{aligned}\n& \\forall k \\in\\{1, \\ldots, K\\} \\\\\n& h\\left(\\Gamma_{\\mathcal{S}}^{-}\\left(i, \\mathbf{A}_{i, k}\\right)\\right)=\\Gamma_{\\mathcal{S}^{\\prime}}^{+}\\left(j, \\mathbf{B}_{j, K-k+1}\\right) \\\\\n& h\\left(\\Gamma_{\\mathcal{S}}^{+}\\left(i, \\mathbf{A}_{i, k}\\right)\\right)=\\Gamma_{\\mathcal{S}^{\\prime}}^{-}\\left(j, \\mathbf{B}_{j, K-k+1}\\right) \\\\\n& h\\left(\\Gamma_{\\mathcal{S}}\\left(i, \\mathbf{A}_{i, k}\\right)\\right)=\\Gamma_{\\mathcal{S}^{\\prime}}\\left(j, \\mathbf{B}_{j, K-k+1}\\right)\n\\end{aligned}\n$$\n\n- conclude that\n\n$$\n\\begin{aligned}\n& \\forall k \\in\\{1, \\ldots, K\\}, \\forall z^{\\prime} \\in \\mathcal{S}^{\\prime} \\\\\n& z_{j}^{\\prime}>\\mathbf{B}_{j, k} \\Longleftrightarrow h^{-1}\\left(z^{\\prime}\\right)_{i}<\\mathbf{A}_{i, K-k+1} \\\\\n& z_{j}^{\\prime}<\\mathbf{B}_{j, k} \\Longleftrightarrow h^{-1}\\left(z^{\\prime}\\right)_{i}>\\mathbf{A}_{i, K-k+1} \\\\\n& z_{j}^{\\prime}=\\mathbf{B}_{j, k} \\Longleftrightarrow h^{-1}\\left(z^{\\prime}\\right)_{i}=\\mathbf{A}_{i, K-k+1}\n\\end{aligned}\n$$\n\nwhich is what we needed to prove in the main grid structure recovery theorem.\n\n# Appendix G. Background on non-removable discontinuities \n\nThe definition of continuity of a function is leveraged in section 5.1.\nDefinition 27 A function $f$ is continuous at a point $x_{0}$ if\n$\\forall \\epsilon>0 ; \\exists \\delta>0: d\\left(x, x_{0}\\right)<\\delta \\Rightarrow\\left|f(x)-f\\left(x_{0}\\right)\\right|<\\epsilon$\nfor $x$ in the domain of $f$ and $d\\left(x, x_{0}\\right)$ being the distance between points $x$ and $x_{0}$.\nThat is, for any positive real number $\\epsilon$, which can be infinitely small, there exists a positive real number $\\delta$ such that for $x$ in the interval $x_{0}-\\delta<x<x_{0}+\\delta$, the function of $x$ will be at the interval $f\\left(x_{0}\\right)-\\epsilon<f(x)<f\\left(x_{0}\\right)+\\epsilon$. So for $f(x)$ to be in a small neighborhood around $f\\left(x_{0}\\right), x$ can be chosen in a small neighborhood around $x_{0}$."
    },
    {
      "markdown": "Definition 28 (Marsden and Hoffman, 1993) A function $f: A \\subset M \\rightarrow N$ is called continuous on the set $B \\subset A$ if $f$ is continuous at each point of $B$. If we just say that $f$ is continuous, we mean that $f$ is continuous on its domain $A$.\n\nThese definitions are relevant because when a function is continuous, Definition 27 will hold at all the points in its domain. However, there can be cases where a function is not continuous at a point, but it is continuous almost everywhere. We define the removable discontinuity of a PDF $p$ at point $x_{0}$ as a discontinuity that can be removed by mapping it to another PDF $p^{\\prime}$ that has the same probability measure. The idea is that the area under the curve is the same in the equivalent PDF without the discontinuity at any interval, as illustrated in Figure 12. More precisely, the PDF $p_{x}$ represents a probability distribution where we can evaluate the probability at an interval by integrating over it, such as $\\operatorname{Pr}[a \\leq X \\leq b]=\\int_{a}^{b} p_{X}(x) d x$ for $a, b$ belonging to a measurable set $\\mathcal{M}^{6}$. Hence, a probability distribution $Q$ maps a measurable set $\\mathcal{M}$ to $[0,1] . Q: \\mathcal{M} \\rightarrow[0,1]$. We notice and exemplify in the figure that multiple PDFs can represent the same probability distribution. When PDFs represent the same probability distribution, we will use the terminology that they belong to the same equivalence class.\n![img-14.jpeg](img-14.jpeg)\n\nFigure 12: The PDF on the left has a removable discontinuity, but it can be mapped to the PDF on the right, which is identical but continuous everywhere. In whatever interval taken within the domain, the area under the PDF is exactly the same for both of them.\n\nA non-removable discontinuity is the type of discontinuity that cannot be removed because the discontinuity affects the area below the PDF and therefore all the PDFs in the same equivalence class present a discontinuity at $x_{0}$, as illustrated in Figure 13.\n\n[^0]\n[^0]:    6. We refer to (Capinski and Kopp, 2013) Chapter 2, definition 2.3, for coverage on measurable sets."
    },
    {
      "markdown": "![img-15.jpeg](img-15.jpeg)\n\nFigure 13: Example of a PDF with a non-removable discontinuity. The area under the PDF is affected by the discontinuity.\n\n# Appendix H. On the necessity of axis-aligned landmarks in the probability density $p_{Z}$ \n\nOur justification is built on a powerful result, Theorem 6 from Buchholz et al. (2022). We restate and revisit the intuitions behind the result below. We use the notation $\\Phi_{*} \\mathbb{P}$ to denote the pushforward of $\\mathbb{P}$ under $\\Phi$.\n\nTheorem 29 Let $p_{Z}$ be a twice differentiable probability density with bounded gradient. Suppose that $x=\\Phi(z)$ where the distribution $\\mathbb{P}$ of $z$ has density $p_{Z}$ and $\\Phi$ is a diffeomorphism with $\\operatorname{det} D \\Phi(z)=1$ for $z \\in \\mathbb{R}^{d}$. Then there is a family of functions $\\Phi_{t}: \\mathbb{R} \\times \\mathbb{R}^{d} \\rightarrow \\mathbb{R}^{d}$ indexed by $t$ with $\\Phi_{0}=q$ and $q_{t} \\neq \\Phi_{0}$ for $t \\neq 0$ such that $\\operatorname{det} D \\Phi_{t}(z)=1$ and $\\left(\\Phi_{t}\\right)_{*} \\mathbb{P}=\\Phi_{*} \\mathbb{P}$.\n\nConsider the case when $\\Phi$ is an identity map. In that case, the above theorem implies that there exists a family of volume preserving transformations different from the identity map such that $\\left(\\Phi_{t}\\right)_{*} \\mathbb{P}=\\mathbb{P}$.\n\nWe revisit the intuition behind the proof, which will be reused for the rest of this section. We define the flow of a vector field as a map $\\Phi: \\mathbb{R} \\times \\mathbb{R}^{d} \\rightarrow \\mathbb{R}^{d}$ such that\n\n$$\n\\Phi_{0}(z)=z, \\partial_{t} \\Phi_{t}(z)=X\\left(\\Phi_{t}(z)\\right)\n$$\n\nWe write $\\Phi(t, z)$ as $\\Phi_{t}(z)$ in the above expression. We can interpret $\\Phi_{t}(z)$ as the position of a particle, which started at $z$ at time $t=0$. Then $X\\left(\\Phi_{t}(z)\\right)$ is the velocity of the particle at time $t$. Define a vector field $X^{i j}: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}^{d}$ as\n\n$$\nX_{k}^{i j}= \\begin{cases}\\partial p_{j} & k=i \\\\ -\\partial p_{i} & k=j \\\\ 0 & \\text { otherwise }\\end{cases}\n$$\n\nwhere $X_{k}^{i j}$ is the $k^{t h}$ component of the vector field. Observe that $X^{i j}$ is orthogonal to the isolines of the density $p_{Z}$ along the plane corresponding to components $i$ and $j$. Under the flows described in equation (4), the probability measures evolve in time and satisfy the continuity equation stated next. Formally stated, the density associated with $\\left(q_{t}\\right)_{*} \\mathbb{P}$ satisfies.\n\n$$\n\\partial p_{t}+\\operatorname{Div}\\left(p_{t} X^{i j}\\right)=0, \\quad p_{0}=p\n$$"
    },
    {
      "markdown": "![img-16.jpeg](img-16.jpeg)\n\nFigure 14: Two dimensional illustration to explain why isolines cannot cross the axis.\n\nObserve that $\\operatorname{Div}\\left(X^{i j}\\right)=\\partial_{i} \\partial_{j} p-\\partial_{j} \\partial_{i} p=0$. Also, observe that $\\operatorname{Div}\\left(p X^{i j}\\right)=X_{i j} \\cdot \\nabla p=0$. From $\\operatorname{Div}\\left(p X^{i j}\\right)=0$, we can infer that the $\\left(q_{t}\\right)_{*} \\mathbb{P}=\\mathbb{P}$ and from $\\operatorname{Div}\\left(X^{i j}\\right)=0, \\Phi_{t}$ is a volume preserving diffeomorphism. Consider the following autoencoder where the decoder is $\\hat{f}_{t}=f \\circ q_{t}$ and the encoder is $\\tilde{g}_{t}=q_{t}^{-1} \\circ f^{-1}$. Observe that these autoencoders achieve perfect reconstruction. The encoder fails at identifying the underlying latents as the estimated latents are related to the true latents by the map $\\Phi_{t}^{-1}(\\cdot)$. Thus we have seen that even if the learner knows the true density $p_{Z}$, there exists a family of autoencoders that cannot achieve identification.\n\nSuppose $Z=\\left[Z_{1}, Z_{2}\\right]$. Consider that $p_{Z}$ is a density defined over the unit square centered at origin. In Figure 14, we show one such density. Suppose we are interested in separating the points in the four quadrants, where each quadrant provides a distinct quantization for all the values of $z$ assumed in it. We consider a family of densities $p_{Z}$, whose isolines crosses $z_{1}=0$ at least once. We further also assume that these densities $p_{Z}$ are differentiable over the support and have a bounded gradient. The isoline of this density crosses $z_{1}=0$. Consider two points shown in pink and blue colors in Figure 14. At $t=0$, the pink point is to the right of $z_{1}=0$ and the blue point is to the left of $z_{1}=0$. Under the flow defined in equation (4), we would argue that after some time $\\tau$ has elapsed, the pink point moves to the left of $z_{1}=0$, while the blue point is still to the left of $z_{1}=0$. Under the map $\\Phi_{\\tau}(z)=\\left(q_{\\tau}^{1}(z), q_{\\tau}^{2}(z)\\right)$, the two points are both to the left of $z_{1}=0$. If all the points $\\Phi_{\\tau}^{1}(z)<0$ are associated with the same quantization, then the pink point and the blue point are associated with same quantization, while their true quantization is different. We provide further details on the construction. We can assume that the pink point at $t=0$ is very close to $z_{1}=0$ but on the right of $z_{1}=0$. Further, we assume that the magnitude of the flow along the negative $z_{1}$ direction (which is the vector tangent to the isoline) in the neighborhood of the pink point at $t=0$ is bounded below by at least $\\epsilon_{1}$. As a result, the point moves at least $\\tau \\epsilon$ distance along $z_{1}$ in time $\\tau$. We can assume that the pink point started with $z_{1}<\\epsilon \\tau$ and thus it crosses to the left of $z_{1}=0$ after $\\tau$ amount of time has elapsed. At the same time, consider the blue point to the left of $z_{1}=0$ at time $t=0$. We assume that the flow along the $z_{1}$ direction in the $\\rho$ radius neighborhood of the blue point at $t=0$ points from right to left, i.e., it is in the negative $z_{1}$ direction. We choose $\\tau$ to be sufficiently small, $\\tau<\\rho / \\gamma$, where $\\gamma$ is the largest value that the velocity under the flow can take (this follows from the assumption that the density has a bounded gradient). Under this constraint, the blue point stays to the left of $z_{1}=0$ after $\\tau$ amount of time has elapsed."
    },
    {
      "markdown": "The above argument explains that if the isolines of $p_{Z}$ cross the grids, then quantized identification is not possible even if we know $p_{Z}$. As a result, we need to focus on the densities $p_{Z}$ whose isolines are restricted to each of the four quadrants. Consider the family of densities $p_{Z}$ with axis-aligned discontinuities. Suppose the density in each of the quadrants is continuous, then for this class of densities the isolines cannot cross any of the axis $z_{1}=0$ and $z_{2}=0$. For this class of densities, our theory established quantized factor identification guarantees even without requiring knowledge of $p_{Z}$. Could there be other densities beyond discontinuous densities with axis-aligned landmarks that permit quantized identification? This is a fairly non-trivial and important question left for future work."
    }
  ],
  "usage_info": {
    "pages_processed": 39,
    "doc_size_bytes": 3668622
  },
  "_metadata": {
    "model_version": "0.0.0",
    "llm_model": "mistral-ocr-2505",
    "query_data": {
      "pdf": "/Users/satyaortiz-gagne/CODE/paperext/data/cache/fulltext/bdf256716856943743e485c013d5a07c/fulltext.pdf"
    },
    "model_id": "parsepdf"
  }
}