{
  "pages": [
    {
      "markdown": "# Lower and Upper Bounds on the Pseudo-Dimension of Tensor Network Models \n\nBehnoush Khavari<br>DIRO \\& Mila<br>Université de Montréal<br>behnoush.khavari@umontreal.ca\n\nGuillaume Rabusseau<br>DIRO \\& Mila, CIFAR AI Chair<br>Université de Montréal<br>grabus@iro.umontreal.ca\n\n\n#### Abstract\n\nTensor network (TN) methods have been a key ingredient of advances in condensed matter physics and have recently sparked interest in the machine learning community for their ability to compactly represent very high-dimensional objects. TN methods can for example be used to efficiently learn linear models in exponentially large feature spaces [56]. In this work, we derive upper and lower bounds on the VC-dimension and pseudo-dimension of a large class of TN models for classification, regression and completion. Our upper bounds hold for linear models parameterized by arbitrary TN structures, and we derive lower bounds for common tensor decomposition models (CP, Tensor Train, Tensor Ring and Tucker) showing the tightness of our general upper bound. These results are used to derive a generalization bound which can be applied to classification with low-rank matrices as well as linear classifiers based on any of the commonly used tensor decomposition models. As a corollary of our results, we obtain a bound on the VC-dimension of the matrix product state classifier introduced in [56] as a function of the so-called bond dimension (i.e. tensor train rank), which answers an open problem listed by Cirac, Garre-Rubio and Pérez-García in [13].\n\n\n## 1 Introduction\n\nTensor networks (TNs) have emerged in the quantum physics community as a mean to compactly represent wave functions of large quantum systems [45, 5, 52]. Their introduction in physics can be traced back to the work of Penrose [47] and Feynman [15]. Akin to matrix factorization, TN methods rely on factorizing a high-order tensor into small factors and have recently gained interest from the machine learning community for their ability to efficiently represent and perform operations on very high-dimensional data and high-order tensors. They have been for example successfully used for compressing models [43, 69, 42, 29, 70], developing new insights on the expressiveness of deep neural networks [14, 31] and designing novel approaches to supervised [56, 18] and unsupervised [55, 25, 39] learning. Most of these approaches leverage the fact that TN can be used to efficiently parameterize high-dimensional linear maps, which is appealing from two perspectives: it makes it possible to learn models in exponentially large feature spaces and it acts as a regularizer, controlling the capacity of the class of hypotheses considered for learning.\nWhile the expressive power of TN models has been studied recently [17, 2], the focus has mainly been on the representation capacity of TN models, but not on their ability to generalize in the context of supervised learning tasks. In this work, we study the generalization ability of TN models by deriving lower and upper bounds on the VC-dimension and pseudo-dimension of TN models commonly used for classification, completion and regression, from which bounds on the generalization gap of TN models can be derived. Using the general framework of tensor networks, we derive a general upper bound for models parameterized by arbitrary TN structures, which applies to all commonly used tensor decomposition models [20] such as CP [27], Tucker [59] and tensor train (TT)) [46], as well"
    },
    {
      "markdown": "as more sophisticated structures including hierarchical Tucker [19, 23], tensor ring (TR) [73] and projected entangled state pairs (PEPS) [60].\nOur analysis proceeds mainly in two steps. First, we formally define the notion of TN learning model by disentangling the underlying graph structure of a TN from its parameters (the core tensors, or factors, involved in the decomposition). This allows us to define, in a conceptually simple way, the hypothesis class $\\mathcal{H}_{G}$ corresponding to the family of linear models whose weights are represented using an arbitrary TN structure $G$. We then proceed to deriving upper bounds on the VC/pseudodimension and generalization error of the class $\\mathcal{H}_{G}$. These bounds follow from a classical result from Warren [66] which was previously used to obtain generalization bounds for neural networks [3], matrix completion [54] and tensor completion [41]. The bounds we derive naturally relate the capacity of $\\mathcal{H}_{G}$ to the underlying graph structure $G$ through the number of nodes and effective number of parameters of the TN. To assess the tightness of our general upper bound, we derive lower bounds for particular TN structures (rank-one, CP, Tucker, TT and TR). These lower bounds show that, for completion, regression and classification, our general upper bound is tight up to a log factor for rank-one, TT and TR tensors, and is tight up to a constant for matrices. Lastly, as a corollary of our results, we obtain a bound on the VC-dimension of the tensor train classifier introduced in [56], which answers one of the open problems listed by Cirac, Garre-Rubio and Pérez-García in [13].\n\nRelated work Machine learning models using low-rank parametrization of the weights have been investigated (mainly from a practical perspective) for various decomposition models, including low-rank matrices [36, 49, 67], CP [1, 37, 7], Tucker [35, 16, 26, 50], tensor train [48, 10, 44, 56, 18, $53,11,65,68]$ and PEPS [12]. From a more theoretical perspective, generalization bounds for matrix and tensor completion have been derived in [54, 41] (based on the Tucker format for the tensor case). A bound on the VC-dimension of low-rank matrix classifiers was derived in [67] and a bound on the pseudo-dimension of regression functions whose weights have low Tucker rank was given in [50] (for both these cases, we show that our results improve over these previous bounds, see Section 4.2). To the best of our knowledge the VC-dimension of tensor train classifiers has not been studied in the past, but the statistical consistency of the convex relaxation of the tensor completion problem was studied in $[58,57]$ for the Tucker decomposition and in [28] for the tensor train decomposition. Lastly, in [38] the authors study the complexity of learning with tree tensor networks using the notion of metric entropy and covering numbers. They provide generalization bounds which are qualitatively similar to ours, but their results only hold for TN structures whose underlying graph is a tree (thus excluding models such as CP, tensor ring and PEPS) and they do not provide lower bounds.\n\nSummary of contributions We introduce a unifying framework for TN-based learning models, which generalizes a wide range of models based on tensor factorization for completion, classification and regression. This framework allows us to consider the class $\\mathcal{H}_{G}$ of low-rank TN models for a given arbitrary TN structure $G$ (Section 3). We provide general upper bounds on the pseudo-dimension and $V C$-dimension of the hypothesis class $\\mathcal{H}_{G}$ for arbitrary $T N$ structure $G$ for regression, classification and completion. Our results naturally relate the capacity of $\\mathcal{H}_{G}$ to the number of parameters of the underlying TN structure $G$ (Section 4.1). From these results, we derive a generalization bound for TN-based classifiers parameterized by arbitrary TN structures (Theorem 4). We compare our results to previous bounds for specific decomposition models and show that our general upper bound is always of the same order and sometimes even improves on previous bounds (Section 4.2). We derive several lower bounds showing that our general upper bound is tight up to a log factor for particular TN structures (Section 5). A summary of the lower bounds derived in this work, as well as upper bounds implied by our general result for particular TN structures, can be found in Table 1 at the end of the paper.\n\n# 2 Preliminaries \n\nIn this section, we present basic notions of tensor algebra and tensor networks as well as generalization bounds based on combinatorial complexity measures. We start by introducing some notations. For any integer $k$ we use $[k]$ to denote the set of integers from 1 to $k$. We use lower case bold letters for vectors (e.g. $\\mathbf{v} \\in \\mathbb{R}^{d_{1}}$ ), upper case bold letters for matrices (e.g. $\\mathbf{M} \\in \\mathbb{R}^{d_{1} \\times d_{2}}$ ) and bold calligraphic letters for higher order tensors (e.g. $\\mathcal{T} \\in \\mathbb{R}^{d_{1} \\times d_{2} \\times d_{3}}$ ). The inner product of two $k$-th order tensors $\\mathcal{S}, \\mathcal{T} \\in \\mathbb{R}^{d_{1} \\times \\cdots \\times d_{k}}$ is defined by $\\langle\\mathcal{T}, \\mathcal{S}\\rangle=\\sum_{i_{1}=1}^{d_{1}} \\cdots \\sum_{i_{k}=1}^{d_{k}} \\mathcal{T}_{i_{1} \\ldots i_{k}} \\mathcal{S}_{i_{1} \\ldots i_{k}}$. The outer product of"
    },
    {
      "markdown": "Figure 1: Tensor network representation of common operations on matrices and vectors.\ntwo vectors $\\mathbf{u} \\in \\mathbb{R}^{d_{1}}$ and $\\mathbf{v} \\in \\mathbb{R}^{d_{2}}$ is denoted by $\\mathbf{u} \\otimes \\mathbf{v} \\in \\mathbb{R}^{d_{1} \\times d_{2}}$ with elements $(\\mathbf{u} \\otimes \\mathbf{v})_{i, j}=\\mathbf{u}_{i} \\mathbf{v}_{j}$. The outer product generalizes to an arbitrary number of vectors. We use the notation $\\left(\\mathbb{R}^{d}\\right)^{\\otimes p}$ to denote the space of $p$-th order hypercubic tensors of size $d \\times d \\times \\cdots \\times d$. We denote by $\\mathcal{Y}^{\\mathcal{X}}$ the space of functions $f: \\mathcal{X} \\mapsto \\mathcal{Y} . \\operatorname{sign}(\\cdot)$ stands for the sign function. Finally, given a graph $G=(V, E)$ and a vertex $v \\in V$, we denote by $E_{v}=\\{e \\in E \\mid v \\in e\\}$ the set of edges incident to the vertex $v$.\n\n# 2.1 Tensors and Tensor Networks \n\nTensor networks A tensor $\\mathcal{T} \\in \\mathbb{R}^{d_{1} \\times \\cdots \\times d_{p}}$ can simply be seen as a multidimensional array $\\left(\\mathcal{T}_{i_{1}, \\cdots, i_{p}}: i_{n} \\in\\left[d_{n}\\right], n \\in[p]\\right)$. Complex operations on tensors can be intuitively represented using the graphical notation of tensor network (TN) diagrams [5, 45]. In tensor networks, a $p$-th order tensor is illustrated as a node with $p$ edges (or legs) in a graph $\\stackrel{\\text { (i) }}{=} \\stackrel{\\text { (j) }}{=}$. An edge between two nodes of a TN represents a contraction over the corresponding modes of the two tensors. Consider the following simple TN with two nodes: $\\stackrel{\\text { (i) }}{=} \\stackrel{\\text { (j) }}{=} \\stackrel{\\text { (k) }}{=}$. The first node represents a matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ and the second one a vector $\\mathbf{x} \\in \\mathbb{R}^{n}$. Since this TN has one dangling leg (i.e. an edge which is not connected to any other node), it represents a first order tensor, i.e. a vector. The edge between the second leg of $\\mathbf{A}$ and the leg of $\\mathbf{x}$ corresponds to a contraction between the second mode of $\\mathbf{A}$ and the first mode of $\\mathbf{x}$. Hence, the resulting TN represents the classical matrix-product, which can be seen by calculating the $i$-th component of this TN: $i-\\stackrel{\\text { (i) }}{=} \\stackrel{\\text { (j) }}{=}=\\sum_{j} \\mathbf{A}_{i j} \\mathbf{x}_{j}=(\\mathbf{A x})_{i}$. Other examples of TN representations of common operations on matrices and vectors can be found in Figure 1. A special case of TN is the tensor train decomposition [46] which factorizes a $n$-th order tensor $\\mathcal{T}$ in the form $\\stackrel{\\text { (i) }}{=} \\stackrel{\\text { (j) }}{=} \\stackrel{\\text { (k) }}{=} \\stackrel{\\text { (j) }}{=} \\stackrel{\\text { (k) }}{=} \\stackrel{\\text { (j) }}{=} \\stackrel{\\text { (k) }}{=} \\text {. This corresponds to }$\n\n$$\n\\mathcal{T}_{i_{1}, i_{2}, \\ldots, i_{n}}=\\sum_{\\alpha_{1}=1}^{r_{1}} \\cdots \\sum_{\\alpha_{n-1}=1}^{r_{n-1}}\\left(\\mathcal{G}_{1}\\right)_{i_{1}, \\alpha_{1}}\\left(\\mathcal{G}_{2}\\right)_{\\alpha_{1}, i_{2}, \\alpha_{2}} \\cdots\\left(\\mathcal{G}_{n-1}\\right)_{\\alpha_{n-2}, i_{n-1}, \\alpha_{n-1}}\\left(\\mathcal{G}_{n}\\right)_{\\alpha_{n-1}, i_{n}}\n$$\n\nwhere the tuple $\\left(r_{i}\\right)_{i=1}^{n-1}$ associated with the TT representation is called TT-rank.\nTensor network structures A tensor network (TN) can be fundamentally decomposed in two constituent parts: a tensor network structure, which describes its graphical structure, and a set of core tensors assigned to each node. For example, the tensor in $\\mathbb{R}^{d_{1} \\times d_{2} \\times d_{3} \\times d_{4}}$ represented by the TN $\\stackrel{\\text { (i) }}{=} \\stackrel{\\text { (j) }}{=} \\stackrel{\\text { (j) }}{=}$ is obtained by assigning the core tensors $\\mathcal{T} \\in \\mathbb{R}^{d_{1} \\times d_{2} \\times R}$ and $\\mathcal{S} \\in \\mathbb{R}^{R \\times d_{3} \\times d_{4}}$ to the nodes of the TN structure $\\stackrel{\\text { (i) }}{=} \\stackrel{\\text { (j) }}{=}$.\nFormally, a tensor network structure is given by a graph $G=(V, E, \\operatorname{dim})$ where edges are labeled by integers: $V$ is the set of vertices, $E \\subset V \\cup(V \\times V)$ is a set of edges containing both classical edges $(e \\in V \\times V)$ and singleton edges $(e \\in V)$ and $\\operatorname{dim}: E \\rightarrow \\mathbb{N}$ assigns a dimension to each edge in the graph. The set of singleton edges $\\delta_{G}=E \\cap V$ corresponds to the dangling legs of a TN. Given a TN structure $G$, one obtains a tensor by assigning a core tensor $\\mathcal{T}^{v} \\in \\bigotimes_{e \\in E_{v}} \\mathbb{R}^{\\operatorname{dim}(e)}$ to each vertex $v$ in the graph, where $E_{v}=\\{e \\in E \\mid v \\in e\\}$. The resulting tensor, denoted by $T N\\left(G,\\left\\{\\mathcal{T}^{v}\\right\\}_{v \\in V}\\right)$, is a tensor of order $\\left|\\delta_{G}\\right|$ in the tensor product space $\\bigotimes_{e \\in \\delta_{G}} \\mathbb{R}^{\\operatorname{dim}(e)}$. Given a tensor structure $G=(V, E, \\operatorname{dim})$, the set of all tensors that can be obtained by assigning core tensors to the vertices of $G$ is denoted by $\\mathcal{T}(G) \\subset \\bigotimes_{e \\in \\delta_{G}} \\mathbb{R}^{\\operatorname{dim}(e)}$ :\n\n$$\n\\mathcal{T}(G)=\\left\\{T N\\left(G,\\left\\{\\mathcal{T}^{v}\\right\\}_{v \\in V}\\right): \\mathcal{T}^{v} \\in \\bigotimes_{e \\in E_{v}} \\mathbb{R}^{\\operatorname{dim}(e)}, v \\in V\\right\\}\n$$"
    },
    {
      "markdown": "![img-0.jpeg](img-0.jpeg)\n\nFigure 2: TN representation of common decomposition models for 4th order and 9th order tensors. For CP, the black dot represents a hyperedge corresponding to a joint contraction over 4 indices.\n\nAs an illustration, one can check that the set of $m \\times n$ matrices of rank at most $r$ is equal to $\\mathcal{T}(\\otimes \\otimes)$. Similarly, the set of 4 th order $d$-dimensional tensors of TT rank at most $r$ is equal to $\\mathcal{T}(\\otimes \\otimes \\otimes)$.\n\nFinally, for a given graph structure $G$, the number of parameters of any member of the family $\\mathcal{T}(G)$ in Equation (2) (which is the total number of entries of the core tensors $\\left\\{\\mathcal{T}^{c}\\right\\}_{v \\in V}$ ) is given by\n\n$$\nN_{G}=\\sum_{v \\in V} \\prod_{e \\in E_{v}} \\operatorname{dim}(e)\n$$\n\nThis will be a central quantity in the generalization bounds and bounds on the VC-dimension of TN models we derive in Section 4.\n\nCommon tensor network structures In Figure 2, we show the tensor network structures associated with classical tensor decomposition models such as CP, Tucker [59] and tensor train (TT) [46], also known as matrix product state (MPS) [45, 52]. For the case of the Candecomp/Parafac (CP) decomposition [27], note that the TN structure is a hyper-graph rather than a graph. We introduced the notion of TN structure focusing on graphs for clarity of exposition in the previous paragraph, but our formalism and results can be straightforwardly extended to hyper-graph TN structures. In addition, we include the tensor ring (TR) [73] (also known as periodic MPS) and PEPS decompositions which have initially emerged in quantum physics and recently gained interest in the machine learning community (see e.g., [12, 62, 63, 71]). We also show the hierarchical Tucker decomposition initially introduced in $[19,23]$.\n\n# 2.2 Generalization Bound and Complexity Measures \n\nThe goal of supervised learning is to learn a function $f$ mapping inputs $x \\in \\mathcal{X}$ to outputs $y \\in \\mathcal{Y}$ from a sample of input-output examples $S=\\left\\{\\left(x_{1}, y_{1}\\right), \\cdots,\\left(x_{n}, y_{n}\\right)\\right\\}$ drawn independently and identically (i.i.d.) from an unknown distribution $D$, where each $y_{i} \\simeq f\\left(x_{i}\\right)$. Given a set of hypotheses $\\mathcal{H} \\subset \\mathcal{Y}^{\\mathcal{X}}$, one natural objective is to find the hypothesis $h \\in \\mathcal{H}$ minimizing the risk $R(h)=\\mathbb{E}_{(x, y) \\sim D} \\ell(h(x), y)$ where $\\ell: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}_{+}$is a loss function measuring the quality of the predictions made by $h$. However, since the distribution $D$ is unknown, machine learning algorithms often rely on the empirical risk minimization principle which consists in finding the hypothesis $h \\in \\mathcal{H}$ that minimizes the empirical risk $\\hat{R}_{S}(h)=\\frac{1}{n} \\sum_{i=1}^{n} \\ell\\left(h\\left(x_{i}\\right), y_{i}\\right)$. It is easy to see that the empirical risk is an unbiased estimator of the risk and one of the concerns of learning theory is to provide guarantees on the quality of this estimator. Such guarantees include generalization bounds, which are probabilistic bounds on the generalization gap $R(h)-\\hat{R}_{S}(h)$. The generalization gap naturally depends on the size of the sample $S$, but also on the richness (or capacity, complexity) of the hypothesis class $\\mathcal{H}$.\nIn this work, our focus is on uniform generalization bounds, which bound the generalization gap uniformly for any hypothesis $h \\in \\mathcal{H}$ as a function of the size of the training sample and of the complexity of the hypothesis class $\\mathcal{H}$. While there are many ways of measuring the complexity of $\\mathcal{H}$, including VC-dimension, Rademacher complexity, metric entropy and covering numbers, we focus on the $V C$-dimension for classification tasks and its counterpart for real-valued functions, the pseudo-dimension, for completion and regression tasks."
    },
    {
      "markdown": "Definition 1. Let $\\mathcal{H} \\subset\\{-1,+1\\}^{\\mathcal{X}}$ be a hypothesis class. The growth function $\\Pi_{\\mathcal{H}}: \\mathbb{N} \\rightarrow \\mathbb{N}$ of $\\mathcal{H}$ is defined by\n\n$$\n\\Pi_{\\mathcal{H}}(n)=\\sup _{S=\\left\\{x_{1}, \\ldots, s_{n}\\right\\} \\subset \\mathcal{X}}\\left|\\left\\{\\left(h\\left(x_{1}\\right), \\ldots, h\\left(x_{n}\\right)\\right) \\mid h \\in \\mathcal{H}\\right\\}\\right|\n$$\n\nThe VC-dimension of $\\mathcal{H}, d_{\\mathrm{VC}}(\\mathcal{H})$, is the largest number of points $x_{1}, \\cdots, x_{n}$ shattered by $\\mathcal{H}$, i.e., for which $\\left|\\left\\{\\left(h\\left(x_{1}\\right), \\ldots, h\\left(x_{n}\\right)\\right) \\mid h \\in \\mathcal{H}\\right\\}\\right|=2^{n}$. In other words: $d_{\\mathrm{VC}}(\\mathcal{H})=\\sup \\left\\{n \\mid \\Pi_{\\mathcal{H}}(n)=2^{n}\\right\\}$.\n\nFor a real-valued hypothesis class $\\mathcal{H} \\subset \\mathbb{R}^{\\mathcal{X}}$, we say that $\\mathcal{H}$ pseudo-shatters the points $x_{1}, \\ldots, x_{n} \\in \\mathcal{X}$ with thresholds $t_{1}, \\ldots, t_{n} \\in \\mathbb{R}$, if for every binary labeling of the points $\\left(s_{1}, \\ldots, s_{n}\\right) \\in\\{-1,+1\\}^{n}$, there exists $h \\in \\mathcal{H}$ s.t. $h\\left(x_{i}\\right)<t$, if and only if $s_{i}=-1$.\nThe pseudo-dimension of a real-valued hypothesis class $\\mathcal{H} \\subset \\mathbb{R}^{\\mathcal{X}}, \\operatorname{Pdim}(\\mathcal{H})$, is the supremum over $n$ for which there exist $n$ points that are pseudo-shattered by $\\mathcal{H}$ (with some thresholds).\n\nPseudo-dimension and VC-dimension are combinatorial measures of complexity (or capacity) which can be used to derive classical uniform generalization bounds over a hypothesis class (see, e.g., [6, 40,3]). By definition, the pseudo-dimension is related to the notion of VC-dimension by the relation\n\n$$\n\\operatorname{Pdim}(\\mathcal{H})=d_{\\mathrm{VC}}\\left(\\{(x, t) \\mapsto \\operatorname{sign}(h(x)-t) \\mid h \\in \\mathcal{H}\\}\\right)\n$$\n\nwhich holds for any $\\mathcal{H} \\subset \\mathbb{R}^{\\mathcal{X}}$.\n\n# 3 Tensor Networks for Supervised Learning \n\nIn this section, we formalize the general notion of tensor network models. We then show how it encompasses classical models such as low-rank matrix completion [8, 9, 22, 51], classification [36, 49, 67], and tensor train based models [56, 18, 53, 11, 65, 68].\n\n### 3.1 Tensor Network Learning Models\n\nConsider a classification problem where the input space $\\mathcal{X}$ is the space of $p$-th order tensors $\\mathbb{R}^{d_{1} \\times d_{2} \\times \\cdots \\times d_{p}}$. One motivation for TN models is that the tensor product space $\\mathcal{X}$ can be exponentially large, thus learning a linear model in this space is often not feasible. Indeed, the number of parameters of a linear classifier $h: \\mathcal{X} \\mapsto \\operatorname{sign}\\left(\\langle\\mathcal{X}, \\boldsymbol{\\mathcal { W }}\\rangle\\right)$, where $\\boldsymbol{\\mathcal { W }} \\in \\mathbb{R}^{d_{1} \\times \\cdots \\times d_{p}}$ is the tensor weight parameters, grows exponentially with $p$. TN models parameterize $\\boldsymbol{\\mathcal { W }}$ as a low-rank TN, thus reducing the number of parameters needed to represent a model $h$. Our objective is to derive generalization bounds for the class of such hypotheses parameterized by low-rank tensor networks for classification, regression and completion tasks.\n\nFormally, let $G=(V, E, \\operatorname{dim})$ be a TN structure for tensors of shape $d_{1} \\times \\cdots \\times d_{p}$, i.e. where the set of singleton edges $\\delta_{G}=E \\cap V=\\left\\{v_{1}, \\cdots, v_{p}\\right\\}$ and $\\operatorname{dim}\\left(v_{i}\\right)=d_{i}$ for each $i \\in[p]$. We are interested in the class of models whose weight tensors are represented in the TN structure $G$ :\n\n$$\n\\begin{aligned}\n\\mathcal{H}_{G}^{\\text {regression }}=\\{h: \\boldsymbol{\\mathcal { X } \\mapsto }\\langle\\boldsymbol{\\mathcal { W }}, \\boldsymbol{\\mathcal { X }}\\rangle \\mid \\boldsymbol{\\mathcal { W }} \\in \\mathcal{T}(G)\\} \\\\\n\\mathcal{H}_{G}^{\\text {classif }}=\\{h: \\boldsymbol{\\mathcal { X } \\mapsto \\operatorname { s i g n } ( \\langle \\boldsymbol { W } , \\boldsymbol { X } \\rangle ) \\mid \\boldsymbol { W }} \\in \\mathcal{T}(G)\\} \\\\\n\\mathcal{H}_{G}^{\\text {completion }}=\\left\\{h:\\left(i_{1}, \\cdots, i_{p}\\right) \\mapsto \\boldsymbol{\\mathcal { W }}_{i_{1}, \\cdots, i_{p}} \\mid \\boldsymbol{\\mathcal { W }} \\in \\mathcal{T}(G)\\right\\}\n\\end{aligned}\n$$\n\nIn Equation (6) for the completion hypothesis class, $p$-th order tensors are interpreted as real-valued functions $f:\\left[d_{1}\\right] \\times \\cdots \\times\\left[d_{p}\\right] \\mapsto \\mathbb{R}$ over the indices of the tensor. $\\mathcal{H}_{G}^{\\text {completion }}$ is thus a class of functions over the indices domain, for which the notion of pseudo-dimension is well-defined. This treatment of completion as a supervised learning task was considered previously to derive generalization bounds for matrix and tensor completion [54, 41].\n\nThe benefit of TN models comes from the drastic reduction in parameters when the TN structure $G$ is low-rank, in the sense that the number of parameters $N_{G}$ is small compared to $d_{1} d_{2} \\cdots d_{p}$. In addition to allowing one to represent linear models in exponentially large spaces, this compression controls the capacity of the corresponding hypothesis class $\\mathcal{H}_{G}$.\n\n### 3.2 Examples\n\nTo illustrate some TN models, we now present several examples of models based on common TN structures: low-rank matrices and tensor trains."
    },
    {
      "markdown": "Low-rank matrices As discussed in Section 2.1, if we define the TN structure\n\n$$\nG_{\\text {mat }}(r)=\\otimes \\otimes \\otimes\n$$\n\nthen $\\mathcal{T}\\left(G_{\\text {mat }}(r)\\right)$ is the set of matrices in $\\mathbb{R}^{d_{1} \\times d_{2}}$ of rank at most $r$. The hypothesis class $\\mathcal{H}_{G_{\\text {mat }}(r)}^{\\text {completion }}$ then corresponds to the classical problem of low-rank matrix completion [8, 9, 22, 51]. Similarly $\\mathcal{H}_{G_{\\text {mat }}(r)}^{\\text {classif }}$ corresponds to the hypothesis class of low-rank matrix classifiers. This hypothesis class was previously considered, notably to compactly represent the parameters of support vector machines for matrix inputs [36, 49, 67]. Lastly, for the regression case, $\\mathcal{H}_{G_{\\text {mat }}(r)}^{\\text {regression }}$ is the set of functions $\\left\\{h: \\mathbf{X} \\mapsto \\operatorname{Tr}\\left(\\mathbf{W} \\mathbf{X}^{\\top}\\right) \\mid \\operatorname{rank}(\\mathbf{W}) \\leq r\\right\\}$. Learning hypotheses from this class is relevant in, e.g., quantum tomography, where it is known as the low-rank trace regression problem [24, 64, 30, 33].\n\nTensor train tensors The tensor train (TT) decomposition model [46] also known as matrix product state (MPS) in the quantum physics community [45, 52], has a number of parameters that grows only linearly with the order of the tensor. This makes the TT format an appealing model for compressing the parameters of ML models [56, 44, 17, 43]. We now present the tensor train classifier model which was introduced in [56] and subsequently explored in [18]. Given a vector input $\\mathbf{x} \\in \\mathbb{R}^{p}$, Stoudenmire and Schwab [56] propose to map $\\mathbf{x}$ into a high-dimensional space of $p$-th order tensors $\\mathcal{X}=\\mathbb{R}^{d \\times \\cdots \\times d}$ by applying a local feature map $\\phi: \\mathbb{R} \\rightarrow \\mathbb{R}^{d}$ to each component of the vector $\\mathbf{x}$ and taking their outer product: $\\Phi(\\mathbf{x})=\\phi\\left(\\mathbf{x}_{1}\\right) \\otimes \\phi\\left(\\mathbf{x}_{2}\\right) \\otimes \\cdots \\otimes \\phi\\left(\\mathbf{x}_{p}\\right) \\in\\left(\\mathbb{R}^{d}\\right)^{\\otimes p}$.\nInstead of relying on the so-called kernel trick, Stoudenmire and Schwab propose to directly learn the parameters $\\boldsymbol{\\mathcal { V }}$ of a linear model $h: \\mathbf{x} \\mapsto \\operatorname{sign}(\\langle\\mathcal{W}, \\Phi(\\mathbf{x})\\rangle)$ in the exponentially large feature space $\\mathcal{X}$. The learning problem is made tractable by paremeterizing $\\mathcal{W}$ as a low-rank TT tensor (see Equation (1)). Letting\n\n$$\nG_{\\mathrm{TT}}\\left(r_{1}, \\cdots, r_{p-1}\\right)=\\bigotimes_{i=1}^{n} \\bigotimes_{j=1}^{n} \\cdots \\bigotimes_{p-1}^{n} \\bigotimes_{j=1}^{n}\n$$\n\nthe hypothesis class considered in [56] is $\\mathcal{H}_{G_{\\mathrm{TT}} \\mathrm{TT}\\left(r_{1}, \\cdots, r_{p-1}\\right)}^{\\text {classif }}$. In addition to the approach of [56], which was extended in [18] and [53], tensor train classifiers were also previously considered in [11, $65,68]$. Similarly, the hypothesis class $\\mathcal{H}_{G_{\\mathrm{TT}}\\left(r_{1}, \\cdots, r_{p-1}\\right)}^{\\text {completion }}$ corresponds to the low-rank TT completion problem $[21,48,61]$.\n\nOther TN models Lastly, we mention that our formalism can be applied to any tensor models having a low-rank structure, including CP, Tucker, tensor ring and PEPS. As mentioned previously, for the case of the CP decomposition, the graph $G$ of the TN structure is in fact a hyper-graph with $|V|=p$ nodes and $N_{G}=p d r$ parameters for a weight tensor in $\\left(\\mathbb{R}^{d}\\right)^{\\otimes p}$ with CP rank at most $r$. Several TN learning models using these decomposition models have been proposed previously, including [26, 50] for regression in the Tucker format, [12] for classification using the PEPS model, $[37,7]$ for classification with the CP decomposition and $[62,72]$ for tensor completion with TR.\n\n# 4 Pseudo-dimension and Generalization Bounds for Tensor Network Models \n\nIn this section, we give a general upper bound on the VC-dimension and pseudo-dimension of hypothesis classes parameterized by arbitrary TN structures for regression, classification and completion. We then discuss corollaries of this general upper bound for common TN models including low-rank matrices and TT tensors, and compare them with existing results. Examples of particular upper bounds that can be derived from our general result can be found in Table 1.\n\n### 4.1 Upper Bounds on the VC-dimension, Pseudo-dimension and Generalization Gap\n\nThe following theorem states one of our main results which upper bounds the VC and pseudodimension of models parameterized by arbitrary TN structures.\nTheorem 2. Let $G=(V, E, \\operatorname{dim})$ be a tensor network structure and let $\\mathcal{H}_{G}^{\\text {regression }}, \\mathcal{H}_{G}^{\\text {classif }}, \\mathcal{H}_{G}^{\\text {completion }}$ be the corresponding hypothesis classes defined in Equations (4-6), where each model has $N_{G}$ parameters (see Equation (3)).\nThen, $\\operatorname{Pdim}\\left(\\mathcal{H}_{G}^{\\text {regression }}\\right), \\quad d_{\\mathrm{VC}}\\left(\\mathcal{H}_{G}^{\\text {classif }}\\right)$ and $\\operatorname{Pdim}\\left(\\mathcal{H}_{G}^{\\text {completion }}\\right)$ are all upper bounded by $2 N_{G} \\log (12|V|)$."
    },
    {
      "markdown": "These bounds naturally relate the capacity of the TN classes $\\mathcal{H}_{G}^{\\text {regression }}, \\mathcal{H}_{G}^{\\text {classif }}, \\mathcal{H}_{G}^{\\text {completion }}$ to the number of parameters $N_{G}$ of the underlying TN structure $G$. Following the analysis of [54] for matrix completion and its extension to the Tucker decomposition model presented in [41], the proof of this theorem leverages Warren's theorem which bounds the number of sign patterns a system of polynomial equations can take.\nTheorem 3 ([66]). The number of sign patterns of $n$ real polynomials, each of degree at most $v$, over $N$ variables is at most $\\left(\\frac{4 \\mathrm{vx} n}{N}\\right)^{N}$ for all $n>N>2$ (where $e$ is Euler's number).\n\nThe proof of Theorem 2 fundamentally relies on Warren's theorem to bound the number of sign patterns that can be achieved by hypotheses in $\\mathcal{H}_{G}^{\\text {regression }}$ on a set of $n$ input examples $\\boldsymbol{\\mathcal { X }}_{1}, \\cdots, \\boldsymbol{\\mathcal { X }}_{n}$. Indeed, the set of predictions $y_{i}=h\\left(\\boldsymbol{\\mathcal { X }}_{i}\\right)$ for $i \\in[n]$ realizable by hypotheses $h \\in \\mathcal{H}_{G}^{\\text {regression }}$ can be seen as a set of $n$ polynomials of degree $|V|$ over $N_{G}$ variables. The variables of the polynomials are the entries of the core tensors $\\left\\{\\mathcal{T}^{\\circ}\\right\\}_{v \\in V}$. The upper bound on the number of sign patterns obtained from Warren's theorem can then be leveraged to obtain a bound on the pseudo-dimension of the hypothesis class $\\mathcal{H}_{G}^{\\text {regression }}$, which in turn implies the upper bounds on $d_{\\mathrm{VC}}\\left(\\mathcal{H}_{G}^{\\text {classif }}\\right)$ and $\\operatorname{Pdim}\\left(\\mathcal{H}_{G}^{\\text {completion }}\\right)$. The complete proof of Theorem 2 can be found in Appendix A.1.1.\nNote that Theorem 2 implies that for a fixed number of parameters $N_{G}$, the VC-dimension grows with the number of vertices in the TN, thus a higher-order tensorization increases the capacity as measured by the VC dimension. This supports a common observation that higher-order tensorizations of high-dimensional data generally result in a model with better learning capacity.\nThe bounds on the VC-dimension and pseudo-dimension presented in Theorem 2 can be leveraged to derive bounds on the generalization error of the corresponding learning models; see for example [40]. In the following theorem, we derive such a generalization bound for classifiers parameterized by arbitrary TN structures.\nTheorem 4. Let $S$ be a sample of size $n$ drawn from a distribution $D$ and let $\\ell$ be a loss bounded by 1. Then, for any $\\delta>0$, with probability at least $1-\\delta$ over the choice of $S$, for any $h \\in \\mathcal{H}_{G}^{\\text {classif }}$,\n\n$$\nR(h)<\\hat{R}_{S}(h)+2 \\sqrt{\\frac{2}{n}\\left(N_{G} \\log \\frac{8 \\mathrm{~en}|V|}{N_{G}}+\\log \\frac{4}{\\delta}\\right)}\n$$\n\nThe proof of this theorem, which can be found in Appendix A.1.2, relies on a symmetrization lemma and a corollary of Hoeffding's inequality. It follows from this theorem that, with high probability, the generalization gap $R(h)-\\hat{R}_{S}(h)$ of any hypothesis $h \\in \\mathcal{H}_{G}^{\\text {classif }}$ is in $\\mathcal{O}\\left(\\sqrt{\\frac{N_{G} \\log (n)}{n}}\\right)$. This bound naturally relates the sample complexity of the hypothesis class with its expressiveness. The notion of richness of the hypothesis class appearing in this bound reflects the structure of the underlying TN through the number of parameters $N_{G}$. Using classical results (see, e.g., Theorem 10.6 in [40]), similar generalization bounds for regression and classification with arbitrary TN structures can be obtained from the bounds on the pseudo-dimension of $\\mathcal{H}_{G}^{\\text {regression }}$ and $\\mathcal{H}_{G}^{\\text {completion }}$ derived in Theorem 2. To examine this upper bound in practice, we perform an experiment with low-rank TT classifiers on synthetic data which can be found in Appendix B.\nIn the next subsection, we present corollaries of our results for particular TN structures, including low-rank matrix completion and the TT classifiers introduced in [56].\n\n# 4.2 Special cases \n\nWe now discuss special cases of Theorems 2 and 4 and compare them with existing results.\nLow-rank matrices Let $G_{\\text {mat }}(r)=\\frac{\\partial}{\\partial} \\partial \\partial \\partial$ and $\\mathcal{T}\\left(G_{\\text {mat }}(r)\\right)$ be the set of $d_{1} \\times d_{2}$ matrices of rank at most $r$. In this case, we have $|V|=2$ and $N_{G_{\\text {mat }(r)}}=r\\left(d_{1}+d_{2}\\right)$, and Theorems 2 and 4 give the following result.\nCorollary 5. $\\operatorname{Pdim}\\left(\\mathcal{H}_{G_{\\text {mat }}(r)}^{\\text {regression }}\\right), d_{\\mathrm{VC}}\\left(\\mathcal{H}_{G_{\\text {mat }}(r)}^{\\text {classif }}\\right)$ and $\\operatorname{Pdim}\\left(\\mathcal{H}_{G_{\\text {mat }}(r)}^{\\text {completion }}\\right)$ are all upper bounded by $10 r\\left(d_{1}+d_{2}\\right)$. Moreover, with high probability over the choice of a sample $S$ of size $n$ drawn i.i.d."
    },
    {
      "markdown": "from a distribution $D$, the generalization gap $R(h)-\\hat{R}_{S}(h)$ of any hypothesis $h \\in \\mathcal{H}_{G_{\\text {max }}(r)}^{\\text {classif }}$ is in $\\mathcal{O}\\left(\\sqrt{\\frac{r\\left(d_{1}+d_{2}\\right) \\log (n)}{n}}\\right)$.\n\nThis bound improves on the one given in [67] where the VC-dimension of $\\mathcal{H}_{G_{\\text {max }}(r)}^{\\text {classif }}$ is bounded by $r\\left(d_{1}+d_{2}\\right) \\log \\left(r\\left(d_{1}+d_{2}\\right)\\right)$ (see Theorem 2 in [67]). For the matrix completion case, our upper bound improves on the bound $\\operatorname{Pdim}\\left(\\mathcal{H}_{G_{\\text {max }}(r)}^{\\text {completion }}\\right) \\leq r\\left(d_{1}+d_{2}\\right) \\log \\frac{1 \\ln d_{1}}{r}$ derived in [54]. In Section 5, we will derive lower bounds showing that the upper bounds on the VC/pseudo-dimension of Corollary 5 are tight up to the constant factor 10 for matrix completion, regression and classification.\n\nTensor train Let $G_{\\mathrm{TT}}(r)=\\bigodot^{\\prime} \\bigodot^{\\prime} \\cdots \\cdot \\bigodot^{\\prime} \\bigodot^{\\prime} \\cdots$ of TT rank at most $r$. In this case, we have $|V|=p$ and $N_{G}=\\mathcal{O}\\left(d p r^{2}\\right)$ where $d=\\max _{i} d_{i}$. For this class of hypotheses, Theorems 2 and 4 give the following result.\nCorollary 6. $\\operatorname{Pdim}\\left(\\mathcal{H}_{G_{\\mathrm{TT}}(r)}^{\\text {regression }}\\right), d_{\\mathrm{VC}}\\left(\\mathcal{H}_{G_{\\mathrm{TT}}(r)}^{\\text {classif }}\\right)$ and $\\operatorname{Pdim}\\left(\\mathcal{H}_{G_{\\mathrm{TT}}(r)}^{\\text {completion }}\\right)$ are all in $\\mathcal{O}\\left(d p r^{2} \\log (p)\\right)$, where $d=\\max _{i} d_{i}$. Moreover, with high probability over the choice of a sample $S$ of size $n$ drawn i.i.d. from a distribution $D$, the generalization gap $R(h)-\\hat{R}_{S}(h)$ of any hypothesis $h \\in \\mathcal{H}_{G_{\\mathrm{TT}}(r)}^{\\text {classif }}$ is in $\\mathcal{O}\\left(\\sqrt{\\frac{d p r^{2} \\log (n)}{n}}\\right)$.\n\nThis result applies for the MPS model introduced in [56] and thus answers the open problem listed as Question 13 in [13]. To the best of our knowledge, the VC-dimension of tensor train classifier models has not been studied previously and our work is the first to address this open question. The lower bounds we derive in Section 5 show that the upper bounds on the VC/pseudo-dimension of Corollary 6 are tight up to a $\\mathcal{O}(\\log (p))$ factor.\n\nTucker We briefly compare our result with the ones proved in [41] for tensor completion and in [50] for tensor regression using the Tucker decomposition. For a Tucker decomposition with maximum rank $r$ for tensors of size $d_{1} \\times \\cdots \\times d_{p}$ with maximal dimension $d=\\max _{i} d_{i}$, the number of parameters is in $\\mathcal{O}\\left(r^{p}+d p r\\right)$ and the number of vertices in the TN structure is $p+1$. In this case, Theorems 2 and 4 show that the VC/pseudo-dimensions are in $\\mathcal{O}\\left(\\left(r^{p}+d p r\\right) \\log (p)\\right)$ and the generalization gap is in $\\mathcal{O}\\left(\\sqrt{\\frac{\\left(r^{p}+d p r\\right) \\log (n)}{n}}\\right)$ with high probability for any classifer parameterized by a low-rank Tucker tensor. It is worth observing that in contrast with the tensor train decomposition, all bounds have an exponential dependency on the tensor order $p$. In [41], the authors give an upper bound on the analogue of the growth function for tensor completion problems which is equivalent to ours. In [50], the pseudo-dimension of regression functions whose weight parameters have low Tucker rank is upper-bounded by $\\mathcal{O}\\left(\\left(r^{p}+d r p\\right) \\log \\left(p d^{p-1}\\right)\\right)$, which is looser than our bound due to the term $d^{p-1}$ (though a similar argument to the one we use in the proof of Theorem 4 can be used to tighten the bound given in [50]).\n\nTree tensor networks Lastly, we compare our result with the ones presented in [38] where the authors study the complexity of learning with tree tensor networks using metric entropy and covering numbers. The results presented in [38] only hold for TN structures whose underlying graph $G$ is a tree. Let $G$ be a tree and $\\ell$ be a loss function which is both bounded and Lipschitz. Under these assumptions, it is shown in [38] that, for any $h \\in \\mathcal{H}_{G}^{\\text {regression }}$, with high probability over the choice of a sample $S$ of size $n$ drawn i.i.d. from a distribution $D$, the generalization gap $R(h)-\\hat{R}(h)$ is in $\\tilde{\\mathcal{O}}\\left(\\sqrt{N_{G}} / n\\right)$. Theorem 4 gives a similar upper bound in $\\tilde{\\mathcal{O}}\\left(\\sqrt{N_{G}} / n\\right)$ on the generalization gap of low-rank tensor classifiers. However, our results hold for any TN structure $G$. Thus, in contrast with our general upper bound (Theorem 2), the bounds from [38] cannot be applied to TN structures containing cycles such as tensor ring and PEPS.\n\n# 5 Lower Bounds \n\nWe now present lower bounds on the VC and pseudo-dimensions of standard TN models: rank-one, CP, Tucker, TT and TR."
    },
    {
      "markdown": "Table 1: Summary of our results for common TN structures. Both lower and upper bounds hold for the VC/pseudo-dimension of $\\mathcal{H}_{G}^{\\text {classif }}, \\mathcal{H}_{G}^{\\text {completion }}$ and $\\mathcal{H}_{G}^{\\text {regression }}$ for the corresponding TN structure $G$ (see Equations (4-6)). The upper bounds follow from applying our general upper bound (Theorem 2) to each TN structure. The lower bounds are proved for each TN structure specifically. Each lower bound is followed by the condition under which it holds in parenthesis (small font). Note that the two bounds for TT and TR hold for both TN structures.\n\n| Decomposition | rank one <br> $\\underbrace{}_{d}$ | CP <br> $\\underbrace{}_{d}$ | Tucker <br> $\\underbrace{}_{d}$ | $\\mathrm{TT} / \\mathrm{TR}$ <br> $\\underbrace{}_{d}$ |\n| :--: | :--: | :--: | :--: | :--: |\n| Lower Bound (condition) | $(d-1) p$ | $r d \\quad\\left(r \\leq d^{p-1}\\right)$ | $r^{p} \\quad(r \\leq d)$ | $\\frac{r^{2} d}{\\frac{p(r^{2} d-1)}{3}} \\quad \\begin{gathered} (r \\leq d)^{\\left(\\frac{p}{2}\\right)} ; p \\geq 3) \\\\ (r=d, r / s \\in \\mathbb{N}) \\end{gathered}$ |\n| Upper bound | $2 d p \\log (12 p)$ | $2 p r d \\log (12 p)$ | $2\\left(r^{p}+p r d\\right) \\log (24 p)$ | $2 p r^{2} d \\log (12 p)$ |\n\nTheorem 7. The VC-dimension and pseudo-dimension of the classification, regression and completion hypothesis classes defined in Equations (4-6) for the rank-one, CP, Tucker, TT and TR tensor network structures satisfy the lower bounds presented in Table 1.\nThese lower bounds show that the general upper bound of Theorem 2 is tight up to a $\\mathcal{O}(\\log (p))$ factor for rank-one, TT and TR tensors and is tight up to a constant for low-rank matrices.\n\nThe proof of this theorem can be found in Appendix A.2. These lower bounds show that our general upper bound is nearly optimal (up to a $\\log$ factor in $p$ ) for rank-one, TT and TR tensors. Indeed, for rank-one tensors we have $(d-1) p \\leq \\mathcal{C}^{\\text {rank-one }} \\leq 2 d p \\log (12 p)$ and for TT and TR tensors of rank $r=d$ whose order $p$ is a multiple of 3 we have $p\\left(r^{2} d-1\\right) / 3 \\leq \\mathcal{C}_{r}^{\\mathrm{TT} / \\mathrm{TR}} \\leq p r^{2} d \\cdot 2 \\log (12 p)$, where $\\mathcal{C}^{\\text {rank-one }}$ (resp. $\\mathcal{C}_{r}^{\\mathrm{TT} / \\mathrm{TR}}$ ) denotes any of the VC/pseudo-dimension of the regression, classification and completion hypothesis classe associated with rank-one tensors (resp. rank $r$ TT and TR tensors). In addition, the lower bound for the CP case shows that our general upper bounds are tight up to a constant for matrices. Indeed, for $p=2$ and $r \\leq d$ the bounds for the CP case give $r d \\leq \\mathcal{C}_{r}^{\\text {matrix }} \\leq$ $20 r d$ where $\\mathcal{C}_{r}^{\\text {matrix }}$ denotes the VC/pseudo-dimension of the hypothesis classes associated with $d \\times d$ matrices of rank at most $r$.\n\n# 6 Conclusion \n\nWe derived a general upper bound on the VC and pseudo-dimension of a large class of tensor models parameterized by arbitrary tensor network structures for classification, regression and completion. We showed that this general bound can be applied to obtain bounds on the complexity of relevant machine learning models such as matrix and tensor completion, trace regression and TT-based linear classifiers. In particular, our result leads to an improved upper bound on the VC-dimension of lowrank matrices for completion tasks. As a corollary of our results, we answer the open question listed in [13] on the VC-dimension of the MPS classification model introduced in [56]. To demonstrate the tightness of our general upper bound, we derived a series of lower bounds for specific TN structures, notably showing that our bound is tight up to a constant for low-rank matrix models for completion, regression and classification.\n\nFuture directions include deriving tighter upper bounds and/or lower bounds for the specific TN structures. This includes investigating whether our general upper bound can be tightened by removing the log factor in the number of vertices of the TN structure, deriving a stronger lower bound for CP (we conjecture our lower bound can be improved by a factor $p$ for CP ), and loosening the condition under which our stronger lower bound holds for TT and TR (for TR, we conjecture that a lower bound of $\\bar{\\Omega}\\left(p r^{2} d\\right)$ holds for any $p \\geq 3$ and $r \\leq d^{k}$ for some value of $k>1$ ). Studying other complexity measures (e.g. Rademacher complexity) and extending recent data-dependant generalization bounds for overparameterized deep neural networks, such as the ones used in [4,34], to TN learning models is worth pursuing. Finally, building upon the connection between the depth of convolutional arithmetic circuits and tensor network structures introduced in [14], it is interesting to connect our result on the VC-dimension of tensor networks to the expressiveness and generalization ability of neural networks."
    },
    {
      "markdown": "# Acknowledgements \n\nWe thank Maude Lizaire for feedbacks and the anonymous reviewers for the useful comments. This research is supported by the Canadian Institute for Advanced Research (CIFAR AI chair program) and the Natural Sciences and Engineering Research Council of Canada (Discovery program, RGPIN-2019-05949).\n\n## References\n\n[1] Evrim Acar, Daniel M Dunlavy, Tamara G Kolda, and Morten Mørup. Scalable tensor factorizations for incomplete data. Chemometrics and Intelligent Laboratory Systems, 106(1):41-56, 2011.\n[2] Sandesh Adhikary, Siddarth Srinivasan, Jacob Miller, Guillaume Rabusseau, and Byron Boots. Quantum tensor networks, stochastic processes, and weighted automata. In The 24th International Conference on Artificial Intelligence and Statistics, volume 130, 2021.\n[3] Martin Anthony and Peter L Bartlett. Neural network learning: Theoretical foundations. cambridge university press, 2009.\n[4] Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets via a compression approach. In International Conference on Machine Learning, pages 254-263. PMLR, 2018.\n[5] Jacob Biamonte and Ville Bergholm. Tensor networks in a nutshell. arXiv preprint arXiv:1708.00006, 2017.\n[6] Olivier Bousquet, Stéphane Boucheron, and Gábor Lugosi. Introduction to statistical learning theory. In Summer School on Machine Learning, pages 169-207. Springer, 2003.\n[7] Deng Cai, Xiaofei He, Ji-Rong Wen, Jiawei Han, and Wei-Ying Ma. Support tensor machines for text categorization. Technical report, 2006.\n[8] Emmanuel J Candès and Benjamin Recht. Exact matrix completion via convex optimization. Foundations of Computational mathematics, 9(6):717-772, 2009.\n[9] Emmanuel J Candès and Terence Tao. The power of convex relaxation: Near-optimal matrix completion. IEEE Transactions on Information Theory, 56(5):2053-2080, 2010.\n[10] Chuan Chen, Zhe-Bin Wu, Zi-Tai Chen, Zi-Bin Zheng, and Xiong-Jun Zhang. Auto-weighted robust low-rank tensor completion via tensor-train. Information Sciences, 567:100-115, 2021.\n[11] Zhongming Chen, Kim Batselier, Johan AK Suykens, and Ngai Wong. Parallelized tensor train learning of polynomial classifiers. IEEE transactions on neural networks and learning systems, 29(10):4621-4632, 2017.\n[12] Song Cheng, Lei Wang, and Pan Zhang. Supervised learning with projected entangled pair states. arXiv preprint arXiv:2009.09932, 2020.\n[13] Juan Ignacio Cirac, José Garre-Rubio, and David Pérez-García. Mathematical open problems in projected entangled pair states. Revista Matemática Complutense, 32(3):579-599, 2019.\n[14] Nadav Cohen, Or Sharir, and Amnon Shashua. On the expressive power of deep learning: A tensor analysis. In Proceedings of the 29th Conference on Learning Theory, volume 49, pages 698-728, 2016.\n[15] Richard P Feynman. Quantum mechanical computers. Foundations of physics, 16(6):507-531, 1986.\n[16] Silvia Gandy, Benjamin Recht, and Isao Yamada. Tensor completion and low-n-rank tensor recovery via convex optimization. Inverse Problems, 27(2):025010, 2011.\n[17] Ivan Glasser, Ryan Sweke, Nicola Pancotti, Jens Eisert, and J. Ignacio Cirac. Expressive power of tensor-network factorizations for probabilistic modeling. In Advances in Neural Information Processing Systems, pages 1496-1508, 2019.\n[18] Ivan Glasser, Nicola Pancotti, and J Ignacio Cirac. From probabilistic graphical models to generalized tensor networks for supervised learning. IEEE Access, 8:68169-68182, 2020.\n[19] Lars Grasedyck. Hierarchical singular value decomposition of tensors. SIAM Journal on Matrix Analysis and Applications, 31(4):2029-2054, 2010."
    },
    {
      "markdown": "[20] Lars Grasedyck, Daniel Kressner, and Christine Tobler. A literature survey of low-rank tensor approximation techniques. GAMM-Mitteilungen, 36(1):53-78, 2013.\n[21] Lars Grasedyck, Melanie Kluge, and Sebastian Kramer. Variants of alternating least squares tensor completion in the tensor train format. SIAM Journal on Scientific Computing, 37(5): A2424-A2450, 2015.\n[22] David Gross. Recovering low-rank matrices from few coefficients in any basis. IEEE Transactions on Information Theory, 57(3):1548-1566, 2011.\n[23] Wolfgang Hackbusch and Stefan Kühn. A new scheme for the tensor representation. Journal of Fourier analysis and applications, 15(5):706-722, 2009.\n[24] Nima Hamidi and Mohsen Bayati. On low-rank trace regression under general sampling distribution. arXiv preprint arXiv:1904.08576, 2019.\n[25] Zhao-Yu Han, Jun Wang, Heng Fan, Lei Wang, and Pan Zhang. Unsupervised generative modeling using matrix product states. Physical Review X, 8(3):031012, 2018.\n[26] Zhi He, Jie Hu, and Yiwen Wang. Low-rank tensor learning for classification of hyperspectral image with limited labeled samples. Signal Processing, 145:12-25, 2018.\n[27] Frank L Hitchcock. The expression of a tensor or a polyadic as a sum of products. Journal of Mathematics and Physics, 6(1-4):164-189, 1927.\n[28] Masaaki Imaizumi, Takanori Maehara, and Kohei Hayashi. On tensor train rank minimization : Statistical efficiency and scalable algorithm. In Advances in Neural Information Processing Systems, pages 3930-3939, 2017.\n[29] Pavel Izmailov, Alexander Novikov, and Dmitry Kropotov. Scalable gaussian processes with billions of inducing inputs via tensor train decomposition. In International Conference on Artificial Intelligence and Statistics, volume 84, 2018.\n[30] Hachem Kadri, Stéphane Ayache, Riikka Huusari, Alain Rakotomamonjy, and Liva Ralaivola. Partial trace regression and low-rank kraus decomposition. In Proceedings of the 37th International Conference on Machine Learning, volume 119, 2020.\n[31] Valentin Khrulkov, Alexander Novikov, and Ivan V. Oseledets. Expressive power of recurrent neural networks. In Proc. of ICLR, 2018.\n[32] Tamara G Kolda and Brett W Bader. Tensor decompositions and applications. SIAM review, 51 (3):455-500, 2009.\n[33] Vladimir Koltchinskii and Dong Xia. Optimal estimation of low rank density matrices. J. Mach. Learn. Res., 16(53):1757-1792, 2015.\n[34] Jingling Li, Yanchao Sun, Jiahao Su, Taiji Suzuki, and Furong Huang. Understanding generalization in deep learning via tensor methods. In International Conference on Artificial Intelligence and Statistics, pages 504-515. PMLR, 2020.\n[35] Ji Liu, Przemyslaw Musialski, Peter Wonka, and Jieping Ye. Tensor completion for estimating missing values in visual data. In IEEE 12th International Conference on Computer Vision, 2009.\n[36] Luo Luo, Yubo Xie, Zhihua Zhang, and Wu-Jun Li. Support matrix machines. In Proceedings of the 32nd International Conference on Machine Learning, volume 37, 2015.\n[37] Konstantinos Makantasis, Anastasios D Doulamis, Nikolaos D Doulamis, and Antonis Nikitakis. Tensor-based classification models for hyperspectral data analysis. IEEE Transactions on Geoscience and Remote Sensing, 56(12):6884-6898, 2018.\n[38] Bertrand Michel and Anthony Nouy. Learning with tree tensor networks: complexity estimates and model selection. arXiv preprint arXiv:2007.01165, 2020.\n[39] Jacob Miller, Guillaume Rabusseau, and John Terilla. Tensor networks for probabilistic sequence modeling. In The 24th International Conference on Artificial Intelligence and Statistics, volume 130, 2021.\n[40] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning. MIT press, 2018."
    },
    {
      "markdown": "[41] Maximilian Nickel and Volker Tresp. An analysis of tensor models for learning on structured data. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 272-287. Springer, 2013.\n[42] Alexander Novikov, Anton Rodomanov, Anton Osokin, and Dmitry P. Vetrov. Putting mrfs on a tensor train. In Proceedings of the 31th International Conference on Machine Learning, volume 32, 2014.\n[43] Alexander Novikov, Dmitry Podoprikhin, Anton Osokin, and Dmitry P. Vetrov. Tensorizing neural networks. In Advances in Neural Information Processing Systems 28, 2015.\n[44] Alexander Novikov, Mikhail Trofimov, and Ivan Oseledets. Exponential machines. arXiv preprint arXiv:1605.03795, 2016.\n[45] Román Orús. A practical introduction to tensor networks: Matrix product states and projected entangled pair states. Annals of Physics, 349:117-158, 2014.\n[46] Ivan V Oseledets. Tensor-train decomposition. SIAM Journal on Scientific Computing, 33(5): 2295-2317, 2011.\n[47] Roger Penrose. Applications of negative dimensional tensors. Combinatorial mathematics and its applications, 1:221-244, 1971.\n[48] Ho N Phien, Hoang D Tuan, Johann A Bengua, and Minh N Do. Efficient tensor completion: Low-rank tensor train. arXiv preprint arXiv:1601.01083, 2016.\n[49] Hamed Pirsiavash, Deva Ramanan, and Charless C. Fowlkes. Bilinear classifiers for visual recognition. In Advances in Neural Information Processing Systems, 2009.\n[50] Guillaume Rabusseau and Hachem Kadri. Low-rank regression with tensor responses. In Advances in Neural Information Processing Systems, 2016.\n[51] Benjamin Recht, Maryam Fazel, and Pablo A Parrilo. Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization. SIAM review, 52(3):471-501, 2010.\n[52] Ulrich Schollwöck. The density-matrix renormalization group in the age of matrix product states. Annals of physics, 326(1):96-192, 2011.\n[53] Raghavendra Selvan and Erik B Dam. Tensor networks for medical image classification. arXiv preprint arXiv:2004.10076, 2020.\n[54] Nathan Srebro, Noga Alon, and Tommi S. Jaakkola. Generalization error bounds for collaborative prediction with low-rank matrices. In Advances in Neural Information Processing Systems, 2004.\n[55] E. Miles Stoudenmire. Learning relevant features of data with multi-scale tensor networks. Quantum Science and Technology, 3(3):034003, 2018.\n[56] Edwin Miles Stoudenmire and David J. Schwab. Supervised learning with tensor networks. In Advances in Neural Information Processing Systems, 2016.\n[57] Ryota Tomioka and Taiji Suzuki. Convex tensor decomposition via structured schatten norm regularization. In Advances in Neural Information Processing Systems, 2013.\n[58] Ryota Tomioka, Taiji Suzuki, Kohei Hayashi, and Hisashi Kashima. Statistical performance of convex tensor decomposition. In Advances in Neural Information Processing Systems, 2011.\n[59] Ledyard R Tucker. Some mathematical notes on three-mode factor analysis. Psychometrika, 31 (3):279-311, 1966.\n[60] Frank Verstraete, Valentin Murg, and J Ignacio Cirac. Matrix product states, projected entangled pair states, and variational renormalization group methods for quantum spin systems. Advances in Physics, 57(2):143-224, 2008.\n[61] Wenqi Wang, Vaneet Aggarwal, and Shuchin Aeron. Tensor completion by alternating minimization under the tensor train (tt) model. arXiv preprint arXiv:1609.05587, 2016.\n[62] Wenqi Wang, Vaneet Aggarwal, and Shuchin Aeron. Efficient low rank tensor ring completion. In IEEE International Conference on Computer Vision, 2017.\n[63] Wenqi Wang, Yifan Sun, Brian Eriksson, Wenlin Wang, and Vaneet Aggarwal. Wide compression: Tensor ring nets. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, 2018."
    },
    {
      "markdown": "[64] Yazhen Wang et al. Asymptotic equivalence of quantum state tomography and noisy matrix completion. The Annals of Statistics, 41(5):2462-2504, 2013.\n[65] Yongkang Wang, Weicheng Zhang, Zhuliang Yu, Zhenghui Gu, Hao Liu, Zhaoquan Cai, Congjun Wang, and Shihan Gao. Support vector machine based on low-rank tensor train decomposition for big data applications. In 2017 12th IEEE Conference on Industrial Electronics and Applications (ICIEA), pages 850-853. IEEE, 2017.\n[66] Hugh E Warren. Lower bounds for approximation by nonlinear manifolds. Transactions of the American Mathematical Society, 133(1):167-178, 1968.\n[67] Lior Wolf, Hueihan Jhuang, and Tamir Hazan. Modeling appearances with low-rank SVM. In IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2007.\n[68] Xiaowen Xu, Qiang Wu, Shuo Wang, Ju Liu, Jiande Sun, and Andrzej Cichocki. Whole brain fmri pattern analysis based on tensor neural network. IEEE Access, 6:29297-29305, 2018.\n[69] Yinchong Yang, Denis Krompass, and Volker Tresp. Tensor-train recurrent neural networks for video classification. In Proceedings of the 34th International Conference on Machine Learning, volume 70, 2017.\n[70] Rose Yu, Max Guangyu Li, and Yan Liu. Tensor regression meets gaussian processes. In International Conference on Artificial Intelligence and Statistics, volume 84, 2018.\n[71] Longhao Yuan, Jianting Cao, Xuyang Zhao, Qiang Wu, and Qibin Zhao. Higher-dimension tensor completion via low-rank tensor ring decomposition. In 2018 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), pages 1071-1076. IEEE, 2018.\n[72] Longhao Yuan, Chao Li, Danilo Mandic, Jianting Cao, and Qibin Zhao. Tensor ring decomposition with rank minimization on latent space: An efficient approach for tensor completion. In Proc. of AAAI, volume 33, pages 9151-9158, 2019.\n[73] Qibin Zhao, Guoxu Zhou, Shengli Xie, Liqing Zhang, and Andrzej Cichocki. Tensor ring decomposition. arXiv preprint arXiv:1606.05535, 2016."
    }
  ],
  "usage_info": {
    "pages_processed": 13,
    "doc_size_bytes": 412333
  },
  "_metadata": {
    "model_version": "0.0.0",
    "llm_model": "mistral-ocr-2505",
    "query_data": {
      "pdf": "/Users/satyaortiz-gagne/CODE/paperext/data/cache/fulltext/d5a596c72be72f1b68b54fcaba99d2c5/fulltext.pdf"
    },
    "model_id": "parsepdf"
  }
}